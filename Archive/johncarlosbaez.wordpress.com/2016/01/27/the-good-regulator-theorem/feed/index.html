<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: The Internal Model Principle	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/</link>
	<description></description>
	<lastBuildDate>Wed, 17 Feb 2021 22:59:17 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-169458</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 17 Feb 2021 22:59:17 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-169458</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-169454&quot;&gt;Elias Hasle&lt;/a&gt;.

I&#039;m a nice guy, so I preferred to focus on the interesting idea rather than the problems with making it precise.  But yes:

&lt;blockquote&gt;
  When I tried to read Conant and Ashby’s paper, I got stuck. They use some very basic mathematical notation in nonstandard ways, and they don’t clearly state the hypotheses and conclusion of their theorem.
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-169454">Elias Hasle</a>.</p>
<p>I&#8217;m a nice guy, so I preferred to focus on the interesting idea rather than the problems with making it precise.  But yes:</p>
<blockquote><p>
  When I tried to read Conant and Ashby’s paper, I got stuck. They use some very basic mathematical notation in nonstandard ways, and they don’t clearly state the hypotheses and conclusion of their theorem.
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Elias Hasle		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-169454</link>

		<dc:creator><![CDATA[Elias Hasle]]></dc:creator>
		<pubDate>Wed, 17 Feb 2021 12:53:42 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-169454</guid>

					<description><![CDATA[Thank you for investigating this! I think you are being too nice with the authors. After all, they are clearly abusing the high-ranking label of &quot;theorem&quot; for a sloppily explained hunch. It doesn&#039;t help if their hunch is (partially) right and has inspired other researchers.

It is a fascinating topic, though.]]></description>
			<content:encoded><![CDATA[<p>Thank you for investigating this! I think you are being too nice with the authors. After all, they are clearly abusing the high-ranking label of &#8220;theorem&#8221; for a sloppily explained hunch. It doesn&#8217;t help if their hunch is (partially) right and has inspired other researchers.</p>
<p>It is a fascinating topic, though.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: seanny1986		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-160662</link>

		<dc:creator><![CDATA[seanny1986]]></dc:creator>
		<pubDate>Thu, 09 Apr 2020 02:23:48 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-160662</guid>

					<description><![CDATA[So, I know this is several years old now, but I found it interesting and wanted to share some thoughts. If I&#039;ve understood correctly, doesn&#039;t Lemma 1 tell us that a regulator &lt;em&gt;has&lt;/em&gt; to be a function of the state? Because if it isn&#039;t, it&#039;s essentially acting randomly (or rather, undirected). Following on from this, if the regulator is a function of the state, and we have some governing dynamics s&#039; = f(s,a(s)), then the optimal action will be some inverse function of the dynamics system a(s) = g(s&#039;, s), where I&#039;ll assume that s&#039; is the optimal next step to reaching some goal state G. I&#039;m probably being a bit fast and loose with terminology here.

That said, I&#039;d like to tie this back to reinforcement learning, which is an area I&#039;m a bit more familiar with. In RL, we have &quot;model free&quot; methods that maximize a reward signal using gradient ascent. They&#039;re model free in the sense that they don&#039;t use a transition function to map out a trajectory, but I&#039;d argue that the model is actually implicit in the reward function, because it determines the behavior of the agent. You can mis-specify a reward function, in which case you get an agent that does something wildly unexpected (e.g. suiciding), or you can pick the right set of rewards to get the behavior you want. It&#039;s only when the reward function is congruent with the task and environment that you get something useful.

The interesting thing (to me at least), is that the process of learning in RL is fundamentally tied in with minimizing cross-entropy in some form or another. The policy gradient theorem uses the score function approximator to step function parameters in the direction that maximizes the reward, but this gradient approximator implies that the policy is fitting to a Boltzmann distribution given by the reward function. This turns out to be the case for &lt;em&gt;all&lt;/em&gt; policy gradients through the equivalence between the score function gradient estimate and the change-of-variables gradient estimate. Furthermore, you can link all RL algorithms in this way, including Q value-based methods like Q learning, which turn out to be minimizing KL-divergence rather than cross-entropy (not sure how widely known this is -- it&#039;s something I only figured out recently, and led me to this page).

You can actually do a simple test with RL and fit a distribution directly by minimizing reverse mode KL-divergence instead of cross-entropy (this gives you a slightly modified policy gradient estimator, that maximizes policy entropy as it minimizes cross-entropy). Say you have data generated by some underlying distribution k(x) ~ N(mu, sigma), and you fit a policy to a reward function that rewards prediction of the data -- your policy will only fit k(x) when the weighting of the reward function is the same as k(x) (i.e. if k(x) = f(x) + sigma * eps, where eps ~ N(0, I), then for a reward function -m * (pi(a&#124;x) - y)^2 , m = 0.5/sigma^2 ). That is to say, in this simple case, the reward function has to be a model of the original distribution for the policy to learn the optimal action and entropy (this isn&#039;t the case with the standard PG, which minimizes cross-entropy, and thus fits the mean and becomes increasingly deterministic over time).

Just some thoughts. Happy to share some code and figures if you&#039;re interested.]]></description>
			<content:encoded><![CDATA[<p>So, I know this is several years old now, but I found it interesting and wanted to share some thoughts. If I&#8217;ve understood correctly, doesn&#8217;t Lemma 1 tell us that a regulator <em>has</em> to be a function of the state? Because if it isn&#8217;t, it&#8217;s essentially acting randomly (or rather, undirected). Following on from this, if the regulator is a function of the state, and we have some governing dynamics s&#8217; = f(s,a(s)), then the optimal action will be some inverse function of the dynamics system a(s) = g(s&#8217;, s), where I&#8217;ll assume that s&#8217; is the optimal next step to reaching some goal state G. I&#8217;m probably being a bit fast and loose with terminology here.</p>
<p>That said, I&#8217;d like to tie this back to reinforcement learning, which is an area I&#8217;m a bit more familiar with. In RL, we have &#8220;model free&#8221; methods that maximize a reward signal using gradient ascent. They&#8217;re model free in the sense that they don&#8217;t use a transition function to map out a trajectory, but I&#8217;d argue that the model is actually implicit in the reward function, because it determines the behavior of the agent. You can mis-specify a reward function, in which case you get an agent that does something wildly unexpected (e.g. suiciding), or you can pick the right set of rewards to get the behavior you want. It&#8217;s only when the reward function is congruent with the task and environment that you get something useful.</p>
<p>The interesting thing (to me at least), is that the process of learning in RL is fundamentally tied in with minimizing cross-entropy in some form or another. The policy gradient theorem uses the score function approximator to step function parameters in the direction that maximizes the reward, but this gradient approximator implies that the policy is fitting to a Boltzmann distribution given by the reward function. This turns out to be the case for <em>all</em> policy gradients through the equivalence between the score function gradient estimate and the change-of-variables gradient estimate. Furthermore, you can link all RL algorithms in this way, including Q value-based methods like Q learning, which turn out to be minimizing KL-divergence rather than cross-entropy (not sure how widely known this is &#8212; it&#8217;s something I only figured out recently, and led me to this page).</p>
<p>You can actually do a simple test with RL and fit a distribution directly by minimizing reverse mode KL-divergence instead of cross-entropy (this gives you a slightly modified policy gradient estimator, that maximizes policy entropy as it minimizes cross-entropy). Say you have data generated by some underlying distribution k(x) ~ N(mu, sigma), and you fit a policy to a reward function that rewards prediction of the data &#8212; your policy will only fit k(x) when the weighting of the reward function is the same as k(x) (i.e. if k(x) = f(x) + sigma * eps, where eps ~ N(0, I), then for a reward function -m * (pi(a|x) &#8211; y)^2 , m = 0.5/sigma^2 ). That is to say, in this simple case, the reward function has to be a model of the original distribution for the policy to learn the optimal action and entropy (this isn&#8217;t the case with the standard PG, which minimizes cross-entropy, and thus fits the mean and becomes increasingly deterministic over time).</p>
<p>Just some thoughts. Happy to share some code and figures if you&#8217;re interested.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: seanny1986		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-160636</link>

		<dc:creator><![CDATA[seanny1986]]></dc:creator>
		<pubDate>Wed, 08 Apr 2020 11:44:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-160636</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76800&quot;&gt;Tom Dietterich&lt;/a&gt;.

To what extent is the reward function in Q learning a model of the system, though? Also, what do you mean by optimal? Q learning can learn the optimal Q function, but this is different to learning optimal control. You only get useful behavior when the reward function is congruent with the environment and the task, which suggests that it serves the purpose of a model that maps states X and a goal G to actions U.

As an example, imagine we want the agent to go from A to B as quickly as possible, with some hazard in the environment. We give the agent a negative reward at each timestep to incentivize shorter paths to B, and a positive reward for reaching A. Depending on the relative magnitudes of these two rewards, we&#039;ll get an agent that either goes from A to B, or suicides to prevent further negative reward. The first set of rewards all represent a valid model of behavior given the goal (getting from A to B), and the second doesn&#039;t.

Keep in mind, I&#039;m assuming that &quot;model&quot; here is being used in a more general sense than it typically is in RL, where it&#039;s mainly used to denote a transition function.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76800">Tom Dietterich</a>.</p>
<p>To what extent is the reward function in Q learning a model of the system, though? Also, what do you mean by optimal? Q learning can learn the optimal Q function, but this is different to learning optimal control. You only get useful behavior when the reward function is congruent with the environment and the task, which suggests that it serves the purpose of a model that maps states X and a goal G to actions U.</p>
<p>As an example, imagine we want the agent to go from A to B as quickly as possible, with some hazard in the environment. We give the agent a negative reward at each timestep to incentivize shorter paths to B, and a positive reward for reaching A. Depending on the relative magnitudes of these two rewards, we&#8217;ll get an agent that either goes from A to B, or suicides to prevent further negative reward. The first set of rewards all represent a valid model of behavior given the goal (getting from A to B), and the second doesn&#8217;t.</p>
<p>Keep in mind, I&#8217;m assuming that &#8220;model&#8221; here is being used in a more general sense than it typically is in RL, where it&#8217;s mainly used to denote a transition function.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-77474</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 20 Feb 2016 00:05:28 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-77474</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-77405&quot;&gt;Tom Holroyd&lt;/a&gt;.

By the way, there&#039;s no sign that Newtonian mechanics would work better with a third-order equation!  The simplifications it makes are of a different sort.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-77405">Tom Holroyd</a>.</p>
<p>By the way, there&#8217;s no sign that Newtonian mechanics would work better with a third-order equation!  The simplifications it makes are of a different sort.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Tom Holroyd		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-77405</link>

		<dc:creator><![CDATA[Tom Holroyd]]></dc:creator>
		<pubDate>Tue, 16 Feb 2016 13:24:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-77405</guid>

					<description><![CDATA[Robert Rosen, in his book Anticipatory Systems, says organisms must contain models of the environment, and in that work and later works expands this to say that formalisms in math are models of [part of] the world, and this is exactly why formalisms break down, because they are necessarily simple, and the world is complex. F=ma for example, cuts the formalism off at second order. It works for a while ...]]></description>
			<content:encoded><![CDATA[<p>Robert Rosen, in his book Anticipatory Systems, says organisms must contain models of the environment, and in that work and later works expands this to say that formalisms in math are models of [part of] the world, and this is exactly why formalisms break down, because they are necessarily simple, and the world is complex. F=ma for example, cuts the formalism off at second order. It works for a while &#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: linasv		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76976</link>

		<dc:creator><![CDATA[linasv]]></dc:creator>
		<pubDate>Tue, 02 Feb 2016 01:24:32 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-76976</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76951&quot;&gt;Jon Awbrey&lt;/a&gt;.

Hi Jon,

Some parts of a model need to be &quot;faithful&quot; (or, to borrow a phrase from genetics, &quot;highly conserved&quot;) while other parts can be variable and/or relatively unimportant.

If the model is formal in the sense of Model Theory, then how do I assign probabilities to the various parts to indicate how faithful or variable they are?  Is the result of doing so a &quot;Markov Logic Net&quot; (MLN)? If so, then how does it work?

Now, practical control problems have either linear of ODE models, so if I walk down the Model-Theory path, this suggests employing something like a Satisfiability Modulo-Theories (SMT) framework.  But these are not probabilistic, nor do they seem to be good at tuning my ODE (for example, a Kalman filter).

I am not aware of any texts that discuss model-theory+probability in any enlightened way, although there seem to be a infinite number of special-case, domain-specific treatments that all seem similar but yet all different...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76951">Jon Awbrey</a>.</p>
<p>Hi Jon,</p>
<p>Some parts of a model need to be &#8220;faithful&#8221; (or, to borrow a phrase from genetics, &#8220;highly conserved&#8221;) while other parts can be variable and/or relatively unimportant.</p>
<p>If the model is formal in the sense of Model Theory, then how do I assign probabilities to the various parts to indicate how faithful or variable they are?  Is the result of doing so a &#8220;Markov Logic Net&#8221; (MLN)? If so, then how does it work?</p>
<p>Now, practical control problems have either linear of ODE models, so if I walk down the Model-Theory path, this suggests employing something like a Satisfiability Modulo-Theories (SMT) framework.  But these are not probabilistic, nor do they seem to be good at tuning my ODE (for example, a Kalman filter).</p>
<p>I am not aware of any texts that discuss model-theory+probability in any enlightened way, although there seem to be a infinite number of special-case, domain-specific treatments that all seem similar but yet all different&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Giampy		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76968</link>

		<dc:creator><![CDATA[Giampy]]></dc:creator>
		<pubDate>Mon, 01 Feb 2016 19:10:45 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-76968</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76951&quot;&gt;Jon Awbrey&lt;/a&gt;.

While i&#039;m not 100% sure about what you meant, I think i can see what you are saying, and i think i agree.

I have used the word &quot;model&quot; in a very liberal way, but indeed i didn&#039;t mean to restrict the thoughts to formal, analytic, explicit, mathematical, models.

I really meant that when some knowledge about the environment is incorporated in your regulator, or algorithm, or system, then it&#039;s somehow easier for your system to steer the environment to its advantage (i think you call this &quot;analogical&quot; modeling).

This seems to me to be a very general principle or guideline.

In fact perhaps in biology evolution is a way for living organism to acquire and refine such knowledge, in an implicit and iterative and randomized way.

In engineering, in the last few decades, we have seen an increase in the use of formal (mathematical and computational) modeling in system design. I think it would be nice if the same happened in social sciences or law, for example.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76951">Jon Awbrey</a>.</p>
<p>While i&#8217;m not 100% sure about what you meant, I think i can see what you are saying, and i think i agree.</p>
<p>I have used the word &#8220;model&#8221; in a very liberal way, but indeed i didn&#8217;t mean to restrict the thoughts to formal, analytic, explicit, mathematical, models.</p>
<p>I really meant that when some knowledge about the environment is incorporated in your regulator, or algorithm, or system, then it&#8217;s somehow easier for your system to steer the environment to its advantage (i think you call this &#8220;analogical&#8221; modeling).</p>
<p>This seems to me to be a very general principle or guideline.</p>
<p>In fact perhaps in biology evolution is a way for living organism to acquire and refine such knowledge, in an implicit and iterative and randomized way.</p>
<p>In engineering, in the last few decades, we have seen an increase in the use of formal (mathematical and computational) modeling in system design. I think it would be nice if the same happened in social sciences or law, for example.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Jon Awbrey		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76951</link>

		<dc:creator><![CDATA[Jon Awbrey]]></dc:creator>
		<pubDate>Mon, 01 Feb 2016 04:06:23 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-76951</guid>

					<description><![CDATA[Re: Giampiero Campa

It is one question whether a regulator has “knowledge” of the object system and another question whether that knowledge is embodied in the more specific form of a “model”.  At this point we encounter a variety of meanings for the word “model”. In my experience they divide into two broad classes, “logical models” and “analogical models”.

Logical modeling involves a relation between a theory and anything that satisfies the theory, in practice either the original domain of phenomena from which the theory was derived in the first place or a formal object we construct to satisfy the theory.

Analogical modeling involves a relation between any two things that have similar properties or structures or that satisfy the same theory.

It is possible that a regulator has knowledge, competence, or a capacity for performance that exists in the form of a theory or other data structures without necessarily having either type of model on hand.

I would be the last to deny that models of either sort are extremely useful when we can get them, but there are reasons for thinking that the mirror of nature does not go all the way down to the most primitive structures of adaptive functioning.]]></description>
			<content:encoded><![CDATA[<p>Re: Giampiero Campa</p>
<p>It is one question whether a regulator has “knowledge” of the object system and another question whether that knowledge is embodied in the more specific form of a “model”.  At this point we encounter a variety of meanings for the word “model”. In my experience they divide into two broad classes, “logical models” and “analogical models”.</p>
<p>Logical modeling involves a relation between a theory and anything that satisfies the theory, in practice either the original domain of phenomena from which the theory was derived in the first place or a formal object we construct to satisfy the theory.</p>
<p>Analogical modeling involves a relation between any two things that have similar properties or structures or that satisfy the same theory.</p>
<p>It is possible that a regulator has knowledge, competence, or a capacity for performance that exists in the form of a theory or other data structures without necessarily having either type of model on hand.</p>
<p>I would be the last to deny that models of either sort are extremely useful when we can get them, but there are reasons for thinking that the mirror of nature does not go all the way down to the most primitive structures of adaptive functioning.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76945</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 01 Feb 2016 00:03:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20937#comment-76945</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76917&quot;&gt;Giampiero Campa&lt;/a&gt;.

Okay, I think I get the connection now.  It would be fun to dig deeper into this.  I&#039;m starting to see biology as a bunch of systems modelling each other in order to control what&#039;s going on...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/#comment-76917">Giampiero Campa</a>.</p>
<p>Okay, I think I get the connection now.  It would be fun to dig deeper into this.  I&#8217;m starting to see biology as a bunch of systems modelling each other in order to control what&#8217;s going on&#8230;</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
