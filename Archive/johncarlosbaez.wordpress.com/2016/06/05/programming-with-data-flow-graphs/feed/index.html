<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Programming with Data Flow Graphs	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/</link>
	<description></description>
	<lastBuildDate>Mon, 27 Jun 2016 11:49:26 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Kamau		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-81040</link>

		<dc:creator><![CDATA[Kamau]]></dc:creator>
		<pubDate>Mon, 27 Jun 2016 11:49:26 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-81040</guid>

					<description><![CDATA[I did not know about tensorflow, will check it out, mainly because of it portability across hardware platforms]]></description>
			<content:encoded><![CDATA[<p>I did not know about tensorflow, will check it out, mainly because of it portability across hardware platforms</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Jake		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80603</link>

		<dc:creator><![CDATA[Jake]]></dc:creator>
		<pubDate>Sat, 11 Jun 2016 02:37:36 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80603</guid>

					<description><![CDATA[I think there are a bunch of those &#039;plug components into other components&#039; interfaces out there, the one that comes to mind right now is gnuradio, here&#039;s a recent example: http://gnuradio.org/blog/filtering-time-series-data__elemental-building-blocks/]]></description>
			<content:encoded><![CDATA[<p>I think there are a bunch of those &#8216;plug components into other components&#8217; interfaces out there, the one that comes to mind right now is gnuradio, here&#8217;s a recent example: <a href="http://gnuradio.org/blog/filtering-time-series-data__elemental-building-blocks/" rel="nofollow ugc">http://gnuradio.org/blog/filtering-time-series-data__elemental-building-blocks/</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Carlos Perez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80585</link>

		<dc:creator><![CDATA[Carlos Perez]]></dc:creator>
		<pubDate>Fri, 10 Jun 2016 02:39:07 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80585</guid>

					<description><![CDATA[I think just being able to formalize DL in terms of category theory will help in discussion.

What will be extremely difficult is coming up with an explanation as to why DL depth architecture works.  There is some group theory based proof that explains the higher probability of forming morphisms that have a large limit cycle, however I can&#039;t see how a layer (or several layers) build up into higher classification abstraction.

My goal is not to be too ambitious,  but rather, have a set of design patterns that aid in building DL solutions.]]></description>
			<content:encoded><![CDATA[<p>I think just being able to formalize DL in terms of category theory will help in discussion.</p>
<p>What will be extremely difficult is coming up with an explanation as to why DL depth architecture works.  There is some group theory based proof that explains the higher probability of forming morphisms that have a large limit cycle, however I can&#8217;t see how a layer (or several layers) build up into higher classification abstraction.</p>
<p>My goal is not to be too ambitious,  but rather, have a set of design patterns that aid in building DL solutions.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Carlos Perez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80584</link>

		<dc:creator><![CDATA[Carlos Perez]]></dc:creator>
		<pubDate>Fri, 10 Jun 2016 02:29:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80584</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80581&quot;&gt;John Baez&lt;/a&gt;.

The reason I remarked about the lack of insight on the tensorflow computation graph concept is that it is no different from dataflow parallel computation that was popular in the early 80&#039;s.  Recall the Japanese 5th generation computing project?   The closes equivalent would be Petri-nets.   It&#039;s a means for specifying a computation which really does not give insight into the dynamics of a DL system.

For a DL, I conjecture that there would be 3 categories that need to be defined.  The first would be the Representation category which would be equivalent to probably a vector space.   The second would be the Model category.  The Model category will have morphism that map from one Model to another.  Furthermore, a Model happens to be a morphism in the Representation category.  The 3rd category, I&#039;m a bit fuzzy about, would be a Learner category.   Nevertheless, SGD would be a morphism in the Model category.

I think there is a lot of promise in a category theory for Deep Learning considering done lately.  That is, I do see category theory on bayesian causality as well as some work regarding entropy.  The step to the next level may not be that much of a leap.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80581">John Baez</a>.</p>
<p>The reason I remarked about the lack of insight on the tensorflow computation graph concept is that it is no different from dataflow parallel computation that was popular in the early 80&#8217;s.  Recall the Japanese 5th generation computing project?   The closes equivalent would be Petri-nets.   It&#8217;s a means for specifying a computation which really does not give insight into the dynamics of a DL system.</p>
<p>For a DL, I conjecture that there would be 3 categories that need to be defined.  The first would be the Representation category which would be equivalent to probably a vector space.   The second would be the Model category.  The Model category will have morphism that map from one Model to another.  Furthermore, a Model happens to be a morphism in the Representation category.  The 3rd category, I&#8217;m a bit fuzzy about, would be a Learner category.   Nevertheless, SGD would be a morphism in the Model category.</p>
<p>I think there is a lot of promise in a category theory for Deep Learning considering done lately.  That is, I do see category theory on bayesian causality as well as some work regarding entropy.  The step to the next level may not be that much of a leap.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80582</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 10 Jun 2016 01:14:41 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80582</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80524&quot;&gt;Graham Jones&lt;/a&gt;.

&lt;blockquote&gt;
  Are you aiming for a description of TensorFlow^{TM} or of a typical/vanilla sort of ANN?
&lt;/blockquote&gt;

I&#039;m aiming for description of something in this general vicinity that admits a slick mathematical description.  Being a mathematician, this is a way for me to get my foot in the door.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80524">Graham Jones</a>.</p>
<blockquote><p>
  Are you aiming for a description of TensorFlow^{TM} or of a typical/vanilla sort of ANN?
</p></blockquote>
<p>I&#8217;m aiming for description of something in this general vicinity that admits a slick mathematical description.  Being a mathematician, this is a way for me to get my foot in the door.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80581</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 10 Jun 2016 01:11:43 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80581</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80580&quot;&gt;Carlos Perez&lt;/a&gt;.

&lt;blockquote&gt;
  Interesting that TensorFlow (TF) is mentioned in this blog.
&lt;/blockquote&gt;

It&#039;s because I&#039;m an expert on some aspects of tensor networks, so the name and description caught my attention.

&lt;blockquote&gt;
  As far as whether you can gain any insight looking at TF graphs, I seriously doubt it.
&lt;/blockquote&gt;

It would be helpful to explain why you doubt it.

Since I&#039;m an expert on how various kinds of labeled graphs are used to describe morphisms in various kinds of categories, and how operations on graphs correspond to operations on morphisms, it seems like an obvious thing for me to look into --- especially since it&#039;s something other people are less likely to have spent a lot of time on.

(If you have a super-power, you have to try to use it even in situations where it&#039;s not obviously the best solution to a problem.  This is why Spiderman spends so much time swinging around on webs.)

&lt;blockquote&gt;
  Deep Learning (DL) systems have a common requirement that the layers are differentiable (so as to support SGD). The question I have is what Category theory has to say about this?
&lt;/blockquote&gt;

I&#039;m pretty good at the theory of differentiable categories, so I&#039;d be inclined to try something like that.  But it will take me a year or two --- at &lt;em&gt;least&lt;/em&gt; --- before I have anything very interesting to say about neural networks.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80580">Carlos Perez</a>.</p>
<blockquote><p>
  Interesting that TensorFlow (TF) is mentioned in this blog.
</p></blockquote>
<p>It&#8217;s because I&#8217;m an expert on some aspects of tensor networks, so the name and description caught my attention.</p>
<blockquote><p>
  As far as whether you can gain any insight looking at TF graphs, I seriously doubt it.
</p></blockquote>
<p>It would be helpful to explain why you doubt it.</p>
<p>Since I&#8217;m an expert on how various kinds of labeled graphs are used to describe morphisms in various kinds of categories, and how operations on graphs correspond to operations on morphisms, it seems like an obvious thing for me to look into &#8212; especially since it&#8217;s something other people are less likely to have spent a lot of time on.</p>
<p>(If you have a super-power, you have to try to use it even in situations where it&#8217;s not obviously the best solution to a problem.  This is why Spiderman spends so much time swinging around on webs.)</p>
<blockquote><p>
  Deep Learning (DL) systems have a common requirement that the layers are differentiable (so as to support SGD). The question I have is what Category theory has to say about this?
</p></blockquote>
<p>I&#8217;m pretty good at the theory of differentiable categories, so I&#8217;d be inclined to try something like that.  But it will take me a year or two &#8212; at <em>least</em> &#8212; before I have anything very interesting to say about neural networks.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Carlos Perez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80580</link>

		<dc:creator><![CDATA[Carlos Perez]]></dc:creator>
		<pubDate>Thu, 09 Jun 2016 22:42:21 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80580</guid>

					<description><![CDATA[Interesting that TensorFlow (TF) is mentioned in this blog.  TF is a convenient computational framework that was designed for DL computation.  As far as wether you can gain any insight looking at TF graphs,  I seriously doubt it.

I&#039;m in the process of writing what I call &quot;A Pattern Language for Deep Learning&quot;.   Deep Learning (DL) systems have a common requirement that the layers are differentiable (so as to support SGD).  The question I have is what Category theory has to say about this?  How does a category that supports a &quot;natural transformation&quot; imply the need for differentiable layers?]]></description>
			<content:encoded><![CDATA[<p>Interesting that TensorFlow (TF) is mentioned in this blog.  TF is a convenient computational framework that was designed for DL computation.  As far as wether you can gain any insight looking at TF graphs,  I seriously doubt it.</p>
<p>I&#8217;m in the process of writing what I call &#8220;A Pattern Language for Deep Learning&#8221;.   Deep Learning (DL) systems have a common requirement that the layers are differentiable (so as to support SGD).  The question I have is what Category theory has to say about this?  How does a category that supports a &#8220;natural transformation&#8221; imply the need for differentiable layers?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Jeff		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80547</link>

		<dc:creator><![CDATA[Jeff]]></dc:creator>
		<pubDate>Wed, 08 Jun 2016 13:55:27 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80547</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80524&quot;&gt;Graham Jones&lt;/a&gt;.

&lt;blockquote&gt;
  (A slick mathematical description of all ANNs is about as likely as a slick mathematical description of all bridges.)
  
  Assuming you want the latter… ANNs operate in at least two modes: training and prediction. The graphs will be different, though often the prediction graph is a subgraph of the training graph. Graphs will be directed acyclic graphs. You do get recurrent ANNs but they are a minority pursuit.
&lt;/blockquote&gt;

I would love a slick formalization of these two interacting graphs, because I feel there is something missing from the descriptions offered by those in ML.

(You can formulate it the way TensorFlow and theano do: by formulating a second gradient graph that takes input from the first.)

But since training mode can also be thought of as flipping the arrows, certainly something co____ must be going on? Apologies if this is a naive question; I am a CT neophyte.

In any case, there are relations of the training/prediction phases and the &quot;forward/backward pass&quot; seen in things like Kalman filtering. A potentially clarifying post is offered by Ben Recht: http://www.argmin.net/2016/05/18/mates-of-costate/]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80524">Graham Jones</a>.</p>
<blockquote><p>
  (A slick mathematical description of all ANNs is about as likely as a slick mathematical description of all bridges.)</p>
<p>  Assuming you want the latter… ANNs operate in at least two modes: training and prediction. The graphs will be different, though often the prediction graph is a subgraph of the training graph. Graphs will be directed acyclic graphs. You do get recurrent ANNs but they are a minority pursuit.
</p></blockquote>
<p>I would love a slick formalization of these two interacting graphs, because I feel there is something missing from the descriptions offered by those in ML.</p>
<p>(You can formulate it the way TensorFlow and theano do: by formulating a second gradient graph that takes input from the first.)</p>
<p>But since training mode can also be thought of as flipping the arrows, certainly something co____ must be going on? Apologies if this is a naive question; I am a CT neophyte.</p>
<p>In any case, there are relations of the training/prediction phases and the &#8220;forward/backward pass&#8221; seen in things like Kalman filtering. A potentially clarifying post is offered by Ben Recht: <a href="http://www.argmin.net/2016/05/18/mates-of-costate/" rel="nofollow ugc">http://www.argmin.net/2016/05/18/mates-of-costate/</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Graham Jones		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80524</link>

		<dc:creator><![CDATA[Graham Jones]]></dc:creator>
		<pubDate>Tue, 07 Jun 2016 20:30:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80524</guid>

					<description><![CDATA[John said in the post

&lt;blockquote&gt;
  the edges represent tensors
&lt;/blockquote&gt;

I can&#039;t tell from by looking at TensorFlow whether the edges represent tensors or lists of tensors (possibly of different rank and shape). From a programming point of of view, it seems nearly as easy to allow lists, and more flexible, so I guess lists are allowed. I did see that strings as well as numbers are allowed as elements.

and from a comment:

&lt;blockquote&gt;
  For me, figuring out a slick mathematical description of something is a good way to start learning about it.
&lt;/blockquote&gt;

Are you aiming for a description of TensorFlow$latex ^{TM}$ or of a typical/vanilla sort of ANN? (A slick mathematical description of all ANNs is about as likely as a slick mathematical description of all bridges.)

Assuming you want the latter... ANNs operate in at least two modes: training and prediction. The graphs will be different, though often the prediction graph is a subgraph of the training graph. Graphs will be directed acyclic graphs. You do get recurrent ANNs but they are a minority pursuit.

ANN = artificial neural net]]></description>
			<content:encoded><![CDATA[<p>John said in the post</p>
<blockquote><p>
  the edges represent tensors
</p></blockquote>
<p>I can&#8217;t tell from by looking at TensorFlow whether the edges represent tensors or lists of tensors (possibly of different rank and shape). From a programming point of of view, it seems nearly as easy to allow lists, and more flexible, so I guess lists are allowed. I did see that strings as well as numbers are allowed as elements.</p>
<p>and from a comment:</p>
<blockquote><p>
  For me, figuring out a slick mathematical description of something is a good way to start learning about it.
</p></blockquote>
<p>Are you aiming for a description of TensorFlow<img src="https://s0.wp.com/latex.php?latex=%5E%7BTM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="^{TM}" class="latex" /> or of a typical/vanilla sort of ANN? (A slick mathematical description of all ANNs is about as likely as a slick mathematical description of all bridges.)</p>
<p>Assuming you want the latter&#8230; ANNs operate in at least two modes: training and prediction. The graphs will be different, though often the prediction graph is a subgraph of the training graph. Graphs will be directed acyclic graphs. You do get recurrent ANNs but they are a minority pursuit.</p>
<p>ANN = artificial neural net</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80505</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 06 Jun 2016 23:54:25 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=21988#comment-80505</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80500&quot;&gt;John Baez&lt;/a&gt;.

Good point.  I still feel pretty that some category like this is what &quot;data flow graphs&quot; are supposed to formalize, but you&#039;re right, I don&#039;t see why it should be a symmetric monoidal category!

Right now it seems like a category where there&#039;s a tensor product of objects (which we could make strictly associative, to give it some teeth), but not for general morphisms.  That&#039;s pretty sad.

I&#039;ll have to look into this some more, to learn more precisely what data flow graphs are, and what people do with them.  If they&#039;re really important there should be some mathematically nice way to think about them.

I should point out, for what it&#039;s worth, that all our vector spaces here come with distinguished bases.  So, we could equally well think of the objects as finite sets, and the tensor product of the objects as the cartesian product.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/06/05/programming-with-data-flow-graphs/#comment-80500">John Baez</a>.</p>
<p>Good point.  I still feel pretty that some category like this is what &#8220;data flow graphs&#8221; are supposed to formalize, but you&#8217;re right, I don&#8217;t see why it should be a symmetric monoidal category!</p>
<p>Right now it seems like a category where there&#8217;s a tensor product of objects (which we could make strictly associative, to give it some teeth), but not for general morphisms.  That&#8217;s pretty sad.</p>
<p>I&#8217;ll have to look into this some more, to learn more precisely what data flow graphs are, and what people do with them.  If they&#8217;re really important there should be some mathematically nice way to think about them.</p>
<p>I should point out, for what it&#8217;s worth, that all our vector spaces here come with distinguished bases.  So, we could equally well think of the objects as finite sets, and the tensor product of the objects as the cartesian product.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
