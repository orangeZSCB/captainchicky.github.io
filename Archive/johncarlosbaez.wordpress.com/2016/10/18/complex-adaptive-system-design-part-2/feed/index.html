<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Complex Adaptive System Design (Part 2)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/</link>
	<description></description>
	<lastBuildDate>Fri, 25 Jun 2021 00:46:50 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Complex Adaptive System Design (Part 3) &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-97733</link>

		<dc:creator><![CDATA[Complex Adaptive System Design (Part 3) &#124; Azimuth]]></dc:creator>
		<pubDate>Thu, 17 Aug 2017 08:42:25 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-97733</guid>

					<description><![CDATA[It&#039;s been a long time since I&#039;ve blogged about the &lt;a href=&quot;http://www.darpa.mil/program/complex-adaptive-system-composition-and-design-environment&quot; rel=&quot;nofollow&quot;&gt;Complex Adaptive System Composition and Design Environment&lt;/a&gt; or &lt;strong&gt;CASCADE&lt;/strong&gt; project run by &lt;a href=&quot;https://www.darpa.mil/staff/dr-john-s-paschkewitz&quot; rel=&quot;nofollow&quot;&gt;John Paschkewitz&lt;/a&gt;.  For a reminder, read these:

• &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/10/02/complex-adaptive-system-design-part-1/&quot;&gt;Complex adaptive system design (part 1)&lt;/a&gt;, &lt;em&gt;Azimuth&lt;/em&gt;, 2 October 2016.

• &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/&quot;&gt;Complex adaptive system design (part 2)&lt;/a&gt;, &lt;em&gt;Azimuth&lt;/em&gt;, 18 October 2016.

A lot has happened since then, and I want to explain it.]]></description>
			<content:encoded><![CDATA[<p>It&#8217;s been a long time since I&#8217;ve blogged about the <a href="http://www.darpa.mil/program/complex-adaptive-system-composition-and-design-environment" rel="nofollow">Complex Adaptive System Composition and Design Environment</a> or <strong>CASCADE</strong> project run by <a href="https://www.darpa.mil/staff/dr-john-s-paschkewitz" rel="nofollow">John Paschkewitz</a>.  For a reminder, read these:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2016/10/02/complex-adaptive-system-design-part-1/">Complex adaptive system design (part 1)</a>, <em>Azimuth</em>, 2 October 2016.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/">Complex adaptive system design (part 2)</a>, <em>Azimuth</em>, 18 October 2016.</p>
<p>A lot has happened since then, and I want to explain it.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: jimstuttard		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-89334</link>

		<dc:creator><![CDATA[jimstuttard]]></dc:creator>
		<pubDate>Thu, 30 Mar 2017 15:38:15 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-89334</guid>

					<description><![CDATA[You might find it useful to look at IBM&#039;s Rational XDE https://www-01.ibm.com/software/awdtools/suite/technical/features/ which connects UML and code for round-trip engineering (see wikipedia), they UML generates code skeletons and filled in code can be reversed engineered into diagrams. I found its Java precursor in 2001 to be the only IDE apart from emacs and intellij that I could stand to use; I believe it now supports multiple languages. I can&#039;t imagine that this ExAMS s/w is more powerful. Why did Metron not use it? Possibly because Rational products did and probably do cost an awful lot per seat.]]></description>
			<content:encoded><![CDATA[<p>You might find it useful to look at IBM&#8217;s Rational XDE <a href="https://www-01.ibm.com/software/awdtools/suite/technical/features/" rel="nofollow ugc">https://www-01.ibm.com/software/awdtools/suite/technical/features/</a> which connects UML and code for round-trip engineering (see wikipedia), they UML generates code skeletons and filled in code can be reversed engineered into diagrams. I found its Java precursor in 2001 to be the only IDE apart from emacs and intellij that I could stand to use; I believe it now supports multiple languages. I can&#8217;t imagine that this ExAMS s/w is more powerful. Why did Metron not use it? Possibly because Rational products did and probably do cost an awful lot per seat.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Wes Hansen		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84498</link>

		<dc:creator><![CDATA[Wes Hansen]]></dc:creator>
		<pubDate>Mon, 24 Oct 2016 18:47:07 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84498</guid>

					<description><![CDATA[Okay, now I’m sorta finding this interesting! I’m not a software developer but I do have a bit of experience with PLCs and Saur Danfoss has a special type of function block diagram editor called GUIDE (Graphical User Interface Design Environment?) which is considered a high-level language by IEEE, ISO, and IEC:

http://powersolutions.danfoss.com/products/plus-1-software/#/

Of course it’s part of their PLUS 1 program so geared towards specific control situations but it could provide insight.

Also, you might be interested in the hypergraphs that Ben Goertzel and Company use with their Novamente and OpenCog projects:

Textbook: http://www.springer.com/us/book/9789462390263

Pre-print (free): http://lesswrong.com/lw/kq4/link_engineering_general_intelligence_the/

Of course their hypergraphs are not being used to study complex systems of systems, rather, they are what evolves in their directed evolution approach to AGI. They also have a probabilistic inference engine called PLN which they utilize to make inferences from hypergraphs; something you might find helpful.

From an analysis perspective, a couple of years ago I was reading a book of Humanity + interviews that Goertzel had put together, “Between Ape and Artilect:”

http://goertzel.org/BetweenApeAndArtilect.pdf

and in one of the interviews Goertzel was discussing the problems he and his team were having mathematically modelling what they call cognitive synergy. From “The Hidden Pattern:”

http://www.goertzel.org/HiddenPattern_march_4_06.pdf

“[…] Clustering/reasoning synergy is one among many dozens of examples of such synergies that we have discussed in the previous chapters. And the high-level mind patterns discussed in Chapter one – the dual network and the self, for example – are envisioned as emergent system patterns arising through cooperativity of all the system’s AI modules. The collection of AI modules in the system is intended to be a close-to-minimal set capable of leading to cooperative emergence of these high-level emergent structures. […]”

According to Goertzel, they were trying to use GeoMetrodynamics to model the synergy where the synergy is represented by curvature. This put the idea of synergy as curvature in my head and I came up with what I believe to be a novel idea, which I emailed to Goertzel and Duane Kouba (Dr. Kouba is an award-winning professor at UC Davis who specializes in Diffy Qs).

Basically, you start with a set of initial processes and synergies between those processes. So my thought was to encode the initial synergies in a Riemannian metric and then use Ricci Flow analogs to model each process with its relevant synergies leading to a system of Ricci Flow analogs which acts to transform the manifold on which it lives. This should tell one how the initial system evolves with time, from the perspective of synergy. To me, in designing a system of systems, the top level goal should be efficiency and it seems to me that high efficiency could be achieved via the maximization of synergy, the idea being to find a “close-to-minimal set capable of leading to cooperative emergence of” whatever one’s goal may be. If you find this idea compelling, you might want to email Goertzel and/or Kouba an inquiry as to whether or not they have done anything with it.

Now, with regards to your ethical considerations, in a sense, you can think of synergy as a type of mutually beneficial symbiosis, correct? Just recently, I posted a short comment on Wolfram’s blog suggesting the maximization of mutually beneficial symbiosis as the overall guiding principle for his AI Constitution. Constitutions are generally developed to guide relations between distinct entities and are most often deployed during conflict resolution. So imagine if your study of systems of systems, funded by DARPA or whoever – the military – led to a better and fuller understanding of general synergistic dynamics, conflict emergence, and conflict resolution which could be utilized on a global scale, eventually leading to a drastic reduction in the necessity for militaristic deployment? Now that would be the Karmic Kitty’s meow! Of course I always take the long view . . .]]></description>
			<content:encoded><![CDATA[<p>Okay, now I’m sorta finding this interesting! I’m not a software developer but I do have a bit of experience with PLCs and Saur Danfoss has a special type of function block diagram editor called GUIDE (Graphical User Interface Design Environment?) which is considered a high-level language by IEEE, ISO, and IEC:</p>
<p><a href="http://powersolutions.danfoss.com/products/plus-1-software/#/" rel="nofollow ugc">http://powersolutions.danfoss.com/products/plus-1-software/#/</a></p>
<p>Of course it’s part of their PLUS 1 program so geared towards specific control situations but it could provide insight.</p>
<p>Also, you might be interested in the hypergraphs that Ben Goertzel and Company use with their Novamente and OpenCog projects:</p>
<p>Textbook: <a href="http://www.springer.com/us/book/9789462390263" rel="nofollow ugc">http://www.springer.com/us/book/9789462390263</a></p>
<p>Pre-print (free): <a href="http://lesswrong.com/lw/kq4/link_engineering_general_intelligence_the/" rel="nofollow ugc">http://lesswrong.com/lw/kq4/link_engineering_general_intelligence_the/</a></p>
<p>Of course their hypergraphs are not being used to study complex systems of systems, rather, they are what evolves in their directed evolution approach to AGI. They also have a probabilistic inference engine called PLN which they utilize to make inferences from hypergraphs; something you might find helpful.</p>
<p>From an analysis perspective, a couple of years ago I was reading a book of Humanity + interviews that Goertzel had put together, “Between Ape and Artilect:”</p>
<p><object data="http://goertzel.org/BetweenApeAndArtilect.pdf" type="application/pdf" width="100%" height="800" style="height: 800px;"><p><a href="http://goertzel.org/BetweenApeAndArtilect.pdf">Click to access BetweenApeAndArtilect.pdf</a></p></object></p>
<p>and in one of the interviews Goertzel was discussing the problems he and his team were having mathematically modelling what they call cognitive synergy. From “The Hidden Pattern:”</p>
<p><object data="http://www.goertzel.org/HiddenPattern_march_4_06.pdf" type="application/pdf" width="100%" height="800" style="height: 800px;"><p><a href="http://www.goertzel.org/HiddenPattern_march_4_06.pdf">Click to access HiddenPattern_march_4_06.pdf</a></p></object></p>
<p>“[…] Clustering/reasoning synergy is one among many dozens of examples of such synergies that we have discussed in the previous chapters. And the high-level mind patterns discussed in Chapter one – the dual network and the self, for example – are envisioned as emergent system patterns arising through cooperativity of all the system’s AI modules. The collection of AI modules in the system is intended to be a close-to-minimal set capable of leading to cooperative emergence of these high-level emergent structures. […]”</p>
<p>According to Goertzel, they were trying to use GeoMetrodynamics to model the synergy where the synergy is represented by curvature. This put the idea of synergy as curvature in my head and I came up with what I believe to be a novel idea, which I emailed to Goertzel and Duane Kouba (Dr. Kouba is an award-winning professor at UC Davis who specializes in Diffy Qs).</p>
<p>Basically, you start with a set of initial processes and synergies between those processes. So my thought was to encode the initial synergies in a Riemannian metric and then use Ricci Flow analogs to model each process with its relevant synergies leading to a system of Ricci Flow analogs which acts to transform the manifold on which it lives. This should tell one how the initial system evolves with time, from the perspective of synergy. To me, in designing a system of systems, the top level goal should be efficiency and it seems to me that high efficiency could be achieved via the maximization of synergy, the idea being to find a “close-to-minimal set capable of leading to cooperative emergence of” whatever one’s goal may be. If you find this idea compelling, you might want to email Goertzel and/or Kouba an inquiry as to whether or not they have done anything with it.</p>
<p>Now, with regards to your ethical considerations, in a sense, you can think of synergy as a type of mutually beneficial symbiosis, correct? Just recently, I posted a short comment on Wolfram’s blog suggesting the maximization of mutually beneficial symbiosis as the overall guiding principle for his AI Constitution. Constitutions are generally developed to guide relations between distinct entities and are most often deployed during conflict resolution. So imagine if your study of systems of systems, funded by DARPA or whoever – the military – led to a better and fuller understanding of general synergistic dynamics, conflict emergence, and conflict resolution which could be utilized on a global scale, eventually leading to a drastic reduction in the necessity for militaristic deployment? Now that would be the Karmic Kitty’s meow! Of course I always take the long view . . .</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84425</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 22 Oct 2016 18:05:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84425</guid>

					<description><![CDATA[There are also interesting comments &lt;a href=&quot;https://plus.google.com/117663015413546257905/posts/1DACvL954WK&quot; rel=&quot;nofollow&quot;&gt;on G+&lt;/a&gt;.  A number express skepticism regarding the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Unified_Modeling_Language&quot; rel=&quot;nofollow&quot;&gt;UML&lt;/a&gt; (Unified Modeling Language) for programming.  This makes me wonder 1) how does Metron&#039;s ExAMS software differ from UML, and 2) how does  simulating networks of boats, planes and satellites differ from other sorts of programming?

I have some ideas on these questions, but anyway, here is part of the conversation on G+:

Carsten F&#252;hrmann wrote:

&lt;blockquote&gt;
I must join the ranks of the UML sceptics here. Actually, my scepticism covers a whole range of diagrammatic techniques in software development. As you know, John, I&#039;ve once dabbled in theories of diagrams (for logics), and had I stayed in academia, I&#039;d probably have pursued this diagrammatic approach to logics and programming languages, premonoidal categories and such. But then I changed careers, and became a software developer. And I found to my own consternation that diagrammatic approaches to software, UML and others, are not beneficial to software engineer&#039;s productivity. I thought a lot about the reasons. And I think I figured them out:

Firstly, software systems quicky get so large that visualisations just look like the wires on a super-complex mainboard, or processor dye. By contrast, a programmer&#039;s mind seems to work is work locally, in that they put their attention inside one diagram node (= function/class/module) and must forget about the rest. Then they take it from there and shift there focus to neighboring nodes. Navigation often takes place via something like project-wide full text search, but never by eye-balling an overview graph. One almost never &quot;zooms out&quot;, except, in the role of a software architect, to such a birds-eye perspective that the presentation (graphical vs. text) isn&#039;t all that important. The understanding of the whole often works via verbalizing pervasive concepts rather than visualization.

Secondly, I have yet to find a graphical language where the tool support for local change, called &quot;refactoring&quot; by programmers, is anywhere near as good as that for textual representation of code. I use semi-automatic refactoring tools daily that seem near unportable to diagrams.

Then again, graphs do seem to be crucial in software engineering, but typically in algorithms executed by machines: intermediate output of a compiler represented as a graph, a graph of the heap used by a garbage collector, and so on.

I am super-careful though to not overextend my conclusions. Just because software visualization doesn&#039;t help me and my colleagues doesn&#039;t mean it doesn&#039;t help others. And there are of course areas outside of software engineering, like electronics, control theory, and biology, and physics, where the situation may be very different. And I know of course that you have tons of great materal to show that.﻿
&lt;/blockquote&gt;

Matt McIrvin wrote:

&lt;blockquote&gt;
I remember obsessively graphing the modular structure of the software I was working on when I was first starting out--but, in hindsight, to a large extent it was just because I didn&#039;t have anything like a modern IDE, and the screens on which I did my text editing were tiny. My way of getting a slightly bigger-picture view of the code was to refer to massive paper printouts, and the pictures were a way of getting my mental model of those printouts under control.﻿
&lt;/blockquote&gt;

Carsten wrote:

&lt;blockquote&gt;
 I&#039;ve had a vaguely similar experience: I was trying to visualize the architecture of my company&#039;s software. I have a very modern tool for this, which produces a UML-like presentation, and even lays it out automatically. The resulting graph, when printed at human-readable magnification, has maybe the size of a football field and is a mess of edges crossing each other. I then applied simplifications and different levels of granularity and zooming. But the handling of the mess is so difficult, it&#039;s much faster to just look at the code.﻿
&lt;/blockquote&gt;

John Baez wrote:

&lt;blockquote&gt;
Carsten Führmann wrote: 

&lt;blockquote&gt;
Firstly, software systems quickly get so large that visualisations just look like the wires on a super-complex mainboard, or processor dye.
&lt;/blockquote&gt;

Could the reason be that the code is not organized in a hierarchical way that lets you see and work with useful &quot;coarse-grained views&quot; - views in which the millions of boxes and wires are grouped into fewer, larger units? 

Metron&#039;s ExAMS software lets you simulate complex search-and-rescue operations involving many platforms---ships, aircraft, and satellites---communicating to each other and moving around.   If you want, you can see down into fine-grained details of each electronic device on each platform.   But you usually don&#039;t want to.  You certainly never want to see all that detail for every device on every platform all at once.  What would be the point?

It would be very interesting if ExAMS were better than SysML or UML.  Unfortunately I don&#039;t have technical documentation for ExAMS, so I&#039;m starting by reading about those other systems.

It would also be interesting if graphical methods are inherently better for the applications that Metron is concerned with---simulating operations involving ships, planes and satellites---than what you what you were doing---programming.﻿
&lt;/blockquote&gt;

Carsten wrote:

&lt;blockquote&gt;
Okay, let me reveal my daily work in its full mundaneness :) My company makes software for business, a rather big and versatile software that&#039;s seen thousands of &quot;man-years&quot;. It&#039;s written on the basis of Microsoft&#039;s .Net framework, but it might as well be Java or something like that. At a technical level, the coarsest granularity is the &quot;unit of deployment&quot;, that is, an &quot;assembly&quot; file that may or may not be deployed to the customer. Each assembly contains some &quot;namespaces&quot;. Each namespace contains lots of &quot;classes&quot;. Each class lots of &quot;methods&quot;.

We have maybe a thousand assemblies. They have a dependency hierarchy, in that stuff &lt;i&gt;inside&lt;/i&gt; assembly A may need stuff &lt;i&gt;inside&lt;/i&gt; assembly B. There are actually different types of dependency edges. The most important ones form a directed, acyclic graph. Other kinds of edges form a graphs that are also directed, but need not be acyclic. Then we can zoom inside a typical assembly. There will only be a small number of namespaces per assembly, let&#039;s ignore those.

Each assembly has a few hundred classes and interfaces on average. These classes and interfaces again have &lt;i&gt;lots&lt;/i&gt; of dependencies among each other, and they have fewer, but still many, dependencies to classes and interfaces in other assemblies.

Sometimes, but rarely, one looks at the entirety of the software, at assembly level. Then the UML-style visualization looks a bit like the map of a major city, with bundles of edges flowing between assemblies. If one then represents each bundle of edges by a single edge, one obtains a graph that&#039;s presentable---if you have a monitor the size of a wall and very sharp eyes. But funnily enough, at that level, the visualization is hardly more informative or useful then the structure of named, and sometimes nested, file-system folders containing the assemblies.

In daily programming, one typically focusses on changing the contents of a single assembly or a bunch of assemblies. So you look inside it, at the classes and interfaces. Now you can visualize those. Again, you get something like the map of a major city, except the road planners were on Speed and LSD combined. It&#039;s actually much easier to just look at the (possibly nested) collection of named files, one per class, and the read them in the text editor. When one needs to follow a dependency, the IDE (integrated development environment) provides ways to click a piece of text and teleport you to the dependency.

So, in practice, our software engineers rarely look at (architectural UML) visualizations.

It&#039;s not hard to imagine, though, that in areas other than software engineering, the relative sizes of the entities one deals with, and the abstraction layers, are better suited to visualization.

I think the following paragraph in your previous comment is crucial: &quot;It would also be interesting if graphical methods are inherently better for the applications that Metron is concerned with&quot;. Indeed, it would be interesting! I have no idea, it might well be great! So we have this interesting question: What exactly is it that makes visualizations so useful in some domains, and so unproductive in others?﻿
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>There are also interesting comments <a href="https://plus.google.com/117663015413546257905/posts/1DACvL954WK" rel="nofollow">on G+</a>.  A number express skepticism regarding the use of <a href="https://en.wikipedia.org/wiki/Unified_Modeling_Language" rel="nofollow">UML</a> (Unified Modeling Language) for programming.  This makes me wonder 1) how does Metron&#8217;s ExAMS software differ from UML, and 2) how does  simulating networks of boats, planes and satellites differ from other sorts of programming?</p>
<p>I have some ideas on these questions, but anyway, here is part of the conversation on G+:</p>
<p>Carsten F&uuml;hrmann wrote:</p>
<blockquote><p>
I must join the ranks of the UML sceptics here. Actually, my scepticism covers a whole range of diagrammatic techniques in software development. As you know, John, I&#8217;ve once dabbled in theories of diagrams (for logics), and had I stayed in academia, I&#8217;d probably have pursued this diagrammatic approach to logics and programming languages, premonoidal categories and such. But then I changed careers, and became a software developer. And I found to my own consternation that diagrammatic approaches to software, UML and others, are not beneficial to software engineer&#8217;s productivity. I thought a lot about the reasons. And I think I figured them out:</p>
<p>Firstly, software systems quicky get so large that visualisations just look like the wires on a super-complex mainboard, or processor dye. By contrast, a programmer&#8217;s mind seems to work is work locally, in that they put their attention inside one diagram node (= function/class/module) and must forget about the rest. Then they take it from there and shift there focus to neighboring nodes. Navigation often takes place via something like project-wide full text search, but never by eye-balling an overview graph. One almost never &#8220;zooms out&#8221;, except, in the role of a software architect, to such a birds-eye perspective that the presentation (graphical vs. text) isn&#8217;t all that important. The understanding of the whole often works via verbalizing pervasive concepts rather than visualization.</p>
<p>Secondly, I have yet to find a graphical language where the tool support for local change, called &#8220;refactoring&#8221; by programmers, is anywhere near as good as that for textual representation of code. I use semi-automatic refactoring tools daily that seem near unportable to diagrams.</p>
<p>Then again, graphs do seem to be crucial in software engineering, but typically in algorithms executed by machines: intermediate output of a compiler represented as a graph, a graph of the heap used by a garbage collector, and so on.</p>
<p>I am super-careful though to not overextend my conclusions. Just because software visualization doesn&#8217;t help me and my colleagues doesn&#8217;t mean it doesn&#8217;t help others. And there are of course areas outside of software engineering, like electronics, control theory, and biology, and physics, where the situation may be very different. And I know of course that you have tons of great materal to show that.﻿
</p></blockquote>
<p>Matt McIrvin wrote:</p>
<blockquote><p>
I remember obsessively graphing the modular structure of the software I was working on when I was first starting out&#8211;but, in hindsight, to a large extent it was just because I didn&#8217;t have anything like a modern IDE, and the screens on which I did my text editing were tiny. My way of getting a slightly bigger-picture view of the code was to refer to massive paper printouts, and the pictures were a way of getting my mental model of those printouts under control.﻿
</p></blockquote>
<p>Carsten wrote:</p>
<blockquote><p>
 I&#8217;ve had a vaguely similar experience: I was trying to visualize the architecture of my company&#8217;s software. I have a very modern tool for this, which produces a UML-like presentation, and even lays it out automatically. The resulting graph, when printed at human-readable magnification, has maybe the size of a football field and is a mess of edges crossing each other. I then applied simplifications and different levels of granularity and zooming. But the handling of the mess is so difficult, it&#8217;s much faster to just look at the code.﻿
</p></blockquote>
<p>John Baez wrote:</p>
<blockquote><p>
Carsten Führmann wrote: </p>
<blockquote><p>
Firstly, software systems quickly get so large that visualisations just look like the wires on a super-complex mainboard, or processor dye.
</p></blockquote>
<p>Could the reason be that the code is not organized in a hierarchical way that lets you see and work with useful &#8220;coarse-grained views&#8221; &#8211; views in which the millions of boxes and wires are grouped into fewer, larger units? </p>
<p>Metron&#8217;s ExAMS software lets you simulate complex search-and-rescue operations involving many platforms&#8212;ships, aircraft, and satellites&#8212;communicating to each other and moving around.   If you want, you can see down into fine-grained details of each electronic device on each platform.   But you usually don&#8217;t want to.  You certainly never want to see all that detail for every device on every platform all at once.  What would be the point?</p>
<p>It would be very interesting if ExAMS were better than SysML or UML.  Unfortunately I don&#8217;t have technical documentation for ExAMS, so I&#8217;m starting by reading about those other systems.</p>
<p>It would also be interesting if graphical methods are inherently better for the applications that Metron is concerned with&#8212;simulating operations involving ships, planes and satellites&#8212;than what you what you were doing&#8212;programming.﻿
</p></blockquote>
<p>Carsten wrote:</p>
<blockquote><p>
Okay, let me reveal my daily work in its full mundaneness :) My company makes software for business, a rather big and versatile software that&#8217;s seen thousands of &#8220;man-years&#8221;. It&#8217;s written on the basis of Microsoft&#8217;s .Net framework, but it might as well be Java or something like that. At a technical level, the coarsest granularity is the &#8220;unit of deployment&#8221;, that is, an &#8220;assembly&#8221; file that may or may not be deployed to the customer. Each assembly contains some &#8220;namespaces&#8221;. Each namespace contains lots of &#8220;classes&#8221;. Each class lots of &#8220;methods&#8221;.</p>
<p>We have maybe a thousand assemblies. They have a dependency hierarchy, in that stuff <i>inside</i> assembly A may need stuff <i>inside</i> assembly B. There are actually different types of dependency edges. The most important ones form a directed, acyclic graph. Other kinds of edges form a graphs that are also directed, but need not be acyclic. Then we can zoom inside a typical assembly. There will only be a small number of namespaces per assembly, let&#8217;s ignore those.</p>
<p>Each assembly has a few hundred classes and interfaces on average. These classes and interfaces again have <i>lots</i> of dependencies among each other, and they have fewer, but still many, dependencies to classes and interfaces in other assemblies.</p>
<p>Sometimes, but rarely, one looks at the entirety of the software, at assembly level. Then the UML-style visualization looks a bit like the map of a major city, with bundles of edges flowing between assemblies. If one then represents each bundle of edges by a single edge, one obtains a graph that&#8217;s presentable&#8212;if you have a monitor the size of a wall and very sharp eyes. But funnily enough, at that level, the visualization is hardly more informative or useful then the structure of named, and sometimes nested, file-system folders containing the assemblies.</p>
<p>In daily programming, one typically focusses on changing the contents of a single assembly or a bunch of assemblies. So you look inside it, at the classes and interfaces. Now you can visualize those. Again, you get something like the map of a major city, except the road planners were on Speed and LSD combined. It&#8217;s actually much easier to just look at the (possibly nested) collection of named files, one per class, and the read them in the text editor. When one needs to follow a dependency, the IDE (integrated development environment) provides ways to click a piece of text and teleport you to the dependency.</p>
<p>So, in practice, our software engineers rarely look at (architectural UML) visualizations.</p>
<p>It&#8217;s not hard to imagine, though, that in areas other than software engineering, the relative sizes of the entities one deals with, and the abstraction layers, are better suited to visualization.</p>
<p>I think the following paragraph in your previous comment is crucial: &#8220;It would also be interesting if graphical methods are inherently better for the applications that Metron is concerned with&#8221;. Indeed, it would be interesting! I have no idea, it might well be great! So we have this interesting question: What exactly is it that makes visualizations so useful in some domains, and so unproductive in others?﻿
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: domenico		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84386</link>

		<dc:creator><![CDATA[domenico]]></dc:creator>
		<pubDate>Fri, 21 Oct 2016 19:55:57 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84386</guid>

					<description><![CDATA[Is there an analogy between Complex Adaptive System design and music scores?
Each musician perform an orderly musical sequence in time, that is a established procedure (boxes for press, pinches, musical chord, etc), so that it could be possible to create new music in a more precise method (wire for tie) like a numerical function.
Controlling an orchestra does not seem different from controlling a complex system using complex commands.]]></description>
			<content:encoded><![CDATA[<p>Is there an analogy between Complex Adaptive System design and music scores?<br />
Each musician perform an orderly musical sequence in time, that is a established procedure (boxes for press, pinches, musical chord, etc), so that it could be possible to create new music in a more precise method (wire for tie) like a numerical function.<br />
Controlling an orchestra does not seem different from controlling a complex system using complex commands.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: @whut		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84362</link>

		<dc:creator><![CDATA[@whut]]></dc:creator>
		<pubDate>Thu, 20 Oct 2016 19:16:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84362</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84340&quot;&gt;@whut&lt;/a&gt;.

&lt;i&gt;&quot;What does this mean, exactly?&quot;&lt;/i&gt;

Language designers provide the necessary primitives to be able to create all of the known synchronization constructs.  I consider those the building blocks. Examples include guards, time-outs, semaphores, entries, mutexes, threads, The hardware design languages introduce signals, which are even more primitive.

I haven&#039;t seen the EXaMS tool but I doubt that they have the fine control that I would want.  I could be wrong though.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84340">@whut</a>.</p>
<p><i>&#8220;What does this mean, exactly?&#8221;</i></p>
<p>Language designers provide the necessary primitives to be able to create all of the known synchronization constructs.  I consider those the building blocks. Examples include guards, time-outs, semaphores, entries, mutexes, threads, The hardware design languages introduce signals, which are even more primitive.</p>
<p>I haven&#8217;t seen the EXaMS tool but I doubt that they have the fine control that I would want.  I could be wrong though.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84360</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 20 Oct 2016 17:18:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84360</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84340&quot;&gt;@whut&lt;/a&gt;.

Thanks for the quick intro!  I&#039;ll look into this stuff.  I&#039;m getting ready to teach now, but I&#039;ll have a lot more to say later.

What does this mean, exactly?

&lt;blockquote&gt;
  And it’s more building-block than the ExAMS software from what I can tell.
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84340">@whut</a>.</p>
<p>Thanks for the quick intro!  I&#8217;ll look into this stuff.  I&#8217;m getting ready to teach now, but I&#8217;ll have a lot more to say later.</p>
<p>What does this mean, exactly?</p>
<blockquote><p>
  And it’s more building-block than the ExAMS software from what I can tell.
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: scout		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84351</link>

		<dc:creator><![CDATA[scout]]></dc:creator>
		<pubDate>Thu, 20 Oct 2016 06:27:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84351</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84345&quot;&gt;Eugene&lt;/a&gt;.

Jeltsch cites a couple papers by Jeffery on LTL and functional reactive programming (FRP); an older paper along these lines is &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.2554&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Interaction Categories and the Foundations of Typed Concurrent Programming&lt;/em&gt;&lt;/a&gt; (Abramsky,Gay,Nagarajan 96), there&#039;s also ongoing research into session types and related type systems for concurrent systems with provable &quot;deadlock freedom&quot;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84345">Eugene</a>.</p>
<p>Jeltsch cites a couple papers by Jeffery on LTL and functional reactive programming (FRP); an older paper along these lines is <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.2554" rel="nofollow"><em>Interaction Categories and the Foundations of Typed Concurrent Programming</em></a> (Abramsky,Gay,Nagarajan 96), there&#8217;s also ongoing research into session types and related type systems for concurrent systems with provable &#8220;deadlock freedom&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Eugene		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84345</link>

		<dc:creator><![CDATA[Eugene]]></dc:creator>
		<pubDate>Wed, 19 Oct 2016 19:56:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84345</guid>

					<description><![CDATA[There are papers linking temporal logic and category theory.  For example &quot;Towards a Common Categorical Semantics
for Linear-Time Temporal Logic
and Functional Reactive Programming&quot; by 
Wolfgang Jeltsch (a copy is  &lt;a href=&quot;http://www.ioc.ee/~wolfgang/research/mfps-2012-paper.pdf&quot; rel=&quot;nofollow&quot;&gt; here &lt;/a&gt;).  Jeltsch has more papers along these lines.]]></description>
			<content:encoded><![CDATA[<p>There are papers linking temporal logic and category theory.  For example &#8220;Towards a Common Categorical Semantics<br />
for Linear-Time Temporal Logic<br />
and Functional Reactive Programming&#8221; by<br />
Wolfgang Jeltsch (a copy is  <a href="http://www.ioc.ee/~wolfgang/research/mfps-2012-paper.pdf" rel="nofollow"> here </a>).  Jeltsch has more papers along these lines.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: @whut		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84340</link>

		<dc:creator><![CDATA[@whut]]></dc:creator>
		<pubDate>Wed, 19 Oct 2016 17:01:33 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=22673#comment-84340</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84335&quot;&gt;John Baez&lt;/a&gt;.

These are of course not new ideas, and computer scientists have long been experimenting with how to represent temporal logic in terms of something that is executable. All of robotics and digital logic design has this goal.

The one language that has time integrated into its syntax is Ada for software (and VHDL amongst others for hardware design). Ada in particular has two built-in expressions for delaying execution, one for absolute timing and one for relative timing.

A delay_statement is used to block further execution until a specified expiration time is reached. The expiration time can be specified either as a particular point in time (in a delay_until_statement), or in seconds from the current time (in a delay_relative_statement). The language-defined package Calendar provides definitions for a type Time and associated operations, including a function Clock that returns the current time.

B-N Syntax

&lt;blockquote&gt;
delay_statement ::= delay_until_statement &#124; delay_relative_statement

delay_until_statement ::= &lt;b&gt;delay until&lt;/b&gt; delay_expression;

delay_relative_statement ::= &lt;b&gt;delay&lt;/b&gt; delay_expression;
&lt;/blockquote&gt;

This is not really interesting until the concepts of threads are introduced, whereby Petri Net control flow can then be modeled. The foundational reference for the theory is by Hoare

&lt;blockquote&gt;
Hoare, Charles Antony Richard. Communicating sequential processes. Vol. 178. Englewood Cliffs: Prentice-hall, 1985.
&lt;/blockquote&gt;

Any Petri Net diagram can be transcribed into an Ada program using the built-in syntax.

The one caveat in this is that Ada is meant for writing real-time software. For the timer, it uses what is known as the &quot;wall clock&quot; which ticks away in actual time.   But there is also a way to replace the wall clock with a simulated clock and thus use it in a way that is a pure executable architecture.

This is a low-level nuts-and-bolts simulation concept that has been around for awhile.  And it&#039;s more building-block than the ExAMS software from what I can tell.  Look into VHDL and your head will spin in terms of what can be constructed.  A VHDL model of a logic design is by definition an &quot;executable architecture&quot;, and only when it is synthesized onto a chip is when you get to see it operate in the real world.  All of the fortune in Silicon Valley is built on top of languages such as VHDL, Verilog, and others classified as proprietary intellectual property.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/10/18/complex-adaptive-system-design-part-2/#comment-84335">John Baez</a>.</p>
<p>These are of course not new ideas, and computer scientists have long been experimenting with how to represent temporal logic in terms of something that is executable. All of robotics and digital logic design has this goal.</p>
<p>The one language that has time integrated into its syntax is Ada for software (and VHDL amongst others for hardware design). Ada in particular has two built-in expressions for delaying execution, one for absolute timing and one for relative timing.</p>
<p>A delay_statement is used to block further execution until a specified expiration time is reached. The expiration time can be specified either as a particular point in time (in a delay_until_statement), or in seconds from the current time (in a delay_relative_statement). The language-defined package Calendar provides definitions for a type Time and associated operations, including a function Clock that returns the current time.</p>
<p>B-N Syntax</p>
<blockquote><p>
delay_statement ::= delay_until_statement | delay_relative_statement</p>
<p>delay_until_statement ::= <b>delay until</b> delay_expression;</p>
<p>delay_relative_statement ::= <b>delay</b> delay_expression;
</p></blockquote>
<p>This is not really interesting until the concepts of threads are introduced, whereby Petri Net control flow can then be modeled. The foundational reference for the theory is by Hoare</p>
<blockquote><p>
Hoare, Charles Antony Richard. Communicating sequential processes. Vol. 178. Englewood Cliffs: Prentice-hall, 1985.
</p></blockquote>
<p>Any Petri Net diagram can be transcribed into an Ada program using the built-in syntax.</p>
<p>The one caveat in this is that Ada is meant for writing real-time software. For the timer, it uses what is known as the &#8220;wall clock&#8221; which ticks away in actual time.   But there is also a way to replace the wall clock with a simulated clock and thus use it in a way that is a pure executable architecture.</p>
<p>This is a low-level nuts-and-bolts simulation concept that has been around for awhile.  And it&#8217;s more building-block than the ExAMS software from what I can tell.  Look into VHDL and your head will spin in terms of what can be constructed.  A VHDL model of a logic design is by definition an &#8220;executable architecture&#8221;, and only when it is synthesized onto a chip is when you get to see it operate in the real world.  All of the fortune in Silicon Valley is built on top of languages such as VHDL, Verilog, and others classified as proprietary intellectual property.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
