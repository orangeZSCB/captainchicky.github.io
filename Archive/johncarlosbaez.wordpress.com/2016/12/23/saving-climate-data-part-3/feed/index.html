<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Saving Climate Data (Part 3)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/</link>
	<description></description>
	<lastBuildDate>Wed, 28 Dec 2016 20:03:29 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: gwpl		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86200</link>

		<dc:creator><![CDATA[gwpl]]></dc:creator>
		<pubDate>Wed, 28 Dec 2016 20:03:29 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86200</guid>

					<description><![CDATA[One may want to verify in future if data were not changed - it&#039;s domain of #DataIntegrity.

Historically it was done by timestamping, e.g. by official stamps on post.

Nowadays we have cryptographic timestamps, of two kinds:
* Based on PKI , assymetric cryptography
* Based.on blockchain.

TL;DR- Please timestamp! Making timestamps it&#039;s not big hassle and later makes legally valid claims that data are as day are (e.g. blockchain method of GuardTime prints hash of all hashes in NewsPaper so one can be formally verified in libraries etc). The only moment of making timestamps is now: if we do later we can not proof it was not modified in mean time. After all, I am fan of block chain basedethod but if personally I timestamp with as many providers (including PKI ones) as handy, to be better covered.

lease allow future #DataIntegrity checks and do #DigitalTimestamp of those hashes! With #PKI &#038; wothout (e.g. @GuardTime )

(As I wrote on 
https://twitter.com/GWierzowiecki/status/814196190669602816 )]]></description>
			<content:encoded><![CDATA[<p>One may want to verify in future if data were not changed &#8211; it&#8217;s domain of #DataIntegrity.</p>
<p>Historically it was done by timestamping, e.g. by official stamps on post.</p>
<p>Nowadays we have cryptographic timestamps, of two kinds:<br />
* Based on PKI , assymetric cryptography<br />
* Based.on blockchain.</p>
<p>TL;DR- Please timestamp! Making timestamps it&#8217;s not big hassle and later makes legally valid claims that data are as day are (e.g. blockchain method of GuardTime prints hash of all hashes in NewsPaper so one can be formally verified in libraries etc). The only moment of making timestamps is now: if we do later we can not proof it was not modified in mean time. After all, I am fan of block chain basedethod but if personally I timestamp with as many providers (including PKI ones) as handy, to be better covered.</p>
<p>lease allow future #DataIntegrity checks and do #DigitalTimestamp of those hashes! With #PKI &amp; wothout (e.g. @GuardTime )</p>
<p>(As I wrote on<br />
<a href="https://twitter.com/GWierzowiecki/status/814196190669602816" rel="nofollow ugc">https://twitter.com/GWierzowiecki/status/814196190669602816</a> )</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86148</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 27 Dec 2016 17:14:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86148</guid>

					<description><![CDATA[Sakari put up an article on G+ about this issue:

&#8226; Sakari Maaranen, &lt;a href=&quot;https://plus.google.com/u/0/+SakariMaaranen/posts/F6fV1p9P87W?sfc=true&quot; rel=&quot;nofollow&quot;&gt;Permanent digital publication&lt;/a&gt;, 26 December 2016.

Here&#039;s a tiny taste:

&lt;blockquote&gt;
  This is a draft for comments. It&#039;s requirement level description of long term archival quality electronic publishing.
  
  The main problem with various persistent or permanent document identifiers, like PURLs, DOIs, LSIDs, or INFO URIs, is that they all assume being the one scheme that does it. They may certainly overlap, such that one is defined as a namespace under another, but none of them recognizes that various identification schemes come and go and that the same resource may be identified by many different names over time.
  
  &lt;strong&gt;Permanent identification metadata&lt;/strong&gt;
  
  For permanent record, a digital work needs a separate set of metadata that is not a part of the work itself, but claims a reference to it. The reference is a set of one or more cryptographic hash values of the same work. These hash values may be added or dropped, as long as at least one currently relevant value remains at all times.
  
  In addition to the set of hash values, the metadata may contain any number of any persistent (or transient) identifiers for the work. These may be authenticated or contested by any currently relevant means. The metadata definition must be independent of any particular identification scheme and support all of them, including any future schemes that have not been thought of yet.
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>Sakari put up an article on G+ about this issue:</p>
<p>&bull; Sakari Maaranen, <a href="https://plus.google.com/u/0/+SakariMaaranen/posts/F6fV1p9P87W?sfc=true" rel="nofollow">Permanent digital publication</a>, 26 December 2016.</p>
<p>Here&#8217;s a tiny taste:</p>
<blockquote><p>
  This is a draft for comments. It&#8217;s requirement level description of long term archival quality electronic publishing.</p>
<p>  The main problem with various persistent or permanent document identifiers, like PURLs, DOIs, LSIDs, or INFO URIs, is that they all assume being the one scheme that does it. They may certainly overlap, such that one is defined as a namespace under another, but none of them recognizes that various identification schemes come and go and that the same resource may be identified by many different names over time.</p>
<p>  <strong>Permanent identification metadata</strong></p>
<p>  For permanent record, a digital work needs a separate set of metadata that is not a part of the work itself, but claims a reference to it. The reference is a set of one or more cryptographic hash values of the same work. These hash values may be added or dropped, as long as at least one currently relevant value remains at all times.</p>
<p>  In addition to the set of hash values, the metadata may contain any number of any persistent (or transient) identifiers for the work. These may be authenticated or contested by any currently relevant means. The metadata definition must be independent of any particular identification scheme and support all of them, including any future schemes that have not been thought of yet.
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: hypergeometric		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86116</link>

		<dc:creator><![CDATA[hypergeometric]]></dc:creator>
		<pubDate>Mon, 26 Dec 2016 05:06:12 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86116</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86113&quot;&gt;domenico&lt;/a&gt;.

https://www.youtube.com/watch?v=2fednIJs7aI

YouTube coverage of the UToronto event.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86113">domenico</a>.</p>
<p><iframe class="youtube-player" width="450" height="254" src="https://www.youtube.com/embed/2fednIJs7aI?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></p>
<p>YouTube coverage of the UToronto event.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: domenico		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86113</link>

		<dc:creator><![CDATA[domenico]]></dc:creator>
		<pubDate>Mon, 26 Dec 2016 01:01:17 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86113</guid>

					<description><![CDATA[I think that could be more useful a single secure site - that distributes the climate data database - with files encrypted with pretty good privacy protocol, so the source is secure, and the source is verified.
So that each request could be made on an intermediate site, that make the request with the encrypted site using a site public key-private key registration, and the researcher don&#039;t know nothing of the cryptographic protocol: the intermediate site exercises the deciphering phases on behalf of the client, so that the deciphering is concealed to the clients.
The hash table could fail (with a little probability), but the deciphering ensure that the file is not modified, and only the encrypted site could modify the original files.
There may be multiple copies of the secure database (for example with torrent), to ensure a quick download, but these could be multiple copies of a single secure database]]></description>
			<content:encoded><![CDATA[<p>I think that could be more useful a single secure site &#8211; that distributes the climate data database &#8211; with files encrypted with pretty good privacy protocol, so the source is secure, and the source is verified.<br />
So that each request could be made on an intermediate site, that make the request with the encrypted site using a site public key-private key registration, and the researcher don&#8217;t know nothing of the cryptographic protocol: the intermediate site exercises the deciphering phases on behalf of the client, so that the deciphering is concealed to the clients.<br />
The hash table could fail (with a little probability), but the deciphering ensure that the file is not modified, and only the encrypted site could modify the original files.<br />
There may be multiple copies of the secure database (for example with torrent), to ensure a quick download, but these could be multiple copies of a single secure database</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: hypergeometric		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86097</link>

		<dc:creator><![CDATA[hypergeometric]]></dc:creator>
		<pubDate>Sun, 25 Dec 2016 02:56:06 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86097</guid>

					<description><![CDATA[By the way, I don&#039;t recall &lt;a href=&quot;http://variable-variability.blogspot.com/2016/12/donald-trump-fiddle-climate-temperature-data.html&quot; rel=&quot;nofollow&quot;&gt;where I got this link from&lt;/a&gt;, perhaps from John, perhaps somewhere else. But it does a nice job of sketching the possibilities and why it&#039;s wise to do what we are doing.]]></description>
			<content:encoded><![CDATA[<p>By the way, I don&#8217;t recall <a href="http://variable-variability.blogspot.com/2016/12/donald-trump-fiddle-climate-temperature-data.html" rel="nofollow">where I got this link from</a>, perhaps from John, perhaps somewhere else. But it does a nice job of sketching the possibilities and why it&#8217;s wise to do what we are doing.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86094</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 25 Dec 2016 00:48:34 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86094</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86075&quot;&gt;Tobias Fritz&lt;/a&gt;.

I haven&#039;t looked into this.  I hope our experts do!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86075">Tobias Fritz</a>.</p>
<p>I haven&#8217;t looked into this.  I hope our experts do!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: hypergeometric		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86089</link>

		<dc:creator><![CDATA[hypergeometric]]></dc:creator>
		<pubDate>Sat, 24 Dec 2016 19:54:42 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86089</guid>

					<description><![CDATA[I don&#039;t understand the technology well, but there is the transactional safeguarding system called &lt;a href=&quot;https://en.wikipedia.org/wiki/Blockchain_(database)&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;blockchaining&lt;/em&gt;&lt;/a&gt; which may deal with this kind of thing. I don&#039;t really understand how blockchains deal with deletions and edits, but that&#039;s one more thing on the huge pile of &lt;em&gt;new things to learn&lt;/em&gt;.]]></description>
			<content:encoded><![CDATA[<p>I don&#8217;t understand the technology well, but there is the transactional safeguarding system called <a href="https://en.wikipedia.org/wiki/Blockchain_(database)" rel="nofollow"><em>blockchaining</em></a> which may deal with this kind of thing. I don&#8217;t really understand how blockchains deal with deletions and edits, but that&#8217;s one more thing on the huge pile of <em>new things to learn</em>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Tobias Fritz		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86075</link>

		<dc:creator><![CDATA[Tobias Fritz]]></dc:creator>
		<pubDate>Fri, 23 Dec 2016 23:55:19 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86075</guid>

					<description><![CDATA[I bet that you&#039;ve already looked into &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_timestamping&quot; rel=&quot;nofollow&quot;&gt;trusted timestamping&lt;/a&gt;? You send your hash to a trusted authority, who then concatenates it with a timestamp, hashes the result again, and signs it. Then anyone who trusts the authority&#039;s timekeeping can verify that you&#039;ve had the data before the date of the timestamp.]]></description>
			<content:encoded><![CDATA[<p>I bet that you&#8217;ve already looked into <a href="https://en.wikipedia.org/wiki/Trusted_timestamping" rel="nofollow">trusted timestamping</a>? You send your hash to a trusted authority, who then concatenates it with a timestamp, hashes the result again, and signs it. Then anyone who trusts the authority&#8217;s timekeeping can verify that you&#8217;ve had the data before the date of the timestamp.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Wehrle		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86069</link>

		<dc:creator><![CDATA[John Wehrle]]></dc:creator>
		<pubDate>Fri, 23 Dec 2016 19:18:29 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86069</guid>

					<description><![CDATA[&quot;What if the data keeps changing with time? This is especially true of climate records, where new temperatures and so on are added to a database every day, or month, or year. Then I think we need to ‘time-stamp’ everything. The owners of the original database need to keep a list of digests, with the time each one was made. And when you make a copy, you need to record the time it was made.&quot;

That sounds like a job for a distributed version control system like Git.]]></description>
			<content:encoded><![CDATA[<p>&#8220;What if the data keeps changing with time? This is especially true of climate records, where new temperatures and so on are added to a database every day, or month, or year. Then I think we need to ‘time-stamp’ everything. The owners of the original database need to keep a list of digests, with the time each one was made. And when you make a copy, you need to record the time it was made.&#8221;</p>
<p>That sounds like a job for a distributed version control system like Git.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Julie Sylvia		</title>
		<link>https://johncarlosbaez.wordpress.com/2016/12/23/saving-climate-data-part-3/#comment-86067</link>

		<dc:creator><![CDATA[Julie Sylvia]]></dc:creator>
		<pubDate>Fri, 23 Dec 2016 18:10:05 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23143#comment-86067</guid>

					<description><![CDATA[I am so grateful for your goodwill, hard work, and dedication. Thank you.]]></description>
			<content:encoded><![CDATA[<p>I am so grateful for your goodwill, hard work, and dedication. Thank you.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
