<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information Geometry (Part 6)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/</link>
	<description></description>
	<lastBuildDate>Fri, 22 Mar 2013 21:03:13 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Luisberis Velazquez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-27000</link>

		<dc:creator><![CDATA[Luisberis Velazquez]]></dc:creator>
		<pubDate>Fri, 22 Mar 2013 21:03:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-27000</guid>

					<description><![CDATA[Sorry with type-errors. The symbol $latex \theta$ in the Levi-Civita connections comes from my original work, with deals with parametric family of continuous distributions with a number $latex m$ of shape parameters $latex \theta=(\theta^{1},\theta^{2},\ldots \theta^{m})$. Certainly, most of differential equations of Riemannian geometry are difficult to solve. I spend a lot of time (years) to decide to explore this type of statistical geometry. Recently, I realize that some mathematical derivations are not difficult to perform despite the exact mathematical form of the metric tensor $latex g_{ij}(x)$ is unknown for a concrete distribution $latex d\mu(x)=p(x)dx$.

My fundamental interest on this geometry are its applications on classical statistical mechanics. This type of geometry was inspired on Ruppeiner&#039;s geometry of thermodynamics. The metric tensor of this last formulation was modified replacing the usual derivative $latex \partial_{i}$ by the Levi-Civita covariant derivative:
$latex g_{ij}(x)=-D_{i}D_{j}\mathcal{S}(x)$.
Here, $latex \mathcal{S}(x)$ is the thermodynamic entropy. Since the entropy is a scalar function, the above relation guarantees the covariance of the metric tensor. Moreover, the thermodynamic entropy $latex \mathcal{S}(x)$ enters in mathematical expression of Einstein postulate of classical fluctuation theory, which should be extended as follows:
$latex d\mu(x)=\exp\left(\mathcal{S}(x)\right)d\nu(x)$
to guarantee the scalar character of the entropy. The gaussian distribution that I commented above is an exact improvement of gaussian approximation of classical fluctuation theory of statistical mechanics.]]></description>
			<content:encoded><![CDATA[<p>Sorry with type-errors. The symbol <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta" class="latex" /> in the Levi-Civita connections comes from my original work, with deals with parametric family of continuous distributions with a number <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m" class="latex" /> of shape parameters <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%3D%28%5Ctheta%5E%7B1%7D%2C%5Ctheta%5E%7B2%7D%2C%5Cldots+%5Ctheta%5E%7Bm%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta=(&#92;theta^{1},&#92;theta^{2},&#92;ldots &#92;theta^{m})" class="latex" />. Certainly, most of differential equations of Riemannian geometry are difficult to solve. I spend a lot of time (years) to decide to explore this type of statistical geometry. Recently, I realize that some mathematical derivations are not difficult to perform despite the exact mathematical form of the metric tensor <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}(x)" class="latex" /> is unknown for a concrete distribution <img src="https://s0.wp.com/latex.php?latex=d%5Cmu%28x%29%3Dp%28x%29dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu(x)=p(x)dx" class="latex" />.</p>
<p>My fundamental interest on this geometry are its applications on classical statistical mechanics. This type of geometry was inspired on Ruppeiner&#8217;s geometry of thermodynamics. The metric tensor of this last formulation was modified replacing the usual derivative <img src="https://s0.wp.com/latex.php?latex=%5Cpartial_%7Bi%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_{i}" class="latex" /> by the Levi-Civita covariant derivative:<br />
<img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D%28x%29%3D-D_%7Bi%7DD_%7Bj%7D%5Cmathcal%7BS%7D%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}(x)=-D_{i}D_{j}&#92;mathcal{S}(x)" class="latex" />.<br />
Here, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BS%7D%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{S}(x)" class="latex" /> is the thermodynamic entropy. Since the entropy is a scalar function, the above relation guarantees the covariance of the metric tensor. Moreover, the thermodynamic entropy <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BS%7D%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{S}(x)" class="latex" /> enters in mathematical expression of Einstein postulate of classical fluctuation theory, which should be extended as follows:<br />
<img src="https://s0.wp.com/latex.php?latex=d%5Cmu%28x%29%3D%5Cexp%5Cleft%28%5Cmathcal%7BS%7D%28x%29%5Cright%29d%5Cnu%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu(x)=&#92;exp&#92;left(&#92;mathcal{S}(x)&#92;right)d&#92;nu(x)" class="latex" /><br />
to guarantee the scalar character of the entropy. The gaussian distribution that I commented above is an exact improvement of gaussian approximation of classical fluctuation theory of statistical mechanics.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Luisberis Velazquez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26997</link>

		<dc:creator><![CDATA[Luisberis Velazquez]]></dc:creator>
		<pubDate>Fri, 22 Mar 2013 20:31:45 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-26997</guid>

					<description><![CDATA[Levi-Civita affine connections or the metric connections only depend on the metric tensor:
$latex \Gamma _{ij}^{k}\left( x&#124;\theta \right) =g^{km}(x&#124;\theta)\frac{1}{2}\left[\frac{\partial g_{im}(x&#124;\theta)}{\partial x^{j}}+\frac{\partial g_{jm}(x&#124;\theta)}{\partial x^{i}}-\frac{\partial g_{ij}(x&#124;\theta)}{\partial x^{m}}\right]$,
and they are the only affine connections with a tonsion-less covariant derivative $latex D_{k}$ that obeys the condition of Levi-Civita parallelism:
$latex D_{k}g_{ij}(x)=0$.
The relation between the metric tensor and the probability density represents a set of covariant partial equations of second-order in terms of the metric tensor. This equation is non-linear and self-consistent. As expected, it is difficult to solve in most of practical situations. However, this ansatz allows to introduce a measure $latex d\nu(x)$ for the relative entropy only considering the probability density of interest. 

Surprinsingly, the value of this relative entropy can be expressed in an exactly way as follows:
$latex S(p&#124;q)=\log Z-n/2$,
where $latex n$ is the dimension of the manifold $latex \mathcal{M}$ and $latex Z$ a certain normalization constant. Precisely, a direct consequence of this set of covariant differential equations is the possibility to rewrite the original distribution function:
$latex d\mu(x)=p(x)dx$
as follows:
$latex d\mu(x)=\frac{1}{Z}\exp\left[-\frac{1}{2}\ell^{2}(x,\bar{x})\right]d\nu(x)$.
Here, $latex d\nu(x)$ is the invariant measure:
$latex d\nu(x)=\sqrt{\left&#124; g_{ij}(x)/2\pi
\right&#124;}dx$;
$latex \bar{x}$ denotes the most likely point of the distribution, while $latex \ell^{2}(x,\bar{x})$ denotes the arc-length of geodesics that connects the points $latex x$ and $latex \bar{x}$. Formally, this is a Gaussian distribution defined on a Riemannian manifold $latex \mathcal{M}$. The normalization constant $latex Z$ is reduced to the unity if the manifold $latex \mathcal{M}$ is diffeomorphic to the n-dimensional Euclidean real space $latex \mathbb{R}^{n}$, and it takes different values when the manifold exhibits a curved geometry. Consequently, this type of relative entropy is a global measure of the curvature of the manifold $latex \mathcal{M}$ where the random variables $latex x$ are defined. To my present understanding, curvature accounts for the existence of irreducible statistical correlations, something analogous to the irreducible character of gravity in different references frames due to its connection with curvature.]]></description>
			<content:encoded><![CDATA[<p>Levi-Civita affine connections or the metric connections only depend on the metric tensor:<br />
<img src="https://s0.wp.com/latex.php?latex=%5CGamma+_%7Bij%7D%5E%7Bk%7D%5Cleft%28+x%7C%5Ctheta+%5Cright%29+%3Dg%5E%7Bkm%7D%28x%7C%5Ctheta%29%5Cfrac%7B1%7D%7B2%7D%5Cleft%5B%5Cfrac%7B%5Cpartial+g_%7Bim%7D%28x%7C%5Ctheta%29%7D%7B%5Cpartial+x%5E%7Bj%7D%7D%2B%5Cfrac%7B%5Cpartial+g_%7Bjm%7D%28x%7C%5Ctheta%29%7D%7B%5Cpartial+x%5E%7Bi%7D%7D-%5Cfrac%7B%5Cpartial+g_%7Bij%7D%28x%7C%5Ctheta%29%7D%7B%5Cpartial+x%5E%7Bm%7D%7D%5Cright%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Gamma _{ij}^{k}&#92;left( x|&#92;theta &#92;right) =g^{km}(x|&#92;theta)&#92;frac{1}{2}&#92;left[&#92;frac{&#92;partial g_{im}(x|&#92;theta)}{&#92;partial x^{j}}+&#92;frac{&#92;partial g_{jm}(x|&#92;theta)}{&#92;partial x^{i}}-&#92;frac{&#92;partial g_{ij}(x|&#92;theta)}{&#92;partial x^{m}}&#92;right]" class="latex" />,<br />
and they are the only affine connections with a tonsion-less covariant derivative <img src="https://s0.wp.com/latex.php?latex=D_%7Bk%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_{k}" class="latex" /> that obeys the condition of Levi-Civita parallelism:<br />
<img src="https://s0.wp.com/latex.php?latex=D_%7Bk%7Dg_%7Bij%7D%28x%29%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_{k}g_{ij}(x)=0" class="latex" />.<br />
The relation between the metric tensor and the probability density represents a set of covariant partial equations of second-order in terms of the metric tensor. This equation is non-linear and self-consistent. As expected, it is difficult to solve in most of practical situations. However, this ansatz allows to introduce a measure <img src="https://s0.wp.com/latex.php?latex=d%5Cnu%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu(x)" class="latex" /> for the relative entropy only considering the probability density of interest. </p>
<p>Surprinsingly, the value of this relative entropy can be expressed in an exactly way as follows:<br />
<img src="https://s0.wp.com/latex.php?latex=S%28p%7Cq%29%3D%5Clog+Z-n%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p|q)=&#92;log Z-n/2" class="latex" />,<br />
where <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is the dimension of the manifold <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{M}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> a certain normalization constant. Precisely, a direct consequence of this set of covariant differential equations is the possibility to rewrite the original distribution function:<br />
<img src="https://s0.wp.com/latex.php?latex=d%5Cmu%28x%29%3Dp%28x%29dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu(x)=p(x)dx" class="latex" /><br />
as follows:<br />
<img src="https://s0.wp.com/latex.php?latex=d%5Cmu%28x%29%3D%5Cfrac%7B1%7D%7BZ%7D%5Cexp%5Cleft%5B-%5Cfrac%7B1%7D%7B2%7D%5Cell%5E%7B2%7D%28x%2C%5Cbar%7Bx%7D%29%5Cright%5Dd%5Cnu%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu(x)=&#92;frac{1}{Z}&#92;exp&#92;left[-&#92;frac{1}{2}&#92;ell^{2}(x,&#92;bar{x})&#92;right]d&#92;nu(x)" class="latex" />.<br />
Here, <img src="https://s0.wp.com/latex.php?latex=d%5Cnu%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu(x)" class="latex" /> is the invariant measure:<br />
<img src="https://s0.wp.com/latex.php?latex=d%5Cnu%28x%29%3D%5Csqrt%7B%5Cleft%7C+g_%7Bij%7D%28x%29%2F2%5Cpi+%5Cright%7C%7Ddx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu(x)=&#92;sqrt{&#92;left| g_{ij}(x)/2&#92;pi &#92;right|}dx" class="latex" />;<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;bar{x}" class="latex" /> denotes the most likely point of the distribution, while <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B2%7D%28x%2C%5Cbar%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell^{2}(x,&#92;bar{x})" class="latex" /> denotes the arc-length of geodesics that connects the points <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;bar{x}" class="latex" />. Formally, this is a Gaussian distribution defined on a Riemannian manifold <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{M}" class="latex" />. The normalization constant <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is reduced to the unity if the manifold <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{M}" class="latex" /> is diffeomorphic to the n-dimensional Euclidean real space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7Bn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{n}" class="latex" />, and it takes different values when the manifold exhibits a curved geometry. Consequently, this type of relative entropy is a global measure of the curvature of the manifold <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{M}" class="latex" /> where the random variables <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> are defined. To my present understanding, curvature accounts for the existence of irreducible statistical correlations, something analogous to the irreducible character of gravity in different references frames due to its connection with curvature.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26989</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 22 Mar 2013 17:53:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-26989</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26968&quot;&gt;Luisberis Velazquez&lt;/a&gt;.

Thanks!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26968">Luisberis Velazquez</a>.</p>
<p>Thanks!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26987</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 22 Mar 2013 17:46:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-26987</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26967&quot;&gt;Luisberis Velazquez&lt;/a&gt;.

Hi!  How are you constructing your Levi-Civita connection starting from $latex p?$  I know how to construct a Levi-Civita connection starting from a metric, but you&#039;ve defined your metric starting from a Levi-Civita connection.  The fact that you speak of &#039;the&#039; Levi-Civita connection makes me a little nervous, since there are many.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26967">Luisberis Velazquez</a>.</p>
<p>Hi!  How are you constructing your Levi-Civita connection starting from <img src="https://s0.wp.com/latex.php?latex=p%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p?" class="latex" />  I know how to construct a Levi-Civita connection starting from a metric, but you&#8217;ve defined your metric starting from a Levi-Civita connection.  The fact that you speak of &#8216;the&#8217; Levi-Civita connection makes me a little nervous, since there are many.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Luisberis Velazquez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26968</link>

		<dc:creator><![CDATA[Luisberis Velazquez]]></dc:creator>
		<pubDate>Fri, 22 Mar 2013 03:18:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-26968</guid>

					<description><![CDATA[By the way, this is a great blog!]]></description>
			<content:encoded><![CDATA[<p>By the way, this is a great blog!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Luisberis Velazquez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-26967</link>

		<dc:creator><![CDATA[Luisberis Velazquez]]></dc:creator>
		<pubDate>Fri, 22 Mar 2013 03:14:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-26967</guid>

					<description><![CDATA[To my knowledge, the notion of relative entropy:

$latex S(p,q)= -\int \left[d\mu(x)/d\nu(x)\right]\log\left[d\mu(x)/d\nu(x)\right]d\nu(x)$

was early proposed by Jaynes to extend the notion of information entropy to the framework of continuous distributions. However, I think that the relative entropy is not a fully satisfactory generalization of this concept. A natural question here is how to introduce the measure:

$latex d\nu(x)=q(x)dx$

when no other information is available, except the continuous distribution:

$latex d\mu(x)=p(x)dx.$

Recently, I proposed a way to overcome this difficulty in the framework of Riemannian geometry of fluctuation theory:

http://iopscience.iop.org/1751-8121/45/17/175002/article

This geometry approach introduces a distance notion:

$latex ds^{2}=g_{ij}(x)dx^{i}dx^{j}$

between two infinitely close points $latex x$ and $latex x+dx$, where the metric tensor is obtained from the probability density $latex p(x)$ as follows:

$latex g_{ij}=-\frac{\partial^{2}\log p}{\partial x^{i}\partial x^{j}}+
\Gamma^{k}_{ij}\frac{\partial\log p}{\partial x^{k}} +\frac{\partial\Gamma^{k}_{jk}}{\partial x^{i}}-\Gamma^{k}_{ij}\Gamma^{l}_{kl}$.

Here, the symbol $latex \Gamma^{k}_{ij}$ denote the Levi-Civita affine connections. Formally, this is a set of covariant partial differential equations of second order in terms of the metric tensor. The measure $latex d\nu(x)$ can be defined as follows:

$latex d\nu(x)=\sqrt{\left&#124;g_{ij}(x)/2\pi \right&#124; }dx.$

Apparently, the relative entropy that follows from this ansatz is a global measure of the curvature of the manifold $latex \mathcal{M}$ where the continuous variables $latex x$ are defined. Some preliminary analysis suggest that curvature $latex R(x)$ is closely related to the existence of irreducible statistical correlations among the random variables $latex x$, that is, statistical correlations that survive any coordinate transformations $latex y=\phi(x)$.]]></description>
			<content:encoded><![CDATA[<p>To my knowledge, the notion of relative entropy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29%3D+-%5Cint+%5Cleft%5Bd%5Cmu%28x%29%2Fd%5Cnu%28x%29%5Cright%5D%5Clog%5Cleft%5Bd%5Cmu%28x%29%2Fd%5Cnu%28x%29%5Cright%5Dd%5Cnu%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)= -&#92;int &#92;left[d&#92;mu(x)/d&#92;nu(x)&#92;right]&#92;log&#92;left[d&#92;mu(x)/d&#92;nu(x)&#92;right]d&#92;nu(x)" class="latex" /></p>
<p>was early proposed by Jaynes to extend the notion of information entropy to the framework of continuous distributions. However, I think that the relative entropy is not a fully satisfactory generalization of this concept. A natural question here is how to introduce the measure:</p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cnu%28x%29%3Dq%28x%29dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu(x)=q(x)dx" class="latex" /></p>
<p>when no other information is available, except the continuous distribution:</p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cmu%28x%29%3Dp%28x%29dx.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu(x)=p(x)dx." class="latex" /></p>
<p>Recently, I proposed a way to overcome this difficulty in the framework of Riemannian geometry of fluctuation theory:</p>
<p><a href="http://iopscience.iop.org/1751-8121/45/17/175002/article" rel="nofollow ugc">http://iopscience.iop.org/1751-8121/45/17/175002/article</a></p>
<p>This geometry approach introduces a distance notion:</p>
<p><img src="https://s0.wp.com/latex.php?latex=ds%5E%7B2%7D%3Dg_%7Bij%7D%28x%29dx%5E%7Bi%7Ddx%5E%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ds^{2}=g_{ij}(x)dx^{i}dx^{j}" class="latex" /></p>
<p>between two infinitely close points <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=x%2Bdx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x+dx" class="latex" />, where the metric tensor is obtained from the probability density <img src="https://s0.wp.com/latex.php?latex=p%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x)" class="latex" /> as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D%3D-%5Cfrac%7B%5Cpartial%5E%7B2%7D%5Clog+p%7D%7B%5Cpartial+x%5E%7Bi%7D%5Cpartial+x%5E%7Bj%7D%7D%2B+%5CGamma%5E%7Bk%7D_%7Bij%7D%5Cfrac%7B%5Cpartial%5Clog+p%7D%7B%5Cpartial+x%5E%7Bk%7D%7D+%2B%5Cfrac%7B%5Cpartial%5CGamma%5E%7Bk%7D_%7Bjk%7D%7D%7B%5Cpartial+x%5E%7Bi%7D%7D-%5CGamma%5E%7Bk%7D_%7Bij%7D%5CGamma%5E%7Bl%7D_%7Bkl%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}=-&#92;frac{&#92;partial^{2}&#92;log p}{&#92;partial x^{i}&#92;partial x^{j}}+ &#92;Gamma^{k}_{ij}&#92;frac{&#92;partial&#92;log p}{&#92;partial x^{k}} +&#92;frac{&#92;partial&#92;Gamma^{k}_{jk}}{&#92;partial x^{i}}-&#92;Gamma^{k}_{ij}&#92;Gamma^{l}_{kl}" class="latex" />.</p>
<p>Here, the symbol <img src="https://s0.wp.com/latex.php?latex=%5CGamma%5E%7Bk%7D_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Gamma^{k}_{ij}" class="latex" /> denote the Levi-Civita affine connections. Formally, this is a set of covariant partial differential equations of second order in terms of the metric tensor. The measure <img src="https://s0.wp.com/latex.php?latex=d%5Cnu%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu(x)" class="latex" /> can be defined as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cnu%28x%29%3D%5Csqrt%7B%5Cleft%7Cg_%7Bij%7D%28x%29%2F2%5Cpi+%5Cright%7C+%7Ddx.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu(x)=&#92;sqrt{&#92;left|g_{ij}(x)/2&#92;pi &#92;right| }dx." class="latex" /></p>
<p>Apparently, the relative entropy that follows from this ansatz is a global measure of the curvature of the manifold <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{M}" class="latex" /> where the continuous variables <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> are defined. Some preliminary analysis suggest that curvature <img src="https://s0.wp.com/latex.php?latex=R%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(x)" class="latex" /> is closely related to the existence of irreducible statistical correlations among the random variables <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, that is, statistical correlations that survive any coordinate transformations <img src="https://s0.wp.com/latex.php?latex=y%3D%5Cphi%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y=&#92;phi(x)" class="latex" />.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 11) « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-15763</link>

		<dc:creator><![CDATA[Information Geometry (Part 11) « Azimuth]]></dc:creator>
		<pubDate>Thu, 07 Jun 2012 10:00:05 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-15763</guid>

					<description><![CDATA[For more on relative entropy, read Part 6 of this series [...]]]></description>
			<content:encoded><![CDATA[<p>For more on relative entropy, read Part 6 of this series [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 8) « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-5950</link>

		<dc:creator><![CDATA[Information Geometry (Part 8) « Azimuth]]></dc:creator>
		<pubDate>Thu, 26 May 2011 10:09:21 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-5950</guid>

					<description><![CDATA[Remember what I told you about relative entropy [...]]]></description>
			<content:encoded><![CDATA[<p>Remember what I told you about relative entropy [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 7) « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-4252</link>

		<dc:creator><![CDATA[Information Geometry (Part 7) « Azimuth]]></dc:creator>
		<pubDate>Wed, 02 Mar 2011 02:40:41 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-4252</guid>

					<description><![CDATA[Now I want to describe how the Fisher information metric is related to relative entropy [...]]]></description>
			<content:encoded><![CDATA[<p>Now I want to describe how the Fisher information metric is related to relative entropy [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-4102</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 13 Feb 2011 09:34:39 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2298#comment-4102</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-4085&quot;&gt;David Corfield&lt;/a&gt;.

David wrote:

&lt;blockquote&gt;

Still it seems odd where you write

&lt;blockquote&gt;

The information gain as we go from $latex p$ to $latex q$ is ...

&lt;/blockquote&gt;

I think I&#039;d prefer it the other way around.

&lt;/blockquote&gt;

Oh, definitely!  That was just a typo &#8212; I&#039;ve fixed it now, thanks.  I should mind my $latex p$&#039;s and $latex q$&#039;s.

&lt;blockquote&gt;

There&#039;s an argument that relative entropy is the &lt;a href=&quot;http://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html#c019383&quot; rel=&quot;nofollow&quot;&gt;square of a distance&lt;/a&gt;, agreed a few comments &lt;a href=&quot;http://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html#c019999&quot; rel=&quot;nofollow&quot;&gt;below&lt;/a&gt;.

&lt;/blockquote&gt;

Only after you symmetrize it, of course.  The relative entropy is not symmetric:

$latex S(p,q) \ne S(p,q)$

so it can&#039;t give a metric (of the traditional symmetric sort) when you take its square root.  But Suresh Venkat is claiming that the symmetrized gadget, which he calls the &#039;Jensen-Shannon distance&#039;:

$latex \frac{1}{2} S(p,q) + \frac{1}{2} S(q,p) $

&lt;i&gt;does&lt;/i&gt; give a metric when you take its square root.  I&#039;ll have to check this, or read about it somewhere if I give up.  You just need to check the triangle inequality... the rest is obvious.

But as your n-Caf&#233; comment notes, there&#039;s also another nice metric floating around.  If we define a version of relative entropy based on &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/&quot; rel=&quot;nofollow&quot;&gt;R&#233;nyi entropy&lt;/a&gt; instead of the usual Shannon kind, we get something called the &lt;a href=&quot;http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#R.C3.A9nyi_divergence&quot; rel=&quot;nofollow&quot;&gt;&#039;R&#233;nyi divergence&#039;&lt;/a&gt; which is symmetric &lt;i&gt;only for the special case $latex \alpha = 1/2$&lt;/i&gt;... and while it still doesn&#039;t obey the triangle inequality in that case, it&#039;s a function of something that does.  Not-so-coincidentally, I was just reading about this today.  This thesis has a nice chapter on R&#233;nyi entropy and the corresponding version of relative entropy:

&#8226; T. A. L. van Even, &lt;a href=&quot;https://openaccess.leidenuniv.nl/dspace/handle/1887/15879&quot; rel=&quot;nofollow&quot;&gt;&lt;i&gt;When Data Compression and Statistics Disagree: Two Frequentist Challenges For the Minimum Description Length Principle&lt;/i&gt;&lt;/a&gt;, Chap. 6: R&#233;nyi Divergence, Ph.D. thesis, Leiden University, 2010.  

Yeah, it&#039;s on page 179:

&lt;blockquote&gt;
Only for  $latex \alpha = 1/2$ is R&#233;nyi divergence symmetric in its arguments. Although not itself a metric, it is a function of the square of the Hellinger distance 

$latex \mathrm{Hel}^2(p,q) = \sum_{i = 1}^n  (\sqrt{p_i} - \sqrt{q_i})^2 $

[Gibbs and Su, 2002].
&lt;/blockquote&gt;

Anyway, my goal in this post was pretty limited.  I just wanted to explain the concept of relative entropy a little bit, so people can following what I&#039;m saying when I explain how it&#039;s related to the Fisher information metric.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comment-4085">David Corfield</a>.</p>
<p>David wrote:</p>
<blockquote>
<p>Still it seems odd where you write</p>
<blockquote>
<p>The information gain as we go from <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is &#8230;</p>
</blockquote>
<p>I think I&#8217;d prefer it the other way around.</p>
</blockquote>
<p>Oh, definitely!  That was just a typo &mdash; I&#8217;ve fixed it now, thanks.  I should mind my <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />&#8216;s and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />&#8216;s.</p>
<blockquote>
<p>There&#8217;s an argument that relative entropy is the <a href="http://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html#c019383" rel="nofollow">square of a distance</a>, agreed a few comments <a href="http://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html#c019999" rel="nofollow">below</a>.</p>
</blockquote>
<p>Only after you symmetrize it, of course.  The relative entropy is not symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%5Cne+S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) &#92;ne S(p,q)" class="latex" /></p>
<p>so it can&#8217;t give a metric (of the traditional symmetric sort) when you take its square root.  But Suresh Venkat is claiming that the symmetrized gadget, which he calls the &#8216;Jensen-Shannon distance&#8217;:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+S%28p%2Cq%29+%2B+%5Cfrac%7B1%7D%7B2%7D+S%28q%2Cp%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2} S(p,q) + &#92;frac{1}{2} S(q,p) " class="latex" /></p>
<p><i>does</i> give a metric when you take its square root.  I&#8217;ll have to check this, or read about it somewhere if I give up.  You just need to check the triangle inequality&#8230; the rest is obvious.</p>
<p>But as your n-Caf&eacute; comment notes, there&#8217;s also another nice metric floating around.  If we define a version of relative entropy based on <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" rel="nofollow">R&eacute;nyi entropy</a> instead of the usual Shannon kind, we get something called the <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#R.C3.A9nyi_divergence" rel="nofollow">&#8216;R&eacute;nyi divergence&#8217;</a> which is symmetric <i>only for the special case <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = 1/2" class="latex" /></i>&#8230; and while it still doesn&#8217;t obey the triangle inequality in that case, it&#8217;s a function of something that does.  Not-so-coincidentally, I was just reading about this today.  This thesis has a nice chapter on R&eacute;nyi entropy and the corresponding version of relative entropy:</p>
<p>&bull; T. A. L. van Even, <a href="https://openaccess.leidenuniv.nl/dspace/handle/1887/15879" rel="nofollow"><i>When Data Compression and Statistics Disagree: Two Frequentist Challenges For the Minimum Description Length Principle</i></a>, Chap. 6: R&eacute;nyi Divergence, Ph.D. thesis, Leiden University, 2010.  </p>
<p>Yeah, it&#8217;s on page 179:</p>
<blockquote><p>
Only for  <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = 1/2" class="latex" /> is R&eacute;nyi divergence symmetric in its arguments. Although not itself a metric, it is a function of the square of the Hellinger distance </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BHel%7D%5E2%28p%2Cq%29+%3D+%5Csum_%7Bi+%3D+1%7D%5En++%28%5Csqrt%7Bp_i%7D+-+%5Csqrt%7Bq_i%7D%29%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Hel}^2(p,q) = &#92;sum_{i = 1}^n  (&#92;sqrt{p_i} - &#92;sqrt{q_i})^2 " class="latex" /></p>
<p>[Gibbs and Su, 2002].
</p></blockquote>
<p>Anyway, my goal in this post was pretty limited.  I just wanted to explain the concept of relative entropy a little bit, so people can following what I&#8217;m saying when I explain how it&#8217;s related to the Fisher information metric.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
