<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Network Theory (Part 4)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/</link>
	<description></description>
	<lastBuildDate>Tue, 24 May 2011 02:58:14 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5930</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 24 May 2011 02:58:14 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5930</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5283&quot;&gt;John Baez&lt;/a&gt;.

Hi, Giampiero!  I&#039;m glad you get it and sort of like it.  I&#039;m going to resume posts in this series soon: I&#039;ll start by reviewing how to get the rate equation and master equation from a stochastic Petri net, and how the latter reduces to the former in the limit of &#039;large particle number&#039; (as the physicists would say).  

The reason people work in &#039;label space&#039; instead of &#039;state space&#039;, if I understand what you mean by those terms, is that many different probability distributions $\psi_\ell(t)$ have the same mean values for the number of things of the $\ell$th type, and their time evolution will be different.  Only in a certain limit where there are many things of every type can we describe dynamics in terms of these mean values.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5283">John Baez</a>.</p>
<p>Hi, Giampiero!  I&#8217;m glad you get it and sort of like it.  I&#8217;m going to resume posts in this series soon: I&#8217;ll start by reviewing how to get the rate equation and master equation from a stochastic Petri net, and how the latter reduces to the former in the limit of &#8216;large particle number&#8217; (as the physicists would say).  </p>
<p>The reason people work in &#8216;label space&#8217; instead of &#8216;state space&#8217;, if I understand what you mean by those terms, is that many different probability distributions $\psi_\ell(t)$ have the same mean values for the number of things of the $\ell$th type, and their time evolution will be different.  Only in a certain limit where there are many things of every type can we describe dynamics in terms of these mean values.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Giampiero Campa		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5929</link>

		<dc:creator><![CDATA[Giampiero Campa]]></dc:creator>
		<pubDate>Tue, 24 May 2011 00:59:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5929</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5283&quot;&gt;John Baez&lt;/a&gt;.

OK, i finally had some time to go through this yesterday, and now everything makes sense.

The last formula with the Kronecker&#039;s delta is really what turned all the the lights on for me. I guess i was not really allowing &lt;i&gt;l&#039;&lt;/i&gt; to be an &quot;independent&quot; variable in &lt;i&gt;H&lt;/i&gt;, in some sense.

Maybe a part of it is that it does not come natural for me to reason in the &quot;labeling space&quot;, as opposite to the &quot;state space&quot;, of the system (actually i am not yet too sure about what are the advantages of doing so). I guess it&#039;s because I haven&#039;t studied quantum mechanics long enough :)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5283">John Baez</a>.</p>
<p>OK, i finally had some time to go through this yesterday, and now everything makes sense.</p>
<p>The last formula with the Kronecker&#8217;s delta is really what turned all the the lights on for me. I guess i was not really allowing <i>l&#8217;</i> to be an &#8220;independent&#8221; variable in <i>H</i>, in some sense.</p>
<p>Maybe a part of it is that it does not come natural for me to reason in the &#8220;labeling space&#8221;, as opposite to the &#8220;state space&#8221;, of the system (actually i am not yet too sure about what are the advantages of doing so). I guess it&#8217;s because I haven&#8217;t studied quantum mechanics long enough :)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Damian Allis		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5826</link>

		<dc:creator><![CDATA[Damian Allis]]></dc:creator>
		<pubDate>Sun, 15 May 2011 20:24:47 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5826</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5153&quot;&gt;John Baez&lt;/a&gt;.

Greetings John,

Bruce Smith sent me to try to say something constructive (and a while back, I started thinking + searching for an answer, realized how long it was going to be, then forgot to follow-up).  

I am pleased to report that (1) you’d be hard pressed to find a chemist with a convincing answer to all other chemists, (2) this is more atomic physics than chemistry, which is why you’d easily stump a chemist on this, (3) I have found a few very eloquent and involved discussions that say “it’s spherically symmetric” and “it’s not spherically symmetric,” (which means the answer is either “neither” or a superposition of the two) and (4) it probably doesn’t matter in the grand scheme because the response of the atom to any kind of external stimuli (such as an H atom) would be effectively instantaneous.

Just going by a combination of Hund’s Rule(s) (max multiplicity = lowest E; largest orbital angular momentum = lowest energy for a given multiplicity; outermost subshell half-filled or less = lowest total angular momentum is lowest in E / outermost subshell more than half-filled = highest total angular momentum is lowest in E), the Aufbau Principle (add e- into a p-shell one-at-a-time, so in nitrogen each p-orbital has 1 e- each), and Unsold’s Theorem (the square of the electronic wavefunction for a completely filled or half-filled sub-shell is spherically symmetric), oxygen is NOT spherically symmetric (well, not spherically symmetric if you were standing on the nucleus).  These are the most fundamental “non-computational-quantum-chemical” ways to interpret SINGLE atoms and are all from good olde fashioned spectroscopy back in the day.  I note that these are SINGLE atom cases, where the larger the vacuum the better the result.  Importantly, these rules break down (or don’t apply) when the atom exists in an external potential (such as an H atom, H nucleus, whatever).

My initial thought (as a computational chemist) was that the &quot;I’m scared that in reality, the oxygen atom’s ground state is a superposition of all possible rotated versions of some charismatic easy-to-draw non-spherically symmetric state&quot; was a bit more formally correct as an &lt;i&gt;interpretation&lt;/i&gt; of the system.  That is, all isolated atoms are spherically symmetric, with the electrons existing in a superposition of all linear combinations of the available orbitals, making the &quot;density&quot; of the atom the &quot;ensemble density&quot; of the linear combinations.  More to the point, the potential around an isolated atom would be spherically symmetric (if it is truly isolated), so the electrons have no bias to group in any arrangement one might describe as px, py, pz, whatever.  One obtains a differentiation of &quot;orbital arrangements&quot; in an external potential (introducing the laboratory reference frame), and it is the balance of internal electronic repulsion in the presence of this potential and the stabilization of electronic arrangements that come from the presence of the external potential (two H atoms around an oxygen, for instance) that gives one a reference frame for any separation of electron density into &quot;directions&quot; (along bonds, external field, etc.). 

All that said, orbitals are a pleasant fiction we use to describe the behavior of electrons in our modern one-electron wavefunction treatment in quantum chemistry.  The isolated atom is one of the hardest systems to treat with great accuracy by quantum chemical methods because one must use multireference computational approaches to deal with the &quot;spherical degeneracy&quot; of the electronic states.  I&#039;ll take a methane calculation over isolated carbon any day.

Extra credit – once you’ve formed OH, the electron density of the O will attract the second H, which will then adopt the standard H2O geometry discussed above.  The 104.45 angle is best explained as “the two lone pairs on the “other side” of the oxygen take up more space because you’ve got four repulsing electrons, so they take up more space, giving the H-O-H bend reduction to take up the slack.

I hope that it any way helps.  If not, I’ll be happy to try to attempt a better explanation.

I&#039;m enjoying reading the rest...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5153">John Baez</a>.</p>
<p>Greetings John,</p>
<p>Bruce Smith sent me to try to say something constructive (and a while back, I started thinking + searching for an answer, realized how long it was going to be, then forgot to follow-up).  </p>
<p>I am pleased to report that (1) you’d be hard pressed to find a chemist with a convincing answer to all other chemists, (2) this is more atomic physics than chemistry, which is why you’d easily stump a chemist on this, (3) I have found a few very eloquent and involved discussions that say “it’s spherically symmetric” and “it’s not spherically symmetric,” (which means the answer is either “neither” or a superposition of the two) and (4) it probably doesn’t matter in the grand scheme because the response of the atom to any kind of external stimuli (such as an H atom) would be effectively instantaneous.</p>
<p>Just going by a combination of Hund’s Rule(s) (max multiplicity = lowest E; largest orbital angular momentum = lowest energy for a given multiplicity; outermost subshell half-filled or less = lowest total angular momentum is lowest in E / outermost subshell more than half-filled = highest total angular momentum is lowest in E), the Aufbau Principle (add e- into a p-shell one-at-a-time, so in nitrogen each p-orbital has 1 e- each), and Unsold’s Theorem (the square of the electronic wavefunction for a completely filled or half-filled sub-shell is spherically symmetric), oxygen is NOT spherically symmetric (well, not spherically symmetric if you were standing on the nucleus).  These are the most fundamental “non-computational-quantum-chemical” ways to interpret SINGLE atoms and are all from good olde fashioned spectroscopy back in the day.  I note that these are SINGLE atom cases, where the larger the vacuum the better the result.  Importantly, these rules break down (or don’t apply) when the atom exists in an external potential (such as an H atom, H nucleus, whatever).</p>
<p>My initial thought (as a computational chemist) was that the &#8220;I’m scared that in reality, the oxygen atom’s ground state is a superposition of all possible rotated versions of some charismatic easy-to-draw non-spherically symmetric state&#8221; was a bit more formally correct as an <i>interpretation</i> of the system.  That is, all isolated atoms are spherically symmetric, with the electrons existing in a superposition of all linear combinations of the available orbitals, making the &#8220;density&#8221; of the atom the &#8220;ensemble density&#8221; of the linear combinations.  More to the point, the potential around an isolated atom would be spherically symmetric (if it is truly isolated), so the electrons have no bias to group in any arrangement one might describe as px, py, pz, whatever.  One obtains a differentiation of &#8220;orbital arrangements&#8221; in an external potential (introducing the laboratory reference frame), and it is the balance of internal electronic repulsion in the presence of this potential and the stabilization of electronic arrangements that come from the presence of the external potential (two H atoms around an oxygen, for instance) that gives one a reference frame for any separation of electron density into &#8220;directions&#8221; (along bonds, external field, etc.). </p>
<p>All that said, orbitals are a pleasant fiction we use to describe the behavior of electrons in our modern one-electron wavefunction treatment in quantum chemistry.  The isolated atom is one of the hardest systems to treat with great accuracy by quantum chemical methods because one must use multireference computational approaches to deal with the &#8220;spherical degeneracy&#8221; of the electronic states.  I&#8217;ll take a methane calculation over isolated carbon any day.</p>
<p>Extra credit – once you’ve formed OH, the electron density of the O will attract the second H, which will then adopt the standard H2O geometry discussed above.  The 104.45 angle is best explained as “the two lone pairs on the “other side” of the oxygen take up more space because you’ve got four repulsing electrons, so they take up more space, giving the H-O-H bend reduction to take up the slack.</p>
<p>I hope that it any way helps.  If not, I’ll be happy to try to attempt a better explanation.</p>
<p>I&#8217;m enjoying reading the rest&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Damian Allis		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5833</link>

		<dc:creator><![CDATA[Damian Allis]]></dc:creator>
		<pubDate>Sun, 15 May 2011 02:32:12 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5833</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5826&quot;&gt;Damian Allis&lt;/a&gt;.

Well, I think I&#039;d go with the &quot;simultaneous states of non-spherically symmetric&quot; interpretation because there&#039;s no way to define axes without imposing on the atom.  The olde guard atomic physics would argue non-spherically symmetric because you&#039;ve got two unpaired electrons in two p orbitals, but you&#039;d never be able to &quot;dock&quot; along an unfilled orbital in any kind of chemical trajectory (which was how I interpreted the basis of your question in my first pass of your query).

My tweet-friendly would be &quot;non-spherically symmetric locally, spherical enough at a distance, no way to sneak up and ask at any instant.&quot;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5826">Damian Allis</a>.</p>
<p>Well, I think I&#8217;d go with the &#8220;simultaneous states of non-spherically symmetric&#8221; interpretation because there&#8217;s no way to define axes without imposing on the atom.  The olde guard atomic physics would argue non-spherically symmetric because you&#8217;ve got two unpaired electrons in two p orbitals, but you&#8217;d never be able to &#8220;dock&#8221; along an unfilled orbital in any kind of chemical trajectory (which was how I interpreted the basis of your question in my first pass of your query).</p>
<p>My tweet-friendly would be &#8220;non-spherically symmetric locally, spherical enough at a distance, no way to sneak up and ask at any instant.&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5832</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 15 May 2011 02:11:20 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5832</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5826&quot;&gt;Damian Allis&lt;/a&gt;.

Hi, Damian!  Thanks for trying to help!  I&#039;m still a bit confused, but misery loves company, so I feel much better hearing you say &quot;I’ll take a methane calculation over isolated carbon any day&quot; and scary stuff like:

&lt;blockquote&gt;
The isolated atom is one of the hardest systems to treat with great accuracy by quantum chemical methods because one must use multireference computational approaches to deal with the “spherical degeneracy” of the electronic states. 
&lt;/blockquote&gt;

It makes me feel like I wasn&#039;t just being really dumb.  

To compress your long answer into a soundbite, I guess you&#039;re saying that a completely isolated unexcited oxygen atom &lt;i&gt;is&lt;/i&gt; spherically symmetric.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5826">Damian Allis</a>.</p>
<p>Hi, Damian!  Thanks for trying to help!  I&#8217;m still a bit confused, but misery loves company, so I feel much better hearing you say &#8220;I’ll take a methane calculation over isolated carbon any day&#8221; and scary stuff like:</p>
<blockquote><p>
The isolated atom is one of the hardest systems to treat with great accuracy by quantum chemical methods because one must use multireference computational approaches to deal with the “spherical degeneracy” of the electronic states.
</p></blockquote>
<p>It makes me feel like I wasn&#8217;t just being really dumb.  </p>
<p>To compress your long answer into a soundbite, I guess you&#8217;re saying that a completely isolated unexcited oxygen atom <i>is</i> spherically symmetric.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5290</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 14 Apr 2011 09:16:30 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5290</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5289&quot;&gt;David Corfield&lt;/a&gt;.

Thanks for all the corrections, David!  I&#039;ll fix that stuff.  As usual, a more beautiful version of this blog entry, with solutions to the puzzles, is available on my &lt;a href=&quot;http://math.ucr.edu/home/baez/networks/networks.html&quot; rel=&quot;nofollow&quot;&gt;networks page&lt;/a&gt;.

&lt;blockquote&gt;
So, with a collection of transitions in place in your Petri net you just add these $latex H$ matrices, and all is well since infinitesimal stochastic matrices are closed under addition.
&lt;/blockquote&gt;

Right!  They&#039;re closed under nonnegative linear combinations.  

Hmm.  If we only keep the condition that the columns sum to zero, and drop the condition that the off-diagonal entries are nonnegative, it seems we get not just a vector space of matrices, but even a Lie algebra $latex \mathfrak{g}$.   I&#039;m guessing this because if we take the definition of &#039;stochastic matrix&#039;, and drop the condition that the entries are nonnegative, keeping the condition that the columns sum to 1, we seem to get a Lie group $latex G$.  This is the Lie group of transformations of $latex \mathbb{R}^n$ that preserve the sum of the entries.   The stochastic matrices form a sub-monoid of $latex G$.  And thus the infinitesimal stochastic matrices form a cone in the Lie algebra $latex \mathfrak{g}$.
]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5289">David Corfield</a>.</p>
<p>Thanks for all the corrections, David!  I&#8217;ll fix that stuff.  As usual, a more beautiful version of this blog entry, with solutions to the puzzles, is available on my <a href="http://math.ucr.edu/home/baez/networks/networks.html" rel="nofollow">networks page</a>.</p>
<blockquote><p>
So, with a collection of transitions in place in your Petri net you just add these <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> matrices, and all is well since infinitesimal stochastic matrices are closed under addition.
</p></blockquote>
<p>Right!  They&#8217;re closed under nonnegative linear combinations.  </p>
<p>Hmm.  If we only keep the condition that the columns sum to zero, and drop the condition that the off-diagonal entries are nonnegative, it seems we get not just a vector space of matrices, but even a Lie algebra <img src="https://s0.wp.com/latex.php?latex=%5Cmathfrak%7Bg%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathfrak{g}" class="latex" />.   I&#8217;m guessing this because if we take the definition of &#8216;stochastic matrix&#8217;, and drop the condition that the entries are nonnegative, keeping the condition that the columns sum to 1, we seem to get a Lie group <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" />.  This is the Lie group of transformations of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> that preserve the sum of the entries.   The stochastic matrices form a sub-monoid of <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" />.  And thus the infinitesimal stochastic matrices form a cone in the Lie algebra <img src="https://s0.wp.com/latex.php?latex=%5Cmathfrak%7Bg%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathfrak{g}" class="latex" />.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: David Corfield		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5289</link>

		<dc:creator><![CDATA[David Corfield]]></dc:creator>
		<pubDate>Thu, 14 Apr 2011 08:29:15 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5289</guid>

					<description><![CDATA[In &#039;Puzzle&#039;, the summation in the master equation should be over $latex \ell$ rather than $latex \ell&#039;$.

In the final equation, $latex H_{\ell&#039; \ell} = r \ell^{\underline{m}} \left(\delta_{\ell + n - m,\ell} - \delta_{\ell, \ell}\right)$, you ought to have a couple of instances of $latex \ell&#039;$ on the right.

So, with a collection of transitions in place in your Petri net you just add these $latex H$ matrices, and all is well since infinitesimal stochastic matrices are closed under addition.]]></description>
			<content:encoded><![CDATA[<p>In &#8216;Puzzle&#8217;, the summation in the master equation should be over <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> rather than <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" />.</p>
<p>In the final equation, <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D+%3D+r+%5Cell%5E%7B%5Cunderline%7Bm%7D%7D+%5Cleft%28%5Cdelta_%7B%5Cell+%2B+n+-+m%2C%5Cell%7D+-+%5Cdelta_%7B%5Cell%2C+%5Cell%7D%5Cright%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell} = r &#92;ell^{&#92;underline{m}} &#92;left(&#92;delta_{&#92;ell + n - m,&#92;ell} - &#92;delta_{&#92;ell, &#92;ell}&#92;right)" class="latex" />, you ought to have a couple of instances of <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" /> on the right.</p>
<p>So, with a collection of transitions in place in your Petri net you just add these <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> matrices, and all is well since infinitesimal stochastic matrices are closed under addition.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5283</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 14 Apr 2011 06:24:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5283</guid>

					<description><![CDATA[Okay, I&#039;ll answer the puzzle in this blog entry.  It will look a tiny bit prettier if we switch the roles of $latex \ell$ and $latex \ell&#039;$:

&lt;b&gt;Puzzle.&lt;/b&gt; Suppose we have a stochastic Petri net with $latex k$ states and just one transition, whose rate constant is $latex r$. Suppose the $latex i$th state appears $latex m_i$ times as the input of this transition and $latex n_i$ times as the output. A labelling of this stochastic Petri net is a $latex k$-tuple of natural numbers $latex \ell = (\ell_1, \dots, \ell_k)$ saying how many things are in each state. Let $latex \psi_\ell(t)$ be the probability that the labelling is $latex \ell$ at time $latex t$. Then the master equation looks like this:

$latex \frac{d}{d t} \psi_{\ell&#039;}(t) = \sum_{\ell} H_{\ell&#039; \ell} \psi_{\ell}(t)$

for some matrix of real numbers $latex H_{\ell&#039; \ell}$. What is this matrix?

&lt;b&gt;Answer.&lt;/b&gt;  To compute  $latex H_{\ell&#039; \ell}$ it&#039;s enough to start the Petri net in a definite labelling $latex \ell$ and see how fast the probability of being in some labelling $latex \ell&#039;$ changes.  In other words, if at some time $latex t$ we have

$latex \psi_{\ell}(t) = 1$

then

$latex \frac{d}{d t } \psi_{\ell&#039;}(t) = H_{\ell&#039; \ell}$

at this time.

Now, suppose we have a Petri net that is labelled in some way at some moment.  Then I said the probability that the transition occurs in a short time $latex \Delta t$ is approximately:

&#8226; the rate constant $latex r$, times

&#8226; the time $latex \Delta t$, times

&#8226; the number of ways the transition can occur, which is the product of falling powers $latex \ell_1^{\underline{m_1}} \cdots \ell_k^{\underline{m_k}}.$  Let&#039;s call this product $latex \ell^{\underline{m}}$ for short.

Multiplying these 3 things we get

$latex r \; \ell^{\underline{m}} \; \Delta t$

So, the &lt;i&gt;rate&lt;/i&gt; at which the transition occurs is just:

$latex r \ell^{\underline{m}}$

And when the transition occurs, it eats up $latex m_i$ things in the &lt;i&gt;i&lt;/i&gt;th state, and produces $latex n_i$ things in that state.  So, it carries our system from the original labelling $latex \ell$ to the new labelling 

$latex \ell&#039; = \ell  + n - m$

So, &lt;i&gt;in this case&lt;/i&gt; we have

$latex  \frac{d}{d t} \psi_{\ell&#039;}(t)  = r \ell^{\underline{m}}$

and thus

$latex H_{\ell&#039; \ell} = r \ell^{\underline{m}}$

However, that&#039;s not all: there&#039;s another case to consider!  Since the probability of the Petri net being in this new labelling $latex \ell&#039;$ is going up, the probability of it staying in the original labelling $latex \ell$ must be going down by the same amount.  So we must also have

$latex H_{\ell \ell} = - r \ell^{\underline{m}}$

We can combine both cases into one formula like this:

$latex H_{\ell&#039; \ell} = r \ell^{\underline{m}} \left(\delta_{\ell&#039;, \ell + n - m} - \delta_{\ell&#039;, \ell}\right)$

Here the first term tells us how fast the probability of being in the new labelling is going up.  The second term tells us how fast the probability of staying in the same labelling is going down.  

Note: each column in the matrix $latex H_{\ell&#039; \ell}$ sums to zero, and all the off-diagonal entries are nonnegative.  That&#039;s good: we &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/11/network-theory-part-5/&quot; rel=&quot;nofollow&quot;&gt;now know&lt;/a&gt; that this matrix must be &#039;infinitesimal stochastic&#039;, meaning it must have these properties!]]></description>
			<content:encoded><![CDATA[<p>Okay, I&#8217;ll answer the puzzle in this blog entry.  It will look a tiny bit prettier if we switch the roles of <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" />:</p>
<p><b>Puzzle.</b> Suppose we have a stochastic Petri net with <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> states and just one transition, whose rate constant is <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" />. Suppose the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state appears <img src="https://s0.wp.com/latex.php?latex=m_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m_i" class="latex" /> times as the input of this transition and <img src="https://s0.wp.com/latex.php?latex=n_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i" class="latex" /> times as the output. A labelling of this stochastic Petri net is a <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" />-tuple of natural numbers <img src="https://s0.wp.com/latex.php?latex=%5Cell+%3D+%28%5Cell_1%2C+%5Cdots%2C+%5Cell_k%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell = (&#92;ell_1, &#92;dots, &#92;ell_k)" class="latex" /> saying how many things are in each state. Let <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Cell%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_&#92;ell(t)" class="latex" /> be the probability that the labelling is <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />. Then the master equation looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29+%3D+%5Csum_%7B%5Cell%7D+H_%7B%5Cell%27+%5Cell%7D+%5Cpsi_%7B%5Cell%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{d t} &#92;psi_{&#92;ell&#039;}(t) = &#92;sum_{&#92;ell} H_{&#92;ell&#039; &#92;ell} &#92;psi_{&#92;ell}(t)" class="latex" /></p>
<p>for some matrix of real numbers <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" />. What is this matrix?</p>
<p><b>Answer.</b>  To compute  <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" /> it&#8217;s enough to start the Petri net in a definite labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> and see how fast the probability of being in some labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" /> changes.  In other words, if at some time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7B%5Cell%7D%28t%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_{&#92;ell}(t) = 1" class="latex" /></p>
<p>then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bd+t+%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29+%3D+H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{d t } &#92;psi_{&#92;ell&#039;}(t) = H_{&#92;ell&#039; &#92;ell}" class="latex" /></p>
<p>at this time.</p>
<p>Now, suppose we have a Petri net that is labelled in some way at some moment.  Then I said the probability that the transition occurs in a short time <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t" class="latex" /> is approximately:</p>
<p>&bull; the rate constant <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" />, times</p>
<p>&bull; the time <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t" class="latex" />, times</p>
<p>&bull; the number of ways the transition can occur, which is the product of falling powers <img src="https://s0.wp.com/latex.php?latex=%5Cell_1%5E%7B%5Cunderline%7Bm_1%7D%7D+%5Ccdots+%5Cell_k%5E%7B%5Cunderline%7Bm_k%7D%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell_1^{&#92;underline{m_1}} &#92;cdots &#92;ell_k^{&#92;underline{m_k}}." class="latex" />  Let&#8217;s call this product <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B%5Cunderline%7Bm%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell^{&#92;underline{m}}" class="latex" /> for short.</p>
<p>Multiplying these 3 things we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=r+%5C%3B+%5Cell%5E%7B%5Cunderline%7Bm%7D%7D+%5C%3B+%5CDelta+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r &#92;; &#92;ell^{&#92;underline{m}} &#92;; &#92;Delta t" class="latex" /></p>
<p>So, the <i>rate</i> at which the transition occurs is just:</p>
<p><img src="https://s0.wp.com/latex.php?latex=r+%5Cell%5E%7B%5Cunderline%7Bm%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r &#92;ell^{&#92;underline{m}}" class="latex" /></p>
<p>And when the transition occurs, it eats up <img src="https://s0.wp.com/latex.php?latex=m_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m_i" class="latex" /> things in the <i>i</i>th state, and produces <img src="https://s0.wp.com/latex.php?latex=n_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i" class="latex" /> things in that state.  So, it carries our system from the original labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> to the new labelling </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cell%27+%3D+%5Cell++%2B+n+-+m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039; = &#92;ell  + n - m" class="latex" /></p>
<p>So, <i>in this case</i> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29++%3D+r+%5Cell%5E%7B%5Cunderline%7Bm%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{d t} &#92;psi_{&#92;ell&#039;}(t)  = r &#92;ell^{&#92;underline{m}}" class="latex" /></p>
<p>and thus</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D+%3D+r+%5Cell%5E%7B%5Cunderline%7Bm%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell} = r &#92;ell^{&#92;underline{m}}" class="latex" /></p>
<p>However, that&#8217;s not all: there&#8217;s another case to consider!  Since the probability of the Petri net being in this new labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" /> is going up, the probability of it staying in the original labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> must be going down by the same amount.  So we must also have</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell+%5Cell%7D+%3D+-+r+%5Cell%5E%7B%5Cunderline%7Bm%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell &#92;ell} = - r &#92;ell^{&#92;underline{m}}" class="latex" /></p>
<p>We can combine both cases into one formula like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D+%3D+r+%5Cell%5E%7B%5Cunderline%7Bm%7D%7D+%5Cleft%28%5Cdelta_%7B%5Cell%27%2C+%5Cell+%2B+n+-+m%7D+-+%5Cdelta_%7B%5Cell%27%2C+%5Cell%7D%5Cright%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell} = r &#92;ell^{&#92;underline{m}} &#92;left(&#92;delta_{&#92;ell&#039;, &#92;ell + n - m} - &#92;delta_{&#92;ell&#039;, &#92;ell}&#92;right)" class="latex" /></p>
<p>Here the first term tells us how fast the probability of being in the new labelling is going up.  The second term tells us how fast the probability of staying in the same labelling is going down.  </p>
<p>Note: each column in the matrix <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" /> sums to zero, and all the off-diagonal entries are nonnegative.  That&#8217;s good: we <a href="https://johncarlosbaez.wordpress.com/2011/04/11/network-theory-part-5/" rel="nofollow">now know</a> that this matrix must be &#8216;infinitesimal stochastic&#8217;, meaning it must have these properties!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5228</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 11 Apr 2011 06:58:41 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5228</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5157&quot;&gt;John Baez&lt;/a&gt;.

Jon wrote:

&lt;blockquote&gt;
I’m no chemist, but aren’t the orbitals you drew calculated as the excited states of a single electron around an otherwise bare nucleus? Since they don’t consider any interactions between multiple electrons except the exclusion “force”, it feels like they just don’t have it in them to predict bond angles.
&lt;/blockquote&gt;

Right, they don&#039;t have the ability to predict bond angles.  I was only trying to settle the question of whether a lone oxygen atom is spherically symmetric or not.  And why?  It&#039;s because I told David Corfield this:

&lt;blockquote&gt;
If we’re talking about real-world chemistry, rather than ball-and-stick models of molecules, I don’t think a lone oxygen atom really has two distinct ‘slots’ waiting for hydrogens. It’s more like it has a propensity to attach to a hydrogen, which can come in from any direction and stick… and then the resulting hydroxyl radical OH still has a propensity to attach to another hydrogen, which will then move to form a 104.45° angle to the first.
&lt;/blockquote&gt;

As soon as I wrote this, I started doubting it.  I started wondering: is the oxygen atom really spherically symmetric before the hydrogen comes along, so that the hydrogen can attach itself anywhere with equal probability? Or does a hydrogen atom coming up to an oxygen atom attach to one of two predefined &#039;slots&#039; with more or less well-defined locations?

(Of course &#039;slots&#039; is a somewhat naive term for patterns in electron wavefunctions, but never mind.)

Until someone tells me the angular momentum of a lone oxygen atom, I won&#039;t know the answer for sure.  But I&#039;m now pretty sure the latter picture is closer to the truth.  I believe the 4 electrons in the outermost shell of the oxygen atom &lt;i&gt;break the spherical symmetry&lt;/i&gt; and in fact pick out two preferred axes, lying at a roughly 90&#176; angle to each other.  That&#039;s what those &#039;p orbital&#039; pictures are supposed to suggest.  But of course those pictures are only approximate, since (as you note) they neglect inter-electron forces.

Then, when two hydrogens actually attach to the oxygen, they wind up attaching at an angle of 104.45&#176;, for complicated reasons that take a computer simulation to fully fathom.  Presumably the repulsion of the hydrogens&#039; nuclei is largely to blame.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5157">John Baez</a>.</p>
<p>Jon wrote:</p>
<blockquote><p>
I’m no chemist, but aren’t the orbitals you drew calculated as the excited states of a single electron around an otherwise bare nucleus? Since they don’t consider any interactions between multiple electrons except the exclusion “force”, it feels like they just don’t have it in them to predict bond angles.
</p></blockquote>
<p>Right, they don&#8217;t have the ability to predict bond angles.  I was only trying to settle the question of whether a lone oxygen atom is spherically symmetric or not.  And why?  It&#8217;s because I told David Corfield this:</p>
<blockquote><p>
If we’re talking about real-world chemistry, rather than ball-and-stick models of molecules, I don’t think a lone oxygen atom really has two distinct ‘slots’ waiting for hydrogens. It’s more like it has a propensity to attach to a hydrogen, which can come in from any direction and stick… and then the resulting hydroxyl radical OH still has a propensity to attach to another hydrogen, which will then move to form a 104.45° angle to the first.
</p></blockquote>
<p>As soon as I wrote this, I started doubting it.  I started wondering: is the oxygen atom really spherically symmetric before the hydrogen comes along, so that the hydrogen can attach itself anywhere with equal probability? Or does a hydrogen atom coming up to an oxygen atom attach to one of two predefined &#8216;slots&#8217; with more or less well-defined locations?</p>
<p>(Of course &#8216;slots&#8217; is a somewhat naive term for patterns in electron wavefunctions, but never mind.)</p>
<p>Until someone tells me the angular momentum of a lone oxygen atom, I won&#8217;t know the answer for sure.  But I&#8217;m now pretty sure the latter picture is closer to the truth.  I believe the 4 electrons in the outermost shell of the oxygen atom <i>break the spherical symmetry</i> and in fact pick out two preferred axes, lying at a roughly 90&deg; angle to each other.  That&#8217;s what those &#8216;p orbital&#8217; pictures are supposed to suggest.  But of course those pictures are only approximate, since (as you note) they neglect inter-electron forces.</p>
<p>Then, when two hydrogens actually attach to the oxygen, they wind up attaching at an angle of 104.45&deg;, for complicated reasons that take a computer simulation to fully fathom.  Presumably the repulsion of the hydrogens&#8217; nuclei is largely to blame.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5227</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 11 Apr 2011 05:19:32 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2963#comment-5227</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5224&quot;&gt;Giampiero Campa&lt;/a&gt;.

Giampiero wrote:

&lt;blockquote&gt;
And i actually wonder why we can’t just propagate the random variable \ell in time, which we should be able to do if we have the deterministic evolution equations (and we do) and the probability distribution (and it looks like we assume it).
&lt;/blockquote&gt;

There are a couple of reasons:

First, those deterministic evolution equations (the &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/03/network-theory-part-3/&quot; rel=&quot;nofollow&quot;&gt;&#039;rate equations&#039;&lt;/a&gt;) assume the amount of things we have in each state varies &lt;i&gt;continuously&lt;/i&gt;.  For example, the rate equation for this model

&lt;div align=&quot;center&quot;&gt;
&lt;img width=&quot;280&quot; src=&quot;http://math.ucr.edu/home/baez/networks/wolf-rabbit.png&quot; /&gt;
&lt;/div&gt;

might predict that starting out with 10 rabbits now, we will have 10.5 rabbits one week later.

But now we are studying the &#039;master equation&#039;, which assumes the number of things we have in each state varies &lt;i&gt;discretely&lt;/i&gt;.  No half-rabbits!  The number must be a natural number.

Second, the master equations are really probabilistic, not deterministic.  Starting out with 10 rabbits now, they&#039;ll tell us the &lt;i&gt;probability&lt;/i&gt; that we&#039;ll have 9 or 10 or 11 or some other number of rabbits one week from now.  

So the master equation is really answering a different question than the rate equation.  It just so happens&#8212;we&#039;ll see this later&#8212;that in the limit of large numbers, the results we get from the master equation are very close &lt;i&gt;on average&lt;/i&gt;  to the results we get from the rate equation.

&lt;blockquote&gt;
I am still at a loss on this. 
&lt;/blockquote&gt;

Okay.  If you start with $latex \ell_i$ things in the $latex i$th state, and something happens that eats $latex m_i$ things in the $latex i$th state and produces $latex n_i$ new things in that state, you&#039;re left with 

$latex \ell_i + n_i - m_i$ 

things in the $latex i$th state.  

We can say the same thing faster using more jargon: if our system starts in the labelling $latex \ell$, our transition takes it to the labelling $latex \ell + m - n$.

So, we expect that the matrix entry

$latex H_{\ell&#039; \ell}$

will be nonzero if 

$latex \ell&#039; = \ell + m - n$

(Note that we&#039;ve switched the roles of $latex \ell$ and $latex \ell&#039;$.  In my article I was talking about $latex H_{\ell \ell&#039;}$; now we&#039;re talking about $latex H_{\ell&#039; \ell}$.  It doesn&#039;t really matter as long as we pay attention.)

So, it would be nice to know the formula for $latex H_{\ell&#039; \ell}$ in this case.   The answer is supposed to be contained in these cryptic comments of mine:

&lt;blockquote&gt;
A bit more precisely: suppose we have a Petri net that is labelled in some way at some moment.  Then the probability that a given transition occurs in a short time $latex \Delta t$ is approximately:

&#8226; the rate constant for that transition, times

&#8226; the time $latex \Delta t$, times

&#8226; the number of ways the transition can occur.

More precisely still: this formula is correct up to terms of order $latex (\Delta t)^2$.  So, taking the limit as $latex \Delta t \to 0$, we get a differential equation describing precisely how the probability of the Petri net having a given labelling changes with time!  And this is the &lt;b&gt;master equation&lt;/b&gt;.
&lt;/blockquote&gt;

But be careful: is

$latex \ell&#039; = \ell + m - n$

the &lt;i&gt;only&lt;/i&gt; value of $latex \ell&#039;$ for which $latex H_{\ell&#039; \ell}$ is nonzero?  

The answer to that may become clearer upon reading &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/04/11/network-theory-part-5/&quot; rel=&quot;nofollow&quot;&gt;part 5&lt;/a&gt;.  On the other hand, you can also figure it out just by thinking.  

Here&#039;s another way to ask the question.  The question is, which labellings $latex \ell&#039;$ have their probabilities changing

$latex \frac{d}{dt} \psi_{\ell&#039;}(t) \ne 0$

if at some moment we&#039;re sure the system has the labelling $latex \ell$:

$latex \psi_\ell(t) = 1$

We know that 

$latex \ell&#039; = \ell + m - n$

is one such labelling.  But is it the only one?

Once we settle this, we can work out exactly what

$latex \frac{d}{dt} \psi_{\ell&#039;}(t)$ 

equals.  We know

$latex \frac{d}{dt} \psi_{\ell&#039;}(t) = \sum_\ell H_{\ell&#039; \ell}\; \psi_\ell(t) $

so once we know which matrix entries $latex H_{\ell&#039; \ell}$ are nonzero, we can figure out what they are and we&#039;ll be done!

(By the way, I encourage other folks to join in and help Giampiero solve this puzzle! The equation we&#039;re looking for is called the &#039;chemical master equation&#039;, because it&#039;s the basic equation governing chemical reactions.  So if you&#039;re already an expert on that, this puzzle might be too easy to be enjoyable.  But if you&#039;ve never thought about it, there&#039;s no better way to learn it than to reinvent it.)
]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comment-5224">Giampiero Campa</a>.</p>
<p>Giampiero wrote:</p>
<blockquote><p>
And i actually wonder why we can’t just propagate the random variable \ell in time, which we should be able to do if we have the deterministic evolution equations (and we do) and the probability distribution (and it looks like we assume it).
</p></blockquote>
<p>There are a couple of reasons:</p>
<p>First, those deterministic evolution equations (the <a href="https://johncarlosbaez.wordpress.com/2011/04/03/network-theory-part-3/" rel="nofollow">&#8216;rate equations&#8217;</a>) assume the amount of things we have in each state varies <i>continuously</i>.  For example, the rate equation for this model</p>
<div align="center">
<img width="280" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/wolf-rabbit.png" />
</div>
<p>might predict that starting out with 10 rabbits now, we will have 10.5 rabbits one week later.</p>
<p>But now we are studying the &#8216;master equation&#8217;, which assumes the number of things we have in each state varies <i>discretely</i>.  No half-rabbits!  The number must be a natural number.</p>
<p>Second, the master equations are really probabilistic, not deterministic.  Starting out with 10 rabbits now, they&#8217;ll tell us the <i>probability</i> that we&#8217;ll have 9 or 10 or 11 or some other number of rabbits one week from now.  </p>
<p>So the master equation is really answering a different question than the rate equation.  It just so happens&mdash;we&#8217;ll see this later&mdash;that in the limit of large numbers, the results we get from the master equation are very close <i>on average</i>  to the results we get from the rate equation.</p>
<blockquote><p>
I am still at a loss on this.
</p></blockquote>
<p>Okay.  If you start with <img src="https://s0.wp.com/latex.php?latex=%5Cell_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell_i" class="latex" /> things in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state, and something happens that eats <img src="https://s0.wp.com/latex.php?latex=m_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m_i" class="latex" /> things in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state and produces <img src="https://s0.wp.com/latex.php?latex=n_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i" class="latex" /> new things in that state, you&#8217;re left with </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cell_i+%2B+n_i+-+m_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell_i + n_i - m_i" class="latex" /> </p>
<p>things in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state.  </p>
<p>We can say the same thing faster using more jargon: if our system starts in the labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" />, our transition takes it to the labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell+%2B+m+-+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell + m - n" class="latex" />.</p>
<p>So, we expect that the matrix entry</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" /></p>
<p>will be nonzero if </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cell%27+%3D+%5Cell+%2B+m+-+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039; = &#92;ell + m - n" class="latex" /></p>
<p>(Note that we&#8217;ve switched the roles of <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" />.  In my article I was talking about <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell+%5Cell%27%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell &#92;ell&#039;}" class="latex" />; now we&#8217;re talking about <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" />.  It doesn&#8217;t really matter as long as we pay attention.)</p>
<p>So, it would be nice to know the formula for <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" /> in this case.   The answer is supposed to be contained in these cryptic comments of mine:</p>
<blockquote><p>
A bit more precisely: suppose we have a Petri net that is labelled in some way at some moment.  Then the probability that a given transition occurs in a short time <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t" class="latex" /> is approximately:</p>
<p>&bull; the rate constant for that transition, times</p>
<p>&bull; the time <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t" class="latex" />, times</p>
<p>&bull; the number of ways the transition can occur.</p>
<p>More precisely still: this formula is correct up to terms of order <img src="https://s0.wp.com/latex.php?latex=%28%5CDelta+t%29%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Delta t)^2" class="latex" />.  So, taking the limit as <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t &#92;to 0" class="latex" />, we get a differential equation describing precisely how the probability of the Petri net having a given labelling changes with time!  And this is the <b>master equation</b>.
</p></blockquote>
<p>But be careful: is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cell%27+%3D+%5Cell+%2B+m+-+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039; = &#92;ell + m - n" class="latex" /></p>
<p>the <i>only</i> value of <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" /> for which <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" /> is nonzero?  </p>
<p>The answer to that may become clearer upon reading <a href="https://johncarlosbaez.wordpress.com/2011/04/11/network-theory-part-5/" rel="nofollow">part 5</a>.  On the other hand, you can also figure it out just by thinking.  </p>
<p>Here&#8217;s another way to ask the question.  The question is, which labellings <img src="https://s0.wp.com/latex.php?latex=%5Cell%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039;" class="latex" /> have their probabilities changing</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bdt%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29+%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{dt} &#92;psi_{&#92;ell&#039;}(t) &#92;ne 0" class="latex" /></p>
<p>if at some moment we&#8217;re sure the system has the labelling <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Cell%28t%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_&#92;ell(t) = 1" class="latex" /></p>
<p>We know that </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cell%27+%3D+%5Cell+%2B+m+-+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell&#039; = &#92;ell + m - n" class="latex" /></p>
<p>is one such labelling.  But is it the only one?</p>
<p>Once we settle this, we can work out exactly what</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bdt%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{dt} &#92;psi_{&#92;ell&#039;}(t)" class="latex" /> </p>
<p>equals.  We know</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bdt%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29+%3D+%5Csum_%5Cell+H_%7B%5Cell%27+%5Cell%7D%5C%3B+%5Cpsi_%5Cell%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{dt} &#92;psi_{&#92;ell&#039;}(t) = &#92;sum_&#92;ell H_{&#92;ell&#039; &#92;ell}&#92;; &#92;psi_&#92;ell(t) " class="latex" /></p>
<p>so once we know which matrix entries <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell%27+%5Cell%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell&#039; &#92;ell}" class="latex" /> are nonzero, we can figure out what they are and we&#8217;ll be done!</p>
<p>(By the way, I encourage other folks to join in and help Giampiero solve this puzzle! The equation we&#8217;re looking for is called the &#8216;chemical master equation&#8217;, because it&#8217;s the basic equation governing chemical reactions.  So if you&#8217;re already an expert on that, this puzzle might be too easy to be enjoyable.  But if you&#8217;ve never thought about it, there&#8217;s no better way to learn it than to reinvent it.)</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
