<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>R&eacute;nyi Entropy and Free Energy | Azimuth</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large, noindex, follow' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; R&eacute;nyi Entropy and Free&nbsp;Energy Comments Feed" href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s0.wp.com/_static/??-eJyNkdtSAyEMhl9INkWnOl44PguHiKnAMhCsvL3Zepi1tdUbhj/Jl/yTwL4oN2fGzJC6KrEHyg0qRsPoVZkbH6nJtXYFv2OUnygTD+BnTNigdAuHslT4hPuCQhdpsQbJVIRXraftpMF2ih5snN2LimSrqQMaj4jfjSi72L2M2TVI6MlglKmLo5Uo0QysKmIwbkyJ8t+45Nb6B3Te/MGpNEMuZrFsxtxZhUr+yPa/W1TDlEM7g6/WvuxN4qmYU/MXsD35gHJeKfn8K8a3y0iRMcraUrE1JW+intTHtRfuMT3o2xt9d7/dbK5375l75CE=?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='jetpack_related-posts-js-extra'>
var related_posts_js_options = {"post_heading":"h4"};
</script>
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F02%2F10%2Frnyi-entropy-and-free-energy%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"c031e776ae","isSingular":"1","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"8f1337fe87\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/2011\/02\/10\/rnyi-entropy-and-free-energy\/?replytocom=4032","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F02%2F10%2Frnyi-entropy-and-free-energy%2F%3Freplytocom%3D4032","postID":"2467","shortlink":"https:\/\/wp.me\/pRBZ9-DN","canEditPost":"","editLink":"https:\/\/wordpress.com\/post\/johncarlosbaez.wordpress.com\/2467","statsLink":"https:\/\/wordpress.com\/stats\/post\/2467\/johncarlosbaez.wordpress.com","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,227 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F02%2F10%2Frnyi-entropy-and-free-energy%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s0.wp.com/_static/??-eJyFjcEKwjAQRH/IbQxSPYnfUtu1bMhuYnZD7d/bQj1YBE/D8B4zbsrQJzEUc0FdTmqMqt2ITdCD+6ac7hQRqmJZBDEgeaS9xxVyrCOJuoKxMxxgXd21H+slGuSSXvOHkfSxDqgrDM+KZd6iYZK/EjCNZTnc5Btf/fnkL8fW+za8ASssW38='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel='stylesheet' id='all-css-0-2' href='https://s0.wp.com/wp-content/mu-plugins/highlander-comments/style.css?m=1625210320h&cssminify=yes' type='text/css' media='all' />
<!--[if lt IE 8]>
<link rel='stylesheet' id='highlander-comments-ie7-css'  href='https://s2.wp.com/wp-content/mu-plugins/highlander-comments/style-ie7.css?m=1351637563h&#038;ver=20110606' media='all' />
<![endif]-->
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />
<link rel="canonical" href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" />
<link rel='shortlink' href='https://wp.me/pRBZ9-DN' />
<link rel="alternate" type="application/json+oembed" href="https://public-api.wordpress.com/oembed/?format=json&amp;url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F02%2F10%2Frnyi-entropy-and-free-energy%2F&amp;for=wpcom-auto-discovery" /><link rel="alternate" type="application/xml+oembed" href="https://public-api.wordpress.com/oembed/?format=xml&amp;url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F02%2F10%2Frnyi-entropy-and-free-energy%2F&amp;for=wpcom-auto-discovery" />
<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="article" />
<meta property="og:title" content="R&eacute;nyi Entropy and Free Energy" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" />
<meta property="og:description" content="The Renyi entropy is really just the free energy in disguise." />
<meta property="article:published_time" content="2011-02-10T12:14:04+00:00" />
<meta property="article:modified_time" content="2012-08-24T05:44:20+00:00" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta name="twitter:text:title" content="R&eacute;nyi Entropy and Free&nbsp;Energy" />
<meta name="twitter:card" content="summary" />
<meta property="fb:app_id" content="249643311490" />
<meta property="article:publisher" content="https://www.facebook.com/WordPresscom" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="The Renyi entropy is really just the free energy in disguise." />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<link rel="amphtml" href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/amp/">		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="post-template-default single single-post postid-2467 single-format-standard customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content" class="widecolumn">

  


		<div class="post-2467 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-2467">
			<h2>R&eacute;nyi Entropy and Free&nbsp;Energy</h2>

			<div class="entry">
				<p>I want to keep telling you about information geometry&#8230; but I got sidetracked into thinking about something slightly different, thanks to some fascinating discussions here at the CQT.  </p>
<p>There are a lot of people interested in entropy here, so some of us &mdash; <a href="http://arxiv.org/find/all/1/all:+AND+oscar+dahlsten/0/1/0/all/0/1">Oscar Dahlsten</a>, <a href="http://arxiv.org/find/all/1/all:+AND+mile+gu/0/1/0/all/0/1">Mile Gu</a>, <a href="http://arxiv.org/find/all/1/all:+AND+elisabeth+rieper/0/1/0/all/0/1">Elisabeth Rieper</a>, <a href="http://arxiv.org/find/all/1/all:+AND+wonmin+son/0/1/0/all/0/1">Wonmin Son</a> and me &mdash; decided to start meeting more or less regularly.  I call it the Entropy Club.  I&#8217;m learning a lot of wonderful things, and I hope to tell you about them someday.  But for now, here&#8217;s a little idea I came up with, triggered by our conversations:</p>
<p>&bull; John Baez, <a href="http://arxiv.org/abs/1102.2098">R&eacute;nyi entropy and free energy</a>.</p>
<p>In 1960, Alfred R&eacute;nyi defined a generalization of the usual Shannon entropy that depends on a parameter.  If <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a probability distribution on a finite set, its <b><a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a></b> of order <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is defined to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Cbeta+%3D+%5Cfrac%7B1%7D%7B1+-+%5Cbeta%7D+%5Cln+%5Csum_i+p_i%5E%5Cbeta+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;beta = &#92;frac{1}{1 - &#92;beta} &#92;ln &#92;sum_i p_i^&#92;beta } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+%5Cbeta+%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le &#92;beta &lt; &#92;infty" class="latex" />.   This looks pretty weird at first, and we need <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cne+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;ne 1" class="latex" /> to avoid dividing by zero, but you can show that the R&eacute;nyi entropy approaches the <b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a></b> as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> approaches<br />
1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7B%5Cbeta+%5Cto+1%7D+H_%5Cbeta+%3D+-%5Csum_%7Bi%7D+p_i+%5Cln+p_i+.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lim_{&#92;beta &#92;to 1} H_&#92;beta = -&#92;sum_{i} p_i &#92;ln p_i . " class="latex" /></p>
<p>(A fun puzzle, which I leave to you.)  So, it&#8217;s customary to define <img src="https://s0.wp.com/latex.php?latex=H_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1" class="latex" /> to be the Shannon entropy&#8230; and then the R&eacute;nyi entropy generalizes the Shannon entropy by allowing an adjustable parameter <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />. </p>
<p>But what does it <i>mean?</i></p>
<p>If you ask people what&#8217;s good about the R&eacute;nyi entropy, they&#8217;ll usually say: it&#8217;s additive!  In other words, when you combine two independent probability distributions into a single one, their R&eacute;nyi entropies add.  And that&#8217;s true &mdash; but there are other quantities that have the same property.  So I wanted a better way to think about R&eacute;nyi entropy, and here&#8217;s what I&#8217;ve come up with so far.</p>
<p>Any probability distribution can be seen as the state of thermal equilibrium for some Hamiltonian at some fixed temperature, say <img src="https://s0.wp.com/latex.php?latex=T+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = 1" class="latex" />.  And that Hamiltonian is unique.  Starting with that Hamiltonian, we can then compute the free energy <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> at any temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />, and up to a certain factor this free energy turns out to be the R&eacute;nyi entropy <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" />.  More precisely:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%281+-+T%29+H_%5Cbeta.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = (1 - T) H_&#92;beta. " class="latex" /></p>
<p>So, up to the fudge factor <img src="https://s0.wp.com/latex.php?latex=1+-+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 - T" class="latex" />, R&eacute;nyi entropy is the same as free energy. It seems like a good thing to know &mdash; but I haven&#039;t seen anyone say it anywhere!  Have you?</p>
<p>Let me show you why it&#8217;s true &mdash; the proof is pathetically simple.  We start with our probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />.  We can always write</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{- E_i} " class="latex" /></p>
<p>for some real numbers <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" />.   Let&#8217;s think of these numbers <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> as energies.  Then the state of thermal equilibrium, also known as the <b><a href="http://en.wikipedia.org/wiki/Canonical_ensemble">canonical ensemble</a></b> or <b><a href="http://en.wikipedia.org/wiki/Gibbs_state">Gibbs state</a></b> at inverse temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Be%5E%7B-+%5Cbeta+E_i%7D%7D%7BZ%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{e^{- &#92;beta E_i}}{Z} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is the <b><a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29#Definition">partition function</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_i+e%5E%7B-%5Cbeta+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_i e^{-&#92;beta E_i} " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=Z+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = 1" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />, the Gibbs state reduces to our original probability distribution at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />.</p>
<p>Now in thermodynamics, the quantity</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-+%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = - &#92;frac{1}{&#92;beta} &#92;ln Z " class="latex" /></p>
<p>is called the <b><a href="http://en.wikipedia.org/wiki/Helmholtz_free_energy#Relation_to_the_partition_function">free energy</a></b>.  It&#8217;s important, because it equals the total expected energy of our system, minus the energy in the form of heat.   Roughly speaking, it&#8217;s the energy that you can use.</p>
<p>Let&#8217;s see how the R&eacute;nyi entropy is related to the free energy.  The proof is a trivial calculation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Cbeta+F+%3D+%5Cln+Z+%3D+%5Cln+%5Csum_%7Bi+%5Cin+X%7D+e%5E%7B-%5Cbeta+E_i%7D+%3D+%5Cln+%5Csum_%7Bi+%5Cin+X%7D+p_i%5E%5Cbeta+%3D+%281+-+%5Cbeta%29+H_%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;beta F = &#92;ln Z = &#92;ln &#92;sum_{i &#92;in X} e^{-&#92;beta E_i} = &#92;ln &#92;sum_{i &#92;in X} p_i^&#92;beta = (1 - &#92;beta) H_&#92;beta " class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta+%3D+-++%5Cfrac%7B%5Cbeta%7D%7B1+-+%5Cbeta%7D+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta = -  &#92;frac{&#92;beta}{1 - &#92;beta} F" class="latex" /></p>
<p>at least for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cne+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;ne 1" class="latex" />.  But you can also check that both sides of this equation have well-defined limits as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to 1" class="latex" />.  </p>
<p>The relation between free energy and R&eacute;nyi entropy looks even neater if we solve for <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> and write the answer using <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%281+-+T%29H_%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = (1 - T)H_&#92;beta " class="latex" /></p>
<p>So, what&#8217;s this fact good for?  I&#8217;m not sure yet!  In my paper, I combine it with this equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+E+%5Crangle+-+T+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle E &#92;rangle - T S " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle" class="latex" /> is the expected energy in the Gibbs state at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle+%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Csum_i+E_i+%5C%2C+e%5E%7B-%5Cbeta+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle = &#92;frac{1}{Z} &#92;sum_i E_i &#92;, e^{-&#92;beta E_i} " class="latex" /></p>
<p>while <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the usual Shannon entropy of this Gibbs state.  I also show that all this stuff works quantum-mechanically as well as classically.  But so far, it seems the main benefit is that R&eacute;nyi entropy has become a lot less mysterious.  It&#8217;s not a mutant version of Shannon entropy: it&#8217;s just a familiar friend in disguise.</p>
<div id="jp-post-flair" class="sharedaddy sd-sharing-enabled">
<div id='jp-relatedposts' class='jp-relatedposts' >
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</div></div>
				
				<p class="postmetadata alt">
					<small>
					This entry was posted  on Thursday, February 10th, 2011 at 12:14 pm and is filed under <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>.					You can follow any responses to this entry through the <a href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/feed/'>RSS 2.0</a> feed.
											You can <a href="#respond">leave a response</a>, or <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/trackback/" rel="trackback">trackback</a> from your own site.
					
					</small>
				</p>

				<nav id="nav-below">
					<h3 class="assistive-text">Post navigation</h3>
					<span class="nav-previous"><a href="https://johncarlosbaez.wordpress.com/2011/02/04/carbon-dioxide-puzzles/" rel="prev">&laquo; Previous Post</a></span>
					<span class="nav-next"><a href="https://johncarlosbaez.wordpress.com/2011/02/11/child-earth/" rel="next">Next Post &raquo;</a></span>
				</nav><!-- #nav-below -->

			</div>
		</div>

	<div id="comments">


<h3 id="comments-title">138 Responses to <em>R&eacute;nyi Entropy and Free&nbsp;Energy</em></h3>


<ol class="commentlist">
			<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-4018">
				<div id="div-comment-4018" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/c125fd6bead727c64b5f5ec96abc9c99?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Wonmin</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4018">10 February, 2011 at 4:34 pm</a>		</div>

		<p>This is amazingly interesting and mysteriously easy. </p>
<p>I may point out a couple of points here as below. </p>
<p>In the formula, first of all, there must be a constraint that the energy should be positive definite. That is because the probability should be all the time smaller than one. </p>
<p>Secondly, when the system is in the ground state, (<img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> goes to infinity), the free energy become R&eacute;nyi entropy with <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Crightarrow+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta&#92;rightarrow &#92;infty" class="latex" />. If the energy is zero, then, the Reney entropy becomes infinity while conventional free energy should be zero at the ground state. This looks contradictory except when the ground state energy is not zero. I think the point is worthy to be discussed.</p>
<p>It seems that the physical interpretation of temperature should be carefully chosen if we equate the parameter for R&eacute;nyi entropy to <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4018#respond' data-commentid="4018" data-postid="2467" data-belowelement="div-comment-4018" data-respondelement="respond" data-replyto="Reply to Wonmin" aria-label='Reply to Wonmin'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 highlander-comment" id="comment-4032">
				<div id="div-comment-4032" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4032">11 February, 2011 at 3:49 am</a>		</div>

		<p>Wonmin wrote:</p>
<blockquote>
<p>In the formula, first of all, there must be a constraint that the energy should be positive definite. That is because the probability should be all the time smaller than one. </p>
</blockquote>
<p>Good point.  I wouldn&#8217;t call this a &#8220;constraint&#8221;: I&#8217;m starting with probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and <i>defining</i> the energies <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> by </p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{-E_i} " class="latex" /></p>
<p>so it just works out automatically that </p>
<p><img src="https://s0.wp.com/latex.php?latex=E_i+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i &#92;ge 0 " class="latex" /></p>
<p>It&#8217;s not a constraint I have to impose   But, it might be worth mentioning that the energies come out nonnegative.</p>
<p>You&#8217;re also right that the limits <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to 0" class="latex" /> (infinite temperature, all states equally likely to be occupied) and <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%2B+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to + &#92;infty" class="latex" /> (zero temperature, only lowest-energy states occupied, and all of these equally likely) deserve special attention.  </p>
<p>For one thing, these special limiting cases of R&eacute;nyi entropy play a special role in <a href="http://arxiv.org/abs/0908.0424" rel="nofollow">understanding the work value of information</a>.  For another, the <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to +&#92;infty" class="latex" /> limit of R&eacute;nyi entropy, called the <a href="http://www.google.com/search?q=cryptography+min-entropy" rel="nofollow">min-entropy</a>, is important in cryptography when we study a &#8216;worst-case scenario&#8217; where a determined opponent is trying to break our code.  </p>
<p>(The Shannon entropy, on the other hand, is related to an &#8216;average-case scenario&#8217;.   This is okay for physics because &mdash; to coin a phrase &mdash; nature is subtle, but it&#8217;s not malicious.)</p>
<p>And, finally, it&#8217;s always good to study high- and low-temperature limits!  Things often simplify here, and sometimes the high-temperature limit of one interesting physical system matches the low-temperature limit of another.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4032#respond' data-commentid="4032" data-postid="2467" data-belowelement="div-comment-4032" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="pingback even thread-odd thread-alt depth-1 highlander-comment" id="comment-4019">
				<div id="div-comment-4019" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='http://topsy.com/johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?utm_source=pingback&#038;utm_campaign=L2' rel='external nofollow ugc' class='url'>Tweets that mention Rényi Entropy and Free Energy « Azimuth -- Topsy.com</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4019">10 February, 2011 at 6:54 pm</a>		</div>

		<p>This post was mentioned on Twitter by Blake Stacey: <a href="http://bit.ly/e4gKsX" rel="nofollow ugc">http://bit.ly/e4gKsX</a> </p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4019#respond' data-commentid="4019" data-postid="2467" data-belowelement="div-comment-4019" data-respondelement="respond" data-replyto="Reply to Tweets that mention Rényi Entropy and Free Energy « Azimuth -- Topsy.com" aria-label='Reply to Tweets that mention Rényi Entropy and Free Energy « Azimuth -- Topsy.com'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1 parent highlander-comment" id="comment-4021">
				<div id="div-comment-4021" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4021">10 February, 2011 at 8:32 pm</a>		</div>

		<blockquote><p>So, up to the fudge factor <img src="https://s0.wp.com/latex.php?latex=1+-+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 - T" class="latex" />, Rényi entropy is the same as free energy. It seems like a good thing to know &mdash; but I haven&#8217;t seen anyone say it anywhere! Have you?</p></blockquote>
<p>After kicking myself over not having thought of this &mdash; and getting a little entertainment from forwarding this to a colleague and watching him do the same &mdash; I thought, &#8220;Hey, I should see what <a href="http://www.eurekajournalwatch.org/index.php/Google_Scholar" rel="nofollow">Google Scholar</a> knows about it.&#8221;  After sifting through half a dozen false positives, I found <a href="http://dx.doi.org/10.1016/0960-0779(94)00196-W" rel="nofollow">Klimontovich (1995)</a>.  Section 14.2 has, as equation (81),</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Clog%5Csum_i+%5Cexp%28-%5Cbeta+b_i%29+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Clog%5Csum_i+%28p_i%29%5E%5Cbeta+%3D+%5Cfrac%7B%5Cbeta+-+1%7D%7B%5Cbeta%7D+S_%5Cbeta%5Bp%5D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(&#92;beta) = -&#92;frac{1}{&#92;beta} &#92;log&#92;sum_i &#92;exp(-&#92;beta b_i) = -&#92;frac{1}{&#92;beta} &#92;log&#92;sum_i (p_i)^&#92;beta = &#92;frac{&#92;beta - 1}{&#92;beta} S_&#92;beta[p]," class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=S_%5Cbeta%5Bp%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_&#92;beta[p]" class="latex" /> is Klimontovich&#8217;s notation for the R&eacute;nyi entropy of order <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />.</p>
<p>(Finding out why I did a double-take and had a wry chuckle after seeing where Klimontovich was published is left as an exercise for the interested reader.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4021#respond' data-commentid="4021" data-postid="2467" data-belowelement="div-comment-4021" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-2 highlander-comment" id="comment-4022">
				<div id="div-comment-4022" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4022">10 February, 2011 at 9:20 pm</a>		</div>

		<p>In turn, Klimontovich&#8217;s derivation appears to come largely from Beck and Schlogl&#8217;s <i><a href="http://books.google.com/books?id=8G6B3yPpli8C&amp;lpg=PP1&amp;ots=p_wIJDlIlW&amp;dq=Thermodynamics%20of%20Chaotic%20Systems%2C&amp;pg=PA115#v=onepage&amp;q=Renyi&amp;f=false" rel="nofollow">Thermodynamics of Chaotic Systems</a></i> (1993).</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4022#respond' data-commentid="4022" data-postid="2467" data-belowelement="div-comment-4022" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-2 highlander-comment" id="comment-4030">
				<div id="div-comment-4030" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4030">11 February, 2011 at 3:09 am</a>		</div>

		<p>Darn.  But thanks.  I tried to find previous work on this subject, but I missed this.  </p>
<p>I&#8217;d already let fly and put my paper on the arXiv; I&#8217;ll update it to include these references, an acknowledgement to you, and also some followup ideas I&#8217;ve had.  At least it may help more people catch on!  You&#8217;ve got a few people over here talking about <a href="http://math.ucr.edu/home/baez/corfield/2006/06/tsallis-entropy.html#115034029419704787" rel="nofollow">maximizing R&eacute;nyi entropy</a> over here, and a lot of people talking about <a href="http://en.wikipedia.org/wiki/Principle_of_minimum_energy#Thermodynamic_potentials" rel="nofollow">minimizing free energy over there</a>, and they should get to know each other.</p>
<blockquote><p>
(Finding out why I did a double-take and had a wry chuckle after seeing where Klimontovich was published is left as an exercise for the interested reader.)
</p></blockquote>
<p>Heh.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4030#respond' data-commentid="4030" data-postid="2467" data-belowelement="div-comment-4030" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-2 highlander-comment" id="comment-4031">
				<div id="div-comment-4031" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4031">11 February, 2011 at 3:14 am</a>		</div>

		<p>I think there&#8217;s a lot to be said for (a) making previously obscure ideas less obscure and (b) showing that seemingly unrelated things are really connected.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4031#respond' data-commentid="4031" data-postid="2467" data-belowelement="div-comment-4031" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1 highlander-comment" id="comment-4023">
				<div id="div-comment-4023" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/1f56f97d97b228240272e0ba442cda02?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://tomate.blogsome.com' rel='external nofollow ugc' class='url'>tomate</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4023">10 February, 2011 at 11:08 pm</a>		</div>

		<p>Interesting. To shed some light on the physical meaning, maybe the following might help. If you set <img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cexp+%28-E_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;exp (-E_i)" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> is dimensionless; if <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> has to be an inverse energy (in units where Boltzmann&#8217;s constant <img src="https://s0.wp.com/latex.php?latex=k+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k = 1" class="latex" />) than <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28+-%5Cbeta+E_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp( -&#92;beta E_i)" class="latex" /> does not make much sense. One can fix this by taking an arbitrary reference temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_0" class="latex" /> and define</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cexp+%28-+%5Cbeta_0+E_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;exp (- &#92;beta_0 E_i)" class="latex" /></p>
<p>so that now one has <img src="https://s0.wp.com/latex.php?latex=F+%3D+%28T_0+-+T%29+H_%7BT_0%2FT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = (T_0 - T) H_{T_0/T}" class="latex" />. So 1) Rényi entropy might be related to the scaling of unit measures, and, who knows?, maybe it could even have some role when renormalizing&#8230; 2) when <img src="https://s0.wp.com/latex.php?latex=T_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_0" class="latex" /> is just slightly more than <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />, one gets the temperature gradient which is the driving force of heat diffusion: could then Rényi entropy have some role out of equilibrium?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4023#respond' data-commentid="4023" data-postid="2467" data-belowelement="div-comment-4023" data-respondelement="respond" data-replyto="Reply to tomate" aria-label='Reply to tomate'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-4025">
				<div id="div-comment-4025" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/1f56f97d97b228240272e0ba442cda02?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://tomate.blogsome.com' rel='external nofollow ugc' class='url'>tomate</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4025">10 February, 2011 at 11:15 pm</a>		</div>

		<p>John wrote:</p>
<blockquote><p>
Any probability distribution can be seen as the state of thermal equilibrium for some Hamiltonian at some fixed temperature, say T=1.
</p></blockquote>
<p>Sorry, I didn&#8217;t see that sentence. Forget my previous comment.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4025#respond' data-commentid="4025" data-postid="2467" data-belowelement="div-comment-4025" data-respondelement="respond" data-replyto="Reply to tomate" aria-label='Reply to tomate'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 parent highlander-comment" id="comment-4033">
				<div id="div-comment-4033" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4033">11 February, 2011 at 3:59 am</a>		</div>

		<p>I won&#8217;t forget it &mdash; not because I&#8217;m one to hold a grudge, but because I actually think we can clarify the physics a bit by calling the standard temperature <img src="https://s0.wp.com/latex.php?latex=T_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_0" class="latex" /> instead of 1.  I mentioned this option in the last sentence of the first version of my paper, but I think it&#8217;ll be better if I show people what the resulting formulas actually look like, like your</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-%28T+-+T_0%29+H_%7BT_0%2FT%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = -(T - T_0) H_{T_0/T} " class="latex" /></p>
<p>The mathematician in me is perfectly happy to set as many constants equal to 1 as I can get away with &mdash; but the physicist in me likes the look of this formula better than </p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-%28T+-+1%29+H_%7B1%2FT%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = -(T - 1) H_{1/T} " class="latex" /></p>
<p>This may be the first time I&#8217;ve acknowledged a tomato in a paper.  Your <a href="http://tomate.blogsome.com/chi-siamo/" rel="nofollow">blog</a> may offer clues about your real identity, but Babelfish translates them as follows:</p>
<blockquote>
<p><b>To</b> (lomé) is a emiliano, pedantic physicist with the other people&#8217;s and indulgente ideas with own, defender to the trouble of the scientific method in every field, also not scientific.</p>
<p><b>Ma</b> (latò) it is a Lombardic pianista, a criticone always incazzato for the musical brutture that they encircle to us. </p>
<p><b>Te</b> (mpon) it is buontempone a Veneto, the perennial university student who passes to the time organizing things and cosette (by now not more… but the times of gold will return).</p>
</blockquote>
<p>What&#8217;s an &#8220;emiliano&#8221;?  </p>
<p>Is a &#8220;buontempone&#8221; someone who says <a href="http://en.wikipedia.org/wiki/Let_the_Good_Times_Roll" rel="nofollow">&#8220;let the good times roll&#8221;</a>?</p>
<p>Most importantly: is there one person named Tolomé Malatò Tempon, or three, or none?  </p>
<p>(You can email me if this is top secret information &mdash; or ignore me if you want me to mind my own business!  I just wish my Italian were up to reading this page.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4033#respond' data-commentid="4033" data-postid="2467" data-belowelement="div-comment-4033" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-3 parent highlander-comment" id="comment-4065">
				<div id="div-comment-4065" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/bb43afd67d78b8cb0fe47f3169a49f8c?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Eric Downes</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4065">11 February, 2011 at 6:59 pm</a>		</div>

		<p>Tomate is correct, and it looks like the Renyi entropy is the secant of the temperature derivative of free energy.  But this does not appear to be a gradient, it&#8217;s a normal equilibrium derivative.</p>
<p>If one starts from a completely general state with SI units</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cexp%28-%5Cbeta+E_i%29%2FZ+%5Cquad+%5Cquad+%5Cbeta%3D1%2F%28kT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;exp(-&#92;beta E_i)/Z &#92;quad &#92;quad &#92;beta=1/(kT)" class="latex" /></p>
<p>and uses <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> for the Renyi parameter, one arrives at</p>
<p><img src="https://s0.wp.com/latex.php?latex=k+H_a+%3D+-+%28+F%28T%2Fa%29+-+F%28T%29+%29+%2F+%28T%2Fa+-+T%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k H_a = - ( F(T/a) - F(T) ) / (T/a - T)" class="latex" /></p>
<p>The limit <img src="https://s0.wp.com/latex.php?latex=a+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a &#92;to 1" class="latex" /> recovers</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-%5Cpartial+F+%2F+%5Cpartial+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = -&#92;partial F / &#92;partial T" class="latex" /></p>
<p>Which is from the first law</p>
<p><img src="https://s0.wp.com/latex.php?latex=dF+%3D+-S+dT+-+p+dV+%2B+...&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dF = -S dT - p dV + ..." class="latex" /></p>
<p>This has two uses</p>
<p>1. By calculating Renyi entropy in an equilibrated simulation running at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />, one can get the free energy at <img src="https://s0.wp.com/latex.php?latex=T%2Fa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T/a" class="latex" />.  This is useful if <img src="https://s0.wp.com/latex.php?latex=T%2Fa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T/a" class="latex" /> is, say, very close to absolute zero.</p>
<p>2. Protein chemists tend to use a non-differential form of the first law</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+F+%3D+-S+%5CDelta+T+-p+%5CDelta+V+%2B+...&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta F = -S &#92;Delta T -p &#92;Delta V + ..." class="latex" /></p>
<p>This means that <img src="https://s0.wp.com/latex.php?latex=H%28T%2FT_0%29%28T-T_0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(T/T_0)(T-T_0)" class="latex" /> is actually<br />
an <i>exact</i> expression which extrapolates <img src="https://s0.wp.com/latex.php?latex=-S+dT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-S dT" class="latex" />, and could perform better than the linear estimate.  Of what practical significance this is, I have no idea.</p>
<p>But! I think that is the interesting mathematics: what are functions <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28+f%28x%2Bs%29-f%28x%29+%29%2Fs+%3D+g%28x%3Bs%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="( f(x+s)-f(x) )/s = g(x;s) " class="latex" /></p>
<p>is exact for all s, for some <img src="https://s0.wp.com/latex.php?latex=f%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)" class="latex" />?  <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> stores information about <img src="https://s0.wp.com/latex.php?latex=f%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, such that if you know <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> at a specific point <img src="https://s0.wp.com/latex.php?latex=x_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_0" class="latex" /> and you know <img src="https://s0.wp.com/latex.php?latex=g%28x_0%3Bs%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(x_0;s)" class="latex" />, you now know <img src="https://s0.wp.com/latex.php?latex=f%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)" class="latex" /> at EVERY point.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4065#respond' data-commentid="4065" data-postid="2467" data-belowelement="div-comment-4065" data-respondelement="respond" data-replyto="Reply to Eric Downes" aria-label='Reply to Eric Downes'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4081">
				<div id="div-comment-4081" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4081">12 February, 2011 at 6:09 am</a>		</div>

		<p>This is great stuff!</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even depth-3 highlander-comment" id="comment-4101">
				<div id="div-comment-4101" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/176e9ac7f3a85880bf6333b1c2be8a39?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Frederik De Roo</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4101">13 February, 2011 at 9:14 am</a>		</div>

		<p>John asked:</p>
<blockquote><p>
 What’s an “emiliano”?
</p></blockquote>
<p>It&#8217;s an inhabitant of the region <a href="http://it.wikipedia.org/wiki/Emilia-Romagna" rel="nofollow">Emilia-Romagna</a>.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4101#respond' data-commentid="4101" data-postid="2467" data-belowelement="div-comment-4101" data-respondelement="respond" data-replyto="Reply to Frederik De Roo" aria-label='Reply to Frederik De Roo'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 highlander-comment" id="comment-4110">
				<div id="div-comment-4110" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4110">14 February, 2011 at 3:10 am</a>		</div>

		<p>Everyone interested in these issues should check out Tomate&#8217;s <a href="http://tomate.blogsome.com/2011/02/10/renyi-entropy-and-free-energy/" rel="nofollow">blog entry</a> on R&eacute;nyi entropy and free energy.</p>
<p>Frederik explained what an &#8220;emiliano&#8221; is:</p>
<blockquote>
<p>It’s an inhabitant of the region Emilia-Romagna.</p>
</blockquote>
<p>Tomate also emailed me and explained what a &#8220;buontempone&#8221; is: someone who likes to waste time, living a light and cheery life.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4110#respond' data-commentid="4110" data-postid="2467" data-belowelement="div-comment-4110" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent highlander-comment" id="comment-4029">
				<div id="div-comment-4029" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/8c446c3d1711a4e28dbf00133a229109?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Allen Knutson</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4029">11 February, 2011 at 2:48 am</a>		</div>

		<p>By the way, Renyi had a book on probability theory, that in the original Hungarian was entitled Valoszinusegszamitas (modulo accents). The German translation was entitled Wahrscheinlichkeitsrechnung. I have no point in telling you this, other than my amusement at foolishly long words.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4029#respond' data-commentid="4029" data-postid="2467" data-belowelement="div-comment-4029" data-respondelement="respond" data-replyto="Reply to Allen Knutson" aria-label='Reply to Allen Knutson'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 highlander-comment" id="comment-4043">
				<div id="div-comment-4043" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4043">11 February, 2011 at 8:21 am</a>		</div>

		<p>So, you&#8217;d like the <a href="http://upload.wikimedia.org/wikipedia/commons/4/4e/De-Rinderkennzeichnungs-_und_Rindfleischetikettierungs%C3%BCberwachungsaufgaben%C3%BCbertragungsgesetz.ogg" rel="nofollow">Rinderkennzeichnungs- und Rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz</a>.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4043#respond' data-commentid="4043" data-postid="2467" data-belowelement="div-comment-4043" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-2 parent highlander-comment" id="comment-4059">
				<div id="div-comment-4059" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/face81a4a2e39b202dd43a85aacd59ca?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Mark Meckes</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4059">11 February, 2011 at 2:24 pm</a>		</div>

		<p>I find it pretty amusing to pick apart the etymology of the German title.  The basic roots are &#8220;wahr&#8221;=&#8221;true&#8221;, &#8220;scheinen&#8221;=&#8221;to seem&#8221;, and &#8220;rechnen&#8221;=&#8221;to calculate&#8221; (cognate with English &#8220;to reckon&#8221;); the suffixes &#8220;-lich&#8221; and &#8220;-keit&#8221; are roughly equivalent to &#8220;-ly&#8221; and &#8220;-ness&#8221;. &#8220;wahrscheinlich&#8221;, literally &#8220;true-seeming&#8221;, is German for &#8220;probable&#8221; or &#8220;probably&#8221;, and &#8220;Wahrscheinlichkeit&#8221; is &#8220;probability&#8221;.  Thus &#8220;Wahrscheinlichkeitsrechnung&#8221;, properly translated as something like &#8220;Computation of probabilities&#8221; or &#8220;The calculus of probability&#8221;, can be literally interpreted as &#8220;Reckoning about how true stuff seems to be&#8221;.</p>
<p>Those long German compounds look intimidating, but in some respects German is much friendlier to non-native speakers than English: complicated German words tend to be built out of simple German words, whereas complicated English words tend to be built out of Latin and Greek words.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4059#respond' data-commentid="4059" data-postid="2467" data-belowelement="div-comment-4059" data-respondelement="respond" data-replyto="Reply to Mark Meckes" aria-label='Reply to Mark Meckes'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 highlander-comment" id="comment-4112">
				<div id="div-comment-4112" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4112">14 February, 2011 at 5:20 am</a>		</div>

		<p>Mark wrote:</p>
<blockquote><p>
I find it pretty amusing to pick apart the etymology of the German title. The basic roots are “wahr”=”true”, “scheinen”=”to seem”, and “rechnen”=”to calculate” (cognate with English “to reckon”); the suffixes “-lich” and “-keit” are roughly equivalent to “-ly” and “-ness”. “wahrscheinlich”, literally “true-seeming”, is German for “probable” or “probably”, and “Wahrscheinlichkeit” is “probability”.
</p></blockquote>
<p>By the way, I just noticed that the letter W on Boltzmann&#8217;s tomb in Vienna, which stands for entropy, really comes from the word Wahrscheinlichkeit.</p>
<div align="center">
<a href="http://en.wikipedia.org/wiki/Ludwig_Boltzmann#Physics" rel="nofollow"><br />
<img src="http://upload.wikimedia.org/wikipedia/en/thumb/6/63/Zentralfriedhof_Vienna_-_Boltzmann.JPG/450px-Zentralfriedhof_Vienna_-_Boltzmann.JPG"/><br />
</a>
</div>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4112#respond' data-commentid="4112" data-postid="2467" data-belowelement="div-comment-4112" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even depth-2 parent highlander-comment" id="comment-4069">
				<div id="div-comment-4069" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4069">11 February, 2011 at 11:51 pm</a>		</div>

		<p>I&#8217;ve always liked the French word <i>vraisemblable</i>, meaning likely or probable, and often heard in <i>vraisemblablement</i>, meaning probably.  <i>Vrai</i> is true, <i>sembler</i> is to seem, and <i>-able</i> is more or less the same as in English.  <i>Semblable</i> therefore means seem-able, or similar.  Hence <i>vraisemblable</i> is true-seem-able, or true-similar, or probable.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4069#respond' data-commentid="4069" data-postid="2467" data-belowelement="div-comment-4069" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment odd alt depth-3 highlander-comment" id="comment-4071">
				<div id="div-comment-4071" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/face81a4a2e39b202dd43a85aacd59ca?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Mark Meckes</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4071">12 February, 2011 at 1:04 am</a>		</div>

		<p>That is nice. I never knew vraisemblable, since probability theory is usually instead referred to as &#8220;probabilités&#8221; in French. (The root here is apparently the Latin &#8220;probare&#8221;, &#8220;to test&#8221;.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4071#respond' data-commentid="4071" data-postid="2467" data-belowelement="div-comment-4071" data-respondelement="respond" data-replyto="Reply to Mark Meckes" aria-label='Reply to Mark Meckes'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even depth-2 parent highlander-comment" id="comment-4093">
				<div id="div-comment-4093" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/f1fe3183e03a7e20a67e29c24151863c?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://ber.parawag.net' rel='external nofollow ugc' class='url'>Berényi Péter</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4093">13 February, 2011 at 1:19 am</a>		</div>

		<p>Title of Rényi&#8217;s book (with accents) is &#8220;<a href="http://hu.wikipedia.org/wiki/Val%C3%B3sz%C3%ADn%C5%B1s%C3%A9g-sz%C3%A1m%C3%ADt%C3%A1s" rel="nofollow">Valószínűségszámítás</a>&#8221; in Hungarian, I have a copy in my library. You may be interested in the structure of the word (which is usually translated as Probability Theory).</p>
<p>It is a compound word made of two parts, &#8220;valószínűség&#8221; and &#8220;számítás&#8221;.</p>
<p>Now, the first one is parsed like &#8220;való-szín-ű-ség&#8221;. In math it is a technical term and simply means &#8220;probability&#8221; but in common speech it has connotations like chance, feasibility, likelihood, odds, presumption, plausibility or verisimilitude.</p>
<p>As for its parts, &#8220;való&#8221; is the present participle of the substantive verb &#8220;van&#8221;. &#8220;Valódi&#8221; means actual, genuine, intrinsic, live, real, sheer, sterling, unimitative, something true to the core while &#8220;valóság&#8221; (like being-ness) is reality, actuality, deed, entity, positiveness or truth.</p>
<p>&#8220;Szín&#8221; is simply color, but also means tint, complexion, face or surface. The suffix &#8220;ű&#8221; makes an adjective of it (like tinct or colorous if there were a word like that) while &#8220;ség&#8221; corresponds to the English suffix -ness. All in all it is something having the complexion (or appearance) of reality.</p>
<p>The second one can be parsed as &#8220;szám-ít-ás&#8221;. &#8220;Szám&#8221; means number while the suffix &#8220;ít&#8221; makes a verb of it, so &#8220;számít&#8221; is to count or calculate (but also to matter or reckon). The suffix &#8220;ás&#8221; makes a noun again, &#8220;számítás&#8221; meaning calculus (but reckoning as well).</p>
<p>So the true meaning of the title is more like &#8220;Probability Calculus&#8221;.</p>
<p>By the way, there is not much about entropy in the book (none about Rényi entropy), it is only mentioned (and defined) twice in exercises, following chapters, as it is a (pretty good) textbook (written in 1966).</p>
<p>As you can see Hungarian words have quite some structure. I happen to know the guy (Gábor Prószéky) who is responsible for the Hungarian spell checker. He said it was utterly impossible to follow a dictionary approach to the problem, as he reckoned it would have required some 24 billion entries.</p>
<p>Once upon a time it was also part of my job to count Hungarian word forms and I came up with a slightly different lot, infinity. It really does not make much difference.</p>
<p>If you take a large volume of text from any language, order words according to decreasing frequency of occurrence and consider log frequency as a function of log rank, then after some initial fluctuation it usually settles to a straight line with negative slope of course. It means a distribution according to some power law and this feature looks quite universal. However, the exponent is not language-independent. For some languages it is below -1, so the larger the volume of text analyzed, the closer one gets to a full word list. On the other hand in Hungarian it is slightly above -1, so no matter how much text one takes, new words just keep trickling in. I guess it is a common feature of agglutinative languages including Navajo, Turkish or Finnish, although I have not checked them. By the way, at the sentence level all languages behave like this.</p>
<p>So the prerequisite of a Hungarian spell checker was an automatic morphological parser. And Dr. Prószéky (a mathematician and linguist) in fact had one for Hungarian well before applying it to a real life problem and going into business.</p>
<p>With such a language, no wonder native Hungarian speakers, starting from early childhood, have considerable practice in real time analysis of complex structures :)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4093#respond' data-commentid="4093" data-postid="2467" data-belowelement="div-comment-4093" data-respondelement="respond" data-replyto="Reply to Berényi Péter" aria-label='Reply to Berényi Péter'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 parent highlander-comment" id="comment-4099">
				<div id="div-comment-4099" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4099">13 February, 2011 at 7:47 am</a>		</div>

		<p>Fascinating remarks, Berényi Péter!  And they&#8217;re even related to the main topic of this blog post, information theory, since the statistical study of language &mdash; such as that linear &#8216;log frequency versus log rank&#8217; relation you mentioned, called <a href="http://en.wikipedia.org/wiki/Zipf%27s_law" rel="nofollow">Zipf&#8217;s law</a> &mdash; is connected to information theory!  </p>
<p>While we&#8217;re at it: Hungarian is exciting and strange because it&#8217;s  a <a href="http://en.wikipedia.org/wiki/Finno-Ugric_languages" rel="nofollow">Finno-Ugric language</a>.  Do other Finno-Ugric languages, like Finnish and Estonian, share the unusual properties you mentioned?  I don&#8217;t know if they&#8217;re agglutinative like Hungarian.  </p>
<p>(I could look it up, but I like to think that Google hasn&#8217;t yet made conversation obsolete!)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4099#respond' data-commentid="4099" data-postid="2467" data-belowelement="div-comment-4099" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-4 highlander-comment" id="comment-4106">
				<div id="div-comment-4106" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/f1fe3183e03a7e20a67e29c24151863c?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://ber.parawag.net' rel='external nofollow ugc' class='url'>Berényi Péter</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4106">13 February, 2011 at 7:54 pm</a>		</div>

		<p>Yes, Zipf&#8217;s law. However, as a thorough theoretical understanding of this phenomenon is still missing, it is more like an empirical regularity than a law.</p>
<p>There may be an underlying critical state attained by <a href="http://en.wikipedia.org/wiki/Self-organized_criticality" rel="nofollow">Self Organized Criticality</a> (SOC), like in Sandpile Avalanche Dynamics models. This kind of scale invariant behavior pops up in a wide variety of fields, including climate studies and may have connections to the somewhat mysterious <a href="http://en.wikipedia.org/wiki/Law_of_maximum_entropy_production#Principles_of_maximum_entropy_production_and_minimum_energy_dissipation" rel="nofollow">Maximum Entropy Production Principle</a> (MEPP) of (thermodynamically) open systems.</p>
<p>As for Hungarian, the language is generally considered to belong to the Finno-Ugric language family indeed. However, Finnish is not a close relative of Hungarian. Na&iuml;ve native speakers of either language are unable to identify a single phrase of the other language, and the relation is only uncovered by deep lingustic analysis. By the way, I do not think so called language families evolved along the same path biological evolution followed. In genetics channels of information flow are highly regulated (for so called &#8220;higher species&#8221; it&#8217;s sex) while in memetics it is not, or at least if <a href="http://www.biolinguagem.com/biolinguagem_antropologia/bickerton_1983_creole_languages.pdf" rel="nofollow">Bickerton&#8217;s research on Hawaiian Creole</a> has merit, it is done in an entirely different way.</p>
<p>Therefore it is unlikely languages can be classified according to a 19th century dream about a well defined tree structure. In fact it looks like Hungarian has far reaching connections beyond the Finno-Ugric group including subgroups of the Altaic family like Turcic languages, possibly others as well. Some go as far as looking for connections to ancient Sumerian. Being a somewhat lunatic theory, it is not entirely out of the question though, because Sumerian used to be the lingua franca of the Middle East for three millenia and as such obviously had some influence on the surrounding regions, including the great plains north of the Caucasus Mountains. It was like Latin in medieval Europe (which happened to be the official language of the Hungarian Kingdom for eight hundred years). </p>
<p>As history was turbulent enough all over the world and populations got mixed intermittently, there must have been multiple episodes of partial pidginization followed by abrupt creolization (within a single generation) buried deep down below the surface of all conceivable languages.</p>
<p>I do not trust linguists too much, because I know how bone headed they can be. For example Dr. Pr&oacute;sz&eacute;ky used to work for the Linguistic Institute of HAS (Hungarian Academy of Sciences) in the early 1980s and was tasked with implementing the morphological ruleset of Hungarian in an automatic parser. By that time the science was considered long settled, so he was supposed to just formalize the rules found in textbooks, write some computer code that could interpret them and feed the program with plenty of text to see the result. And that&#8217;s what he did. However, the parsed output was full of all too obvious errors. He went back to the drawing board, repeatedly reformulated the ruleset, alas, to no avail, misparsed wordforms just kept coming. Finally he was forced to abandon textbook knowledge altogether and established a new utilitarian paradigm that judged the value of a ruleset based on its performance alone. The end result was a new ruleset, not akin to anything previously seen, that performed far better than the traditional one.</p>
<p>At that point he decided to publish his results, but met bitter resistance. The uproar was about his ruleset simply did not make sense at all from a linguistic point of view. People were absolutely unwilling to consider the possibility that in this case there might be some problem with linguistic theory itself.</p>
<p>As market demand for a good spell checker arose around the same time, he silently left Academia, started his own R+D company and developed one (with overwhelming success, one should say). Of course the product is built around the morphological ruleset he already had. The approach was developed further and now it is applicable to a wide range of linguistic problems and languages including syntactic analysis and machine translation. He is professor of language technology at P&aacute;zm&aacute;ny P&eacute;ter Catholic University by now.</p>
<p>I am not an expert in Finnish, but I guess a good Finnish spell checker also needs a morphological parser. However, as soon as one goes beyond the word level, any natural language gets mind bogglingly complicated even if 3 years old kids seem to cope well with this kind of complexity.</p>
<p>Currently I am reading <a href="http://www.huffingtonpost.com/albertlaszlo-barabasi/bursts-can-human-behavior_b_556030.html" rel="nofollow">Bursts</a> by Albert-L&aacute;szl&oacute; Barab&aacute;si (another Hungarian, or <a href="http://encyclopedia.thefreedictionary.com/szekler" rel="nofollow">Szekler</a>, to be more precise). Besides being an awesome read in itself, also gives some insight into the field of complex networks and systems.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4113">
				<div id="div-comment-4113" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4113">14 February, 2011 at 5:56 am</a>		</div>

		<p>Berényi wrote:</p>
<blockquote>
<p>Yes, Zipf’s law.  However, as a thorough theoretical understanding of this phenomenon is still missing, it is more like an empirical regularity than a law.</p>
</blockquote>
<p>It&#8217;s possible to derive Zipf&#8217;s law from information-theoretic assumptions:</p>
<p>&bull; J. M. M. J. Vogels, <a href="http://home.deds.nl/~essentiae/Zipf%20entropy.pdf" rel="nofollow">Entropy related remarks on Zipf&#8217;s law for word frequencies</a>.</p>
<p>It would be nice if this law arose from some tendency for communication to maximize information transfer.  But there have been lots of other attempts to derive Zipf&#8217;s law, some of which are summarized in this paper.  It&#8217;s even possible to get Zipf&#8217;s law in &#8216;completely random&#8217; texts:</p>
<p>&bull; Wentian Li, <a href="http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=0B1AC4BC3DB2C33E1091A13E50F879D9?doi=10.1.1.164.8422&amp;rep=rep1&amp;type=pdf" rel="nofollow">Random texts exhibit Zipf’s-law-like word frequency distribution</a></p>
<p>So, I don&#8217;t know what&#8217;s really going on with Zipf&#8217;s law.</p>
<blockquote>
<p>I do not trust linguists too much, because I know how bone headed they can be. </p>
</blockquote>
<p>I should warn you that we don&#8217;t allow insults on this blog, even of this fairly weak sort: &#8220;I know that in some large set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> there exist members <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> who are bone headed&#8221;.   Insults tend to breed further insults, and distract from what I&#8217;m trying to do here.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-4114">
				<div id="div-comment-4114" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/656e05d084448337fb49459225dc525e?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">DavidTweed</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4114">14 February, 2011 at 4:16 pm</a>		</div>

		<p>Zipf&#8217;s law is an example of a <a href="http://www.azimuthproject.org/azimuth/show/Power+law" rel="nofollow">(probability disrbiution which is a) power law</a>. One of the difficulties is that it can be difficult to show purely empirically that a power law is what the data is following (rather than, eg, a log-normal). So I&#8217;d be cautious in assessing with any &#8220;explanatory but non-predictive&#8221; theories which require the istribution to be a power law (rather than a weaker property like being heavy tailed).</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 highlander-comment" id="comment-4115">
				<div id="div-comment-4115" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4115">14 February, 2011 at 7:51 pm</a>		</div>

		<p>And, of course, &#8220;It&#8217;s a power law!&#8221; is hardly the end of any story, even if it&#8217;s true (that is, even if you haven&#8217;t <a href="http://www.ncbi.nlm.nih.gov/pubmed/20023717" rel="nofollow">fooled yourself</a> by doing sloppy statistics, or constructed a meaningless network by forgetting the actual chemistry or biology of the thing you&#8217;re trying to study). </p>
<p>For example, everybody loves &#8220;scale-free networks&#8221;: collections of nodes and links in which the probability that a node has <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> connections falls off as a power-law function of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" />.  But <i>the degree distribution does not by itself characterize a network!</i> Two networks can be quite different but have identical degree distributions. For example, consider the &#8220;clustering coefficient&#8221;, defined as the probability that two neighbours of a node will themselves be directly connected. (It measures the &#8220;cliquishness&#8221; of the network, in a way.) One can build networks with indistinguishable power-law degree distributions but arbitrarily different clustering coefficients.</p>
<p>The NetworkX Python module has a built-in function to do just this: <a href="http://networkx.lanl.gov/reference/generated/networkx.generators.random_graphs.powerlaw_cluster_graph.html#networkx.generators.random_graphs.powerlaw_cluster_graph" rel="nofollow">powerlaw_cluster_graph()</a>.</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even depth-3 highlander-comment" id="comment-4295">
				<div id="div-comment-4295" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/8c446c3d1711a4e28dbf00133a229109?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Allen Knutson</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4295">2 March, 2011 at 3:53 pm</a>		</div>

		<p><i>native speakers of either language are unable to identify a single phrase of the other language</i></p>
<p>I was taught that the common vocabulary was about body parts and fish, such that there was some sentence about grabbing a fish with one&#8217;s hand that was comprehensible to speakers of either language.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4295#respond' data-commentid="4295" data-postid="2467" data-belowelement="div-comment-4295" data-respondelement="respond" data-replyto="Reply to Allen Knutson" aria-label='Reply to Allen Knutson'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt thread-even depth-1 highlander-comment" id="comment-4035">
				<div id="div-comment-4035" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4035">11 February, 2011 at 5:10 am</a>		</div>

		<p>David used to study statistical inference and machine learning, and this led him to become interested in some variants of the ordinary notion of entropy.  So, anyone interested should read his old blog posts on <a href="http://math.ucr.edu/home/baez/corfield/2006/06/tsallis-entropy.html" rel="nofollow">Tsallis entropy</a> and <a href="http://math.ucr.edu/home/baez/corfield/2006/07/renyi-entropy.html" rel="nofollow">R&eacute;nyi entropy</a>.   I just remembered these posts now.  They have some nice references that I should reread, like this:</p>
<p>&bull;  Peter Harrem&ouml;es, <a href="http://arxiv.org/abs/math-ph/0510002" rel="nofollow">Interpretations of Renyi entropies and divergences</a>.</p>
<p>I&#8217;ve explained relative entropy <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="nofollow">here</a>.  Harrem&ouml;es shows that the &#8216;R&eacute;nyi divergence&#8217;, or <i>relative</i> R&eacute;nyi entropy, measures how much a probabilistic mixture of two codes can be compressed.  </p>
<p>Relative R&eacute;nyi entropy is also discussed here:</p>
<p>&bull; E. Lutwak, D. Yang and G. Zhang, <a href="http://www.deaneyang.com/papers/renyi1d.pdf" rel="nofollow">Cramer-Rao and moment-entropy inequalities for Renyi entropy and generalized Fisher information</a>, <i>IEEE Transactions on Information Theory</i> <b>51</b> (2005). 473-478. </p>
<p>and I&#8217;m sure in dozens of other papers &mdash; but I just happen to be getting interested in Cramer-Rao inequalities, so this is nice.  </p>
<p>By the way, I think the name &#8216;Harrem&ouml;es&#8217; is really cool.  It reminds me of Averr&ouml;es.  I didn&#8217;t think people had cool names like that anymore.  <a href="http://en.wikipedia.org/wiki/Averroes" rel="nofollow">&#8216;Averr&ouml;es&#8217;</a> was a Latinization of Ibn Rushd, although this philosopher was also known as Ibin-Ros-din, Filius Rosadis, Ibn-Rusid, Ben-Raxid, Ibn-Ruschod, Den-Resched, Aben-Rassad, Aben-Rois, Aben-Rasd, Aben- Rust, Avenrosdy Avenryz, Adveroys, Benroist, Avenroyth, Averroysta, and &#8220;Hey, you!  Philosopher dude!&#8221;</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4035#respond' data-commentid="4035" data-postid="2467" data-belowelement="div-comment-4035" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent highlander-comment" id="comment-4041">
				<div id="div-comment-4041" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4041">11 February, 2011 at 7:19 am</a>		</div>

		<p>This is really a reply to your <a href="http://golem.ph.utexas.edu/category/2011/02/rnyi_entropy_and_free_energy.html" rel="nofollow"><i>n</i>-Caf&eacute;</a> post, but I see you&#8217;re disallowing comments there, presumably so that they&#8217;re all in one place here.  </p>
<p>Anyway, you point out there a couple of previous blog posts by David Corfield on R&eacute;nyi entropy and its close relative, Tsallis entropy.  I just wanted to point out another n-Caf&eacute; post where R&eacute;nyi entropy came up: <a href="http://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html" rel="nofollow">Entropy, Diversity and Cardinality (Part 1)</a>. </p>
<p>While I&#8217;m writing, here&#8217;s a historical note on what physicists call &#8220;Tsallis entropies&#8221;.  </p>
<p>As far as I know, they were first discovered in the context of information theory:</p>
<p>&bull; J. Havrda and F. Charv&aacute;t, Quantification method of classification processes: Concept of structural alpha-entropy.  <i>Kybernetika</i> <b>3</b> (1967), 30-35.</p>
<p>(It&#8217;s a Czech journal which I had some trouble getting hold of; my friend Jirka Velebil in Prague found a copy for me.)  There&#8217;s a slight caveat here: they were working with logarithms to base 2, as information theorists are wont to do, and accordingly the expression looks a bit different.</p>
<p>I&#8217;ve read somewhere that they were independently discovered around the same time (I think a year or two later) by Z. Dar&oacute;czy.  They are certainly studied in the book</p>
<p>&bull; J. Acz&eacute;l, Z. Dar&oacute;czy, <i>On Measures of Information and Their Characterizations</i>.  Academic, New York, 1975.</p>
<p>In general, this is an excellent reference for entropies of all different kinds (including R&eacute;nyi).  If you ever want a unique characterization theorem for a pre-1975 entropy, you&#8217;re very likely to find it here.</p>
<p>The Havrda-Charv&aacute;t entropies were rediscovered in statistical ecology in the 1980s:</p>
<p>&bull; G.P. Patil and C. Taillie, Diversity as a concept and its measurement.  <i>Journal of the American Statistical Association</i> <b>77</b> (1982), 548-561.</p>
<p>This time it&#8217;s to base e, so the formula is exactly the same as the one used by Tsallis when he rediscovered them (again!) in physics in 1988:</p>
<p>&bull; C. Tsallis, Possible generalization of Boltzmann-Gibbs statistics.  <i>Journal of Statistical physics</i> <b>52</b> (1988), 479-487.</p>
<p>There remains the question of what to call these things.  If you&#8217;re writing for physicists I suppose you call them &#8220;Tsallis entropies&#8221;, because (a) although he was by no means first to write down the formula or study it mathematically, he was apparently first to see its physical significance, (b) historical priority never stopped something being named after the wrong person anyway, and (c) everyone else does.  If you&#8217;re not writing for physicists, it&#8217;s up to you. </p>
<p>Of course, the Havrda-Charv&aacute;t-Dar&oacute;czy-Patil-Taillie-Tsallis entropies are a simple transformation of the R&eacute;nyi entropies, which came earlier than any of them.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4041#respond' data-commentid="4041" data-postid="2467" data-belowelement="div-comment-4041" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 parent highlander-comment" id="comment-4042">
				<div id="div-comment-4042" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4042">11 February, 2011 at 8:09 am</a>		</div>

		<p>Thanks for all those references, Tom!  I&#8217;ve been crossposting a bit between here and the <i>n</i>-Caf&eacute;, but only allowing comments at one place, since it seems a bit silly to have separate conversations in two rooms about the same subject.  Information theory here, category theory there &mdash; that&#8217;s how I&#8217;m trying to handle it. </p>
<p>Your references are really helpful&#8230; I need to get to the bottom of some questions, now.  Like this:</p>
<p>If Tsallis was trying to generalize Boltzmann-Gibbs statistics (&sim; ordinary statistical mechanics) by replacing ordinary entropy with Tsallis entropy, and Tsallis entropy is a simple transform of the R&eacute;nyi entropy, and R&eacute;nyi entropy is just a fudge factor times the free energy (a well-known concept in ordinary statistical mechanics), isn&#8217;t there perhaps less going on here than meets the eye?  A lot of smoke and gunfire, but fewer concepts shooting it out than you might think?</p>
<p>Was perhaps Tsallis secretly just replacing the principle of maximum entropy by the principle of minimum free energy? </p>
<p>That would be odd: it would make his work seem unnecessary.  The idea of minimizing free energy goes back to <a href="http://en.wikipedia.org/wiki/Thermodynamic_free_energy" rel="nofollow">Gibbs</a>, and it amounts to maximizing entropy subject to a constraint: a well-understood idea among physicists.</p>
<p>Presumably there&#8217;s something more to Tsallis&#8217; work than this, but perhaps my null hypothesis should be that there&#8217;s not.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4042#respond' data-commentid="4042" data-postid="2467" data-belowelement="div-comment-4042" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-3 parent highlander-comment" id="comment-4127">
				<div id="div-comment-4127" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.kent.ac.uk/secl/philosophy/staff/corfield.html' rel='external nofollow ugc' class='url'>David Corfield</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4127">15 February, 2011 at 12:40 pm</a>		</div>

		<blockquote>
<p>Information theory here, category theory there &#8211; that&#8217;s how I’m trying to handle it.</p>
</blockquote>
<p>But what to do when they happen at the same time? I was trying to figure out a category theoretic  version of Bayesian networks <a href="http://golem.ph.utexas.edu/category/2007/12/progic_v.html" rel="nofollow">here</a>. Now I see Coecke and Spekkens are <a href="http://arxiv.org/abs/1102.2368" rel="nofollow">applying</a> the picture calculus of monoidal categories to Bayesian inference.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4127#respond' data-commentid="4127" data-postid="2467" data-belowelement="div-comment-4127" data-respondelement="respond" data-replyto="Reply to David Corfield" aria-label='Reply to David Corfield'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4128">
				<div id="div-comment-4128" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4128">15 February, 2011 at 12:59 pm</a>		</div>

		<p>I wonder if their work gives a category-theoretic foundation to the existing theory of Bayesian networks.  That&#8217;s one thing I want to do.  I&#8217;d completely forgotten you wanted to do it!   <img src="https://i1.wp.com/math.ucr.edu/home/baez/emoticons/redface.gif" alt="" /></p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-7175">
				<div id="div-comment-7175" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-7175">19 August, 2011 at 11:07 pm</a>		</div>

		<p>If you put <a href="http://arxiv.org/abs/1102.2368" rel="nofollow">Coecke and Spekkens</a> together with <a href="http://arxiv.org/abs/0911.1763" rel="nofollow">Harper</a>, I suppose you discover that, in the properly simplified scenario, natural selection happens in a dagger-compact category with commutative Frobenius structure.</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt depth-2 parent highlander-comment" id="comment-4054">
				<div id="div-comment-4054" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4054">11 February, 2011 at 11:49 am</a>		</div>

		<p>Hi John.  Personally I tend to recoil a little bit when people send me references, because although of course it can be extremely helpful, I can&#8217;t suppress the small voice saying &#8220;oh no! now I have to look them up!&#8221;  But perhaps you&#8217;re made of sterner stuff.</p>
<p>Anyway, I guess my main points were (i) that Tsallis entropy is in some sense a misnomer (the main reason for giving most of those references), and (ii) the book by Aczel and Dar&oacute;czy is pretty useful for entropy characterization theorems.</p>
<p>I won&#8217;t presume to pronounce on the physics of it.  But for the purposes I&#8217;m interested in, I tend to regard the fundamental quantity as not Tsallis entropy, nor R&eacute;nyi entropy, but <i>the exponential</i> of R&eacute;nyi entropy.  Some reasons are sketched in that <a href="http://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html" rel="nofollow">Caf&eacute; post</a> I cited, where I call this quantity &#8220;cardinality&#8221; (of order <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />).  Thus, a uniform probability distribution on a set of cardinality n has cardinality n.  </p>
<p>But then the &#8220;fudge factor&#8221; that you mention becomes what I suppose we must call a &#8220;fudge power&#8221;, which sounds a bit less innocuous, and also like a sugar high.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4054#respond' data-commentid="4054" data-postid="2467" data-belowelement="div-comment-4054" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-3 parent highlander-comment" id="comment-4061">
				<div id="div-comment-4061" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/8e48939cdd0316335e2a171c4b76ceb8?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.eqnets.com' rel='external nofollow ugc' class='url'>Steve Huntsman</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4061">11 February, 2011 at 3:55 pm</a>		</div>

		<p>Seems to me like Tsallis entropy is for when you don&#8217;t really know how to count DoFs.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4061#respond' data-commentid="4061" data-postid="2467" data-belowelement="div-comment-4061" data-respondelement="respond" data-replyto="Reply to Steve Huntsman" aria-label='Reply to Steve Huntsman'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment odd alt depth-4 highlander-comment" id="comment-4077">
				<div id="div-comment-4077" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dc21eed3ebc499422d6c4a927c71ac4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://mobjectivist.blogspot.com' rel='external nofollow ugc' class='url'>WebHubTel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4077">12 February, 2011 at 2:26 am</a>		</div>

		<blockquote><p>Seems to me like Tsallis entropy is for when you don’t really know how to count DoFs.</p></blockquote>
<p>That makes some sense. I have read that Tsallis entropy is used when you want to explain a power-law distribution. But then again you can always generate a power-law by invoking a <a href="http://en.wikipedia.org/wiki/Ratio_distribution" rel="nofollow"><i>ratio distribution</i></a> argument on two exponentials, which essentially says that you are adding DOF to the system.</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-3 highlander-comment" id="comment-4131">
				<div id="div-comment-4131" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4131">15 February, 2011 at 2:32 pm</a>		</div>

		<p>Tom wrote:</p>
<blockquote>
<p>the book by Aczel and Daróczy is pretty useful for entropy characterization theorems.</p>
</blockquote>
<p>Do you remember if it gives some nice theorems that single out the R&eacute;nyi entropies?  This reference:</p>
<p>&bull; J. Uffink, <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.6392" rel="nofollow">Can the maximum entropy principle be explained as a consistency requirement?</a>, Sec. 6: Justification by consistency: Shore and Johnson, Stud. Hist. Phil. Sci. B26 (1995), 223-261.</p>
<p>shows that Shore and Johnson&#8217;s attempt to characterize the usual Shannon entropy by a certain list of axioms actually fails, but succeeds in characterizing R&eacute;nyi entropies in general.  I would like to see more theorems like this!</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4131#respond' data-commentid="4131" data-postid="2467" data-belowelement="div-comment-4131" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-3 highlander-comment" id="comment-4132">
				<div id="div-comment-4132" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/face81a4a2e39b202dd43a85aacd59ca?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Mark Meckes</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4132">15 February, 2011 at 3:23 pm</a>		</div>

		<p>I happen to have the book by Aczel and Daróczy in front of me (as it happens, because Tom pointed me to it), so I can tell you that they show Rényi entropies are the only additive entropies in some more general class.  Some of the technical results in the same section address quite directly your question below: &#8220;what’s so great about raising a probability to a power?&#8221;</p>
<p><a href="http://arxiv.org/abs/1102.2618" rel="nofollow">This preprint</a> which appeared this morning (and was co-authored by a mathematical sibling of mine) doesn&#8217;t mention entropy explicitly but is about a characterization of <img src="https://s0.wp.com/latex.php?latex=L%5Ep&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^p" class="latex" /> norms which appears to be closely related.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4132#respond' data-commentid="4132" data-postid="2467" data-belowelement="div-comment-4132" data-respondelement="respond" data-replyto="Reply to Mark Meckes" aria-label='Reply to Mark Meckes'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-3 parent highlander-comment" id="comment-4134">
				<div id="div-comment-4134" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4134">15 February, 2011 at 5:52 pm</a>		</div>

		<p>As Mark just said, yes, the book of Acz&eacute;l and Dar&oacute;czy has at least one characterization theorem for R&eacute;nyi entropies.  I think it&#8217;s the same as the one R&eacute;nyi gave in his original paper introducing his entropies.  </p>
<p>I don&#8217;t know whether I&#8217;d use the word &#8220;nice&#8221;.  My current impression (based on only superficial browsing) is that this theorem concerns what R&eacute;nyi calls &#8220;incomplete probability distributions&#8221;, i.e. vectors <img src="https://s0.wp.com/latex.php?latex=%28p_1%2C+%5Cldots%2C+p_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1, &#92;ldots, p_n)" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=p_i+%5Cgeq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;geq 0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+p_i+%5Cleq+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i p_i &#92;leq 1" class="latex" />.  Personally I&#8217;d prefer a theorem that stuck to ordinary distributions (<img src="https://s0.wp.com/latex.php?latex=%5Csum_i+p_i+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i p_i = 1" class="latex" />).  But maybe I&#8217;m not seeing the point.</p>
<p>However, I&#8217;ve just been looking at a different characterization: </p>
<p>&bull; R.D. Routledge, Diversity indices: which ones are admissible?  <i>Journal of Theoretical Biology</i> <b>76</b> (1979), 503-515.</p>
<p>This characterizes the Hill numbers, which are simply what ecologists call the exponential of the R&eacute;nyi entropies.  To my mind, it seems a more satisfying result, but I don&#8217;t have the energy to explain what it says right now.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4134#respond' data-commentid="4134" data-postid="2467" data-belowelement="div-comment-4134" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment odd alt depth-4 highlander-comment" id="comment-4139">
				<div id="div-comment-4139" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/face81a4a2e39b202dd43a85aacd59ca?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Mark Meckes</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4139">16 February, 2011 at 2:55 pm</a>		</div>

		<p>I think (also based on only superficial browsing) that Aczél and Daróczy give multiple characterization theorems for Rényi entropies.  One is Rényi&#8217;s theorem involving functionals of &#8220;incomplete probability distributions&#8221;, but they have another characterization that only deals with ordinary distributions.  The tradeoff is that the latter result makes stronger assumptions on the nature of the functional, restricting to what they call &#8220;average entropies&#8221;.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-4144">
				<div id="div-comment-4144" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4144">16 February, 2011 at 3:17 pm</a>		</div>

		<p>Thanks; I&#8217;ll take a look.  </p>
<p>Incidentally, my copy of Aczél and Daróczy makes me a little bit sad.  It&#8217;s out of print, so I ordered a second-hand copy from Amazon, and when it arrived it turned out that it used to belong to a university library (Wright State University in Ohio).  So they, presumably, no longer have a copy of this nice book.  Also, whereas the price to me was quite substantial, I bet they made at most a trivial amount of money from selling it.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4151">
				<div id="div-comment-4151" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4151">17 February, 2011 at 8:07 am</a>		</div>

		<p>Tom wrote:</p>
<blockquote>
<p>Incidentally, my copy of Aczél and Daróczy makes me a little bit sad.</p>
</blockquote>
<p>Yes, that <i>is</i> sad.  Kinda like buying jewelry and finding it was pawned by a now-destitute queen.  </p>
<p>But most libraries sell books not to make money, but to clear out space.  They are constantly short of space.  Books not sufficiently used eventually get weeded out.  So if you want to feel sorry for Wright State University, you should probably feel sorry that they can&#8217;t afford to expand their library.  At least the book went to someone who will appreciate it!</p>
<p>The U. C. Riverside library is suffering a severe space shortage, and it&#8217;ll only get worse with the proposed <a href="http://www.universityofcalifornia.edu/news/article/24835" rel="nofollow">$500 million budget cut for the University of California system</a>.  On the &#8220;bright side&#8221;, we&#8217;re too broke to buy many new books: the library can&#8217;t easily reduce expenditures on journals, thanks to <a href="http://www.eurekajournalwatch.org/index.php?title=Journal_bundling&amp;redirect=no" rel="nofollow">journal bundling</a>, so the books and library staff take most of the budget hit.</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-4048">
				<div id="div-comment-4048" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4048">11 February, 2011 at 9:12 am</a>		</div>

		<p>Here&#8217;s a half-baked idea which I&#8217;ve had in my head for a while. I&#8217;m posting this here in the hope that someone knows about this.</p>
<p>To begin, it is <a href="http://en.wikipedia.org/wiki/Newton%27s_identities" rel="nofollow">old hat</a> that every permutation-invariant polynomial in the variables <img src="https://s0.wp.com/latex.php?latex=p_1%2C%5Cldots%2Cp_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1,&#92;ldots,p_n" class="latex" /> can be written as a polynomial of the power sums</p>
<p><img src="https://s0.wp.com/latex.php?latex=D%28%5Cbeta%29%3D%5Csum_i+p_i%5E%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D(&#92;beta)=&#92;sum_i p_i^&#92;beta" class="latex" />,</p>
<p>where we take <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> to range over the naturals.</p>
<p>Now, is there an analogous statement for permutation-invariant functions, where then <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> would be allowed to range over the positive reals? Which class of functions would one choose to do this?<br />
In other words, given any &#8216;natural&#8217; function of the probabilities, can it be written as a function of the Rényi entropies?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4048#respond' data-commentid="4048" data-postid="2467" data-belowelement="div-comment-4048" data-respondelement="respond" data-replyto="Reply to Tobias Fritz" aria-label='Reply to Tobias Fritz'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 parent highlander-comment" id="comment-4080">
				<div id="div-comment-4080" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4080">12 February, 2011 at 4:20 am</a>		</div>

		<p>Hi, Tobias!</p>
<blockquote><p>
In other words, given any ‘natural’ function of the probabilities, can it be written as a function of the Rényi entropies?
</p></blockquote>
<p>Yes, and it was thinking about this that led me to realize the relation between Rényi entropy and free energy.  Maybe I should mention this in my paper.</p>
<p>The idea goes like this.  The probabilities <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cdots%2C+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;dots, p_n" class="latex" /> can be recovered from the energies <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{-E_i} " class="latex" /></p>
<p>and these energies can be recovered from the &#8216;density of states&#8217; </p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28E%29+%3D+%5Csum_%7Bi+%3D+1%7D%5En+%5Cdelta%28E+-+E_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(E) = &#92;sum_{i = 1}^n &#92;delta(E - E_i) " class="latex" /></p>
<p>The Laplace transform of the density of states is the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29+%3D+%5Cint_0%5E%5Cinfty+f%28E%29+e%5E%7B-%5Cbeta+E%7D+%5C%2C+d+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(&#92;beta) = &#92;int_0^&#92;infty f(E) e^{-&#92;beta E} &#92;, d E " class="latex" /></p>
<p>so the density of states can be recovered by taking the inverse Laplace transform of the partition function.</p>
<p>Thus, you can recover the probabilities <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cdots+%2C+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;dots , p_n" class="latex" /> if you know the partition function for all <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cin+%5B0%2C+%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;in [0, &#92;infty)" class="latex" />.</p>
<p>But you can recover the partition function <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(&#92;beta)" class="latex" /> if you know the R&eacute;nyi entropy <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" />, thanks to my calculation above:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cln+Z%28%5Cbeta%29+%3D+%281+-+%5Cbeta%29+H_%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln Z(&#92;beta) = (1 - &#92;beta) H_&#92;beta " class="latex" /></p>
<p>So, the R&eacute;nyi entropies <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cin+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;in [0,&#92;infty)" class="latex" /> determine the probabilities <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cdots%2C+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;dots, p_n" class="latex" />, up to a permutation.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4080#respond' data-commentid="4080" data-postid="2467" data-belowelement="div-comment-4080" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-3 highlander-comment" id="comment-4100">
				<div id="div-comment-4100" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4100">13 February, 2011 at 8:07 am</a>		</div>

		<p>Nice! That is certainly the more natural way to phrase it, although my formulation sounds much more sophisticated ;)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4100#respond' data-commentid="4100" data-postid="2467" data-belowelement="div-comment-4100" data-respondelement="respond" data-replyto="Reply to Tobias Fritz" aria-label='Reply to Tobias Fritz'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1 parent highlander-comment" id="comment-4056">
				<div id="div-comment-4056" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/2c35fe624daf48f00e57289b3616c1a7?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.synapse9.com' rel='external nofollow ugc' class='url'>Phil Henshaw</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4056">11 February, 2011 at 1:10 pm</a>		</div>

		<p>Maybe someone would like to comment on the general &#8220;law of continuity&#8221; I found a few years ago.  It follows directly from energy conservation, that it takes the work of evolving a process to make any change.  That then implies there are lots of missing variables from the equations we assume apply to any phenomenon involving energy use, for what amount to the other components of entropy. The main one is the component of &#8220;entropy&#8221; that must necessarily be the energy used to build the system for using energy referred to.  </p>
<p>When you check out that basic map with observation, that all events require energy to build the process they use, it seems to check out in spades.  For me it gives entirely new face to entropy, that it comes from the net energy (surplus) that all macroscopic systems need to develop and operate at all times.  <a href="http://www.synapse9.com/drafts/LawOfContinuity.pdf" rel="nofollow ugc">http://www.synapse9.com/drafts/LawOfContinuity.pdf</a></p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4056#respond' data-commentid="4056" data-postid="2467" data-belowelement="div-comment-4056" data-respondelement="respond" data-replyto="Reply to Phil Henshaw" aria-label='Reply to Phil Henshaw'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-2 highlander-comment" id="comment-4076">
				<div id="div-comment-4076" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dc21eed3ebc499422d6c4a927c71ac4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://mobjectivist.blogspot.com' rel='external nofollow ugc' class='url'>WebHubTel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4076">12 February, 2011 at 2:12 am</a>		</div>

		<p>Phil, I think your theorem of the divergence of individual events looks similar to the principle of maximum entropy. Say you don&#8217;t know the acceleration of an object but you know that it must possess some mean value. Then the accelerations following an exponential distribution will be a conservative approximation given the limited information at your disposal.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4076#respond' data-commentid="4076" data-postid="2467" data-belowelement="div-comment-4076" data-respondelement="respond" data-replyto="Reply to WebHubTel" aria-label='Reply to WebHubTel'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1 highlander-comment" id="comment-4057">
				<div id="div-comment-4057" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">David Corfield</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4057">11 February, 2011 at 1:21 pm</a>		</div>

		<p>The light at the end of the tunnel is getting brighter when I&#8217;ll be able to get back to enjoyable stuff like this.</p>
<p>Have you caught any glimpse of the dual affine connection business of Amari yet? <a href="http://books.google.co.uk/books?id=vc2FWSo7wLUC&amp;pg=PA57" rel="nofollow">Here</a> he relates the associated divergences to Renyi entropy.</p>
<p>They held what looked like a fascinating <a href="http://www.mis.mpg.de/calendar/conferences/2010/infgeo/abstracts.html" rel="nofollow">meeting</a> on Information Geometry in Leipzig last Summer, at which Amari <a href="http://www.mis.mpg.de/calendar/conferences/2010/infgeo/abstract-1891-shun-ichi-amari.html" rel="nofollow">spoke</a>.</p>
<p>&#8220;A nonlinear transformation of a divergence function or a convex function causes a conformal change of the dual geometrical structure. In this context, we can discuss the dual geometry derived from the Tsallis or Renyi entropy. It again gives the dually flat structure to the family of discrete probability distributions or of positive measures.&#8221;</p>
<p>From when I looked at these things, I remember enjoying <a href="http://www.mitpressjournals.org/doi/abs/10.1162/0899766041336477?journalCode=neco" rel="nofollow">Stochastic Reasoning, Free Energy, and Information Geometry</a>.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4057#respond' data-commentid="4057" data-postid="2467" data-belowelement="div-comment-4057" data-respondelement="respond" data-replyto="Reply to David Corfield" aria-label='Reply to David Corfield'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent highlander-comment" id="comment-4060">
				<div id="div-comment-4060" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">David Corfield</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4060">11 February, 2011 at 2:56 pm</a>		</div>

		<p>Whoops, shouldn&#8217;t have used Markdown. As recompense, there&#8217;s a minus sign missing from equation 7 of the arXiv paper.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4060#respond' data-commentid="4060" data-postid="2467" data-belowelement="div-comment-4060" data-respondelement="respond" data-replyto="Reply to David Corfield" aria-label='Reply to David Corfield'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 highlander-comment" id="comment-4075">
				<div id="div-comment-4075" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4075">12 February, 2011 at 1:51 am</a>		</div>

		<p>Thanks, David!  I fixed your previous comment: it&#8217;s straight HTML on this blog, so you type this:</p>
<p>&lt;a href = &#8220;http://www.math.ntnu.no/~stacey/Mathforge/Azimuth/&#8221;&gt;Azimuth Forum&lt;/a&gt;</p>
<p>to create a link like this:</p>
<p><a href="http://www.math.ntnu.no/~stacey/Mathforge/Azimuth/" rel="nofollow">Azimuth Forum</a></p>
<p>(I know you know this, but I want to keep telling everyone reading this blog how it works.)</p>
<p>Thanks for catching a missing sign!  I&#8217;ve been catching lots of sign errors in my calculations.</p>
<p>Wow, a whole book on <i>Stochastic Reasoning, Free Energy, and Information Geometry</i>?  I&#8217;ll have to look at it &mdash; if they don&#8217;t point out the connection between free energy and R&eacute;nyi entropy, something is seriously amiss.   The funny thing is that nobody I know has much intuition for R&eacute;nyi entropy, while every physicist worth their salt has a deep understanding of free energy.  </p>
<p>(Some people working on <a href="http://en.wikipedia.org/wiki/Systems_ecology" rel="nofollow">systems ecology</a> and <a href="http://en.wikipedia.org/wiki/Thermoeconomics" rel="nofollow">thermoeconomics</a> seem to use <a href="http://en.wikipedia.org/wiki/Exergy" rel="nofollow">&#8220;exergy&#8221;</a> as a synonym for free energy.  It&#8217;s a cute name, and it avoids confusion with the <a href="http://en.wikipedia.org/wiki/Free_energy_suppression" rel="nofollow">free energy of conspiracy theories</a> &mdash; that is, energy that we could get for free, if only big business weren&#8217;t preventing us!  But it&#8217;s a shame to have essentially the same concept studied by three communities under three different names: free energy, R&eacute;nyi entropy and exergy.)</p>
<p>I think I&#8217;m sort of beginning to understand the &#8220;dual affine connections&#8221; business, and I&#8217;ll try to talk about that later in my series of posts on information geometry.  So far I&#8217;ve been trying to understand the Fisher information metric from a bunch of different viewpoints; I think this is a good first step. In my <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="nofollow">latest post in that series</a>, I reviewed relative entropy, so that next time I can prove the Fisher information metric is the Hessian of the relative entropy.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4075#respond' data-commentid="4075" data-postid="2467" data-belowelement="div-comment-4075" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-4082">
				<div id="div-comment-4082" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/6b3fa8cd421b5fd2028e3c1e9c32aa7f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Tim van Beek</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4082">12 February, 2011 at 8:53 am</a>		</div>

		<p>When you assume that a reader has not heard about Rényi entropy before, is there a tagline that explains why it is interesting and/or useful?</p>
<p>(I mean, the main blog post is about another way to think about Rényi entropy that could turn out to be useful, but what is the primary reason to think about it at all? Besides that it is additive.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4082#respond' data-commentid="4082" data-postid="2467" data-belowelement="div-comment-4082" data-respondelement="respond" data-replyto="Reply to Tim van Beek" aria-label='Reply to Tim van Beek'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 parent highlander-comment" id="comment-4094">
				<div id="div-comment-4094" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4094">13 February, 2011 at 2:28 am</a>		</div>

		<p>I have <i><b>no idea</b></i> why anyone else thinks R&eacute;nyi entropy is interesting or useful!  This is why I found myself annoyed to see papers discussing it without explaining what it means.  And that&#8217;s why I was happy to discover that it&#8217;s really just a slight modification of the concept of free energy.  Now I understand R&eacute;nyi entropy and I know why it&#8217;s interesting and useful.</p>
<p>Well, I&#8217;m exaggerating slightly.  I <i>do</i> understand a bit about why people care about these special cases of R&eacute;nyi entropy:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=H_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_0" class="latex" /> (the &#8216;max-entropy&#8217;)</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=H_1+%3D+-+%5Csum+p_i+%5Cln+p_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1 = - &#92;sum p_i &#92;ln p_i " class="latex" /> (the &#8216;Shannon entropy&#8217;, the really important one)</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=H_2+%3D+-+2+%5Cln+%5Csum+p_i%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_2 = - 2 &#92;ln &#92;sum p_i^2 " class="latex" /> (the &#8216;collision entropy&#8217;)</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=H_%5Cinfty+%3D+%5Clim_%7B%5Cbeta+%5Cto+%2B%5Cinfty%7D+H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;infty = &#92;lim_{&#92;beta &#92;to +&#92;infty} H_&#92;beta" class="latex" /> (the &#8216;min-entropy&#8217;)</p>
<p>For example, min-entropy and max-entropy show up if you ask an extreme pessimist or optimist how much work they can extract from a situation in which there&#8217;s some randomness (a &#8216;mixed state&#8217;).  This paper gives a nice explanation:</p>
<p>&bull; Oscar Dahlsten, Renato Renner, Elisabeth Rieper and Vlatko Vedral, <a href="http://arxiv.org/abs/0908.0424" rel="nofollow">On the work value of information</a>.</p>
<p>This paper makes a nice attempt to understand R&eacute;nyi entropy in general:</p>
<p>&bull;  Peter Harrem&ouml;es, <a href="http://arxiv.org/abs/math-ph/0510002" rel="nofollow">Interpretations of Renyi entropies and divergences</a>.</p>
<p>It includes a history of this question!</p>
<p>But basically, I think R&eacute;nyi entropy arose when R&eacute;nyi studied the axiomatic derivations of the ordinary Shannon entropy and noticed that if you removed some axioms while keeping the crucial additivity axiom</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p+%5Cotimes+q%29+%3D+S%28p%29+%2B+S%28q%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p &#92;otimes q) = S(p) + S(q) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=p+%5Cotimes+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;otimes q" class="latex" /> is the tensor product of probability distributions, you get a bunch of &#8216;generalized entropies&#8217;, which include the R&eacute;nyi entropies.  </p>
<p>As you know, people like to do that.   Someone gets a good idea: other people generalize it.</p>
<p>So, the idea of R&eacute;nyi entropy has been floating around at least 1960.  And, some researchers on machine learning and statistical inference seem to find it useful:</p>
<p>&bull; D. Erdogmuns and D. Xu, R&eacute;nyi&#8217;s entropy, divergence and their nonparametric estimators, in <i>Information Theoretic Learning: R&eacute;nyi&#8217;s Entropy and Kernel Perspectives</i>,<br />
ed. J. Principe, Springer, 2010, pp. 47-102.</p>
<p>But don&#8217;t ask me why!</p>
<p>Tom Leinster may have more to say about why R&eacute;nyi entropy is a natural measure of biodiversity &mdash; it&#8217;s used for that too.</p>
<p>Here&#8217;s an example of why I&#8217;m happy to see the relation between R&eacute;nyi entropy and free energy.   Free energy is additive, and this makes a lot of sense, physically.  So the additivity of R&eacute;nyi entropy makes sense to me now, because it&#8217;s just a temperature-dependent multiple of free energy.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4094#respond' data-commentid="4094" data-postid="2467" data-belowelement="div-comment-4094" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-3 parent highlander-comment" id="comment-4104">
				<div id="div-comment-4104" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/face81a4a2e39b202dd43a85aacd59ca?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Mark Meckes</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4104">13 February, 2011 at 2:58 pm</a>		</div>

		<p>One vague comment to be made is that the Rényi entropies interpolate in a natural way between the special cases you pointed out, just as <img src="https://s0.wp.com/latex.php?latex=L%5Ep&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^p" class="latex" /> spaces interpolate between <img src="https://s0.wp.com/latex.php?latex=L%5E1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=L%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=L%5E%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^&#92;infty" class="latex" />.  I&#8217;ll be lazy and not bother to explain details myself, but add that if you want to fit Rényi entropies into a bigger mathematical picture, you should spend some time pondering <a href="http://en.wikipedia.org/wiki/Generalized_mean" rel="nofollow">generalized means</a>.</p>
<p>Since I&#8217;ve spent some time understanding Tom&#8217;s work on biodiversity, I can say something about that, too.  Say the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are the relative sizes of populations of distinct species in a community.  (&#8220;Species&#8221; may refer to taxonomical species, or some other way of grouping organisms; likewise sizes may be measures of the number of individuals, or biomass, or something else.)  If you want to measure how diverse the community is, one idea you might have is this: randomly pick an individual and say how &#8220;typical&#8221; that individual&#8217;s species is in the community.  The more typical an average individual is, the less diverse the community.</p>
<p>So how can you quantify the typicality of a species, given only the relative abundances <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />?  The most obvious choice is to use <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> itself: the &#8220;typicality&#8221; of species <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> is precisely how common it is. This leads to <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+p_i+p_i+%3D+%5Csum_i+p_i%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i p_i p_i = &#92;sum_i p_i^2" class="latex" /> as a way to quantify how non-diverse the community is, so that <img src="https://s0.wp.com/latex.php?latex=H_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_2" class="latex" /> turns out to quantify how diverse the community is.</p>
<p>One could just as well, however, measure typicality by <img src="https://s0.wp.com/latex.php?latex=f%28p_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(p_i)" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=f%3A%5B0%2C1%5D+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f:[0,1] &#92;to &#92;mathbb{R}" class="latex" /> is any increasing function. If <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3E+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &gt; 1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+x%5E%7B%5Cbeta+-+1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = x^{&#92;beta - 1}" class="latex" />, this leads to using <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" /> as a reasonable way to measure diversity.  (I leave as an exercise the justification of <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" /> as a measure of diversity for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;le 1" class="latex" />.)  Using different values of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> gives more or less influence to rare species, similar to what John said about <img src="https://s0.wp.com/latex.php?latex=H_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=H_%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;infty" class="latex" /> reflecting the viewpoints of extreme pessimists or optimists.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4104#respond' data-commentid="4104" data-postid="2467" data-belowelement="div-comment-4104" data-respondelement="respond" data-replyto="Reply to Mark Meckes" aria-label='Reply to Mark Meckes'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4109">
				<div id="div-comment-4109" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4109">14 February, 2011 at 2:14 am</a>		</div>

		<p>Mark wrote:</p>
<blockquote>
<p>One vague comment to be made is that the Rényi entropies interpolate in a natural way between the special cases you pointed out, just as <img src="https://s0.wp.com/latex.php?latex=L%5Ep&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^p" class="latex" /> spaces interpolate between <img src="https://s0.wp.com/latex.php?latex=L%5E1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=L%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=L%5E%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^&#92;infty" class="latex" />.</p>
</blockquote>
<p>Indeed, when I started trying to understand the goofy-looking formula for R&eacute;nyi entropy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%5Calpha%28p%29+%3D+%5Cfrac%7B1%7D%7B1+-+%5Calpha%7D+%5Cln+%5Csum_i+p_i%5E%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;alpha(p) = &#92;frac{1}{1 - &#92;alpha} &#92;ln &#92;sum_i p_i^&#92;alpha" class="latex" /></p>
<p>I was reminded of the <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell^&#92;alpha" class="latex" /> norm:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7Cp%5C%7C_%5Calpha+%3D+%28%5Csum_i+p_i%5E%5Calpha%29%5E%7B1%2F%5Calpha%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;|p&#92;|_&#92;alpha = (&#92;sum_i p_i^&#92;alpha)^{1/&#92;alpha}" class="latex" /></p>
<p>(which people usually call the <img src="https://s0.wp.com/latex.php?latex=%5Cell%5Ep&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell^p" class="latex" /> norm, but we&#8217;re getting too many <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />&#8216;s around here).  </p>
<p>But then I wondered: what&#8217;s so great about <i>raising a probability to a power?</i>  As you note, for some applications any increasing function would work about equally well.  Is the R&eacute;nyi entropy just one of an enormous family of equally good (or bad) ideas, or is there really something special about it?</p>
<p>And then I remembered that in statistical mechanics, the probability of a system in equilibrium at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> being in its <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state is:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-E_i+%2F+k+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{-E_i / k T}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> is the energy of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state and <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> is Boltzmann&#8217;s constant.  (We can set this constant to 1 by a wise choice of units, and theoretical physicists usually do, but I think I&#8217;ll leave it in for now.)</p>
<p>This makes it obvious what &#8216;raising probabilities to powers&#8217; might mean:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%5E%5Calpha+%3D+e%5E%7B-+%5Calpha+E_i+%2F+k+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i^&#92;alpha = e^{- &#92;alpha E_i / k T}" class="latex" /></p>
<p>You can think of this as:</p>
<p>&bull; multiplying all the energies <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> of our system by <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /></p>
<p>or if you prefer:</p>
<p>&bull; replacing the temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=T%2F%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T/&#92;alpha" class="latex" />, </p>
<p>or for that matter:</p>
<p>&bull; dividing Boltzmann&#8217;s constant by <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />, which we&#8217;d do if we changed our units of either energy or temperature.</p>
<p>It doesn&#8217;t really matter which we do &mdash; they&#8217;re all just different ways of talking about the same thought.  So let&#8217;s say we&#8217;re rescaling the temperature: </p>
<p><img src="https://s0.wp.com/latex.php?latex=T+%5Cmapsto+T%2F%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;mapsto T/&#92;alpha" class="latex" /></p>
<p>This led me to call the process</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%5Cmapsto+p_i%5E%5Calpha+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;mapsto p_i^&#92;alpha " class="latex" /></p>
<p>&#8216;thermalization&#8217;.  The idea is that as <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;to 0" class="latex" />, we are <i>turning up the temperature</i>.  Physicists all know that as the temperature increases, states of high energy become more likely to be occupied&#8230; and in the limit of infinite temperature, all allowed states become equally likely.  And so, as <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> gets smaller, <img src="https://s0.wp.com/latex.php?latex=p_i%5E%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i^&#92;alpha" class="latex" /> approaches 1, at least if it was nonzero in the first place.</p>
<p>Conversely, as <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;to +&#92;infty" class="latex" /> we are lowering the temperature.  At low temperatures only the lowest-energy states have any significant chance of being occupied.</p>
<p>But of course when we thermalize a probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%5Cmapsto+p_i%5E%5Calpha+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;mapsto p_i^&#92;alpha " class="latex" /></p>
<p>the result is no longer a probability distribution &mdash; the <img src="https://s0.wp.com/latex.php?latex=p_i%5E%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i^&#92;alpha" class="latex" />&#8216;s don&#8217;t sum to 1 &mdash; until we divide by a fudge factor.</p>
<p>But this fudge factor is famous in statistical mechanics: it&#8217;s the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29" rel="nofollow">partition function</a>!  It&#8217;s a remarkably important quantity. <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4080" rel="nofollow">Everything we might want to know about our system is encoded in it!</a>  This is quite remarkable, given that it starts life as a mere &#8216;fudge factor&#8217;, designed to correct a mistake.  </p>
<p>I guess it&#8217;s a case of what Tom Leinster would call <i><a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4054" rel="nofollow">fudge power!</a></i></p>
<div align="center">
<img width="250" src="https://i0.wp.com/thumbs.ifood.tv/files/images/food/mint-fudge-02.jpg" alt="" />
</div>
<p>In particular, the logarithm of the partition function times <img src="https://s0.wp.com/latex.php?latex=-T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-T" class="latex" /> is the <a href="http://en.wikipedia.org/wiki/Thermodynamic_free_energy" rel="nofollow">Helmholtz free energy</a>: the amount of energy that&#8217;s not in the form of heat.  And this logarithm explains the logarithm in the formula for R&eacute;nyi entropy!</p>
<p>And then everything started falling into place.  </p>
<p>But it&#8217;s still falling.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-4119">
				<div id="div-comment-4119" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/5850d628c599d7ac60d7f7889844995a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">John F</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4119">14 February, 2011 at 10:06 pm</a>		</div>

		<p>John B,<br />
I don&#8217;t intend to seem too dense, but it looks to me that the formula for probability is already thermalized:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-+E_i+%2F+k+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{- E_i / k T}" class="latex" /></p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 parent highlander-comment" id="comment-4120">
				<div id="div-comment-4120" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4120">15 February, 2011 at 2:59 am</a>		</div>

		<p>John F. wrote:</p>
<blockquote>
<p>I don’t intend to seem too dense, but it looks to me that the formula for probability is already thermalized</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-+E_i+%2F+k+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{- E_i / k T}" class="latex" /></p>
</blockquote>
<p>The formula for probabilities is just </p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /></p>
<p>We&#8217;re starting with a completely arbitrary probability distribution here: nothing to do with temperature, energy, etcetera.     </p>
<p>The &#8216;thermalization&#8217; trick starts by choosing an arbitrary number <img src="https://s0.wp.com/latex.php?latex=T+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &gt; 0" class="latex" /> and finding real numbers <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-E_i+%2F+k+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{-E_i / k T}" class="latex" /></p>
<p>Such numbers always exist if <img src="https://s0.wp.com/latex.php?latex=p_i+%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;ne 0" class="latex" />, and then they&#8217;re always unique:</p>
<p><img src="https://s0.wp.com/latex.php?latex=E_i+%3D+-+k+T+%5Cln+p_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i = - k T &#92;ln p_i " class="latex" /></p>
<p>Then, to <b>thermalize</b> the probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> we define</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%28%5Calpha%29+%3D+%5Cfrac%7B1%7D%7BZ%28%5Calpha%29%7D+e%5E%7B-+%5Calpha+E_i+%2F+k+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i (&#92;alpha) = &#92;frac{1}{Z(&#92;alpha)} e^{- &#92;alpha E_i / k T}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=Z%28%5Calpha%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(&#92;alpha)" class="latex" /> is chosen to make the new probabilities sum to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Calpha%29+%3D+%5Csum_i+e%5E%7B-+%5Calpha+E_i+%2F+k+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(&#92;alpha) = &#92;sum_i e^{- &#92;alpha E_i / k T}" class="latex" /></p>
<p>We now have a new probability distribution depending on <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />, namely <img src="https://s0.wp.com/latex.php?latex=p_i%28%5Calpha%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(&#92;alpha)" class="latex" />.  </p>
<p>R&eacute;nyi entropy is a funny <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />-dependent entropy that reduces to the usual Shannon entropy when <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = 1" class="latex" />.   To understand R&eacute;nyi entropy, I claim we need to understand the process of thermalizing a probability distribution.</p>
<p>And here&#8217;s one way:</p>
<p>If we think of the original probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> as the thermal equilibrium state of some physical system at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />, the new <img src="https://s0.wp.com/latex.php?latex=p_i%28%5Calpha%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(&#92;alpha)" class="latex" />&#8216;s can be thought of as the result of:</p>
<p>&bull; multiplying all the energies <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> of our system by <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /></p>
<p>or:</p>
<p>&bull; replacing the temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=T%2F%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T/&#92;alpha" class="latex" />, </p>
<p>or:</p>
<p>&bull; dividing Boltzmann&#8217;s constant by <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />, which we&#8217;d do if we changed our units of either energy or temperature.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4120#respond' data-commentid="4120" data-postid="2467" data-belowelement="div-comment-4120" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-4 highlander-comment" id="comment-4125">
				<div id="div-comment-4125" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dc21eed3ebc499422d6c4a927c71ac4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://TheOilConunDrum.com' rel='external nofollow ugc' class='url'>Web Hub Tel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4125">15 February, 2011 at 6:59 am</a>		</div>

		<p>John,<br />
This is kind of out there, but I wonder if the thermalization factor as you defined it can predict deviations from the Einstein relation. I have looked at characterization of charged transport in disordered semiconductors and the curves always show an Einstein relation greater than kT/q. I have thought it had something to do with a highly smeared density of states, where the fundamental relation goes as N/(dN/dE), but you may be on to a unifying explanation. </p>
<p>Transport in disordered material typically shows a power-law response tail and this links back to non-extensive entropy such as Tsallis and Renyi. Is it possible that the carriers are somehow &#8220;thermalized&#8221; away from equilibrium and this is what  could cause a larger effective kT/q?  </p>
<p>I am saying this is way out there because I have yet to find any paper that claims any deviation from the Einstein relation can exist.</p>
<p>The significant link to Azimuth is that these disordered semiconductor materials (amorphous Si:H, etc) are the primary material for making photo-voltaics. Of course there is a huge amount of practical interest surrounding this topic. That essentially explains my interest in the math and physics, as you have to really go out on a limb to hit on a breakthrough.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4126">
				<div id="div-comment-4126" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4126">15 February, 2011 at 8:24 am</a>		</div>

		<p>WebHubTel wrote:</p>
<blockquote>
<p>This is kind of out there, but I wonder if the thermalization factor as you defined it can predict deviations from the Einstein relation. I have looked at characterization of charged transport in disordered semiconductors and the curves always show an Einstein relation greater than kT/q. </p>
</blockquote>
<p>Unfortunately I don&#8217;t understand what you&#8217;re talking about it!  It sounds interesting, but I don&#8217;t even know what the &#8220;Einstein relation&#8221; is, much less anything about disordered semiconductors.  I can learn stuff if you&#8217;re willing to explain it slowly.</p>
<blockquote>
<p>Transport in disordered material typically shows a power-law response tail and this links back to non-extensive entropy such as Tsallis and Renyi. </p>
</blockquote>
<p>Hmm.  I haven&#8217;t yet understood the stuff people keep muttering off in the distance about Tsallis entropy and power laws.  </p>
<p>By the way, the Tsallis entropy is non-extensive but &#8220;convex&#8221; (it behaves as you&#8217;d hope when taking mixtures of states), while the R&eacute;nyi entropy is &#8220;extensive&#8221; (when you set two uncorrelated systems side by side, their entropies add) but nonconvex.   Either one is a function of the other &mdash; so you can have your cake, and eat it too, but not in the same room.</p>
<p>Only in the special case where the R&eacute;nyi entropy reduces to the Boltzmann-Gibbs-Shannon-von Neumann entropy do you get something that&#8217;s both extensive and convex.  </p>
<blockquote>
<p>The significant link to Azimuth is that these disordered semiconductor materials (amorphous Si:H, etc) are the primary material for making photo-voltaics. Of course there is a huge amount of practical interest surrounding this topic.</p>
</blockquote>
<p>Interesting.  Is there some practical use for deviations from the Einstein relation (whatever that is)?</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-4130">
				<div id="div-comment-4130" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dc21eed3ebc499422d6c4a927c71ac4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://TheOilConunDrum.com' rel='external nofollow ugc' class='url'>Web Hub Tel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4130">15 February, 2011 at 2:02 pm</a>		</div>

		<p>(staying on the same indentation level)</p>
<p>About the Einstein relation: it is the ratio between the diffusivity and mobility coefficient of a charged particle or any particle moving under some force. In one sense it gives an idea of how strong random walk fluctuations are in comparison to forced movements. It is usually derived under equilibrium conditions.</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt depth-2 highlander-comment" id="comment-4095">
				<div id="div-comment-4095" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4095">13 February, 2011 at 3:46 am</a>		</div>

		<p>I cared (somewhat tangentially) about R&eacute;nyi entropies a few years ago, because they&#8217;re related to the distribution of node degrees in random spatial networks.  See Eqs. (12) through (16) in Herrmann <i>et al.</i> (2003), &#8220;Connectivity Distribution of Spatial Networks&#8221; <i>Phys. Rev. E</i> <b>68</b>: 026128, <a href="http://arxiv.org/abs/cond-mat/0302544" rel="nofollow">arXiv:cond-mat/0302544</a>.</p>
<p>Basically, if you build a graph by throwing vertices randomly into a box and drawing edges between those which land close to each other, the moments of the resulting graph&#8217;s degree distribution will be given by the R&eacute;nyi entropies of the probability function according to which the vertices were deposited.  (There&#8217;s a caveat about going to the thermodynamic limit, and the formula also brings in the <a href="http://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind" rel="nofollow">Stirling numbers of the second kind</a>.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4095#respond' data-commentid="4095" data-postid="2467" data-belowelement="div-comment-4095" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-2 parent highlander-comment" id="comment-4096">
				<div id="div-comment-4096" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4096">13 February, 2011 at 4:00 am</a>		</div>

		<p>And on the subject of &#8220;throwing things into a box&#8221;, I&#8217;m aware of an application some people cooked up to use R&eacute;nyi entropies in nuclear physics, to estimate the <i>thermodynamic</i> entropy of something whose phase-space properties are known from a limited number of samples.  In this case, the &#8220;box&#8221; is a discretized phase space, and the trick is to calculate <img src="https://s0.wp.com/latex.php?latex=H_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=H_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_3" class="latex" /> from what you&#8217;ve measured, after which you extrapolate down to <img src="https://s0.wp.com/latex.php?latex=H_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1" class="latex" />.  See A. Bialas and W. Czyz (2000), &#8220;Event by event analysis and entropy of multiparticle systems&#8221;, <i>Phys Rev D</i> <b>61</b>: 074021, <a href="http://arxiv.org/abs/hep-ph/9909209" rel="nofollow">arXiv:hep-ph/9909209</a>; also A. Bialas and K. Zalewski, <a href="http://arxiv.org/abs/hep-ph/0602059" rel="nofollow">arXiv:hep-ph/0602059</a>.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4096#respond' data-commentid="4096" data-postid="2467" data-belowelement="div-comment-4096" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 highlander-comment" id="comment-4097">
				<div id="div-comment-4097" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4097">13 February, 2011 at 4:58 am</a>		</div>

		<p>Thanks for all the helpful information, Blake!</p>
<p>In a more mundane way, the collision entropy </p>
<p><img src="https://s0.wp.com/latex.php?latex=H_2+%3D+-+2+%5Cln+%5Csum+p_i%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_2 = - 2 &#92;ln &#92;sum p_i^2 " class="latex" /></p>
<p>shows up when you throw rocks into boxes, with each rock landing in the <i>i </i>th box with probability <i>p<sub></sub><sub>i</sub></i>, and the result of each throw being independent from the rest.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum+p_i%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum p_i^2 " class="latex" /></p>
<p>is the probability that two chosen rocks land in the same box.</p>
<p>Hmm.  I guess the R&eacute;nyi entropies <img src="https://s0.wp.com/latex.php?latex=H_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_n" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=n+%3D+1%2C2%2C3%2C%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 1,2,3,&#92;dots" class="latex" /> all have interpretations of this sort&#8230; and now that I think about it, I bet you&#8217;re talking about the same idea!</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4097#respond' data-commentid="4097" data-postid="2467" data-belowelement="div-comment-4097" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-3 highlander-comment" id="comment-4105">
				<div id="div-comment-4105" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4105">13 February, 2011 at 3:58 pm</a>		</div>

		<p>Yes &mdash; in the one case, &#8220;land in the same box&#8221; means &#8220;be in the same phase-space bin&#8221;, while in the other, it means &#8220;fall close to each other in Euclidean space&#8221;.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4105#respond' data-commentid="4105" data-postid="2467" data-belowelement="div-comment-4105" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-3 highlander-comment" id="comment-160878">
				<div id="div-comment-160878" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='https://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-160878">19 April, 2020 at 12:37 am</a>		</div>

		<p>I&#8217;ve been thinking about all this again in the context of trying to invent final-exam problems for my statistical-physics course (it&#8217;ll be a take-home exam&#8230;), and I noticed something I believe I missed before. This bit in the &#8220;Rényi entropy and free energy&#8221; paper caught my attention:</p>
<blockquote><p>Of course, it is already well-known that the von Neumann entropy is the derivative of <img src="https://s0.wp.com/latex.php?latex=-F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-F" class="latex" /> with respect to temperature.  What we see now is that the Rényi entropy is the difference quotient approximating this derivative. Instead of the slope of the tangent line, it is the slope of the secant line.</p></blockquote>
<p>This seems to be doing the same thing, morally speaking, as Eq. (20) in Bialas and Czyz (arXiv:hep-ph/9909209).</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+H_2+%2B+%28H_2+-+H_3%29+%2B+%28H_2+-+3H_3+%2B+H_4%29+%2B+%5Ccdots+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = H_2 + (H_2 - H_3) + (H_2 - 3H_3 + H_4) + &#92;cdots " class="latex" /></p>
<p>They do a polynomial extrapolation of the sequence <img src="https://s0.wp.com/latex.php?latex=%5C%7BH_k%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{H_k&#92;}" class="latex" /> down to <img src="https://s0.wp.com/latex.php?latex=k+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k = 1" class="latex" />, which sounds like another way of saying to take the collection of secant lines and extrapolate to what the tangent line should be.</p>
<p>This is also reminiscent of something I found while playing around with <a href="https://arxiv.org/abs/1409.4708" rel="nofollow ugc">higher-order generalizations of mutual information</a>, where a binomial transform also arose &#8230; but that&#8217;s getting beyond the scope even of a take-home exam.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=160878#respond' data-commentid="160878" data-postid="2467" data-belowelement="div-comment-160878" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-2 highlander-comment" id="comment-4098">
				<div id="div-comment-4098" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4098">13 February, 2011 at 6:25 am</a>		</div>

		<p>By the way, in response to <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4082" rel="nofollow">Tim&#8217;s question</a>, I should add:</p>
<p>It may seem a bizarre and unproductive research strategy to study concepts precisely because nobody has explained why they&#8217;re important,  or what they really mean.  But for a long time, I&#8217;ve spent a fraction of my time on this slightly decadent approach, as kind of hobby.  </p>
<p>It can actually turn up interesting results.  For example, after decades of struggling to understand the octonions &mdash; a quirky algebraic gadget that fills everyone who meets it with a mixture of curiosity and revulsion &mdash; it eventually became clear that classical superstring theory Lagrangians exist only in 3, 4, 6, and 10 dimensions precisely because the 4 normed division algebras allow the construction of Lie super-2-groups extending the super-Poincar&eacute; group in these dimensions&#8230; with the 10-dimensional, physically interesting case arising from the octonions!</p>
<p>So, when I heard David Corfield start talking about generalized entropies like <a href="http://math.ucr.edu/home/baez/corfield/2006/06/tsallis-entropy.html" rel="nofollow">Tsallis entropy</a> and <a href="http://math.ucr.edu/home/baez/corfield/2006/07/renyi-entropy.html" rel="nofollow">R&eacute;nyi entropy</a>, my instinctive repugnance upon meeting concepts that seemed like &#8216;generalization for the sake of generalization&#8217; was mixed with a desire to understand why anyone cared about them, and to fit them into the framework of ideas I already understood.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4098#respond' data-commentid="4098" data-postid="2467" data-belowelement="div-comment-4098" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt thread-odd thread-alt depth-1 parent highlander-comment" id="comment-4121">
				<div id="div-comment-4121" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4121">15 February, 2011 at 3:38 am</a>		</div>

		<p>I have drastically rewritten my paper based on the comments here, especially those by Blake Stacey, Eric Downes, and &#8216;tomate&#8217;.   </p>
<p>I also had a new idea of my own.   I&#8217;m calling the R&eacute;nyi entropy <img src="https://s0.wp.com/latex.php?latex=S_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_q" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" /> now, because the R&eacute;nyi entropy is almost the <a href="http://en.wikipedia.org/wiki/Q-derivative" rel="nofollow"><i>q</i>-derivative</a> of free energy!  This is cool because <i>q</i>-derivatives show up when we <a href="http://en.wikipedia.org/wiki/Q-analog" rel="nofollow"><i>q</i>-deform</a> existing mathematical structures (like groups) to get new ones (like quantum groups).  It&#8217;s now clear that the R&eacute;nyi entropy is a <i>q</i>-deformed version of the ordinary entropy.</p>
<p>The new version is here:</p>
<p>&bull; <a href="http://arxiv.org/abs/1102.2098" rel="nofollow">R&eacute;nyi entropy and free energy</a>.  </p>
<p>The idea is now to take an arbitrary probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and write it as the state of thermal equilibrium for some physical system at some chosen temperature <img src="https://s0.wp.com/latex.php?latex=T_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_0" class="latex" />.  Let <img src="https://s0.wp.com/latex.php?latex=F%28T%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(T)" class="latex" /> be the free energy of this system at an arbitrary temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />.</p>
<p>By definition, the R&eacute;nyi entropy of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S_q+%3D+%5Cfrac%7B1%7D%7B1+-+q%7D+%5Cln+%5Csum_i+p_i%5Eq+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_q = &#92;frac{1}{1 - q} &#92;ln &#92;sum_i p_i^q " class="latex" /></p>
<p>But Eric Downes noted that</p>
<p><img src="https://s0.wp.com/latex.php?latex=S_q+%3D+-+%5C%3B+%5Cfrac%7BF%28T%29+-+F%28T_0%29%7D%7BT+-+T_0%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_q = - &#92;; &#92;frac{F(T) - F(T_0)}{T - T_0} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is a temperature with </p>
<p><img src="https://s0.wp.com/latex.php?latex=T_0%2FT+%3D+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_0/T = q" class="latex" /></p>
<p>Thanks to this formula, the R&eacute;nyi entropy is revealed to have a simple physical meaning.   This was pointed out by &#8216;tomate&#8217;.</p>
<p>Start with a physical system in thermal equilibrium at some temperature.  What is its R&eacute;nyi entropy?  To find out,  &#8216;quench&#8217; the system, suddenly dividing the temperature by <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  </p>
<p>Then:</p>
<blockquote>
<p><b>The maximum amount of work the system can do as it moves to thermal equilibrium at the new temperature, divided by the change in temperature, equals its R&eacute;nyi entropy.</b></p>
</blockquote>
<p>The formula </p>
<p><img src="https://s0.wp.com/latex.php?latex=S_q+%3D+-+%5C%3B+%5Cfrac%7BF%28T%29+-+F%28T_0%29%7D%7BT+-+T_0%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_q = - &#92;; &#92;frac{F(T) - F(T_0)}{T - T_0} " class="latex" /></p>
<p>can be written as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S_q%28T_0%29+%3D+-+%5C%3B+%5Cfrac%7BF%28q%5E%7B-1%7D+T_0%29+-+F%28T_0%29%7D%7Bq%5E%7B-1%7D+T_0+-+T_0%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_q(T_0) = - &#92;; &#92;frac{F(q^{-1} T_0) - F(T_0)}{q^{-1} T_0 - T_0} " class="latex" /></p>
<p>The <i>q</i>-derivative of a function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cfrac%7Bdf%7D%7Bdx%7D%5Cright%29_q+%3D+%5Cfrac%7Bf%28qx%29+-+f%28x%29%7D%7Bqx+-+x%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left(&#92;frac{df}{dx}&#92;right)_q = &#92;frac{f(qx) - f(x)}{qx - x} " class="latex" /></p>
<p>So, it&#8217;s clear that &mdash; except for some small stuff that&#8217;s explained in the paper &mdash; the R&eacute;nyi entropy is essentially a <i>q</i>-derivative of free energy.</p>
<p>As <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to 1" class="latex" />, the <i>q</i>-derivative reduces to an ordinary derivative, and the R&eacute;nyi entropy reduces to the ordinary Shannon entropy.</p>
<p>I don&#8217;t know what the connection between R&eacute;nyi and <i>q</i>-deformation really means, but it suggests there&#8217;s a bigger picture lurking in the background.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4121#respond' data-commentid="4121" data-postid="2467" data-belowelement="div-comment-4121" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-2 parent highlander-comment" id="comment-4123">
				<div id="div-comment-4123" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4123">15 February, 2011 at 4:37 am</a>		</div>

		<p>I saw the revised paper go up on the arXiv earlier today &mdash; thank you very much for mentioning my comment!</p>
<p>And, now I know what the <i>q</i> in <i>q</i>-deformation stands for: <a href="http://en.wikipedia.org/wiki/Quenching" rel="nofollow">quenching</a>!  (-:</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4123#respond' data-commentid="4123" data-postid="2467" data-belowelement="div-comment-4123" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 highlander-comment" id="comment-4124">
				<div id="div-comment-4124" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4124">15 February, 2011 at 5:45 am</a>		</div>

		<p>Sure &mdash; thanks for spotting those references and maybe alerting Eric Downes to this stuff!  </p>
<p>I&#8217;m not sure I have the guts to mention in this paper that <i>q</i> stands for &#8220;quenching&#8221; &mdash; but I&#8217;d definitely use that line in a talk.</p>
<p>For now, I&#8217;m happy just to be able to use the word <i>quench</i> in a paper. It&#8217;s so incredibly macho.</p>
<div align="center">
<a href="http://www.arscives.com/bladesign/kristanto.htm" rel="nofollow"><br />
<img src="https://i0.wp.com/www.arscives.com/bladesign/images1/quenching_small.jpg" alt="" /><br />
</a></div>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4124#respond' data-commentid="4124" data-postid="2467" data-belowelement="div-comment-4124" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-3 parent highlander-comment" id="comment-4136">
				<div id="div-comment-4136" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4136">15 February, 2011 at 7:45 pm</a>		</div>

		<p>I think there&#8217;s a missing &#8220;tr&#8221; in the equation in between (7) and (8).  The R&eacute;nyi entropy at temperature <img src="https://s0.wp.com/latex.php?latex=T_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_0" class="latex" /> should be</p>
<p><img src="https://s0.wp.com/latex.php?latex=S_q%28T_0%29+%3D+%5Cfrac%7B1%7D%7B1+-+q%7D%5Clog%5C%2C%5Chbox%7Btr%7D%28%5Crho_%7BT_0%7D%5Eq%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_q(T_0) = &#92;frac{1}{1 - q}&#92;log&#92;,&#92;hbox{tr}(&#92;rho_{T_0}^q)." class="latex" /></p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4136#respond' data-commentid="4136" data-postid="2467" data-belowelement="div-comment-4136" data-respondelement="respond" data-replyto="Reply to Blake Stacey" aria-label='Reply to Blake Stacey'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4138">
				<div id="div-comment-4138" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4138">16 February, 2011 at 3:07 am</a>		</div>

		<p>Thanks &#8211; fixed!  You can see an ever-improving version of the paper <a href="http://math.ucr.edu/home/baez/renyi.pdf" rel="nofollow">on my website</a>.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-4147">
				<div id="div-comment-4147" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/dd50e8b0462cc7fb7fa5bb0785e4f2a4?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.sunclipse.org' rel='external nofollow ugc' class='url'>Blake Stacey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4147">16 February, 2011 at 8:18 pm</a>		</div>

		<p>Another, smaller point (or pair of them): in the graf beginning &#8220;After the author noticed this result&#8221;, the name &#8220;Schlogl&#8221; should be &#8220;Schl&ouml;gl&#8221;.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-4150">
				<div id="div-comment-4150" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4150">17 February, 2011 at 8:01 am</a>		</div>

		<p>&Ouml;&ouml;f!  What a pun!</p>
<p>Thanks &mdash; fixed!</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 highlander-comment" id="comment-4178">
				<div id="div-comment-4178" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e522cb5eda111a1afa7f345f8423c13c?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Marc Harper</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4178">18 February, 2011 at 11:16 pm</a>		</div>

		<p>John,</p>
<p>You may be interested in a more general form of entropy that generalizes Tsallis/Renyi entropies to a functional parameter given in <a href="http://arxiv.org/abs/math-ph/0402005" title="Estimators, escort probabilities, and phi-exponential families in statistical physics" rel="nofollow">this paper</a> by Jan Naudts. See section 4 for the definitions of the generalized logarithms and entropies.</p>
<p>These guys are related to generalized versions of Fisher Information, gradient systems, machine learning, evolutionary game theory, and more.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4178#respond' data-commentid="4178" data-postid="2467" data-belowelement="div-comment-4178" data-respondelement="respond" data-replyto="Reply to Marc Harper" aria-label='Reply to Marc Harper'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1 parent highlander-comment" id="comment-4195">
				<div id="div-comment-4195" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/716fa9d449b2090e185ff19c2fb5bb2d?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://threeplusone.com' rel='external nofollow ugc' class='url'>Gavin Crooks</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4195">23 February, 2011 at 12:31 am</a>		</div>

		<p>Hay, this looks like fun. Can I play? Whad&#8217;ya know, Renyi entropy also shows up in non-equilibrium statistical dynamics. I&#8217;ve written up a quick note explaining this:</p>
<p><a href="http://threeplusone.com/pubs/technote/CrooksTechNote009-TrajDiv.pdf" rel="nofollow">Trajectory divergences in non-equilibrium statistical dynamics</a></p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4195#respond' data-commentid="4195" data-postid="2467" data-belowelement="div-comment-4195" data-respondelement="respond" data-replyto="Reply to Gavin Crooks" aria-label='Reply to Gavin Crooks'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-2 highlander-comment" id="comment-4211">
				<div id="div-comment-4211" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4211">27 February, 2011 at 12:58 am</a>		</div>

		<p>Fascinating paper!  You sure know a lot of different entropy-related concepts.  I&#8217;m just catching up&#8230;</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4211#respond' data-commentid="4211" data-postid="2467" data-belowelement="div-comment-4211" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1 parent highlander-comment" id="comment-5300">
				<div id="div-comment-5300" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5300">14 April, 2011 at 3:27 pm</a>		</div>

		<p>Here&#8217;s a little idea on how to view entropy as a functor. Let&#8217;s define to two monoidal categories, call them <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />, such that Shannon and Rényi entropies are (strict) monoidal functors <img src="https://s0.wp.com/latex.php?latex=C+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C &#92;to D" class="latex" />. An object of <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is defined to be a finite set together with a probability distribution. A morphism of <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is defined to be a function which is compatible with the probabilities in the obvious way. (Such a function is necessarily surjective.) For <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />, we take the poset of non-negative real numbers in the reversed order. The monoidal structure on <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is given by cartesian products of sets and forming the product of the probability distributions. On <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />, it is simply the addition of real numbers.</p>
<p>Then what is a strict monoidal functor from <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />? It assigns to each probability distribution <img src="https://s0.wp.com/latex.php?latex=p%3D%28p_1%2C...%2Cp_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p=(p_1,...,p_n)" class="latex" /> a non-negative number <img src="https://s0.wp.com/latex.php?latex=H%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(p)" class="latex" /> such that:<br />
(1) it is invariant under permutations,<br />
(2) it is non-increasing under the operation of grouping some outcomes to a single one;<br />
(3) it satisfies additivity for independent distributions.</p>
<p>The Rényi entropies have all of these properties; as does any positive linear combination of Rényi entropies. So, are there any other strict monoidal functors from <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> besides these? Has this categorical point of view been written down anywhere?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5300#respond' data-commentid="5300" data-postid="2467" data-belowelement="div-comment-5300" data-respondelement="respond" data-replyto="Reply to Tobias Fritz" aria-label='Reply to Tobias Fritz'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-2 highlander-comment" id="comment-5302">
				<div id="div-comment-5302" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">David Corfield</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5302">14 April, 2011 at 3:59 pm</a>		</div>

		<p>I wonder if Censov&#8217;s (or Chensov&#8217;s) work bears on this. It was mentioned <a href="http://golem.ph.utexas.edu/category/2006/09/category_theory_and_philosophy.html#c004627" rel="nofollow">here</a>, and <a href="http://www.cs.cmu.edu/~lebanon/papers/cenJournal.pdf" rel="nofollow">here</a>. Frank Hansen <a href="http://arxiv.org/abs/1102.2989" rel="nofollow">writes</a>:</p>
<blockquote><p>
Chentsov proved [3] that the Fisher-Rao metric is the only Riemannian metric, defined on the tangent space, that is decreasing under Markov morphisms. Since Markov morphisms represent coarse graining or randomization, it means that the Fisher information is the only Riemannian metric possessing the attractive property that distinguishability of probability distributions becomes more difficult when they are observed through a noisy channel.
</p></blockquote>
<p>[3] N.N. Censov, Statistical Decision Rules and Optimal Inferences, Transl. Math. Monogr., volume 53. Amer. Math. Soc., Providence, 1982.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5302#respond' data-commentid="5302" data-postid="2467" data-belowelement="div-comment-5302" data-respondelement="respond" data-replyto="Reply to David Corfield" aria-label='Reply to David Corfield'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-2 highlander-comment" id="comment-5304">
				<div id="div-comment-5304" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5304">14 April, 2011 at 6:25 pm</a>		</div>

		<p>Interesting question.  And it&#8217;s also interesting that any such functor H induces a functor from (categories enriched in C) to (metric spaces).</p>
<p>I think you&#8217;re missing one small thing.  In the definition of C, you say that morphisms are functions &#8220;compatible with the probabilities in the obvious way&#8221;, which I take to mean measure-preserving.  This does <i>not</i> imply that the function is surjective.  Rather, it implies that the complement of the image has measure 0.  So when you refer in condition (2) to &#8220;the operation of grouping some outcomes to a single one&#8221;, the term &#8220;some&#8221; must include the possibility of &#8220;none&#8221;.  In other words, (2) means not only</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28p_1%2C+%5Cldots%2C+p_%7Bn+-+1%7D%2C+p_n%2C+p_%7Bn+%2B+1%7D%29+%5Cgeq+H%28p_1%2C+%5Cldots%2C+p_%7Bn+-+1%7D%2C+p_n+%2B+p_%7Bn+%2B+1%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(p_1, &#92;ldots, p_{n - 1}, p_n, p_{n + 1}) &#92;geq H(p_1, &#92;ldots, p_{n - 1}, p_n + p_{n + 1})" class="latex" /></p>
<p>but also</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28p_1%2C+%5Cldots%2C+p_%7Bn+-+1%7D%29+%5Cgeq+H%28p_1%2C+%5Cldots%2C+p_%7Bn+-+1%7D%2C+0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(p_1, &#92;ldots, p_{n - 1}) &#92;geq H(p_1, &#92;ldots, p_{n - 1}, 0)" class="latex" /></p>
<p>(which together imply that the last inequality is actually an equality).  But that&#8217;s fine: all the Rényi entropies satisfy this further condition.</p>
<p>It strikes me that you don&#8217;t have a continuity condition anywhere, and continuity often appears in axiomatizations of entropy.  But perhaps you can get it for free.  </p>
<p>Have you seen <a href="http://www.maths.gla.ac.uk/~tl/Renyi.pdf" rel="nofollow">Rényi&#8217;s original paper</a>?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5304#respond' data-commentid="5304" data-postid="2467" data-belowelement="div-comment-5304" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-2 parent highlander-comment" id="comment-5305">
				<div id="div-comment-5305" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5305">14 April, 2011 at 6:59 pm</a>		</div>

		<p>Your conditions are satisfied not only by the Rényi entropies of order <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5B0%2C+%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in [0, &#92;infty)" class="latex" />, but in fact by those of order <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5B-%5Cinfty%2C+%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in [-&#92;infty, &#92;infty]" class="latex" /> (hence positive linear combinations of these).  So if the answer to your first question (&#8220;are there any other&#8230;?&#8221;) is to be &#8220;no&#8221;, then you&#8217;d better include that full range of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />s.</p>
<p>When doing entropy of negative order, one has to be a bit careful about zero probabilities.  Thus, for <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%28-%5Cinfty+%2C+%5Cinfty%29%5Csetminus%5C%7B1%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in (-&#92;infty , &#92;infty)&#92;setminus&#92;{1&#92;}" class="latex" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_q%28p%29+%3D+%5Cfrac%7B1%7D%7B1+-+q%7D+%5Clog+%5Csum_%7Bi%3A+p_i+%3E+0%7D+p_i%5Eq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_q(p) = &#92;frac{1}{1 - q} &#92;log &#92;sum_{i: p_i &gt; 0} p_i^q" class="latex" />.</p>
<p>Also</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7B-%5Cinfty%7D%28p%29+%3D+-%5Clog+%28%5Cmin_%7Bi%3A+p_i+%3E+0%7D+p_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{-&#92;infty}(p) = -&#92;log (&#92;min_{i: p_i &gt; 0} p_i)" class="latex" /></p>
<p>and as usual</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%5Cinfty%28p%29+%3D+-%5Clog+%28%5Cmax_i+p_i%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;infty(p) = -&#92;log (&#92;max_i p_i)." class="latex" /></p>
<p>(In the last expression, it makes no difference whether the maximum ranges over all <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> or only those for which <img src="https://s0.wp.com/latex.php?latex=p_i+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &gt; 0" class="latex" />.)</p>
<p>Anyway, it would be really nice if your hunch turned out to be right.  Among other things, it would provide a nice companion to these <a href="http://golem.ph.utexas.edu/category/2011/03/characterizing_the_pnorms.html" rel="nofollow">two</a> <a href="http://golem.ph.utexas.edu/category/2011/03/characterizing_the_generalized.html" rel="nofollow">theorems</a>.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5305#respond' data-commentid="5305" data-postid="2467" data-belowelement="div-comment-5305" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment odd alt depth-3 highlander-comment" id="comment-5442">
				<div id="div-comment-5442" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5442">21 April, 2011 at 7:54 pm</a>		</div>

		<p>Actually, the above conjecture is not quite right, for essentially trivial reasons. There are certain <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#Inequalities_between_different_values_of_.CE.B1" rel="nofollow">linear inequalities</a> between the <img src="https://s0.wp.com/latex.php?latex=H_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;alpha" class="latex" /> for different values of <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />. Taking for example <img src="https://s0.wp.com/latex.php?latex=H_1-H_2%5Cgeq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1-H_2&#92;geq 0" class="latex" />, it is clear that <img src="https://s0.wp.com/latex.php?latex=H_1-H_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1-H_2" class="latex" /> also is a monoidal functor from C to D! Assuming that there are no linear dependencies between Rényi entropies, this shows that there are other such functors which are not *positive* linear combinations or integrals of Rényi entropies.</p>
<p>It still remains unclear (to me at least) whether there are any such functors which lie outside the (closed) linear hull of Rényi entropies, but I bet that the answer is yes&#8230;</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5442#respond' data-commentid="5442" data-postid="2467" data-belowelement="div-comment-5442" data-respondelement="respond" data-replyto="Reply to Tobias Fritz" aria-label='Reply to Tobias Fritz'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-3 parent highlander-comment" id="comment-5444">
				<div id="div-comment-5444" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5444">21 April, 2011 at 11:06 pm</a>		</div>

		<p>I agree that <img src="https://s0.wp.com/latex.php?latex=H_1+%5Cgeq+H_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1 &#92;geq H_2" class="latex" /> (indeed, <img src="https://s0.wp.com/latex.php?latex=H_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;alpha" class="latex" /> is decreasing in <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />).  But is it so clear that <img src="https://s0.wp.com/latex.php?latex=H_1+-+H_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1 - H_2" class="latex" /> is monoidal?  E.g. is it clear that <img src="https://s0.wp.com/latex.php?latex=H_1+-+H_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1 - H_2" class="latex" /> decreases when you group some outcomes to a single one?  I haven&#8217;t tried to calculate it, but I don&#8217;t see why it should be true at first glance.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5444#respond' data-commentid="5444" data-postid="2467" data-belowelement="div-comment-5444" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5460">
				<div id="div-comment-5460" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5460">22 April, 2011 at 8:37 am</a>		</div>

		<p>Ah, you&#8217;re right, it&#8217;s not clear that the difference is functorial. It might have been too late last night&#8230;</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even depth-2 highlander-comment" id="comment-5306">
				<div id="div-comment-5306" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5306">14 April, 2011 at 7:41 pm</a>		</div>

		<p>One more thing: when you speak of &#8220;positive linear combinations&#8221; of Rényi entropies, you&#8217;re going to need to allow <i>infinite</i> linear combinations, i.e. integrals.  </p>
<p>So in summary, I think a plausible form of your (implicit) conjecture is something like this: every strict monoidal functor H from C to D is of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28p%29+%3D+%5Cint_%7B%5B-%5Cinfty%2C+%5Cinfty%5D%7D+H_q%28p%29+d%5Cmu%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(p) = &#92;int_{[-&#92;infty, &#92;infty]} H_q(p) d&#92;mu(q)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> is a finite positive Borel measure on <img src="https://s0.wp.com/latex.php?latex=%5B-%5Cinfty%2C+%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[-&#92;infty, &#92;infty]" class="latex" />.  (This integral does make sense: for each p, the integrand is continuous and bounded.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5306#respond' data-commentid="5306" data-postid="2467" data-belowelement="div-comment-5306" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 parent highlander-comment" id="comment-5308">
				<div id="div-comment-5308" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5308">15 April, 2011 at 1:13 am</a>		</div>

		<p>Hi, Tobias!  That&#8217;s a beautiful idea&mdash;I haven&#8217;t seen it expressed in categorical language like that before!  If Tom Leinster hasn&#8217;t seen it before, it must be new.  And thanks, Tom, for making it even more beautiful!   </p>
<p>I read your comments when I woke up last night and couldn&#8217;t get back to sleep because my head was whirring with thoughts.   Someone had told me an amazing relationship between special relativity and the fact that planets have elliptic, parabolic or hyperbolic orbits in the inverse square force law, and I was trying to figure out what it meant.  But your comments made my insomnia even worse.  I came up with some interesting ideas.  Alas, they seem to have developed some flaws in the clear light of day.  But they still seem worth mentioning.</p>
<p>Tobias conjectured a way to characterize all positive linear combinations of R&eacute;nyi entropies. It might be even nicer to characterize the R&eacute;nyi entropies themselves.  </p>
<p>Tobias&#8217; proposed characterization is in terms of symmetric monoidal functors.   The main idea behind the adjective &#8216;symmetric monoidal&#8217; here is that the entropy of a product of finite probability measure spaces is the sum of the entropies of the individual spaces:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28X+%5Ctimes+Y%29+%3D+S%28X%29+%2B+S%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X &#92;times Y) = S(X) + S(Y)" class="latex" /></p>
<p>So, <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is sort of analogous to a homomorphism of abelian groups (or even better, commutative monoids).    </p>
<p>Now, a linear combination of abelian group homomorphisms is again an abelian group homomorphism.  That&#8217;s why Tobias&#8217; idea can&#8217;t characterize only R&eacute;nyi entropies; positive linear combinations of R&eacute;nyi entropies also work.  But linear combinations of <i>ring</i> homomorphisms are not again ring homomorphisms.  So, let&#8217;s try to think of R&eacute;nyi entropies as something more like <i>ring</i> homomorphisms.  </p>
<p>Right now they carry multiplication to addition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28X+%5Ctimes+Y%29+%3D+S%28X%29+%2B+S%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X &#92;times Y) = S(X) + S(Y)" class="latex" /></p>
<p>This is a bit awkward, so let&#8217;s  take the exponential of entropy&mdash;and just for now, let&#8217;s call it <b>extropy</b>.  The name here is a <a href="http://en.wikipedia.org/wiki/Extropianism#Extropy" rel="nofollow">joke</a>, but the idea has been advocated by <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2053" rel="nofollow">David Ellerman</a> here on this blog, and by Tom Leinster elsewhere.</p>
<p>So, for any R&eacute;nyi entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />, let</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28X%29+%3D+%5Cexp%28S%28X%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(X) = &#92;exp(S(X)) " class="latex" /></p>
<p>Now we have </p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28X+%5Ctimes+Y%29+%3D+E%28X%29+%5Ctimes+E%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(X &#92;times Y) = E(X) &#92;times E(Y)" class="latex" /></p>
<p>and we can hope that <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> extends to something like a ring homomorphism, by also obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28X+%2B+Y%29+%3D+E%28X%29+%2B+E%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(X + Y) = E(X) + E(Y)" class="latex" /></p>
<p>But what is <img src="https://s0.wp.com/latex.php?latex=X+%2B+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X + Y" class="latex" />?  For this I think it&#8217;s best to switch from finite <i>probability</i> measure spaces to finite measure spaces.  After all, if we have two measure spaces <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" />, their disjoint union <img src="https://s0.wp.com/latex.php?latex=X+%2B+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X + Y" class="latex" /> becomes a measure space in an obvious way&#8212;but if <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> have total measure 1, <img src="https://s0.wp.com/latex.php?latex=X+%2B+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X + Y" class="latex" /> will have total measure 2.  We could try tricks to get around this, but I don&#8217;t feel like it.</p>
<p>So, let <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> be the category of finite measure spaces.  This is a bit like a ring.  More precisely, it&#8217;s a <a href="http://ncatlab.org/nlab/show/rig+category" rel="nofollow">&#8216;rig category&#8217;</a> with <img src="https://s0.wp.com/latex.php?latex=X+%2B+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X + Y" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=X+%5Ctimes+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X &#92;times Y" class="latex" /> defined in the obvious ways.  </p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> be the set of nonnegative real numbers made into a category with a unique morphism <img src="https://s0.wp.com/latex.php?latex=x+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;to y" class="latex" /> whenever <img src="https://s0.wp.com/latex.php?latex=x+%5Cle+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;le y" class="latex" />.  This becomes a rig category with <img src="https://s0.wp.com/latex.php?latex=x+%2B+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x + y" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=x+%5Ctimes+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;times y" class="latex" /> defined in the usual way.</p>
<p>So, now we can ask: what are the rig functors <img src="https://s0.wp.com/latex.php?latex=C+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C &#92;to D" class="latex" />?  </p>
<p>But then comes a surprise, I think&#8230;  </p>
<p>Maybe I&#8217;ll leave this question as a puzzle before trying to regroup and march on: </p>
<p><b>Puzzle.</b> Do the R&eacute;nyi extropies define rig functors <img src="https://s0.wp.com/latex.php?latex=E+%3A+C+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E : C &#92;to D" class="latex" />?  Can you completely classify such functors?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5308#respond' data-commentid="5308" data-postid="2467" data-belowelement="div-comment-5308" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-3 highlander-comment" id="comment-5310">
				<div id="div-comment-5310" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5310">15 April, 2011 at 3:36 am</a>		</div>

		<p>By the way, I&#8217;m so pleased with this stuff right now that I feel we should write a joint paper on it.  I start more papers than I finish, but&#8230;</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5310#respond' data-commentid="5310" data-postid="2467" data-belowelement="div-comment-5310" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-3 parent highlander-comment" id="comment-5316">
				<div id="div-comment-5316" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5316">15 April, 2011 at 11:30 am</a>		</div>

		<p>I have the feeling that I&#8217;m missing the point here&#8230; but I&#8217;ll give it a go.  </p>
<p>The exponential of the Rényi entropy of order q is <img src="https://s0.wp.com/latex.php?latex=E_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_q" class="latex" />, defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=E_q%28p%29+%3D+%28%5Csum_%7Bi%3A+p_i+%3E+0%7D+p_i%5Eq%29%5E%7B1%2F%281+-+q%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_q(p) = (&#92;sum_{i: p_i &gt; 0} p_i^q)^{1/(1 - q)}" class="latex" /></p>
<p>for finite probability distributions p.  Here I&#8217;m assuming <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%28-%5Cinfty%2C+1%29+%5Ccup+%281%2C+%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in (-&#92;infty, 1) &#92;cup (1, &#92;infty)" class="latex" />; the other values of <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5B-%5Cinfty%2C+%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in [-&#92;infty, &#92;infty]" class="latex" /> are handled in the usual way.  </p>
<p>I don&#8217;t know the definition of the Rényi entropy of a finite <i>measure</i> space.  One could of course normalize the measure space to become a probability space and take <i>its</i> entropy, but that might not be the right thing to do.  </p>
<p>Anyway, if <img src="https://s0.wp.com/latex.php?latex=E%3A+C+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E: C &#92;to D" class="latex" /> is to preserve +, then for any sequence <img src="https://s0.wp.com/latex.php?latex=a_1%2C+%5Cldots%2C+a_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_1, &#92;ldots, a_n" class="latex" /> of nonnegative reals, we must have</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28a_1%2C+%5Cldots%2C+a_n%29+%3D+E%28a_1%29+%2B+%5Ccdots+%2B+E%28a_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(a_1, &#92;ldots, a_n) = E(a_1) + &#92;cdots + E(a_n)" class="latex" /></p>
<p>where on the right-hand side, <img src="https://s0.wp.com/latex.php?latex=a_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_i" class="latex" /> denotes the measure space consisting of a single point with measure <img src="https://s0.wp.com/latex.php?latex=a_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_i" class="latex" />.  Whatever the Rényi extropy of a measure space is, it seems unlikely that it satisfies such a formula.  In fact, I reckon I could prove it&#8217;s impossible: for if we take the <img src="https://s0.wp.com/latex.php?latex=a_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_i" class="latex" />s to add up to 1 then we know what the left-hand side is, and it surely can&#8217;t be decomposed as a sum of functions of the <img src="https://s0.wp.com/latex.php?latex=a_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_i" class="latex" />s.</p>
<p>So, John asks, what <i>are</i> the (strict) rig functors <img src="https://s0.wp.com/latex.php?latex=E%3A+C+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E: C &#92;to D" class="latex" />?  Well, by the displayed formula above, such functors are entirely determined by what they do to singleton measure spaces.  This determines a function <img src="https://s0.wp.com/latex.php?latex=a%5Cmapsto+E%28a%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a&#92;mapsto E(a)" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%5B0%2C+%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0, &#92;infty)" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C+%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0, &#92;infty)" class="latex" />.  Multiplicativity gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28ab%29+%3D+E%28a%29E%28b%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(ab) = E(a)E(b)" class="latex" />, &nbsp; <img src="https://s0.wp.com/latex.php?latex=E%281%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(1) = 1" class="latex" /></p>
<p>and functoriality gives </p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28a%29+%2B+E%28b%29+%5Cgeq+E%28a+%2B+b%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(a) + E(b) &#92;geq E(a + b)" class="latex" />, &nbsp; <img src="https://s0.wp.com/latex.php?latex=E%280%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(0) = 0" class="latex" />.</p>
<p>The &#8220;obvious&#8221; solutions to the functional equations <img src="https://s0.wp.com/latex.php?latex=E%28ab%29+%3D+E%28a%29E%28b%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(ab) = E(a)E(b)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=E%281%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(1) = 1" class="latex" /> are those of the form <img src="https://s0.wp.com/latex.php?latex=E%28a%29+%3D+a%5Et&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(a) = a^t" class="latex" /> where t is a real constant.  There are also non-obvious solutions, at least assuming the axiom of choice, but I suspect the subadditivity property of E excludes those.  It also implies that <img src="https://s0.wp.com/latex.php?latex=t+%5Cleq+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;leq 1" class="latex" />.  (If <img src="https://s0.wp.com/latex.php?latex=t+%5Cleq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;leq 0" class="latex" /> then we interpret <img src="https://s0.wp.com/latex.php?latex=0%5Et&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0^t" class="latex" /> as 0.)  So then,</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28a_1%2C+%5Cldots%2C+a_n%29+%3D+%5Csum_%7Bi%3A+a_i+%3E+0%7D+a_i%5Et&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(a_1, &#92;ldots, a_n) = &#92;sum_{i: a_i &gt; 0} a_i^t" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=t+%5Cin+%28-%5Cinfty%2C+1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;in (-&#92;infty, 1]" class="latex" />.  These are all rig functors from C to D, and I suspect they&#8217;re the only ones.</p>
<p>By the way, I assume that when you wrote &#8220;a unique morphism <img src="https://s0.wp.com/latex.php?latex=x+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;to y" class="latex" /> whenever <img src="https://s0.wp.com/latex.php?latex=x+%5Cleq+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;leq y" class="latex" />&#8221; in your definition of D, you meant the opposite inequality.  (That&#8217;s what Tobias had.)  If not, I guess we get <img src="https://s0.wp.com/latex.php?latex=t+%5Cgeq+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;geq 1" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=t+%5Cleq+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;leq 1" class="latex" />.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5316#respond' data-commentid="5316" data-postid="2467" data-belowelement="div-comment-5316" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-4 highlander-comment" id="comment-5327">
				<div id="div-comment-5327" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5327">16 April, 2011 at 6:57 am</a>		</div>

		<p>Tom wrote:</p>
<blockquote>
<p>By the way, I assume that when you wrote “a unique morphism <img src="https://s0.wp.com/latex.php?latex=x+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;to y" class="latex" /> whenever <img src="https://s0.wp.com/latex.php?latex=x+%5Cleq+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;leq y" class="latex" />” in your definition of <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />, you meant the opposite inequality. (That’s what Tobias had.) </p>
</blockquote>
<p>Yeah, I meant whatever he meant.   But in fact, right now for a minute I feel like only including <i>isomorphisms</i> in my category <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" />!  So I&#8217;ll take the <a href="http://ncatlab.org/nlab/show/core" rel="nofollow">&#8216;core&#8217;</a> of the category <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> we had before: I&#8217;ll throw out all the morphisms except isomorphisms.  I&#8217;ll call the core <img src="https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C_0" class="latex" />.</p>
<p>Then I believe all the strict rig functors </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%3A+C_0+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z: C_0 &#92;to D" class="latex" /></p>
<p>are of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z_t%28a_1%2C+%5Cdots%2C+a_n%29+%3D+%5Csum_%7Bi+%3A+a_i+%3E+0%7D+a_i%5Et+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z_t(a_1, &#92;dots, a_n) = &#92;sum_{i : a_i &gt; 0} a_i^t " class="latex" /></p>
<p>where now <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> can be any real number.  This is the answer I was wanting you to get.</p>
<p>Of course, when I first got this answer I was bummed out, because this isn&#8217;t the R&eacute;nyi entropy.  But I quickly recovered, because this is something even more important than the R&eacute;nyi entropy: it&#8217;s the <i>partition function!</i></p>
<p>The logarithm of the partition function is also famous: it&#8217;s called the <i>free energy</i>.  So this sends <img src="https://s0.wp.com/latex.php?latex=%5Ctimes&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;times" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%2B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="+" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />, sort of like entropy does.</p>
<p>And the R&eacute;nyi entropy is a q-deformed derivative of the partition function, as explained <a href="http://arxiv.org/abs/1102.2098" rel="nofollow">in the paper this blog entry is about</a>.</p>
<p>So all these important concepts show up in an interlocked way when we start thinking about functors from <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> (or its core) to <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />!</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 parent highlander-comment" id="comment-5336">
				<div id="div-comment-5336" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5336">16 April, 2011 at 12:37 pm</a>		</div>

		<p>Ah, I see.  Question: how do you exclude the &#8220;non-obvious&#8221; solutions to the functional equations <img src="https://s0.wp.com/latex.php?latex=E%28ab%29+%3D+E%28a%29E%28b%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(ab) = E(a)E(b)" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=E%281%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(1) = 1" class="latex" />?  As you know, if we assume the axiom of choice then it&#8217;s possible to construct solutions <i>not</i> of the form <img src="https://s0.wp.com/latex.php?latex=E%28a%29+%3D+a%5Et&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(a) = a^t" class="latex" />.  </p>
<p>Some extra hypothesis on E is needed in order to exclude these.  A long list of possibilities is known: continuity, order-preservation, measurability, &#8230;</p>
<p>But maybe you want to leave that little niggle for another day and think about the bigger picture.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-4 highlander-comment" id="comment-5340">
				<div id="div-comment-5340" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5340">16 April, 2011 at 3:40 pm</a>		</div>

		<p>I don&#8217;t mind treating <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> as categories internal to Top and assuming all our functors are continuous.  In fact I felt the urge to do this for some other aspect of the problem, which I&#8217;ll try to tackle here after some sleep!</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 parent highlander-comment" id="comment-5341">
				<div id="div-comment-5341" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5341">16 April, 2011 at 6:13 pm</a>		</div>

		<p>OK, sounds good.  </p>
<p>Is there any reason not to treat <img src="https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C_0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> as just <i>rigs</i> in Top?  (Here <img src="https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C_0" class="latex" /> is the rig of isomorphism classes of finite measure spaces.)  If we&#8217;re not using any of the non-invertible maps in C then it seems to me that we might as well do this.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-4 highlander-comment" id="comment-5346">
				<div id="div-comment-5346" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5346">17 April, 2011 at 12:38 am</a>		</div>

		<p>Tom wrote:</p>
<blockquote><p>
Is there any reason not to treat <img src="https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C_0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> as just rigs in Top? (Here <img src="https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C_0" class="latex" /> is the rig of isomorphism classes of finite measure spaces.)
</p></blockquote>
<p>I don&#8217;t know yet.  But I&#8217;d hate to say that as soon as a category becomes a groupoid you might as well decategorify it and make it into a set&mdash;what would Ronnie Brown say?   And Tobias&#8217; results seem to use the whole structure of <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> in a nice way.  So it seems we should be trying to classify functors <img src="https://s0.wp.com/latex.php?latex=C_0+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C_0 &#92;to D" class="latex" /> with various nice properties, including extending to functors <img src="https://s0.wp.com/latex.php?latex=C+%5Cto+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C &#92;to D" class="latex" />.  (This is just a <i>property</i> here: such extensions are unique if they exist.)   It seems that when we do, we&#8217;ll find many of the famous concepts from probability theory and statistical mechanics.  </p>
<p>David C.&#8217;s point is also good: it&#8217;s wise to consider several variants of <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> hand in hand: finite measure spaces, finite probability measure spaces, and maybe even finite sub-probability measure spaces.   It&#8217;d be good to have a version such that &#8216;normalizing&#8217; a measure and making it into a probability measure becomes a functor.   R&eacute;nyi entropy for probability measures is nicely understood starting from the free energy of measures whose normalizations are those probability measures.</p>
<p>I have some more coherent remarks to make, but not now&mdash;I&#8217;ve been commandeered to get up and explore some neighborhoods in Singapore we haven&#8217;t been to yet, and that sounds like a great idea!</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1 parent highlander-comment" id="comment-5314">
				<div id="div-comment-5314" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5314">15 April, 2011 at 10:25 am</a>		</div>

		<p>David, Tom, John: thanks for your feedback! Very intriguing food for thought! And sorry for being a bit slow in replying &#8212; a couple of papers on the drawing board are crying for attention&#8230;</p>
<p>@David: That could be an interesting connection! I will go to the library later and check out Censov&#8217;s stuff. What confuses me, though, is that entropy is not monotone under Markov morphisms: it increases under randomization, but decreases under coarse-graining.</p>
<p>@Tom: wow, I had not thought about categories *enriched* over C! Very cool, by &#8220;base change&#8221; entropy maps every such category into a Lawvere metric space. So, what is a category enriched over C? Ignoring technicalities about cardinality, it&#8217;s just an ordinary category together with a probability measure on each hom-set. This is a &#8216;random morphism&#8217;! There is a compatibility condition which requires the following: for any h:X&#8211;&gt;Z and any other object Y, the probability P(h) equals the sum over all P(g)*P(f) where the composition of f:X&#8211;&gt;Y and g:Y&#8211;&gt;Z equals h. For example when the category is a finite group, this gives the normalized counting measure as the unique possibility. I wonder whether such probability measures always exist on a (say, finite) category, and how unique they are? Is there any relation to your work on Euler characteristics of categories?</p>
<p>Then, yes, you got me there, one should take care of stuff with measure 0.</p>
<p>About continuity, I would indeed hope that this comes out for free. Presently, I am confident that it is at least enough to assume continuity of the binary entropy around (1,0). For then one can consider various coarse-grainings of the product of a distribution p=(p_1,&#8230;,p_n) with the distribution (1-\epsilon,\epsilon) which give a distribution close to p with entropy close H(p). Without having worked out the details yet, I guess that this implies continuity.</p>
<p>@John: I agree that exponentiated entropy or &#8216;extropy&#8217; is nicer than entropy itself. Counting a number of states is more natural than specifying it in terms of &#8216;bits&#8217; or &#8216;nats&#8217;, isn&#8217;t it?<br />
However when dropping the normalization and going from probability spaces to measure spaces, we loose functoriality of the entropy! In the original category C, functoriality means that the entropy decreases under coarse-grainings. But if we take a probability space and include it as a subset of a measure space, then the entropy increases! So, we cannot choose the orientation on the poset of real numbers in a way consistent with functoriality&#8230; But still, the idea of rig homomorphism seems something to keep in mind.</p>
<p>In any case, I would be very happy to write a joint paper about this! It strucks me as surprising that this seems to be new. I will put down an outline in LaTeX over the next few days and email it to all of you, ok?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5314#respond' data-commentid="5314" data-postid="2467" data-belowelement="div-comment-5314" data-respondelement="respond" data-replyto="Reply to Tobias Fritz" aria-label='Reply to Tobias Fritz'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-2 highlander-comment" id="comment-5320">
				<div id="div-comment-5320" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5320">15 April, 2011 at 1:17 pm</a>		</div>

		<p>Great; I look forward to seeing where this goes.  </p>
<p>Regarding enrichment in C, I&#8217;m going to guess that not every finite category can be equipped with such a probability measure, and that when such measures do exist they needn&#8217;t be unique.  But I don&#8217;t think that makes the picture any less nice.</p>
<p>Regarding relation to Euler characteristic: yes, I think there&#8217;s something there.  Suppose you have a monoidal category C and you have some notion of the &#8220;size&#8221; of <i>objects of</i> C.  Then there arises, in an automatic way, a notion of &#8220;size&#8221; (or magnitude, or Euler characteristic) for <i>categories enriched in</i> C.  </p>
<p>Here, we know what the size of an object of C is: it&#8217;s the exponential of Shannon entropy.  (At least, that&#8217;s the most obvious choice; you could try Rényi too.)  So you get a notion of the size of a &#8220;probabilistic&#8221; category, i.e. a category enriched in C.</p>
<p>No time to say more for now, but can explain at more length if anyone wants.  (And might explain at more length even if no one wants.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5320#respond' data-commentid="5320" data-postid="2467" data-belowelement="div-comment-5320" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-2 parent highlander-comment" id="comment-5350">
				<div id="div-comment-5350" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5350">17 April, 2011 at 3:35 pm</a>		</div>

		<p>Tobias wrote:</p>
<blockquote>
<p>It strikes me as surprising that this seems to be new. </p>
</blockquote>
<p>I&#8217;m assuming that if it&#8217;s new to Tom, who has thought a lot about entropies and likes category theory, it&#8217;s new.</p>
<blockquote>
<p>I will put down an outline in LaTeX over the next few days and email it to all of you, ok?</p>
</blockquote>
<p>Great.  I wrote some material today and put it <a href="http://www.ncatlab.org/johnbaez/show/Entropy+as+a+Functor" rel="nofollow">here</a>.  It doesn&#8217;t go too far, but at least it sketches how to start with a rig functor and get the R&eacute;nyi entropy.  I&#8217;m hoping that this will be helpful somehow.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5350#respond' data-commentid="5350" data-postid="2467" data-belowelement="div-comment-5350" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-3 highlander-comment" id="comment-5353">
				<div id="div-comment-5353" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5353">17 April, 2011 at 8:46 pm</a>		</div>

		<blockquote>
<p>I&#8217;m assuming that if it&#8217;s new to Tom [&#8230;] it&#8217;s new.</p>
</blockquote>
<p>Dangerous.  I wouldn&#8217;t trust myself on that.</p>
<p>It might be worth having a look at <a href="http://www.maths.gla.ac.uk/~tl/Lawvere_Entropy.pdf" rel="nofollow">this 1984 preprint</a> of Lawvere.  I haven&#8217;t read it myself.  A thorough literature search would also include the work of Giry and Lawvere on the interaction of category theory and probability theory.  Again, I haven&#8217;t read that stuff.  This is why you shouldn&#8217;t rely on my knowledge of the literature :-)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5353#respond' data-commentid="5353" data-postid="2467" data-belowelement="div-comment-5353" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-3 parent highlander-comment" id="comment-5372">
				<div id="div-comment-5372" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5372">18 April, 2011 at 6:46 pm</a>		</div>

		<p>Excellent! I am still waiting to find a few spare hours in which to write a page or two about this.</p>
<p>Lawvere&#8217;s paper looks like it might actually contain these ideas. Haven&#8217;t read it yet, though.</p>
<p>Some more food for thought:</p>
<p>(1) The &#8220;basic inequalities&#8221; of information theory are the following, expressed in terms of joint Shannon entropy: </p>
<p>&bull; positivity of entropy, <img src="https://s0.wp.com/latex.php?latex=H%28X%29%5Cgeq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(X)&#92;geq 0" class="latex" /><br />
&bull;  positivity of conditional entropy, <img src="https://s0.wp.com/latex.php?latex=H%28X%29%5Cleq+H%28XY%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(X)&#92;leq H(XY)" class="latex" /><br />
&bull; positivity of mutual information, <img src="https://s0.wp.com/latex.php?latex=H%28XY%29%5Cleq+H%28X%29%2BH%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(XY)&#92;leq H(X)+H(Y)" class="latex" /><br />
&bull;  positivity of conditional mutual information, <img src="https://s0.wp.com/latex.php?latex=H%28Y%29+%2B+H%28XYZ%29+%5Cleq+H%28XY%29+%2B+H%28YZ%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(Y) + H(XYZ) &#92;leq H(XY) + H(YZ)" class="latex" /></p>
<p>By the requirement that the entropy should be a monoidal functor, one obtains a categorical explanation/proof of the first two properties. (The second follows from considering the morphism &#8216;take the marginal&#8217; from the joint distribution <img src="https://s0.wp.com/latex.php?latex=%28X%2CY%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,Y)" class="latex" /> to the distribution of <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.)</p>
<p>So now the question is: is there a categorical interpretation of the latter two properties? The third one looks a lot like Shannon entropy would be a <i>lax</i> monoidal functor, doesn&#8217;t it? And what about the fourth one?</p>
<p>(2) In information-theoretic terms, the intuition is to think of Shannon entropy as a sort of cardinality. Does that mean that Shannon entropy is somehow a decategorification, analogous to like the natural numbers are a decategorification of the category of finite sets? Is there a  construction for monoidal categories which generalizes Shannon&#8217;s idea of considering many copies of a morphism and using this to define invariants (entropy and channel capacity)?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5372#respond' data-commentid="5372" data-postid="2467" data-belowelement="div-comment-5372" data-respondelement="respond" data-replyto="Reply to Tobias Fritz" aria-label='Reply to Tobias Fritz'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-4 highlander-comment" id="comment-5373">
				<div id="div-comment-5373" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5373">18 April, 2011 at 6:48 pm</a>		</div>

		<p>BTW, I still haven&#8217;t figured out how to use LaTeX on this blog and don&#8217;t want to bother the elves every time I post something. Are there any instructions available?</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5374">
				<div id="div-comment-5374" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">David Corfield</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5374">18 April, 2011 at 8:03 pm</a>		</div>

		<p>You just put &#8216;$latex&#8217;, then a space, then your LaTeX, then &#8216;$&#8217;.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-5375">
				<div id="div-comment-5375" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5375">18 April, 2011 at 9:18 pm</a>		</div>

		<p>There are further tips on maths in WordPress <a href="http://sbseminar.wordpress.com/including-equations-in-comments/" rel="nofollow">here</a> and <a href="http://en.support.wordpress.com/latex/" rel="nofollow">here</a>.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-5378">
				<div id="div-comment-5378" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5378">19 April, 2011 at 12:58 am</a>		</div>

		<p>I fixed up the LaTeX on your most recent post, Tobias.  As David noted, I just needed to replace all expressions like this:</p>
<p>&#036;H(X)&#036;</p>
<p>with expressions like this:</p>
<p>&#036;latex H(X)&#036;</p>
<p>The space is crucial.  </p>
<p>And don&#8217;t use double dollar signs: they don&#8217;t work.  <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/cry.gif" alt="" /></p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 parent highlander-comment" id="comment-5426">
				<div id="div-comment-5426" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5426">21 April, 2011 at 2:16 am</a>		</div>

		<p>John, I edited your nLab page to add some new material.  (I&#8217;m saying this here in case anyone, such as Tobias, wants to take a look.)</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-5428">
				<div id="div-comment-5428" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5428">21 April, 2011 at 5:29 am</a>		</div>

		<p><img src="https://i2.wp.com/math.ucr.edu/home/baez/emoticons/thumbsup.gif" alt="" /></p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-3 highlander-comment" id="comment-5379">
				<div id="div-comment-5379" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5379">19 April, 2011 at 1:12 am</a>		</div>

		<p>Tobias wrote:</p>
<blockquote>
<p>Lawvere’s paper looks like it might actually contain these ideas. Haven’t read it yet, though.</p>
</blockquote>
<p>I don&#8217;t see him mentioning Shannon entropy or R&eacute;nyi entropy.  He seems to be talking about entropy, not from the viewpoint of statistical mechanics (where you start from a set of probability measures and write down a formula for the entropy of such a measure), but from the viewpoint of axiomatic thermodynamics (where you posit entropy as a function on some unspecified set or usually manifold, and impose axioms on it).  </p>
<p>Since I&#8217;m not very knowledgeable about axiomatic thermodynamics, and Lawvere is doing his usual thing of pushing the subject he&#8217;s talking about to new heights of abstraction, it would take me a while to extract useful information from this paper.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5379#respond' data-commentid="5379" data-postid="2467" data-belowelement="div-comment-5379" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 parent highlander-comment" id="comment-5380">
				<div id="div-comment-5380" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5380">19 April, 2011 at 1:33 am</a>		</div>

		<p>Tobias wrote:</p>
<blockquote>
<p>2) In information-theoretic terms, the intuition is to think of Shannon entropy as a sort of cardinality. Does that mean that Shannon entropy is somehow a decategorification, analogous to like the natural numbers are a decategorification of the category of finite sets? </p>
</blockquote>
<p>Something like that is true.  The big difference is that unlike sets, which form a rig category, probability measure spaces have a natural product but not a natural coproduct.  This is why I like measure spaces.  I am trying to push the viewpoint that &#8216;the&#8217; decategorification of the category of finite measure spaces is not the entropy or even the exponential of entropy, but the <i>partition function</i>.</p>
<p>The idea is to start with any finite set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> equipped with a measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> whose only set of measure zero is the empty set.</p>
<p>We can always write <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> as some function</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%5Cexp%28-E%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = &#92;exp(-E)" class="latex" /></p>
<p>times counting measure, where </p>
<p><img src="https://s0.wp.com/latex.php?latex=E+%3A+S+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E : S &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is some function called the <b>energy</b>.</p>
<p>Then we can think of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as sitting inside a 1-parameter family of measures given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28T%29+%3D+%5Cexp%28-E+%2F+T%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(T) = &#92;exp(-E / T)" class="latex" /></p>
<p>times counting measure, where <img src="https://s0.wp.com/latex.php?latex=T+%5Cin+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;in &#92;mathbb{R}" class="latex" /> is called the <b>temperature</b>.  </p>
<p>Then the <b>partition function</b> of the original finite measure space <img src="https://s0.wp.com/latex.php?latex=X+%3D+%28S%2C%5Cmu%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X = (S,&#92;mu)" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28X%29+%3D+%5Csum_%7Bi+%5Cin+S%7D+%5Cexp%28-E_i+%2F+T%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(X) = &#92;sum_{i &#92;in S} &#92;exp(-E_i / T)" class="latex" /></p>
<p>I&#8217;ve suppressed the variable <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> on the left side; <img src="https://s0.wp.com/latex.php?latex=Z%28X%2CT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(X,T)" class="latex" /> would be more accurate but it&#8217;s annoyingly long.   The partition function obeys</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28X+%2B+Y%29+%3D+Z%28X%29+%2B+Z%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(X + Y) = Z(X) + Z(Y)" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28X+%5Ctimes+Y%29+%3D+Z%28X%29+%5Ctimes+Z%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(X &#92;times Y) = Z(X) &#92;times Z(Y)" class="latex" /></p>
<p>as we&#8217;d expect from a decategorification.  </p>
<p>Moreover, as I told you <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-4048" rel="nofollow">here</a>, we can recover the original finite measure space <img src="https://s0.wp.com/latex.php?latex=X+%3D+%28S%2C%5Cmu%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X = (S,&#92;mu)" class="latex" /> up to isomorphism from its partition function!  So, we may call the partition function <i>the</i> decategorification of the category of finite measure spaces (whose only set of measure zero is the empty set).</p>
<p>If you don&#8217;t like the parenthetical remark you can allow infinite energies and say <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cinfty%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-&#92;infty) = 0" class="latex" />.  However, in my <a href="http://ncatlab.org/johnbaez/show/Entropy+as+a+Functor" rel="nofollow">writeup of our work so far</a>, I&#8217;ve been disallowing nonempty sets of measure zero.  This is a small detail which can be fine-tuned later to maximize overall elegance.</p>
<p>By the way: this <a href="http://ncatlab.org/johnbaez/show/Entropy+as+a+Functor" rel="nofollow">writeup</a> asks what you and Tom think about certain ideas, e.g. whether you can understand them and if you like them!  I&#8217;d appreciate some comments.</p>
<p>Also: your last comment has a lot of great ideas in it, but too many for me to process.  Being a slow pedestrian sort, I would like at first to focus on your original goal of characterizing R&eacute;nyi entropy as a functor!   </p>
<p>Since R&eacute;nyi entropy is a kind of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />-derivative of the logarithm of the partition function, it contains almost all the same information as the partition function.  And for the same reason, I&#8217;m hoping my category-theoretic treatment of the partition function will connect nicely to a category-theoretic treatment of the  e R&eacute;nyi entropy!  So, I&#8217;m hoping my writeup so far, which focuses on the partition function, is not really a huge digression from your original goal.  However, I am still confused about <b>why we&#8217;d want to take a perfectly nice rig homomorphism and then take the <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />-derivative of its logarithm!</b></p>
<p>Now, some things Tom wrote suggest that while probability measure spaces don&#8217;t have coproduct, we can still take two of them, say <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" />, together two numbers <img src="https://s0.wp.com/latex.php?latex=x%2Cy+%5Cin+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x,y &#92;in [0,1]" class="latex" /> that sum to 1, and form a probability measure space I&#8217;ll call <img src="https://s0.wp.com/latex.php?latex=x+X+%2B+y+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x X + y Y" class="latex" />, where we multiply the probability measure on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, and that on <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />.  More generally we can do this for <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> probability measure space using a point on the <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-simplex.  I think there&#8217;s some way to formalize this using operads, which I forget.  </p>
<p>But, it could be that this extra structure explains why we want to take the <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />-derivative of the logarithm of the partition function, and think of that as our favorite invariant of a probability measure space!  Anyone have any good ideas?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5380#respond' data-commentid="5380" data-postid="2467" data-belowelement="div-comment-5380" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-4 highlander-comment" id="comment-5381">
				<div id="div-comment-5381" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5381">19 April, 2011 at 3:07 am</a>		</div>

		<p>Yikes, all this interesting stuff.  I&#8217;m struggling to keep up.  </p>
<p>Two quick points.  One: in conversations with Mark Meckes, I&#8217;ve learned the important lesson that there&#8217;s no such thing as the entropy of a probability measure.  It&#8217;s really the entropy of one probability measure <i>relative</i> to another.  We&#8217;re misled by the fact that in the familiar examples, there&#8217;s a canonical reference measure: for finite sets, counting measure, and for <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{R}" class="latex" />, Lebesgue measure.</p>
<p>Second point: there&#8217;s an operadic characterization of entropy.  (I figured this out after some conversations with Steve Lack; as far as I know, it&#8217;s new.)  </p>
<p>Take the operad <img src="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta" class="latex" /> whose space of n-ary operations is <img src="https://s0.wp.com/latex.php?latex=%5CDelta_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n" class="latex" />, the probability distributions on an n-element set.  It&#8217;s a symmetric topological operad.  We can consider its algebras in Cat(Top), the category of categories internal to Top.  One such algebra is the topological monoid <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BR%7D%2C+%2B%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;mathbf{R}, +)" class="latex" />, regarded as a one-object topological category.</p>
<p>Now, given any operad P, there&#8217;s a notion of <i>lax map</i> between two P-algebras in Cat.  In particular, you can consider lax maps <img src="https://s0.wp.com/latex.php?latex=1+%5Cto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;to X" class="latex" /> whose domain is the terminal category 1, where <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a P-algebra in Cat.  Let&#8217;s call such a thing a <i>lax point</i> of <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  (The concept of &#8220;monoid in a monoidal category&#8221; is an example of a lax point, when you take P to be the terminal operad; lax points aren&#8217;t just something invented for this purpose.)</p>
<p>The theorem now is that the lax points of the <img src="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta" class="latex" />-algebra <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BR%7D%2C+%2B%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;mathbf{R}, +)" class="latex" /> are exactly the scalar multiples of Shannon entropy. </p>
<p>I might try to say that in a less hurried way another time.  I&#8217;ll also try to give you some feedback on what you wrote, John.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5405">
				<div id="div-comment-5405" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5405">19 April, 2011 at 11:55 pm</a>		</div>

		<p>OK, I&#8217;ve got a better idea of what you&#8217;re doing now, John.  I like the idea that the cardinality or decategorification is the entire partition function.  </p>
<p>Here are a couple of semi-notational queries.  First, you use temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />, but I&#8217;d understood previously that it&#8217;s better from a physics point of view to use inverse temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" />.  It also seems to be more convenient from a purely mathematical point of view.  Is there any reason why you&#8217;re sticking with <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />?  Or can we switch to <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />?</p>
<p>Second, is there a particular reason for working with <i>the logarithm of</i> <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />, rather than <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> itself?  I mean, you put <img src="https://s0.wp.com/latex.php?latex=E_i+%3D+-%5Cln+p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i = -&#92;ln p_i" class="latex" /> and </p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%28T%29+%3D+%5Cexp%28-E_i%2FT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(T) = &#92;exp(-E_i/T)" class="latex" /></p>
<p>when you could instead just say</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%28T%29+%3D+p_i%5E%7B1%2FT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(T) = p_i^{1/T}" class="latex" />.</p>
<p>Or if we work with <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> instead, that becomes the even simpler</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%28%5Cbeta%29+%3D+p_i%5E%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(&#92;beta) = p_i^&#92;beta" class="latex" />.</p>
<p>To me it seems easier to skip the <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" />s altogether and work directly with the <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />s.  </p>
<p>If working with energies is just something so deeply ingrained in every physicist that it seems more natural that way, then fine, I can understand.  But I&#8217;d like to find out whether there&#8217;s also a mathematical reason for doing so.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-4 highlander-comment" id="comment-5461">
				<div id="div-comment-5461" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5461">20 April, 2011 at 4:52 am</a>		</div>

		<p>Tom wrote:</p>
<blockquote><p>
Is there any reason why you’re sticking with <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />? Or can we switch to <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />?
</p></blockquote>
<p>That&#8217;s fine; I&#8217;d love to wake up tomorrow and discover that you edited that page so you like it better.  </p>
<p>If we use <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />, I&#8217;ll need to explain that in physics applications it&#8217;s <img src="https://s0.wp.com/latex.php?latex=1%2FkT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/kT" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> it Boltzmann&#8217;s constant and <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is temperature.   But that&#8217;s fine; it just takes a sentence, and maybe all this &#8216;physical interpretation&#8217; stuff belongs in its own separate section.  For me it&#8217;s absolutely crucial, but for other readers it might be distracting or (worse) intimidating.</p>
<blockquote><p>
Second, is there a particular reason for working with the logarithm of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />, rather than <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> itself?
</p></blockquote>
<p>I guess that for me the idea of taking a probability and raising it to, say, the 1.7th power seemed very bizarre and unnatural until I had the revelation that led to my R&eacute;nyi entropy paper.</p>
<p>I mean, I understood perfectly well when we might want to <i>add</i> probabilities, and when we might want to <i>multiply</i> them.  So it made perfect sense to <i>square</i> a probability: if I have a 1/52 chance of drawing an ace of spades from a deck of cards, I have a 1/52<sup></sup><sup>2</sup> chance of drawing an ace of spades from each of two decks.  But raising a probability to some noninteger power like 1.7?  <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/confused.gif" alt="" /></p>
<p>Then I noticed that it&#8217;s a perfectly standard operation, but physicists call it &#8216;dividing the temperature by 1.7&#8217;, i.e. replacing</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-E%2FT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-E/T)" class="latex" /></p>
<p>by </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-1.7+E+%2F+T%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-1.7 E / T)" class="latex" /></p>
<p>This is something they do all the time!   I have an easy comfortable intuition for what it means to &#8216;chill out&#8217; a physical system: improbable states become more improbable as it gets colder.  So, all of a sudden <img src="https://s0.wp.com/latex.php?latex=p%5E%7B1.7%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^{1.7}" class="latex" /> stopped looking so bizarre.</p>
<p>(There&#8217;s an important subtlety I&#8217;m skimming over here: when we take a probability distribution and raise all the probabilities to some power, we don&#8217;t get a probability distribution any more: we need to normalize the result!  And this is what the physicists do.)</p>
<p>So anyway: go ahead and change </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+E_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-&#92;beta E_i)" class="latex" /></p>
<p>to </p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%5E%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i^&#92;beta" class="latex" /> </p>
<p>if you like.  At some point I&#8217;ll need to add an explanation of &#8216;what the heck is really going on&#8217;, so that people who like physics can more deeply appreciate what we&#8217;re doing.  But I&#8217;d need to do that no matter what!  So, you might as well make the math as elegant as you can.</p>
<p>If you&#8217;d let me &#8216;borrow&#8217; your result on Shannon entropy and operads, I might like to add that to this paper.  (If you&#8217;re planning to publish it somewhere else, we can cite that).  I bet it&#8217;s an important part of the overall story.   There&#8217;s got to be some nice interpretation of R&eacute;nyi entropy as a functor (say, valued in functions of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />), and then an explanation of how this functor becomes &#8216;better&#8217; when we restrict it to <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />.</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1 highlander-comment" id="comment-5315">
				<div id="div-comment-5315" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">David Corfield</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5315">15 April, 2011 at 11:04 am</a>		</div>

		<p>Regarding normalization, it often seems that there are reasons to keep three associated spaces under consideration when considering a space <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />: the cone of finite measures, the truncated cone of subprobability measures, and the latter&#8217;s face of probability measures. The differential geometry of the Fisher-Rao metric applies to each. </p>
<p>These three cropped up in our conversation <a href="http://golem.ph.utexas.edu/category/2010/09/what_is_this_category_enriched.html" rel="nofollow">here</a>. There are certain advantages in not restricting to normalized measures, e.g., <a href="http://golem.ph.utexas.edu/category/2010/09/what_is_this_category_enriched.html#c034715" rel="nofollow">this one</a>.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5315#respond' data-commentid="5315" data-postid="2467" data-belowelement="div-comment-5315" data-respondelement="respond" data-replyto="Reply to David Corfield" aria-label='Reply to David Corfield'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 highlander-comment" id="comment-5318">
				<div id="div-comment-5318" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">David Corfield</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5318">15 April, 2011 at 11:59 am</a>		</div>

		<p>I always seem to be the one wanting an indexed category approach. We have a base category of sets, and fibred above each is the cone of finite measures. There are multiplication and sums of fibres ending up as fibres over products and disjoint union of sets, respectively.</p>
<p>I never can decide as to which morphisms to have in the base category: functions, relations, stochastic relations, conditional probability distributions. Then there would be interesting inducings going on between the fibres. And I guess the possibility of some hyperdoctrinal-type quantifier adjoints. We looked at this a little in the <a href="http://golem.ph.utexas.edu/category/2007/09/progic_ii.html" rel="nofollow">Progic</a> thread.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5318#respond' data-commentid="5318" data-postid="2467" data-belowelement="div-comment-5318" data-respondelement="respond" data-replyto="Reply to David Corfield" aria-label='Reply to David Corfield'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1 parent highlander-comment" id="comment-5367">
				<div id="div-comment-5367" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5367">18 April, 2011 at 1:18 pm</a>		</div>

		<p>Here&#8217;s something that you might find interesting in connection with entropy, John.  Fact: the positive probability distributions on a finite set naturally form a real vector space.  </p>
<p>I don&#8217;t know how well known this is, or how relevant it is to what we&#8217;re doing, but here goes.  Let n be a positive integer.  Write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta_n+%3D+%5C%7B+p+%5Cin+%5B0%2C+%5Cinfty%29%5En+%5Ccolon+%5Csum+p_i+%3D+1+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n = &#92;{ p &#92;in [0, &#92;infty)^n &#92;colon &#92;sum p_i = 1 &#92;}" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta_n%5E%5Ccirc+%3D+%5C%7B+p+%5Cin+%280%2C+%5Cinfty%29%5En+%5Ccolon+%5Csum+p_i+%3D+1+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n^&#92;circ = &#92;{ p &#92;in (0, &#92;infty)^n &#92;colon &#92;sum p_i = 1 &#92;}" class="latex" />.</p>
<p>The statement is that <img src="https://s0.wp.com/latex.php?latex=%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n^&#92;circ" class="latex" /> is naturally a real vector space.  To say it briefly, it&#8217;s canonically isomorphic to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BR%7D%5En%2F%5Clangle%281%2C+%5Cldots%2C+1%29%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{R}^n/&#92;langle(1, &#92;ldots, 1)&#92;rangle" class="latex" /> via the bijection <img src="https://s0.wp.com/latex.php?latex=%5Cexp%3A+%5Cmathbf%7BR%7D+%5Cto+%280%2C+%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp: &#92;mathbf{R} &#92;to (0, &#92;infty)" class="latex" />.</p>
<p>In more detail: exponential provides a bijection between <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{R}^n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%280%2C+%5Cinfty%29%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0, &#92;infty)^n" class="latex" />.  Thus, <img src="https://s0.wp.com/latex.php?latex=%280%2C+%5Cinfty%29%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0, &#92;infty)^n" class="latex" /> becomes a real vector space whose &#8220;zero&#8221; is <img src="https://s0.wp.com/latex.php?latex=%281%2C+%5Cldots%2C+1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1, &#92;ldots, 1)" class="latex" />, whose &#8220;addition&#8221; is pointwise multiplication, and whose &#8220;scalar multiplication&#8221; assigns to a real number <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and a point <img src="https://s0.wp.com/latex.php?latex=%28a_1%2C+%5Cldots%2C+a_n%29+%5Cin+%280%2C+%5Cinfty%29%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(a_1, &#92;ldots, a_n) &#92;in (0, &#92;infty)^n" class="latex" /> the point <img src="https://s0.wp.com/latex.php?latex=%28a_1%5E%5Clambda%2C+%5Cldots%2C+a_n%5E%5Clambda%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(a_1^&#92;lambda, &#92;ldots, a_n^&#92;lambda)" class="latex" />.  </p>
<p>Now, the subspace <img src="https://s0.wp.com/latex.php?latex=%5Clangle%281%2C+%5Cldots%2C+1%29%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle(1, &#92;ldots, 1)&#92;rangle" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{R}^n" class="latex" /> corresponds to the subspace <img src="https://s0.wp.com/latex.php?latex=W+%3D+%5C%7B%28t%2C+%5Cldots%2C+t%29+%5Ccolon+t+%5Cin+%280%2C+%5Cinfty%29+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W = &#92;{(t, &#92;ldots, t) &#92;colon t &#92;in (0, &#92;infty) &#92;}" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%280%2C+%5Cinfty%29%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0, &#92;infty)^n" class="latex" />.  The cosets of the subspace W are the rays <img src="https://s0.wp.com/latex.php?latex=%5C%7B+ta+%5Ccolon+t+%5Cin+%280%2C+%5Cinfty%29%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{ ta &#92;colon t &#92;in (0, &#92;infty)&#92;}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=a+%5Cin+%280%2C+%5Cinfty%29%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a &#92;in (0, &#92;infty)^n" class="latex" />.  (So our quotient is quite like the familiar quotient that gets you projective space.)  Each such ray meets <img src="https://s0.wp.com/latex.php?latex=%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n^&#92;circ" class="latex" /> exactly once, so we can identify the quotient space <img src="https://s0.wp.com/latex.php?latex=%280%2C+%5Cinfty%29%5En%2FW&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0, &#92;infty)^n/W" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n^&#92;circ" class="latex" />.  This gives <img src="https://s0.wp.com/latex.php?latex=%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n^&#92;circ" class="latex" /> its vector space structure.</p>
<p>Explicitly, the vector space structure on <img src="https://s0.wp.com/latex.php?latex=%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta_n^&#92;circ" class="latex" /> is as follows:</p>
<p>* The &#8220;zero&#8221; is <img src="https://s0.wp.com/latex.php?latex=%281%2Fn%2C+%5Cldots%2C+1%2Fn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1/n, &#92;ldots, 1/n)" class="latex" />.</p>
<p>* The &#8220;sum&#8221; of p and r is <img src="https://s0.wp.com/latex.php?latex=%28p_1+r_1%2C+%5Cldots%2C+p_n+r_n%29%2F%5Csum+p_i+r_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1 r_1, &#92;ldots, p_n r_n)/&#92;sum p_i r_i" class="latex" />.</p>
<p>* &#8220;Scalar multiplication&#8221; takes a scalar <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5Cmathbf%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda &#92;in &#92;mathbf{R}" class="latex" /> and a point <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in &#92;Delta_n^&#92;circ" class="latex" />, and produces the point <img src="https://s0.wp.com/latex.php?latex=%28p_1%5E%5Clambda%2C+%5Cldots%2C+p_n%5E%5Clambda%29%2F%5Csum+p_i%5E%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1^&#92;lambda, &#92;ldots, p_n^&#92;lambda)/&#92;sum p_i^&#92;lambda" class="latex" />.</p>
<p>There&#8217;s a linear map <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BR%7D%5En+%5Cto+%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{R}^n &#92;to &#92;Delta_n^&#92;circ" class="latex" /> given by </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%28e%5E%7Bx_1%7D%2C+%5Cldots%2C+e%5E%7Bx_n%7D%29%2F%5Csum+e%5E%7Bx_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;mapsto (e^{x_1}, &#92;ldots, e^{x_n})/&#92;sum e^{x_i}" class="latex" />.</p>
<p>It&#8217;s surjective, and its kernel is <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%281%2C+%5Cldots%2C+1%29+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (1, &#92;ldots, 1) &#92;rangle" class="latex" />, so <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BR%7D%5En%2F%5Clangle+%281%2C+%5Cldots%2C+1%29+%5Crangle+%5Ccong+%5CDelta_n%5E%5Ccirc&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{R}^n/&#92;langle (1, &#92;ldots, 1) &#92;rangle &#92;cong &#92;Delta_n^&#92;circ" class="latex" />.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5367#respond' data-commentid="5367" data-postid="2467" data-belowelement="div-comment-5367" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-2 highlander-comment" id="comment-5368">
				<div id="div-comment-5368" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5368">18 April, 2011 at 1:50 pm</a>		</div>

		<p>Hi, Tom!  Thanks for all that.  You&#8217;ll note that in my paper on R&eacute;nyi entropy, I do what all physicists always do: I identify a strictly positive measure <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on a finite set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> with a point <img src="https://s0.wp.com/latex.php?latex=E+%5Cin+%5Cmathbb%7BR%7D%5ES&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E &#92;in &#92;mathbb{R}^S" class="latex" /> via</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cexp%28-E_i%2F+T%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;exp(-E_i/ T) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is a positive constant we get to choose.</p>
<p>Physicists call <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> &#8216;temperature&#8217; and <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> &#8216;energy&#8217; so that nonphysicists are scared to learn this useful trick.</p>
<p>But you&#8217;re going further&#8230; I&#8217;ll have to think about what I can with that.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5368#respond' data-commentid="5368" data-postid="2467" data-belowelement="div-comment-5368" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-2 highlander-comment" id="comment-5369">
				<div id="div-comment-5369" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5369">18 April, 2011 at 1:58 pm</a>		</div>

		<p>Hi John!  You don&#8217;t scare me.  Actually, I was well aware that the expressions coming up in my last comment look  similar to the expressions appearing in your paper.  That&#8217;s why I was hoping it might be helpful.  </p>
<p>(There also seems to be some stuff about magnitude lurking in the background here.  See Example 2.2.3(ii) of <a href="http://arxiv.org/abs/1012.5857" rel="nofollow">The magnitude of a metric space</a>, if you feel like it.)</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5369#respond' data-commentid="5369" data-postid="2467" data-belowelement="div-comment-5369" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent highlander-comment" id="comment-5397">
				<div id="div-comment-5397" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://2.gravatar.com/avatar/e12fed9193da121c6337ce250d548759?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">David Corfield</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5397">19 April, 2011 at 10:52 am</a>		</div>

		<p>Did people here ever read <a href="http://mathoverflow.net/questions/20740/is-there-an-introduction-to-probability-theory-from-a-structuralist-categorical-p/20820#20820" rel="nofollow">Pavlov&#8217;s account</a> of how to approach measure theory in a category theoretic way? John&#8217;s supervisor is involved.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5397#respond' data-commentid="5397" data-postid="2467" data-belowelement="div-comment-5397" data-respondelement="respond" data-replyto="Reply to David Corfield" aria-label='Reply to David Corfield'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment odd alt depth-2 parent highlander-comment" id="comment-5400">
				<div id="div-comment-5400" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/49e4bc6445aa00cd61eb470ad2270ef0?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn">Tim van Beek</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5400">19 April, 2011 at 12:50 pm</a>		</div>

		<p>Ugh, I did not know that theorem of Segal, nor the term &#8220;localized&#8221; measure space, but knowing that, and that these form an equivalent category to the W-star-algebras is certainly important.</p>
<p>As Mark Meckes wrote in a complementary answer over at mathoverflow, in probability theory the probability spaces are usually swept under the rug&#8230;</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5400#respond' data-commentid="5400" data-postid="2467" data-belowelement="div-comment-5400" data-respondelement="respond" data-replyto="Reply to Tim van Beek" aria-label='Reply to Tim van Beek'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-3 highlander-comment" id="comment-5462">
				<div id="div-comment-5462" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5462">22 April, 2011 at 9:05 am</a>		</div>

		<p>Tim wrote:</p>
<blockquote>
<p>these form an equivalent category to the W-star-algebras&#8230;</p>
</blockquote>
<p>Of course you mean <i>commutative</i> W*-algebras.</p>
<p>Working with Segal forced me to read <i>Integrals and Operators</i> by Segal and Kunze, which explains the &#8216;algebraic&#8217; approach to integration theory.  But they don&#8217;t express things in terms of an equivalence of categories, since Segal didn&#8217;t like categories.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5462#respond' data-commentid="5462" data-postid="2467" data-belowelement="div-comment-5462" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt thread-even depth-1 parent highlander-comment" id="comment-5463">
				<div id="div-comment-5463" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5463">22 April, 2011 at 9:27 am</a>		</div>

		<p>Here are some more thoughts, mainly for Tobias Fritz and Tom Leinster, but also for David Corfield and anyone else who is interested.  I&#8217;ve been writing stuff on the nLab, but I need another burst of energy to overcome some obstacles, and maybe a bit more chatting here will help.  </p>
<p>Tom <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5381" rel="nofollow">mentioned</a> that &#8220;the lax points of the <img src="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta" class="latex" />-algebra <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BR%7D%2C+%2B%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;mathbb{R}, +)" class="latex" /> are exactly the scalar multiples of Shannon entropy&#8221;.  While I still need to unravel that a bit, I figure that it should be pretty close to Fadeev&#8217;s theorem about Shannon entropy mentioned on the first page here:</p>
<p>&bull; Alfréd Rényi, <a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.bsmsp/1200512181" rel="nofollow">  Measures of information and entropy</a>, in Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability 1960, pp. 547–561.</p>
<p>But how close, exactly?</p>
<p>Let me state Fadeev&#8217;s theorem.  I&#8217;ll modify the statement slightly to make it sound a bit more like a category theorist said it. </p>
<p>Here goes:</p>
<blockquote>
<p><b>Theorem.</b> Let <img src="https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C_0" class="latex" /> be the groupoid of finite probability measure spaces, and regard <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> as a category with a unique morphism from <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> iff <img src="https://s0.wp.com/latex.php?latex=x+%5Cge+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;ge y" class="latex" />.  Treat both of these as topological categories (categories internal to <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BTop%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Top}" class="latex" />) in the obvious way.   Suppose that</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%3A+C+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H: C &#92;to [0,&#92;infty)" class="latex" /> </p>
<p>is a continous functor.  Moreover, suppose that <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> obeys the &#8216;magical property&#8217;.  Then <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is a nonnegative multiple of Shannon entropy.</p>
</blockquote>
<p>So, what&#8217;s the &#8216;magical property&#8217;?  To state this, let&#8217;s write a probability measure on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cdots%2Cn%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1,&#92;dots,n&#92;}" class="latex" /> as an <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-tuple of numbers <img src="https://s0.wp.com/latex.php?latex=p_i+%5Cin+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;in [0,1]" class="latex" /> with </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi+%3D+1%7D%5En+p_i+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i = 1}^n p_i = 1" class="latex" /></p>
<p>Then the <b>magical property</b> is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28t+p_1%2C+%281-t%29p_1%2C+p_2%2C+%5Cdots%2C+p_n%29+%3D+H%28p_1%2C+%5Cdots%2C+p_n%29+%2B+p_1+H%28t%2C+1-t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(t p_1, (1-t)p_1, p_2, &#92;dots, p_n) = H(p_1, &#92;dots, p_n) + p_1 H(t, 1-t)" class="latex" /></p>
<p>What&#8217;s the nice way to state this magic property using category theory?  </p>
<p>How is this related to Tom&#8217;s result?</p>
<p>Fadeev&#8217;s result mentions continuity, Tom&#8217;s doesn&#8217;t.  Oh, no &#8211; I think Tom&#8217;s result secretly <i>does</i>.  Are your operads topological, Tom?  I think so.</p>
<p>Fadeev&#8217;s &#8216;magical property&#8217; seems to have something to with how <img src="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta" class="latex" /> is an operad: we&#8217;re taking a probability measure on a 2-point set and a probability measure on an <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-point set and &#8216;glomming them together&#8217; and getting a probability measure on a <img src="https://s0.wp.com/latex.php?latex=%28n%2B1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(n+1)" class="latex" />-point set. </p>
<p>Okay, so now I&#8217;m guessing Tom&#8217;s result really <i>is</i> Fadeev&#8217;s theorem, stated in a slick way.</p>
<p>Am I on the right track, Tom?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5463#respond' data-commentid="5463" data-postid="2467" data-belowelement="div-comment-5463" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-2 parent highlander-comment" id="comment-5465">
				<div id="div-comment-5465" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5465">22 April, 2011 at 10:30 am</a>		</div>

		<p>Finding a categorical interpretation of the magical property is a fun puzzle! I think I can provide an answer.</p>
<p>As Tom has emphasized, it is often better to think in terms of relative entropy. So let&#8217;s try to take a category like FinMeas and assign entropies to the morphisms instead of the objects! Actually, let me revert to working with normalized measures, so that the category now is FinProb, finite probability spaces with measure-preserving functions. Optimally, we would expect to get a functor FinProb &#8212;&gt; R, where now R is the abelian group of real numbers under addition, regarded as a category with one object. But that would be too good to be true, wouldn&#8217;t it?</p>
<p>But that is precisely what relative Shannon entropy is!! For an object <img src="https://s0.wp.com/latex.php?latex=%5C%7Bp_1%2C%5Cldots%2Cp_n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{p_1,&#92;ldots,p_n&#92;}" class="latex" /> and a map <img src="https://s0.wp.com/latex.php?latex=f%3A%5Bn%5D%5Crightarrow+%5Bm%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f:[n]&#92;rightarrow [m]" class="latex" />, the relative entropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28f%29%3D%5Csum_i+p_i%5Clog%5Cfrac%7Bp_%7Bf%28i%29%7D%7D%7Bp_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(f)=&#92;sum_i p_i&#92;log&#92;frac{p_{f(i)}}{p_i}" class="latex" /></p>
<p>A simple calculation shows that this indeed maps composition of measure-preserving maps to sums of relative entropies. The magical property is the special case of this when we consider the object <img src="https://s0.wp.com/latex.php?latex=%5C%7Btp_1%2C%281-t%29p_1%2Cp_2%2C%5Cldots%2Cp_n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{tp_1,(1-t)p_1,p_2,&#92;ldots,p_n&#92;}" class="latex" />, the morphism which identifies the first two elements, and from there the morphism to the terminal one-element probability space.</p>
<p>I hope that this makes sense. In fact, Lawvere&#8217;s paper deals with this kind of functor! So, it might be a good idea to try and understand that.</p>
<p>BTW, sorry for not being able to keep up with the pace here&#8230; I see new comments appear faster than I can read them, and the <a href="http://www.ncatlab.org/johnbaez/show/Entropy+as+a+Functor" rel="nofollow">nlab page</a> also has expanded at a frightening rate ;) And starting tomorrow I will be offline until Wednesday&#8230;</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5465#respond' data-commentid="5465" data-postid="2467" data-belowelement="div-comment-5465" data-respondelement="respond" data-replyto="Reply to Tobias Fritz" aria-label='Reply to Tobias Fritz'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 highlander-comment" id="comment-5467">
				<div id="div-comment-5467" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5467">22 April, 2011 at 10:37 am</a>		</div>

		<p>You <i>are</i> keeping up with the pace, Tobias!  You answered my puzzle shortly after I asked it!  We&#8217;re coming up with so much stuff, none of us feels like we&#8217;re keeping up with the other two.  </p>
<p>I really like the idea of studying <i>relative</i> entropy as a functor, in the way you&#8217;ve described.  You&#8217;ll have to give me until Wednesday to think about this.  I can&#8217;t keep up with the pace!  <img src="https://i1.wp.com/math.ucr.edu/home/baez/emoticons/tongue.gif" alt="" /></p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5467#respond' data-commentid="5467" data-postid="2467" data-belowelement="div-comment-5467" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-3 parent highlander-comment" id="comment-5616">
				<div id="div-comment-5616" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5616">28 April, 2011 at 8:02 pm</a>		</div>

		<p>Tobias, I don&#8217;t understand your formula for H(f).  Maybe that&#8217;s because I don&#8217;t understand the set-up.</p>
<p>You say &#8220;for an object <img src="https://s0.wp.com/latex.php?latex=%5C%7Bp_1%2C+%5Cldots%2C+p_n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{p_1, &#92;ldots, p_n&#92;}" class="latex" />&#8220;.  I guess this is an object of the category FinProb you&#8217;d just mentioned, i.e. a finite probability space.  You also say &#8220;a map <img src="https://s0.wp.com/latex.php?latex=f%3A+%5Bn%5D+%5Cto+%5Bm%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: [n] &#92;to [m]" class="latex" />&#8220;.  Presumably <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D+%3D+%5C%7B1%2C+%5Cldots%2C+n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[n] = &#92;{1, &#92;ldots, n&#92;}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is simply a map of finite sets.  This gives a pushforward measure <img src="https://s0.wp.com/latex.php?latex=f_%2A%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_*(p)" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Bm%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[m]" class="latex" />.</p>
<p>Now in the formula for H(f), we have a sum running over <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />.  Since there are terms <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> in the summation, I assume that <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> runs over <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[n]" class="latex" />.  But then there&#8217;s also a term <img src="https://s0.wp.com/latex.php?latex=p_%7Bf%28i%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_{f(i)}" class="latex" />, which then doesn&#8217;t make sense.  Maybe you mean <img src="https://s0.wp.com/latex.php?latex=%28f_%2A%28p%29%29_%7Bf%28i%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f_*(p))_{f(i)}" class="latex" />?  Applying the definition of pushforward measure, the formula then becomes</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28f%29++%3D+%5Csum_%7Bi+%5Cin+%5Bn%5D%7D+p_i+%5Clog++%5Cfrac%7B%5Csum_%7Bi%27+%5Cin+%5Bn%5D%3A+f%28i%27%29+%3D+f%28i%29%7D+p_%7Bi%27%7D%7D%7Bp_i%7D.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(f)  = &#92;sum_{i &#92;in [n]} p_i &#92;log  &#92;frac{&#92;sum_{i&#039; &#92;in [n]: f(i&#039;) = f(i)} p_{i&#039;}}{p_i}. " class="latex" /></p>
<p>That looks a bit unlikely, so I guess that&#8217;s <i>not</i> what you meant, and I&#8217;m puzzled.</p>
<p>I&#8217;m also puzzled because I&#8217;d understood relative entropy to be something that was defined for a pair of probability measures on the same (finite) set.  How does that fit into your definition of the relative entropy of a map?</p>
<p>Are you, perhaps, taking a measure <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[n]" class="latex" />, another measure <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Bm%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[m]" class="latex" />, and a map <img src="https://s0.wp.com/latex.php?latex=f%3A+%5Bn%5D+%5Cto+%5Bm%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: [n] &#92;to [m]" class="latex" /> of finite sets, and then considering the entropy of <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=f_%2A%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_*(p)" class="latex" />?</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5616#respond' data-commentid="5616" data-postid="2467" data-belowelement="div-comment-5616" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5617">
				<div id="div-comment-5617" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5617">28 April, 2011 at 8:21 pm</a>		</div>

		<p>Sorry for having been unclear on this. It may seem unlikely, but your guesses in your initial three paragraphs are correct.</p>
<p>You are right in reminding me that relative entropy refers to a situation with two probability measures on the same set. So it might be better to call this the &#8216;entropy of a map&#8217;. I would suggest to think of it as quantifying the information loss one incurs by applying the map.</p>
<p>Does this make sense to you?</p>
<p>I am currently working on writing this down on the <a href="http://www.ncatlab.org/johnbaez/show/Entropy+as+a+Functor#a_functorial_characterization_of_shannon_entropy_17" rel="nofollow">nlab page</a>. I hope that it&#8217;s not complete bogus :P</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-4 highlander-comment" id="comment-5618">
				<div id="div-comment-5618" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5618">29 April, 2011 at 6:51 am</a>		</div>

		<p>Hi, T&amp;T!  </p>
<p>I&#8217;m glad you&#8217;re writing more stuff on the nLab.  I&#8217;ve been distracted by the Desargues graph </p>
<div align="center">
<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/DesarguesGraph.svg/220px-DesarguesGraph.svg.png" />
</div>
<p>and its role in chemistry&mdash;you&#8217;ll see what I mean someday soon.</p>
<p>I too was confused by Tobias&#8217; use of the term &#8216;relative entropy&#8217;, until I saw he was talking about some sort of entropy of a map instead of the usual notion.</p>
<p>One funny thing is that R&eacute;nyi&#8217;s <i>own</i> characterization of R&eacute;nyi entropies (Theorem 3 on page 55 <a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.bsmsp/1200512181#page=9" rel="nofollow">here</a>) uses the <i>usual</i> concept of relative entropy.  So, one thing I want to do is understand <i>that</i> result better using more category theory!    But what&#8217;s a nice way to think about the category of finite sets with <i>two</i> probability measures on them.</p>
<p>I will attempt to rejoin the fray as soon as I pull myself away from my current obsession.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5621">
				<div id="div-comment-5621" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5621">29 April, 2011 at 8:39 am</a>		</div>

		<p>Actually, my idea for finding a categorical interpretation of the &#8216;magical property&#8217; <b>is</b> complete bogus, as I noticed while writing it out in detail on the nLab page.</p>
<p>If <img src="https://s0.wp.com/latex.php?latex=f%3AX%5Crightarrow+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f:X&#92;rightarrow Y" class="latex" /> is measure-preserving, then my definition boils down to saying that</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28f%29%3DH%28X%29-H%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(f)=H(X)-H(Y)" class="latex" /></p>
<p>and then it is a triviality that <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is functorial! In particular, one can do precisely the same thing for Rényi entropy.</p>
<p>When we start with some probability space <img src="https://s0.wp.com/latex.php?latex=X%3D%28tp_1%2C%281-t%29p_1%2Cp_2%5Cldots%2Cp_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X=(tp_1,(1-t)p_1,p_2&#92;ldots,p_n)" class="latex" />, define <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with the first two elements identified, takes <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> to be the projection map from <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" />, then we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%28tp_1%2C%281-t%29p_1%2Cp_2%2C%5Cldots%2Cp_n%29+%3D+H%28p_1%2C%5Cldots%2Cp_n%29+%2B+H%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(tp_1,(1-t)p_1,p_2,&#92;ldots,p_n) = H(p_1,&#92;ldots,p_n) + H(f)" class="latex" /></p>
<p>and this <img src="https://s0.wp.com/latex.php?latex=H%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(f)" class="latex" /> corresponds to the remaining term in the magical property.</p>
<p>However, what is missing is the identification of this third term as <img src="https://s0.wp.com/latex.php?latex=p_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1" class="latex" /> times a binary entropy!</p>
<p>At some point I would like to understand the operadic formulation of the magical property. Is your (Tom&#8217;s) book the right place for doing that, given that I understand the basic ideas behind operads?</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-5622">
				<div id="div-comment-5622" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://1.gravatar.com/avatar/a69ff5f5b8fdc40526f286344b59ef4f?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://users.icfo.es/Tobias.Fritz/' rel='external nofollow ugc' class='url'>Tobias Fritz</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5622">29 April, 2011 at 10:30 am</a>		</div>

		<p>I erroneously used the term &#8216;relative entropy&#8217; for the following reason: let&#8217;s consider two random variables <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" />, which are objects in the category. Then also the joint variable <img src="https://s0.wp.com/latex.php?latex=%28X%2CY%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,Y)" class="latex" /> is an object in the category, and we may consider the projection  morphism <img src="https://s0.wp.com/latex.php?latex=%28X%2CY%29%5Cto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,Y)&#92;to X" class="latex" />.</p>
<p>Then the entropy of this projection morphism is the relative entropy of <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />. So the entropy of a map generalizes the concept of relative entropy.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5624">
				<div id="div-comment-5624" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5624">29 April, 2011 at 8:36 pm</a>		</div>

		<p>Ah!  Excellent.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-4 highlander-comment" id="comment-5625">
				<div id="div-comment-5625" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5625">30 April, 2011 at 1:10 am</a>		</div>

		<p>Tobias wrote:</p>
<blockquote><p>
At some point I would like to understand the operadic formulation of the magical property. Is your (Tom’s) book the right place for doing that, given that I understand the basic ideas behind operads?
</p></blockquote>
<p>Did you look at the material Tom wrote <a href="http://ncatlab.org/johnbaez/show/Entropy+as+a+Functor#shannon_entropy" rel="nofollow">on the nLab?</a>  It&#8217;s just a sketch, so we should fill in details.  I&#8217;m not sure I understand every detail, but I feel sure I <i>could</i>.  So, maybe you can ask some questions here and I&#8217;ll try to answer them by filling in details on the nLab.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5626">
				<div id="div-comment-5626" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5626">30 April, 2011 at 1:21 am</a>		</div>

		<p>Tobias, sorry, I missed your question about my book.  No, I don&#8217;t think it&#8217;s the right place to get the background for that result on operads and entropy.  </p>
<p>Off the top of my head I can&#8217;t think where the definition of lax map of algebras for an operad is written down.  It&#8217;s essentially a special case of the notion of lax map of algebras for a 2-monad, but that doesn&#8217;t really help.  What I should do at some point is simply write it out.  The trouble is that it involves diagrams, which are a bit of a pain in this medium.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even depth-4 highlander-comment" id="comment-5627">
				<div id="div-comment-5627" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5627">30 April, 2011 at 1:44 am</a>		</div>

		<p>Tom: here&#8217;s what I&#8217;m confused about.  The <i>laxness</i> of the map between algebras would seem to be a way to express an <i>inequality</i>, since there&#8217;s a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" />, or <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" />, whenever one number is greater than or equal to another.  But none of Fadeev&#8217;s axioms for Shannon entropy involve inequalities!</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt depth-4 highlander-comment" id="comment-5629">
				<div id="div-comment-5629" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5629">30 April, 2011 at 4:04 am</a>		</div>

		<p>John: you&#8217;re confused because you&#8217;re treating <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> as a poset, whereas I&#8217;m treating it as a monoid (viewed as a 1-object category).  So a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> is not an inequality: it&#8217;s a real number.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-4 highlander-comment" id="comment-5630">
				<div id="div-comment-5630" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5630">30 April, 2011 at 6:13 am</a>		</div>

		<p>Just when I was in the mood for typing up some diagrams (and <i>that&#8217;s</i> not common), the nLab seems to be down.  So I latexed them instead.  In fact, I did this operadic characterization of Shannon entropy in very nearly complete detail.  It&#8217;s <a href="http://www.maths.gla.ac.uk/~tl/operadic_entropy.pdf" rel="nofollow">here</a>.</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-4 highlander-comment" id="comment-5637">
				<div id="div-comment-5637" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5637">1 May, 2011 at 2:51 am</a>		</div>

		<p>Thanks, Tom!  I have more to say, but I feel like carrying this conversation <a href="http://golem.ph.utexas.edu/category/2011/05/categorytheoretic_characteriza.html" rel="nofollow">over to the n-Caf&eacute;</a>, since it&#8217;s getting a bit heavy on the category theory, so people will enjoy it more there than here.</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even depth-2 parent highlander-comment" id="comment-5466">
				<div id="div-comment-5466" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/3f761ffe05c1c24605981803edd055a6?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://www.maths.gla.ac.uk/~tl' rel='external nofollow ugc' class='url'>Tom Leinster</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5466">22 April, 2011 at 10:33 am</a>		</div>

		<p>Just about to get on the train to the 92nd Peripatetic Seminar on Sheaves and Logic in Oxford &#8212; think of it like the Hogwarts Express &#8212; so will answer briefly.  Yes, that result on &#8220;lax points&#8221; is a dressing-up of the (Fadeev) result in Rényi&#8217;s paper.  Yes, my operads were topological.  Yes, the magical property is about operadic composition (but stated a bit more economically).</p>
<p>If you unravel the definition of lax point, you find that there are three conditions: one on composition in the operad, one on the unit in the operad, and one on the symmetric group action.  (And because everything&#8217;s in Top, you get continuity too.)  The symmetry one is symmetry, and the unit one is redundant.  But what I think is the cool part is that the composition one is essentially the magical property.  </p>
<p>So, that&#8217;s how you can state the magical property using category theory.  </p>
<p>(There are some other ways to state it using category theory; Steve Lack and I figured out a couple.  But the one about lax points is the slickest formulation that I know of.)</p>
<p>I&#8217;ll type up the details if you don&#8217;t get there before me, but you can guess them anyway.  If you know &#8212; or can guess &#8212; the definition of a lax map of algebras for a 2-monad, then you have everything you need to unravel my statement.</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5466#respond' data-commentid="5466" data-postid="2467" data-belowelement="div-comment-5466" data-respondelement="respond" data-replyto="Reply to Tom Leinster" aria-label='Reply to Tom Leinster'>Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor odd alt depth-3 highlander-comment" id="comment-5470">
				<div id="div-comment-5470" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5470">22 April, 2011 at 10:44 am</a>		</div>

		<p>Hmm, so you and Tobias are proposing seemingly different explanations of the magical property!  If they&#8217;re both correct, we have a bit of thinking to do.  The nice thing about his is that it brings relative entropy into the game, which seems to be crucial for characterizing R&eacute;nyi entropy &mdash; at least the way R&eacute;nyi did it.</p>
<p>Have fun at the Hogwarts Academy of Sheaves and Logic!  </p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5470#respond' data-commentid="5470" data-postid="2467" data-belowelement="div-comment-5470" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-johncarlosbaez bypostauthor even thread-odd thread-alt depth-1 highlander-comment" id="comment-5464">
				<div id="div-comment-5464" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' />			<cite class="fn"><a href='http://math.ucr.edu/home/baez/' rel='external nofollow ugc' class='url'>John Baez</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5464">22 April, 2011 at 10:10 am</a>		</div>

		<p>Okay, next I want to take this paper:</p>
<p>&bull; Alfréd Rényi, <a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.bsmsp/1200512181" rel="nofollow">  Measures of information and entropy</a>, in Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability 1960, pp. 547–561.</p>
<p>and try to translate his characterization of R&eacute;nyi entropies into category-theoretic language.  As with Fadeev&#8217;s characterization of Shannon entropy, I won&#8217;t entirely succeed.  In fact this time I&#8217;ll succeed even less!  But it&#8217;s worth trying&#8230;</p>
<p>Actually he gives two characterizations, Theorem 2 on page 553 and Theorem 3 on page 555.  The first theorem is a bit unsatisfactory because it involves the function </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%5Calpha%28x%29+%3D+2%5E%7B%28%5Calpha+-+1%29+x%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_&#92;alpha(x) = 2^{(&#92;alpha - 1) x}" class="latex" /></p>
<p>which looks suspiciously like something you&#8217;d see in the definition of R&eacute;nyi entropy.  The second one is nicer but it characterizes the <i>relative</i> R&eacute;nyi entropy of one probability measure with respect to another.  Of course faithful readers of this blog know I consider relative entropy to be more fundamental than &#8216;plain old&#8217; entropy.  So, I should be happy.  But it sounds like we may need a fancier category where an object consists of <i>two</i> probablity measures on the same finite set, and that&#8217;s slightly off-putting.</p>
<p>Anyway, let me try the first theorem.  There&#8217;s another twist involved in this theorem: it&#8217;s not about probability measures on finite sets, it&#8217;s about what <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5315" rel="nofollow">David</a> calls &#8216;subprobability measures&#8217;, meaning nonnegative measures whose total mass is <img src="https://s0.wp.com/latex.php?latex=%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;le 1" class="latex" />.  R&eacute;nyi calls them &#8216;generalized probability measures&#8217;, but I like David&#8217;s term better.</p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=X+%3D+%28S%2C%5Cmu%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X = (S,&#92;mu)" class="latex" /> is a finite subprobability measure space, meaning that <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is a finite set and <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> is a nonnegative measure on <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=w%28X%29+%3D+%5Cint_S+d+%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w(X) = &#92;int_S d &#92;mu " class="latex" /></p>
<p>is <img src="https://s0.wp.com/latex.php?latex=%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;le 1" class="latex" />.  And suppose <img src="https://s0.wp.com/latex.php?latex=Y+%3D+%28T%2C%5Cnu%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y = (T,&#92;nu)" class="latex" /> is another finite subprobability measure space.  Then the disjoint union of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> gets a subprobability measure on it iff</p>
<p><img src="https://s0.wp.com/latex.php?latex=w%28X%29+%2B+w%28Y%29+%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w(X) + w(Y) &#92;le 1" class="latex" />.  </p>
<p>and when this happens, we&#8217;ll call the resulting subprobability measure space <img src="https://s0.wp.com/latex.php?latex=X+%2B+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X + Y" class="latex" />.   It&#8217;s annoying that this <img src="https://s0.wp.com/latex.php?latex=%2B+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="+ " class="latex" /> operation is only partially defined, but let&#8217;s soldier on&#8230;</p>
<blockquote>
<p><b>Theorem.</b> Let <img src="https://s0.wp.com/latex.php?latex=C%27_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C&#039;_0" class="latex" /> be the groupoid of finite subprobability measure spaces, and regard <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> as a category with a unique morphism from <img src="https://s0.wp.com/latex.php?latex=x+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;to y" class="latex" /> iff <img src="https://s0.wp.com/latex.php?latex=x+%5Cge+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;ge y" class="latex" />. Treat both of these as topological categories in the obvious way. Suppose that</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%3A+C+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H: C &#92;to [0,&#92;infty)" class="latex" /></p>
<p>is a continuous functor.   Moreover supppose that:</p>
<p>1)  <img src="https://s0.wp.com/latex.php?latex=H%28X+%5Ctimes+Y%29+%3D+H%28X%29+%2B+H%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(X &#92;times Y) = H(X) + H(Y)" class="latex" /> </p>
<p>2) <img src="https://s0.wp.com/latex.php?latex=H%28X%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H(X) = 1" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is the one-point set equipped with a measure with <img src="https://s0.wp.com/latex.php?latex=w%28X%29+%3D+1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w(X) = 1/2" class="latex" />.</p>
<p>3) Whenever <img src="https://s0.wp.com/latex.php?latex=X+%2B+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X + Y" class="latex" /> is defined, </p>
<p><img src="https://s0.wp.com/latex.php?latex=w%28X+%2B+Y%29+g_%5Calpha%28H%28X+%2B+Y%29%29+%3D+w%28X%29+g_%5Calpha%28H%28X%29%29+%2B+w%28Y%29+g_%5Calpha%28H%28Y%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w(X + Y) g_&#92;alpha(H(X + Y)) = w(X) g_&#92;alpha(H(X)) + w(Y) g_&#92;alpha(H(Y))" class="latex" /></p>
</blockquote>
<p>Here <img src="https://s0.wp.com/latex.php?latex=g_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_&#92;alpha" class="latex" /> is that function I mentioned!</p>
<p>So: can we make this more pretty?  </p>
<p>Opportunities abound, because it&#8217;s so darn ugly.  Condition 1) is beautiful, condition 2) is a dumb little normalization condition, and condition 3) is the 800-pound gorilla that needs to be trained and beautified.  </p>
<p>For what it&#8217;s worth, the &#8216;total mass function&#8217;</p>
<p><img src="https://s0.wp.com/latex.php?latex=w%3A+C%27_0+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w: C&#039;_0 &#92;to [0,&#92;infty)" class="latex" /></p>
<p>is a rig functor:</p>
<p><img src="https://s0.wp.com/latex.php?latex=w%28X+%2B+Y%29+%3D+w%28X%29+%2B+w%28Y%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w(X + Y) = w(X) + w(Y) " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=w%28X+%5Ctimes+Y%29+%3D+w%28X%29+%5Ctimes+w%28Y%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w(X &#92;times Y) = w(X) &#92;times w(Y)" class="latex" /></p>
<p>and it&#8217;s also continuous.  I know what all continuous rig functors from <img src="https://s0.wp.com/latex.php?latex=C%27_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C&#039;_0" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> are, in case that helps.  </p>
<p>And for what it&#8217;s worth, the functions <img src="https://s0.wp.com/latex.php?latex=g_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_&#92;alpha" class="latex" /> are all the continuous functions from <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> that send addition to multiplication and obey some other tiny property.</p>
<p>So, there&#8217;s a lot of stuff about addition and multiplication being preserved, or one getting changed to the other, going on here!</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5464#respond' data-commentid="5464" data-postid="2467" data-belowelement="div-comment-5464" data-respondelement="respond" data-replyto="Reply to John Baez" aria-label='Reply to John Baez'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="pingback odd alt thread-even depth-1 highlander-comment" id="comment-5628">
				<div id="div-comment-5628" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://johncarlosbaez.wordpress.com/2011/04/30/crooks-fluctuation-theorem/' rel='external nofollow ugc' class='url'>Crooks’ Fluctuation Theorem « Azimuth</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-5628">30 April, 2011 at 3:55 am</a>		</div>

		<p>John Baez&#8217; recent relation between free energy and R&eacute;nyi entropy is a nice potential competitor for the efficient calculation of free energy differences [&#8230;]</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=5628#respond' data-commentid="5628" data-postid="2467" data-belowelement="div-comment-5628" data-respondelement="respond" data-replyto="Reply to Crooks’ Fluctuation Theorem « Azimuth" aria-label='Reply to Crooks’ Fluctuation Theorem « Azimuth'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="pingback even thread-odd thread-alt depth-1 highlander-comment" id="comment-6144">
				<div id="div-comment-6144" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/' rel='external nofollow ugc' class='url'>A Characterization of Entropy « Azimuth</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-6144">2 June, 2011 at 10:49 am</a>		</div>

		<p>Tsallis entropy is a close relative of R&eacute;nyi entropy, which I discussed here earlier. Just as R&eacute;nyi entropy is a kind of q-derivative of the free energy, the Tsallis entropy is a kind of q-derivative of the partition function [&#8230;]</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=6144#respond' data-commentid="6144" data-postid="2467" data-belowelement="div-comment-6144" data-respondelement="respond" data-replyto="Reply to A Characterization of Entropy « Azimuth" aria-label='Reply to A Characterization of Entropy « Azimuth'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="pingback odd alt thread-even depth-1 highlander-comment" id="comment-12689">
				<div id="div-comment-12689" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='http://threeplusone.com/gec/archives/20' rel='external nofollow ugc' class='url'>Article published: Measures of trajectory ensemble disparity in nonequilibrium statistical dynamics | Gavin E. Crooks</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-12689">20 January, 2012 at 8:39 pm</a>		</div>

		<p>[&#8230;] actually be experimentally measured in real systems. I was finally inspired to write this up due to John Baez, who recently discussed the significance of Renyi entropy to equilbrium statistical [&#8230;]</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=12689#respond' data-commentid="12689" data-postid="2467" data-belowelement="div-comment-12689" data-respondelement="respond" data-replyto="Reply to Article published: Measures of trajectory ensemble disparity in nonequilibrium statistical dynamics | Gavin E. Crooks" aria-label='Reply to Article published: Measures of trajectory ensemble disparity in nonequilibrium statistical dynamics | Gavin E. Crooks'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="pingback even thread-odd thread-alt depth-1 highlander-comment" id="comment-16497">
				<div id="div-comment-16497" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/' rel='external nofollow ugc' class='url'>The Mathematics of Biodiversity (Part 4) « Azimuth</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-16497">2 July, 2012 at 9:58 am</a>		</div>

		<p>[&#8230;] I have written about R&eacute;nyi entropy and its role in thermodynamics before on this blog. I&#8217;ll also talk about it later in this conference, and I&#8217;ll show you my slides. [&#8230;]</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=16497#respond' data-commentid="16497" data-postid="2467" data-belowelement="div-comment-16497" data-respondelement="respond" data-replyto="Reply to The Mathematics of Biodiversity (Part 4) « Azimuth" aria-label='Reply to The Mathematics of Biodiversity (Part 4) « Azimuth'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="pingback odd alt thread-even depth-1 highlander-comment" id="comment-18734">
				<div id="div-comment-18734" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://johncarlosbaez.wordpress.com/2012/08/24/more-second-laws-of-thermodynamics/' rel='external nofollow ugc' class='url'>More Second Laws of Thermodynamics « Azimuth</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comment-18734">24 August, 2012 at 2:14 pm</a>		</div>

		<p>[&#8230;] so we&#8217;re continuing some conversations about entropy that we started last year, back when the Entropy Club was active. But now Jamie Vicary and Brendan Fong are involved [&#8230;]</p>

		<div class="reply"><a rel='nofollow' class='comment-reply-link' href='https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=18734#respond' data-commentid="18734" data-postid="2467" data-belowelement="div-comment-18734" data-respondelement="respond" data-replyto="Reply to More Second Laws of Thermodynamics « Azimuth" aria-label='Reply to More Second Laws of Thermodynamics « Azimuth'>Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ol>



	<div id="respond" class="comment-respond">
		<h3 id="reply-title" class="comment-reply-title">Leave a Reply to <a href="#comment-4032">John Baez</a> <small><a rel="nofollow" id="cancel-comment-reply-link" href="/2011/02/10/rnyi-entropy-and-free-energy/#respond">Cancel reply</a></small></h3><form action="https://johncarlosbaez.wordpress.com/wp-comments-post.php" method="post" id="commentform" class="comment-form"><input type="hidden" id="highlander_comment_nonce" name="highlander_comment_nonce" value="43cb85f550" /><input type="hidden" name="_wp_http_referer" value="/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4032" />
<input type="hidden" name="hc_post_as" id="hc_post_as" value="guest" />

<div class="comment-form-field comment-textarea">
	<label for="comment">Enter your comment here...</label>
	<div id="comment-form-comment"><textarea id="comment" name="comment" title="Enter your comment here..."></textarea></div>
</div>

<div id="comment-form-identity">
	<div id="comment-form-nascar">
		<p>Fill in your details below or click an icon to log in:</p>
		<ul>
			<li class="selected" style="display:none;">
				<a href="#comment-form-guest" id="postas-guest" class="nascar-signin-link"
                   title="Login via Guest">
									</a>
			</li>
			<li>
				<a href="#comment-form-load-service:WordPress.com" id="postas-wordpress" class="nascar-signin-link"
                   title="Login via WordPress.com">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24" ><rect x="0" fill="none" width="24" height="24"/><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"/></g></svg>				</a>
			</li>
			<li>
			<iframe id="googleplus-sign-in" name="googleplus-sign-in" src="https://public-api.wordpress.com/connect/?googleplus-sign-in=https%3A%2F%2Fjohncarlosbaez.wordpress.com&#038;color_scheme=light" width="24" height="24" scrolling="no" allowtransparency="true" seamless="seamless" frameborder="0"></iframe>
			</li>
			<li>
				<a href="#comment-form-load-service:Twitter" id="postas-twitter" class="nascar-signin-link"
                   title="Login via Twitter">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24" ><rect x="0" fill="none" width="24" height="24"/><g><path fill="#1DA1F2" d="M22.23 5.924c-.736.326-1.527.547-2.357.646.847-.508 1.498-1.312 1.804-2.27-.793.47-1.67.812-2.606.996C18.325 4.498 17.258 4 16.078 4c-2.266 0-4.103 1.837-4.103 4.103 0 .322.036.635.106.935-3.41-.17-6.433-1.804-8.457-4.287-.353.607-.556 1.312-.556 2.064 0 1.424.724 2.68 1.825 3.415-.673-.022-1.305-.207-1.86-.514v.052c0 1.988 1.415 3.647 3.293 4.023-.344.095-.707.145-1.08.145-.265 0-.522-.026-.773-.074.522 1.63 2.038 2.817 3.833 2.85-1.404 1.1-3.174 1.757-5.096 1.757-.332 0-.66-.02-.98-.057 1.816 1.164 3.973 1.843 6.29 1.843 7.547 0 11.675-6.252 11.675-11.675 0-.178-.004-.355-.012-.53.802-.578 1.497-1.3 2.047-2.124z"/></g></svg>				</a>
			</li>
			<li>
				<a href="#comment-form-load-service:Facebook" id="postas-facebook" class="nascar-signin-link"
                   title="Login via Facebook">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24" ><rect x="0" fill="none" width="24" height="24"/><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"/></g></svg>				</a>
			</li>
		</ul>
	</div>

	<div id="comment-form-guest" class="comment-form-service selected">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
<a href="https://gravatar.com/site/signup/" target="_blank">				<img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" alt="Gravatar" width="25" class="no-grav" />
</a>			</div>

				<div class="comment-form-fields">
				<div class="comment-form-field comment-form-email">
					<label for="email">Email <span class="required">(required)</span> <span class="nopublish">(Address never made public)</span></label>
					<div class="comment-form-input"><input id="email" name="email" type="email" value="" /></div>
				</div>
				<div class="comment-form-field comment-form-author">
					<label for="author">Name <span class="required">(required)</span></label>
					<div class="comment-form-input"><input id="author" name="author" type="text" value="" /></div>
				</div>
				<div class="comment-form-field comment-form-url">
					<label for="url">Website</label>
					<div class="comment-form-input"><input id="url" name="url" type="url" value="" /></div>
				</div>
			</div>
			
		</div>
	</div>

	<div id="comment-form-wordpress" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" alt="WordPress.com Logo" width="25" class="no-grav" />
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="wp_avatar" id="wordpress-avatar" class="comment-meta-wordpress" value="" />
				<input type="hidden" name="wp_user_id" id="wordpress-user_id" class="comment-meta-wordpress" value="" />
				<input type="hidden" name="wp_access_token" id="wordpress-access_token" class="comment-meta-wordpress" value="" />
						<p class="comment-form-posting-as pa-wordpress">
			<strong></strong>
			You are commenting using your WordPress.com account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( 'wordpress' );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24" ><rect x="0" fill="none" width="24" height="24"/><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"/></g></svg></span>
		</p>
					</div>
	
		</div>
	</div>

	<div id="comment-form-googleplus" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" alt="Google photo" width="25" class="no-grav" />
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="googleplus_avatar" id="googleplus-avatar" class="comment-meta-googleplus" value="" />
				<input type="hidden" name="googleplus_user_id" id="googleplus-user_id" class="comment-meta-googleplus" value="" />
				<input type="hidden" name="googleplus_access_token" id="googleplus-access_token" class="comment-meta-googleplus" value="" />
						<p class="comment-form-posting-as pa-googleplus">
			<strong></strong>
			You are commenting using your Google account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( 'googleplus' );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" x="0px" y="0px" viewBox="0 0 60 60" ><path fill="#519bf7" d="M56.3,30c0,-1.6 -0.2,-3.4 -0.6,-5h-3.1H42.2H30v10.6h14.8C44,39.3 42,42 39.1,43.9l8.8,6.8C53,46 56.3,39 56.3,30z" /><path fill="#3db366" d="M30,57.5c6.7,0 13.1,-2.4 17.9,-6.8l-8.8,-6.8c-2.5,1.6 -5.6,2.4 -9.1,2.4c-7.2,0 -13.3,-4.7 -15.4,-11.2l-9.3,7.1C9.8,51.3 19.1,57.5 30,57.5z" /><path fill="#fdc600" d="M5.3,42.2l9.3,-7.1c-0.5,-1.6 -0.8,-3.3 -0.8,-5.1s0.3,-3.5 0.8,-5.1l-9.3,-7.1C3.5,21.5 2.5,25.6 2.5,30S3.5,38.5 5.3,42.2z" /><path fill="#f15b44" d="M40.1,17.4l8,-8C43.3,5.1 37,2.5 30,2.5C19.1,2.5 9.8,8.7 5.3,17.8l9.3,7.1c2.1,-6.5 8.2,-11.1 15.4,-11.1C33.9,13.7 37.4,15.1 40.1,17.4z" /></svg></span>
		</p>
					</div>
	
		</div>
	</div>

	<div id="comment-form-twitter" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" alt="Twitter picture" width="25" class="no-grav" />
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="twitter_avatar" id="twitter-avatar" class="comment-meta-twitter" value="" />
				<input type="hidden" name="twitter_user_id" id="twitter-user_id" class="comment-meta-twitter" value="" />
				<input type="hidden" name="twitter_access_token" id="twitter-access_token" class="comment-meta-twitter" value="" />
						<p class="comment-form-posting-as pa-twitter">
			<strong></strong>
			You are commenting using your Twitter account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( 'twitter' );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24" ><rect x="0" fill="none" width="24" height="24"/><g><path fill="#1DA1F2" d="M22.23 5.924c-.736.326-1.527.547-2.357.646.847-.508 1.498-1.312 1.804-2.27-.793.47-1.67.812-2.606.996C18.325 4.498 17.258 4 16.078 4c-2.266 0-4.103 1.837-4.103 4.103 0 .322.036.635.106.935-3.41-.17-6.433-1.804-8.457-4.287-.353.607-.556 1.312-.556 2.064 0 1.424.724 2.68 1.825 3.415-.673-.022-1.305-.207-1.86-.514v.052c0 1.988 1.415 3.647 3.293 4.023-.344.095-.707.145-1.08.145-.265 0-.522-.026-.773-.074.522 1.63 2.038 2.817 3.833 2.85-1.404 1.1-3.174 1.757-5.096 1.757-.332 0-.66-.02-.98-.057 1.816 1.164 3.973 1.843 6.29 1.843 7.547 0 11.675-6.252 11.675-11.675 0-.178-.004-.355-.012-.53.802-.578 1.497-1.3 2.047-2.124z"/></g></svg></span>
		</p>
					</div>
	
		</div>
	</div>

	<div id="comment-form-facebook" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="" alt="Facebook photo" width="25" class="no-grav" />
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="fb_avatar" id="facebook-avatar" class="comment-meta-facebook" value="" />
				<input type="hidden" name="fb_user_id" id="facebook-user_id" class="comment-meta-facebook" value="" />
				<input type="hidden" name="fb_access_token" id="facebook-access_token" class="comment-meta-facebook" value="" />
						<p class="comment-form-posting-as pa-facebook">
			<strong></strong>
			You are commenting using your Facebook account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( 'facebook' );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24" ><rect x="0" fill="none" width="24" height="24"/><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"/></g></svg></span>
		</p>
					</div>
	
		</div>
	</div>


	<div id="comment-form-load-service" class="comment-form-service">
		<div class="comment-form-posting-as-cancel"><a href="javascript:HighlanderComments.cancelExternalWindow();">Cancel</a></div>
		<p>Connecting to %s</p>
	</div>

</div>

<script type="text/javascript">
var highlander_expando_javascript = function () {

	function hide( sel ) {
		var el = document.querySelector( sel );
		if ( el ) {
			el.style.setProperty( 'display', 'none' );
		}
	}

	function show( sel ) {
		var el = document.querySelector( sel );
		if ( el ) {
			el.style.removeProperty( 'display' );
		}
	}

	var input = document.createElement( 'input' );
	var comment = document.querySelector( '#comment' );

	if ( input && comment && 'placeholder' in input ) {
		var label = document.querySelector( '.comment-textarea label' );
		if ( label ) {
			var text = label.textContent;
			label.parentNode.removeChild( label );
			comment.setAttribute( 'placeholder', text );
		}
	}

	// Expando Mode: start small, then auto-resize on first click + text length
	hide( '#comment-form-identity' );
	hide( '#comment-form-subscribe' );
	hide( '#commentform .form-submit' );

	if ( comment ) {
		comment.style.height = '10px';

		var handler = function () {
			comment.style.height = HighlanderComments.initialHeight + 'px';
			show( '#comment-form-identity' );
			show( '#comment-form-subscribe' );
			show( '#commentform .form-submit' );
			HighlanderComments.resizeCallback();

			comment.removeEventListener( 'focus', handler );
		};

		comment.addEventListener( 'focus', handler );
	}
}

if ( document.readyState !== 'loading' ) {
	highlander_expando_javascript();
} else {
	if ( typeof window.jQuery === 'function' ) {
		// Use jQuery's `ready` if available.
		// This solves some scheduling issues between this script and the main highlander script.
		jQuery( document ).ready( highlander_expando_javascript );
	} else {
		// If not available, add a vanilla event listener.
		document.addEventListener( 'DOMContentLoaded', highlander_expando_javascript );
	}
}

</script>

<div id="comment-form-subscribe">
	<p class="comment-subscription-form"><input type="checkbox" name="subscribe" id="subscribe" value="subscribe" style="width: auto;"/> <label class="subscribe-label" id="subscribe-label" for="subscribe" style="display: inline;">Notify me of new comments via email.</label></p><p class="post-subscription-form"><input type="checkbox" name="subscribe_blog" id="subscribe_blog" value="subscribe" style="width: auto;"/> <label class="subscribe-label" id="subscribe-blog-label" for="subscribe_blog"  style="display: inline;">Notify me of new posts via email.</label></p></div>




<p class="form-submit wp-block-button"><input name="submit" type="submit" id="comment-submit" class="submit wp-block-button__link" value="Post Comment" /> <input type='hidden' name='comment_post_ID' value='2467' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='4032' />
</p><p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="3c704dc608" /></p>
<input type="hidden" name="genseq" value="1632617990" />
<input type="hidden" id="ak_js" name="ak_js" value="213"/><textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100" style="display: none !important;"></textarea><script>document.getElementById( "ak_js" ).setAttribute( "value", ( new Date() ).getTime() );</script></form>	</div><!-- #respond -->
	<div style="clear: both"></div><p class="akismet_comment_form_privacy_notice">This site uses Akismet to reduce spam. <a href="https://akismet.com/privacy/" target="_blank" rel="nofollow noopener">Learn how your comment data is processed</a>.</p>
</div><!-- #comments -->
	
	</div>



	<div id="sidebar">
				<ul>

		
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/07/29/structured-vs-decorated-cospans-part-2/">Structured vs Decorated Cospans (Part&nbsp;2)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="amarashiki" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/#comment-172555">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Robert A. Wilson" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="https://robwilson1.wordpress.com" rel="nofollow"><img alt='' src='https://2.gravatar.com/avatar/24dc7e2371491f36fd4538b3474920b5?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="https://robwilson1.wordpress.com" rel="nofollow">Robert A. Wilson</a> on <a href="https://johncarlosbaez.wordpress.com/2021/04/04/the-koide-formula/#comment-172553">The Koide Formula</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172545">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Wolfgang" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://1.gravatar.com/avatar/d3c6d7ec8069e25c08a0a11263581925?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">Wolfgang on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172543">Classical Mechanics versus The&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (478)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (204)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,227 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/?replytocom=4032"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="8f1337fe87" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,176,979 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-c125fd6bead727c64b5f5ec96abc9c99">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-dd50e8b0462cc7fb7fa5bb0785e4f2a4">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-1f56f97d97b228240272e0ba442cda02">
	</div>
	<div class="grofile-hash-map-bb43afd67d78b8cb0fe47f3169a49f8c">
	</div>
	<div class="grofile-hash-map-176e9ac7f3a85880bf6333b1c2be8a39">
	</div>
	<div class="grofile-hash-map-8c446c3d1711a4e28dbf00133a229109">
	</div>
	<div class="grofile-hash-map-face81a4a2e39b202dd43a85aacd59ca">
	</div>
	<div class="grofile-hash-map-3f761ffe05c1c24605981803edd055a6">
	</div>
	<div class="grofile-hash-map-f1fe3183e03a7e20a67e29c24151863c">
	</div>
	<div class="grofile-hash-map-656e05d084448337fb49459225dc525e">
	</div>
	<div class="grofile-hash-map-e12fed9193da121c6337ce250d548759">
	</div>
	<div class="grofile-hash-map-8e48939cdd0316335e2a171c4b76ceb8">
	</div>
	<div class="grofile-hash-map-dc21eed3ebc499422d6c4a927c71ac4f">
	</div>
	<div class="grofile-hash-map-a69ff5f5b8fdc40526f286344b59ef4f">
	</div>
	<div class="grofile-hash-map-2c35fe624daf48f00e57289b3616c1a7">
	</div>
	<div class="grofile-hash-map-6b3fa8cd421b5fd2028e3c1e9c32aa7f">
	</div>
	<div class="grofile-hash-map-5850d628c599d7ac60d7f7889844995a">
	</div>
	<div class="grofile-hash-map-e522cb5eda111a1afa7f345f8423c13c">
	</div>
	<div class="grofile-hash-map-716fa9d449b2090e185ff19c2fb5bb2d">
	</div>
	<div class="grofile-hash-map-49e4bc6445aa00cd61eb470ad2270ef0">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	<div class="grofile-hash-map-24dc7e2371491f36fd4538b3474920b5">
	</div>
	<div class="grofile-hash-map-d3c6d7ec8069e25c08a0a11263581925">
	</div>
	</div>
<script id='highlander-comments-js-extra'>
var HighlanderComments = {"loggingInText":"Logging In\u2026","submittingText":"Posting Comment\u2026","postCommentText":"Post Comment","connectingToText":"Connecting to %s","commentingAsText":"%1$s: You are commenting using your %2$s account.","logoutText":"Log Out","loginText":"Log In","connectURL":"https:\/\/johncarlosbaez.wordpress.com\/public.api\/connect\/?action=request","logoutURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?action=logout&_wpnonce=598fb2ebe8","homeURL":"https:\/\/johncarlosbaez.wordpress.com\/","postID":"2467","gravDefault":"identicon","enterACommentError":"Please enter a comment","enterEmailError":"Please enter your email address here","invalidEmailError":"Invalid email address","enterAuthorError":"Please enter your name here","gravatarFromEmail":"This picture will show whenever you leave a comment. Click to customize it.","logInToExternalAccount":"Log in to use details from one of these accounts.","change":"Change","changeAccount":"Change Account","comment_registration":"","userIsLoggedIn":"","isJetpack":"","text_direction":"ltr"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/_static/??/wp-content/js/jquery/jquery.autoresize.js,/wp-content/mu-plugins/highlander-comments/script.js?m=1626677336j'></script>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFTs0OgjAMfiHHAgcTDsZnGVtDNuk2207g7S2JXIyJp/58v3atxpcskMUmtgFe0UPdusQXq1DMfmkB+MB8QVSaIajL3mHMJ+nUYzN1aXPMbF1Q3EyOLDoWIN2MkPMP/hapcXo2oP0zurVqkKlUtl2j9Mfyq42egBOEP0XWGGYQttwm9hSrxJKPDne89ddhHPthuPbpDZCYYT4='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','post':'2467','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZVYlhiMkYuKz1XUnVaX0lsJmh3L2xxcHNhKzVFcDhfWklkZkImNjl5OFdzKy5CNHlwT1QvQmduQ2xKY1Z0bVI3JW9+MythaEZ2SG9naWl+fHhYbzFTJlVDVm13LF16UnU3VHlTVnNmPU44bVMzVDZ3fndDSS50XTZYUS0yOC1nemZ5ZHlkRkIwbmNmflpJRXM5Z2hrbU4lU1Y3NT1QeUpuK0h2ZFRfS0pLVmlnXT9QNSVLdEdDeD12Tyt0eWZKS0pfRHdWWW5uWGZJWU5tUjdnb3d+clQsdDJpJg=='}]);
_stq.push([ 'clickTrackerInit', '12777403', '2467' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>