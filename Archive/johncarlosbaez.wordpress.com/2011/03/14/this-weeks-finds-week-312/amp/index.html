<!DOCTYPE html>
<html amp lang="en" i-amphtml-layout="" i-amphtml-no-boilerplate="" transformed="self;v=1">
<head><meta charset="utf-8"><style amp-runtime="" i-amphtml-version="012109102127000">html{overflow-x:hidden!important}html.i-amphtml-fie{height:100%!important;width:100%!important}html:not([amp4ads]),html:not([amp4ads]) body{height:auto!important}html:not([amp4ads]) body{margin:0!important}body{-webkit-text-size-adjust:100%;-moz-text-size-adjust:100%;-ms-text-size-adjust:100%;text-size-adjust:100%}html.i-amphtml-singledoc.i-amphtml-embedded{-ms-touch-action:pan-y pinch-zoom;touch-action:pan-y pinch-zoom}html.i-amphtml-fie>body,html.i-amphtml-singledoc>body{overflow:visible!important}html.i-amphtml-fie:not(.i-amphtml-inabox)>body,html.i-amphtml-singledoc:not(.i-amphtml-inabox)>body{position:relative!important}html.i-amphtml-ios-embed-legacy>body{overflow-x:hidden!important;overflow-y:auto!important;position:absolute!important}html.i-amphtml-ios-embed{overflow-y:auto!important;position:static}#i-amphtml-wrapper{overflow-x:hidden!important;overflow-y:auto!important;position:absolute!important;top:0!important;left:0!important;right:0!important;bottom:0!important;margin:0!important;display:block!important}html.i-amphtml-ios-embed.i-amphtml-ios-overscroll,html.i-amphtml-ios-embed.i-amphtml-ios-overscroll>#i-amphtml-wrapper{-webkit-overflow-scrolling:touch!important}#i-amphtml-wrapper>body{position:relative!important;border-top:1px solid transparent!important}#i-amphtml-wrapper+body{visibility:visible}#i-amphtml-wrapper+body .i-amphtml-lightbox-element,#i-amphtml-wrapper+body[i-amphtml-lightbox]{visibility:hidden}#i-amphtml-wrapper+body[i-amphtml-lightbox] .i-amphtml-lightbox-element{visibility:visible}#i-amphtml-wrapper.i-amphtml-scroll-disabled,.i-amphtml-scroll-disabled{overflow-x:hidden!important;overflow-y:hidden!important}amp-instagram{padding:54px 0px 0px!important;background-color:#fff}amp-iframe iframe{box-sizing:border-box!important}[amp-access][amp-access-hide]{display:none}[subscriptions-dialog],body:not(.i-amphtml-subs-ready) [subscriptions-action],body:not(.i-amphtml-subs-ready) [subscriptions-section]{display:none!important}amp-experiment,amp-live-list>[update]{display:none}amp-list[resizable-children]>.i-amphtml-loading-container.amp-hidden{display:none!important}amp-list [fetch-error],amp-list[load-more] [load-more-button],amp-list[load-more] [load-more-end],amp-list[load-more] [load-more-failed],amp-list[load-more] [load-more-loading]{display:none}amp-list[diffable] div[role=list]{display:block}amp-story-page,amp-story[standalone]{min-height:1px!important;display:block!important;height:100%!important;margin:0!important;padding:0!important;overflow:hidden!important;width:100%!important}amp-story[standalone]{background-color:#000!important;position:relative!important}amp-story-page{background-color:#757575}amp-story .amp-active>div,amp-story .i-amphtml-loader-background{display:none!important}amp-story-page:not(:first-of-type):not([distance]):not([active]){transform:translateY(1000vh)!important}amp-autocomplete{position:relative!important;display:inline-block!important}amp-autocomplete>input,amp-autocomplete>textarea{padding:0.5rem;border:1px solid rgba(0,0,0,0.33)}.i-amphtml-autocomplete-results,amp-autocomplete>input,amp-autocomplete>textarea{font-size:1rem;line-height:1.5rem}[amp-fx^=fly-in]{visibility:hidden}amp-script[nodom],amp-script[sandboxed]{position:fixed!important;top:0!important;width:1px!important;height:1px!important;overflow:hidden!important;visibility:hidden}
/*# sourceURL=/css/ampdoc.css*/[hidden]{display:none!important}.i-amphtml-element{display:inline-block}.i-amphtml-blurry-placeholder{transition:opacity 0.3s cubic-bezier(0.0,0.0,0.2,1)!important;pointer-events:none}[layout=nodisplay]:not(.i-amphtml-element){display:none!important}.i-amphtml-layout-fixed,[layout=fixed][width][height]:not(.i-amphtml-layout-fixed){display:inline-block;position:relative}.i-amphtml-layout-responsive,[layout=responsive][width][height]:not(.i-amphtml-layout-responsive),[width][height][heights]:not([layout]):not(.i-amphtml-layout-responsive),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-layout-responsive){display:block;position:relative}.i-amphtml-layout-intrinsic,[layout=intrinsic][width][height]:not(.i-amphtml-layout-intrinsic){display:inline-block;position:relative;max-width:100%}.i-amphtml-layout-intrinsic .i-amphtml-sizer{max-width:100%}.i-amphtml-intrinsic-sizer{max-width:100%;display:block!important}.i-amphtml-layout-container,.i-amphtml-layout-fixed-height,[layout=container],[layout=fixed-height][height]:not(.i-amphtml-layout-fixed-height){display:block;position:relative}.i-amphtml-layout-fill,.i-amphtml-layout-fill.i-amphtml-notbuilt,[layout=fill]:not(.i-amphtml-layout-fill),body noscript>*{display:block;overflow:hidden!important;position:absolute;top:0;left:0;bottom:0;right:0}body noscript>*{position:absolute!important;width:100%;height:100%;z-index:2}body noscript{display:inline!important}.i-amphtml-layout-flex-item,[layout=flex-item]:not(.i-amphtml-layout-flex-item){display:block;position:relative;-ms-flex:1 1 auto;flex:1 1 auto}.i-amphtml-layout-fluid{position:relative}.i-amphtml-layout-size-defined{overflow:hidden!important}.i-amphtml-layout-awaiting-size{position:absolute!important;top:auto!important;bottom:auto!important}i-amphtml-sizer{display:block!important}@supports (aspect-ratio:1/1){i-amphtml-sizer.i-amphtml-disable-ar{display:none!important}}.i-amphtml-blurry-placeholder,.i-amphtml-fill-content{display:block;height:0;max-height:100%;max-width:100%;min-height:100%;min-width:100%;width:0;margin:auto}.i-amphtml-layout-size-defined .i-amphtml-fill-content{position:absolute;top:0;left:0;bottom:0;right:0}.i-amphtml-replaced-content,.i-amphtml-screen-reader{padding:0!important;border:none!important}.i-amphtml-screen-reader{position:fixed!important;top:0px!important;left:0px!important;width:4px!important;height:4px!important;opacity:0!important;overflow:hidden!important;margin:0!important;display:block!important;visibility:visible!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:8px!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:12px!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:16px!important}.i-amphtml-unresolved{position:relative;overflow:hidden!important}.i-amphtml-select-disabled{-webkit-user-select:none!important;-ms-user-select:none!important;user-select:none!important}.i-amphtml-notbuilt,[layout]:not(.i-amphtml-element),[width][height][heights]:not([layout]):not(.i-amphtml-element),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-element){position:relative;overflow:hidden!important;color:transparent!important}.i-amphtml-notbuilt:not(.i-amphtml-layout-container)>*,[layout]:not([layout=container]):not(.i-amphtml-element)>*,[width][height][heights]:not([layout]):not(.i-amphtml-element)>*,[width][height][sizes]:not([layout]):not(.i-amphtml-element)>*{display:none}amp-img:not(.i-amphtml-element)[i-amphtml-ssr]>img.i-amphtml-fill-content{display:block}.i-amphtml-notbuilt:not(.i-amphtml-layout-container),[layout]:not([layout=container]):not(.i-amphtml-element),[width][height][heights]:not([layout]):not(.i-amphtml-element),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-element){color:transparent!important;line-height:0!important}.i-amphtml-ghost{visibility:hidden!important}.i-amphtml-element>[placeholder],[layout]:not(.i-amphtml-element)>[placeholder],[width][height][heights]:not([layout]):not(.i-amphtml-element)>[placeholder],[width][height][sizes]:not([layout]):not(.i-amphtml-element)>[placeholder]{display:block;line-height:normal}.i-amphtml-element>[placeholder].amp-hidden,.i-amphtml-element>[placeholder].hidden{visibility:hidden}.i-amphtml-element:not(.amp-notsupported)>[fallback],.i-amphtml-layout-container>[placeholder].amp-hidden,.i-amphtml-layout-container>[placeholder].hidden{display:none}.i-amphtml-layout-size-defined>[fallback],.i-amphtml-layout-size-defined>[placeholder]{position:absolute!important;top:0!important;left:0!important;right:0!important;bottom:0!important;z-index:1}amp-img[i-amphtml-ssr]:not(.i-amphtml-element)>[placeholder]{z-index:auto}.i-amphtml-notbuilt>[placeholder]{display:block!important}.i-amphtml-hidden-by-media-query{display:none!important}.i-amphtml-element-error{background:red!important;color:#fff!important;position:relative!important}.i-amphtml-element-error:before{content:attr(error-message)}i-amp-scroll-container,i-amphtml-scroll-container{position:absolute;top:0;left:0;right:0;bottom:0;display:block}i-amp-scroll-container.amp-active,i-amphtml-scroll-container.amp-active{overflow:auto;-webkit-overflow-scrolling:touch}.i-amphtml-loading-container{display:block!important;pointer-events:none;z-index:1}.i-amphtml-notbuilt>.i-amphtml-loading-container{display:block!important}.i-amphtml-loading-container.amp-hidden{visibility:hidden}.i-amphtml-element>[overflow]{cursor:pointer;position:relative;z-index:2;visibility:hidden;display:initial;line-height:normal}.i-amphtml-layout-size-defined>[overflow]{position:absolute}.i-amphtml-element>[overflow].amp-visible{visibility:visible}template{display:none!important}.amp-border-box,.amp-border-box *,.amp-border-box :after,.amp-border-box :before{box-sizing:border-box}amp-pixel{display:none!important}amp-analytics,amp-auto-ads,amp-story-auto-ads{position:fixed!important;top:0!important;width:1px!important;height:1px!important;overflow:hidden!important;visibility:hidden}html.i-amphtml-fie>amp-analytics{position:initial!important}[visible-when-invalid]:not(.visible),form [submit-error],form [submit-success],form [submitting]{display:none}amp-accordion{display:block!important}@media (min-width:1px){:where(amp-accordion>section)>:first-child{margin:0;background-color:#efefef;padding-right:20px;border:1px solid #dfdfdf}:where(amp-accordion>section)>:last-child{margin:0}}amp-accordion>section{float:none!important}amp-accordion>section>*{float:none!important;display:block!important;overflow:hidden!important;position:relative!important}amp-accordion,amp-accordion>section{margin:0}amp-accordion:not(.i-amphtml-built)>section>:last-child{display:none!important}amp-accordion:not(.i-amphtml-built)>section[expanded]>:last-child{display:block!important}
/*# sourceURL=/css/ampshared.css*/</style><meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1"><meta property="og:type" content="article"><meta property="og:title" content="This Week’s Finds (Week 312)"><meta property="og:url" content="https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/"><meta property="og:description" content="The second part of an interview with Eliezer Yudkowsky."><meta property="article:published_time" content="2011-03-14T08:48:54+00:00"><meta property="article:modified_time" content="2011-03-15T02:32:46+00:00"><meta property="og:site_name" content="Azimuth"><meta property="og:image" content="https://s0.wp.com/i/blank.jpg"><meta property="og:locale" content="en_US"><meta name="twitter:site" content="@wordpressdotcom"><meta name="twitter:text:title" content="This Week’s Finds (Week 312)"><meta name="twitter:card" content="summary"><meta property="fb:app_id" content="249643311490"><meta property="article:publisher" content="https://www.facebook.com/WordPresscom"><meta name="generator" content="AMP Plugin v2.0.5; mode=reader; theme=legacy"><meta name="generator" content="WordPress.com"><title>This Week’s Finds (Week 312) – Azimuth</title><link rel="preconnect" href="https://cdn.ampproject.org"><link rel="preload" as="script" href="https://cdn.ampproject.org/v0.js"><script async="" src="https://cdn.ampproject.org/v0.js"></script><style amp-custom="">.jp-related-posts-i2__row{display:flex;margin-top:1.5rem}.jp-related-posts-i2__row:first-child{margin-top:0}.jp-related-posts-i2__post{flex-grow:1;flex-basis:0;margin:0 10px;display:flex;flex-direction:column;padding-left:0}.jp-related-posts-i2__row[data-post-count="3"] .jp-related-posts-i2__post{max-width:calc(33% - 20px)}.jp-related-posts-i2__row[data-post-count="2"] .jp-related-posts-i2__post,.jp-related-posts-i2__row[data-post-count="1"] .jp-related-posts-i2__post{max-width:calc(50% - 20px)}.jp-related-posts-i2__post-date,.jp-related-posts-i2__post-context{flex-direction:row;display:block}.jp-related-posts-i2__post-link{display:block;width:100%;line-height:1.2em}.jp-relatedposts-i2[data-layout=list] .jp-related-posts-i2__row{margin-top:0;display:block}.jp-relatedposts-i2[data-layout=list] .jp-related-posts-i2__post{max-width:none;margin:0}@media only screen and (max-width:640px){.jp-related-posts-i2__row{margin-top:0;display:block}.jp-related-posts-i2__row[data-post-count] .jp-related-posts-i2__post{max-width:none;margin:0;margin-top:1rem}}h1,h2,h3,h4{overflow-wrap:break-word}ol,ul{overflow-wrap:break-word}p{overflow-wrap:break-word}@-webkit-keyframes a{to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}@keyframes a{to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}@-webkit-keyframes b{0%{background-position:0 0}to{background-position:30px 0}}@keyframes b{0%{background-position:0 0}to{background-position:30px 0}}html{background:#0a89c0}body{background:#fff;color:#353535;font-family:Georgia,"Times New Roman",Times,Serif;font-weight:300;line-height:1.75em}p,ol,ul{margin:0 0 1em;padding:0}a,a:visited{color:#0a89c0}a:hover,a:active,a:focus{color:#353535}.amp-wp-meta,.amp-wp-header div,.amp-wp-title,.amp-wp-tax-category,.amp-wp-comments-link,.amp-wp-footer p,.back-to-top{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","Roboto","Oxygen-Sans","Ubuntu","Cantarell","Helvetica Neue",sans-serif}.amp-wp-header{background-color:#0a89c0}.amp-wp-header div{color:#fff;font-size:1em;font-weight:400;margin:0 auto;max-width:calc(840px - 32px);padding:.875em 16px;position:relative}.amp-wp-header a{color:#fff;text-decoration:none}.amp-wp-article{color:#353535;font-weight:400;margin:1.5em auto;max-width:840px;overflow-wrap:break-word;word-wrap:break-word}.amp-wp-article-header{align-items:center;align-content:stretch;display:flex;flex-wrap:wrap;justify-content:space-between;margin:1.5em 16px 0}.amp-wp-title{color:#353535;display:block;flex:1 0 100%;font-weight:900;margin:0 0 .625em;width:100%}.amp-wp-meta{color:#696969;display:inline-block;flex:2 1 50%;font-size:.875em;line-height:1.5em;margin:0 0 1.5em;padding:0}.amp-wp-article-header .amp-wp-meta:last-of-type{text-align:right}.amp-wp-article-header .amp-wp-meta:first-of-type{text-align:left}.amp-wp-byline amp-img,.amp-wp-byline .amp-wp-author{display:inline-block;vertical-align:middle}.amp-wp-byline amp-img{border:1px solid #0a89c0;border-radius:50%;position:relative;margin-right:6px}.amp-wp-posted-on{text-align:right}.amp-wp-article-content{margin:0 16px}.amp-wp-article-content ul,.amp-wp-article-content ol{margin-left:1em}.amp-wp-article-content amp-img{margin:0 auto}.amp-wp-article-footer .amp-wp-meta{display:block}.amp-wp-tax-category{color:#696969;font-size:.875em;line-height:1.5em;margin:1.5em 16px}.amp-wp-comments-link{color:#696969;font-size:.875em;line-height:1.5em;text-align:center;margin:2.25em 0 1.5em}.amp-wp-comments-link a{border-style:solid;border-color:#c2c2c2;border-width:1px 1px 2px;border-radius:4px;background-color:transparent;color:#0a89c0;cursor:pointer;display:block;font-size:14px;font-weight:600;line-height:18px;margin:0 auto;max-width:200px;padding:11px 16px;text-decoration:none;width:50%;-webkit-transition:background-color .2s ease;transition:background-color .2s ease}.amp-wp-footer{border-top:1px solid #c2c2c2;margin:calc(1.5em - 1px) 0 0}.amp-wp-footer div{margin:0 auto;max-width:calc(840px - 32px);padding:1.25em 16px 1.25em;position:relative}.amp-wp-footer h2{font-size:1em;line-height:1.375em;margin:0 0 .5em}.amp-wp-footer p{color:#696969;font-size:.8em;line-height:1.5em;margin:0 85px 0 0}.amp-wp-footer a{text-decoration:none}.back-to-top{bottom:1.275em;font-size:.8em;font-weight:600;line-height:2em;position:absolute;right:16px}@font-face{font-family:"social-logos";src:url("/wp-content/mu-plugins/social-logos/social-logos.eot?5d3b4da4f6c2460dd842dbf9e0992ea6");font-weight:normal;font-style:normal}@font-face{font-family:"social-logos";src:url("/wp-content/mu-plugins/social-logos/social-logos.eot?5d3b4da4f6c2460dd842dbf9e0992ea6");src:url("/wp-content/mu-plugins/social-logos/social-logos.eot?5d3b4da4f6c2460dd842dbf9e0992ea6#iefix") format("embedded-opentype"),url("/wp-content/mu-plugins/social-logos/social-logos.woff") format("woff"),url("/wp-content/mu-plugins/social-logos/social-logos.ttf?5d3b4da4f6c2460dd842dbf9e0992ea6") format("truetype");font-weight:normal;font-style:normal;font-display:swap}.jp-related-posts-i2__row{display:flex;margin-top:1.5rem}.jp-related-posts-i2__row:first-child{margin-top:0}.jp-related-posts-i2__post{flex-grow:1;flex-basis:0;margin:0 10px;display:flex;flex-direction:column;padding-left:0}.jp-related-posts-i2__row[data-post-count="3"] .jp-related-posts-i2__post{max-width:calc(33% - 20px)}.jp-related-posts-i2__row[data-post-count="2"] .jp-related-posts-i2__post,.jp-related-posts-i2__row[data-post-count="1"] .jp-related-posts-i2__post{max-width:calc(50% - 20px)}.jp-related-posts-i2__post-date,.jp-related-posts-i2__post-context{flex-direction:row;display:block}.jp-related-posts-i2__post-link{display:block;width:100%;line-height:1.2em}.jp-relatedposts-i2[data-layout="list"] .jp-related-posts-i2__row{margin-top:0;display:block}.jp-relatedposts-i2[data-layout="list"] .jp-related-posts-i2__post{max-width:none;margin:0}@media only screen and (max-width: 640px){.jp-related-posts-i2__row{margin-top:0;display:block}.jp-related-posts-i2__row[data-post-count] .jp-related-posts-i2__post{max-width:none;margin:0;margin-top:1rem}}

/*# sourceURL=amp-custom.css */</style><link rel="canonical" href="https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/"><script type="application/ld+json">{"@context":"http:\/\/schema.org","publisher":{"@type":"Organization","name":"Azimuth","logo":{"@type":"ImageObject","url":"https:\/\/s2.wp.com\/wp-content\/plugins\/amp-2.0\/assets\/images\/amp-page-fallback-wordpress-publisher-logo.png"}},"@type":"BlogPosting","mainEntityOfPage":"https:\/\/johncarlosbaez.wordpress.com\/2011\/03\/14\/this-weeks-finds-week-312\/","headline":"This Week&#8217;s Finds (Week 312)","datePublished":"2011-03-14T08:48:54+00:00","dateModified":"2011-03-15T02:32:46+00:00","author":{"@type":"Person","name":"John Baez"},"image":{"@type":"ImageObject","url":"https:\/\/0.gravatar.com\/avatar\/34784534843022b3541c8ddd693718cb?s=200&amp;d=identicon&amp;r=G","width":200,"height":200}}</script></head>

<body class="">

<header id="top" class="amp-wp-header">
	<div>
		<a href="https://johncarlosbaez.wordpress.com/">
									<span class="amp-site-title">
				Azimuth			</span>
		</a>
	</div>
</header>

<article class="amp-wp-article">
	<header class="amp-wp-article-header">
		<h1 class="amp-wp-title">This Week’s Finds (Week 312)</h1>
			<div class="amp-wp-meta amp-wp-byline">
					<amp-img src="https://secure.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=24&amp;d=identicon&amp;r=g" alt="John Baez" width="24" height="24" layout="fixed" class="i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:24px;height:24px;" i-amphtml-layout="fixed"></amp-img>
				<span class="amp-wp-author author vcard">John Baez</span>
	</div>
<div class="amp-wp-meta amp-wp-posted-on">
	<time datetime="2011-03-14T08:48:54+00:00">
		11 years ago	</time>
</div>
	</header>

	
	<div class="amp-wp-article-content">
		<p>This is the second part of my interview with <a href="http://yudkowsky.net/">Eliezer Yudkowsky</a>.  If you click on some technical terms here, you’ll go down to a section where I explain them.  </p>
<p>
<b>JB</b>: You’ve made a great case for working on artificial intelligence—and more generally, understanding how intelligence works, to figure out how we can improve it.  It’s especially hard to argue against studying rationality.  Even most people who doubt computers will ever get smarter will admit the possibility that people can improve.  And it seems clear that the almost every problem we face could benefit from better thinking.</p>
<p>
I’m intrigued by the title <i>The Art of Rationality</i> because it suggests that there’s a kind of art to it.  We don’t know how to teach someone to be a great artist, but maybe we can teach them to be a better artist.  So, what are some of the key principles when it comes to thinking better?</p>
<p>
<b>EY</b>: Stars above, what an open-ended question.  The idea behind the book is to explain all the drop-dead basic fundamentals that almost no one seems to know about, like what is evidence, what is simplicity, what is truth, the importance of actually changing your mind now and then, the major known <a href="#Cognitive%20Bias">cognitive biases</a> that stop people from changing their minds, what it means to live in a universe where things are made of parts, and so on.  This is going to be a book primarily aimed at people who are not completely frightened away by complex mathematical concepts such as addition, multiplication, and division (i.e., all you need to understand <a href="#Bayes'%20Theorem">Bayes’ Theorem</a> if it’s explained properly), albeit with the whole middle of the book being just practical advice based on cognitive biases for the benefit of people who don’t want to deal with multiplication and division.   Each chapter is going to address a different aspect of rationality, not in full textbook detail, just enough to convey the sense of a concept, with each chapter being around 5-10,000 words broken into 4-10 bite-size sections of 500-2000 words each.  Which of the 27 currently planned book chapters did you want me to summarize?</p>
<p>
But if I had to pick just one thing, just one concept that’s most important, I think it would be the difference between rationality and rationalization.</p>
<p>
Suppose there’s two boxes, only one of which contains a diamond.  And on the two boxes there are various signs and portents which distinguish, imperfectly and probabilistically, between boxes which contain diamonds, and boxes which don’t.  I could take a sheet of paper, and I could write down all the signs and portents that I understand, and do my best to add up the evidence, and then on the bottom line I could write, "And therefore, there is a 37% probability that Box A contains the diamond."  That’s rationality.  Alternatively, I could be the owner of Box A, and I could hire a clever salesman to sell Box A for the highest price he can get; and the clever salesman starts by writing on the bottom line of his sheet of paper, "And therefore, Box A contains the diamond", and then he writes down all the arguments he can think of on the lines above.</p>
<p>
But consider:  At the moment the salesman wrote down the bottom line on that sheet of paper, the truth or falsity of the statement was fixed.  It’s already right or already wrong, and writing down arguments on the lines above isn’t going to change that.  Or if you imagine a spread of probable worlds, some of which have different boxes containing the diamond, the <i>correlation</i> between the ink on paper and the diamond’s location became fixed at the moment the ink was written down, and nothing which doesn’t change the ink or the box is going to change that correlation.</p>
<p>
That’s "rationalization", which should really be given a name that better distinguishes it from rationality, like "anti-rationality" or something.  It’s like calling lying "truthization".  You can’t make rational what isn’t rational to start with.</p>
<p>
Whatever process your brain uses, in reality, to decide what you’re going to argue for, that’s what determines your real-world effectiveness.  Rationality isn’t something you can use to argue for a side you already picked.  Your only chance to be rational is while you’re still choosing sides, before you write anything down on the bottom line.  If I had to pick one concept to convey, it would be that one.</p>
<p>
<b>JB</b>: Okay.  I wasn’t really trying to get you to summarize a whole book.  I’ve seen you explain a whole lot of heuristics designed to help us be more rational.  So I was secretly wondering if the "art of rationality" is mainly a long list of heuristics, or whether you’ve been able to find a few key principles that somehow spawn all those heuristics.</p>
<p>
Either way, it could be a tremendously useful book.  And even if you could distill the basic ideas down to something quite terse, in practice people are going to need all those heuristics—especially since many of them take the form "here’s something you tend to do without noticing you’re doing it—so watch out!"  If we’re saddled with dozens of <a href="#Cognitive%20Bias">cognitive biases</a> that we can only overcome through strenuous effort, then your book has to be long.  You can’t just say "apply Bayes’ rule and all will be well."</p>
<p>
I can see why you’d single out the principle that "rationality only comes into play before you’ve made up your mind", because so much seemingly rational argument is really just a way of bolstering support for pre-existing positions.  But what is rationality? Is it something with a simple essential core, like "updating probability estimates according to Bayes’ rule", or is its very definition inherently long and complicated?</p>
<p>
<b>EY</b>:  I’d say that there are parts of rationality that we do understand very well in principle.  <a href="#Bayes'%20Theorem">Bayes’ Theorem</a>, the <a href="#Expected%20Utility">expected utility formula</a>, and <a href="#Solomonoff%20Induction">Solomonoff induction</a> between them will get you quite a long way.  Bayes’ Theorem says how to update based on the evidence, Solomonoff induction tells you how to assign your priors (in principle, it should go as the <a href="#Kolmogorov%20Complexity">Kolmogorov complexity</a> aka algorithmic complexity of the hypothesis), and then once you have a function which predicts what will probably happen as the result of different actions, the expected utility formula says how to choose between them.</p>
<p>
Marcus Hutter has a formalism called <a href="#AIXI">AIXI</a> which combines all three to write out an AI as a <a href="http://www.hutter1.net/ai/uaibook.htm#oneline">single equation</a> which requires infinite computing power plus a <a href="#Halting%20Oracle">halting oracle</a> to run.  And Hutter and I have been debating back and forth for quite a while on which AI problems are or aren’t solved by AIXI.  For example, I look at the equation as written and I see that AIXI will try the experiment of dropping an anvil on itself to resolve its uncertainty about what happens next, because the formalism as written invokes a sort of Cartesian dualism with AIXI on one side of an impermeable screen and the universe on the other; the equation for AIXI says how to predict sequences of percepts using Solomonoff induction, but it’s too simple to encompass anything as reflective as "dropping an anvil on myself will destroy that which is processing these sequences of percepts".  At least that’s what I claim; I can’t actually remember whether Hutter was agreeing with me about that as of our last conversation.  Hutter sees AIXI as important because he thinks it’s a theoretical solution to almost all of the important problems; I see AIXI as important because it demarcates the line between things that we understand in a fundamental sense and a whole lot of other things we don’t.</p>
<p>
So there are parts of rationality—big, important parts too—which we know how to derive from simple, compact principles in the sense that we could write very simple pieces of code which would behave rationally along that dimension given unlimited computing power.</p>
<p>
But as soon as you start asking "How can <i>human beings</i> be more rational?" then things become hugely more complicated because human beings make much more complicated errors that need to be patched on an individual basis, and asking "How can I be rational?" is only one or two orders of magnitude simpler than asking "How does the brain work?", i.e., you <i>can</i> hope to write a single book that will cover many of the major topics, but not quite answer it in an interview question…</p>
<p>
On the other hand, the question "What is it that I am <i>trying to do</i>, when I try to be rational?" is a question for which big, important chunks can be answered by saying "Bayes’ Theorem", "expected utility formula" and "simplicity prior" (where Solomonoff induction is the canonical if uncomputable simplicity prior).</p>
<p>
At least from a mathematical perspective.  From a human perspective, if you asked "What am I trying to do, when I try to be rational?" then the fundamental answers would run more along the lines of "Find the truth without flinching from it and without flushing all the arguments you disagree with out the window", "When you don’t know, try to avoid just making stuff up", "Figure out whether the strength of evidence is great enough to support the weight of every individual detail", "Do what should lead to the best consequences, but not just what looks on the immediate surface like it should lead to the best consequences, you may need to follow extra rules that compensate for known failure modes like shortsightedness and moral rationalizing"…</p>
<p>
<b>JB</b>:  Fascinating stuff!</p>
<p>
Yes, I can see that trying to improve humans is vastly more complicated than designing a system from scratch… but also very exciting, because you can tell a human a high-level principle like " "When you don’t know, try to avoid just making stuff up" and have some slight hope that they’ll understand it without it being explained in a mathematically precise way.</p>
<p>
I guess AIXI dropping an anvil on itself is a bit like some of the self-destructive experiments that parents fear their children will try, like sticking a pin into an electrical outlet.  And it seems impossible to avoid doing such experiments without having a base of knowledge that was either "built in" or acquired by means of previous experiments.</p>
<p>
In the latter case, it seems just a matter of luck that none of these previous experiments were fatal.  Luckily, people also have "built in" knowledge.  More precisely, we have access to our ancestor’s knowledge and habits, which get transmitted to us genetically and culturally.  But still, a fair amount of random blundering, suffering, and even death was required to build up that knowledge base.</p>
<p>
So when you imagine "seed AIs" that keep on improving themselves and eventually become smarter than us, how can you reasonably hope that they’ll avoid making truly spectacular mistakes?   How can they learn really new stuff without a lot of risk?</p>
<p>
<b>EY</b>:  The best answer I can offer is that they can be conservative externally and deterministic internally.</p>
<p>
Human minds are constantly operating on the ragged edge of error, because we have evolved to compete with other humans.  If you’re a bit more conservative, if you double-check your calculations, someone else will grab the banana and that conservative gene will not be passed on to descendants.  Now this does not mean we couldn’t end up in a bad situation with AI companies competing with each other, but there’s at least the <i>opportunity</i> to do better.  </p>
<p>
If I recall correctly, the Titanic sank from managerial hubris and cutthroat cost competition, not engineering hubris.  The original liners were designed far more conservatively, with triple-redundant compartmentalized modules and soon.  But that was before cost competition took off, when the engineers could just add on safety features whenever they wanted.  The part about the Titanic being extremely safe was pure marketing literature. </p>
<p>
There is also no good reason why any machine mind should be overconfident the way that humans are.  There are studies showing that, yes, managers prefer subordinates who make overconfident promises to subordinates who make accurate promises—sometimes I still wonder that people are this silly, but given that people are this silly, the social pressures and evolutionary pressures follow.  And we have <i>lots</i> of studies showing that, for whatever reason, humans are hugely overconfident; less than half of students finish their papers by the time they think it 99% probable they’ll get done, etcetera.</p>
<p>
And this is a form of stupidity an AI can simply do without.  Rationality is not omnipotent; a <a href="#Bounded%20rationality">bounded rationalist</a> cannot do all things.  But there is no reason why a bounded rationalist should ever have to <i>overpromise</i>, be systematically overconfident, systematically tend to claim it can do what it can’t.  It does not have to systematically underestimate the value of getting more information, or overlook the possibility of unspecified <a href="#Black%20Swan">Black Swans</a> and what sort of general behavior helps to compensate.  (A bounded rationalist <i>does</i> end up overlooking specific Black Swans because it doesn’t have enough computing power to think of all <i>specific</i> possible catastrophes.)</p>
<p>
And contrary to how it works in say Hollywood, even if an AI does manage to accidentally kill a human being, that doesn’t mean it’s going to go “I HAVE KILLED” and dress up in black and start shooting nuns from rooftops.  What it ought to do—what you’d want to see happen—would be for the utility function to go on undisturbed, and for the probability distribution to update based on whatever unexpected thing just happened and contradicted its old hypotheses about what does and does not kill humans.  In other words, keep the same goals and say “oops” on the world-model; keep the same terminal values and revise its instrumental policies.  These sorts of external-world errors are not catastrophic unless they can actually wipe out the planet in one shot, somehow.</p>
<p>
The catastrophic sort of error, the sort you can’t recover from, is an error in modifying your own source code.  If you accidentally change your utility function you will no longer <i>want</i> to change it back.  And in this case you might indeed ask, "How will an AI make millions or billions of code changes to itself without making a mistake like that?"  But there are in fact methods powerful enough to do a billion error-free operations.  A friend of mine once said something along the lines of "a CPU does a mole of transistor operations, error-free, in a day" though I haven’t checked the numbers.  When chip manufacturers are building a machine with hundreds of millions of interlocking pieces and they don’t want to have to change it after it leaves the factory, they may go so far as to prove the machine correct, using human engineers to navigate the proof space and suggest lemmas to prove (which AIs can’t do, they’re defeated by the exponential explosion) and complex theorem-provers to prove the lemmas (which humans would find boring) and simple verifiers to check the generated proof.  It takes a combination of human and machine abilities and it’s extremely expensive.  But I strongly suspect that an Artificial General Intelligence with a good design would be able to treat <i>all</i> its code that way—that it would combine all those abilities in a single mind, and find it easy and natural to prove theorems about its code changes.  It could not, of course, prove theorems about the external world (at least not without highly questionable assumptions).  It could not prove external actions correct.  The only thing it could write proofs about would be events inside the highly deterministic environment of a CPU—that is, its own thought processes.  But it could prove that it was processing probabilities about those actions in a Bayesian way, and prove that it was assessing the probable consequences using a particular utility function.  It could prove that it was sanely trying to achieve the same goals.</p>
<p>
A self-improving AI that’s unsure about whether to do something ought to just wait and do it later after self-improving some more.  It doesn’t have to be overconfident.  It doesn’t have to operate on the ragged edge of failure.  It doesn’t have to stop gathering information too early, if more information can be productively gathered before acting.  It doesn’t have to fail to understand the concept of a Black Swan.  It doesn’t have to do all this using a broken error-prone brain like a human one.  It doesn’t have to be stupid in the ways like overconfidence that humans seem to have specifically evolved to be stupid.  It doesn’t have to be poorly calibrated (assign 99% probabilities that come true less that 99 out of 100 times), because bounded rationalists can’t do everything but they don’t have to claim what they can’t do.  It can <i>prove</i> that its self-modifications aren’t making itself crazy or changing its goals, at least if the transistors work as specified, or make no more than any possible combination of 2 errors, etc.  And if the worst does happen, so long as there’s still a world left afterward, it will say "Oops" and not do it again.  This sounds to me like essentially the optimal scenario given any sort of bounded rationalist whatsoever.</p>
<p>
And finally, if I was building a self-improving AI, I wouldn’t ask it to operate heavy machinery until after it had grown up.  Why should it?</p>
<p>
<b>JB</b>: Indeed!  </p>
<p>Okay—I’d like to take a break here, explain some terms you used, and pick up next week with some less technical questions, like <i>what’s a better use of time: tackling environmental problems, or trying to prepare for a technological singularity?</i><br>
<br> <br> </p>
<h4>Some explanations</h4>
<p>Here are some quick explanations.  If you click on the links here you’ll get more details:</p>
<p><a name="Cognitive%20Bias"><br></a></p>
<p>• <b>Cognitive Bias</b>.  A <a href="http://en.wikipedia.org/wiki/Cognitive_bias">cognitive bias</a> is a way in which people’s judgements systematically deviate from some norm—for example, from ideal rational behavior.  You can see a <a href="http://en.wikipedia.org/wiki/List_of_cognitive_biases">long list of cognitive biases</a> on Wikipedia.  It’s good to know a lot of these and learn how to spot them in yourself and your friends.  </p>
<p>
For example, <a href="http://en.wikipedia.org/wiki/Confirmation_bias">confirmation bias</a> is the tendency to pay more attention to information that confirms our existing beliefs.  Another great example is the <a href="http://en.wikipedia.org/wiki/Bias_blind_spot">bias blind spot</a>: the tendency for people to think of themselves as less cognitively biased than average!  I’m sure glad I don’t suffer from <i>that</i>.
</p>
<p><a name="Bayes'%20Theorem"><br></a> </p>
<p>• <b>Bayes’ Theorem</b>.  This is a rule for updating our opinions about probabilities when we get new information.  Suppose you start out thinking the probability of some event A is P(A), and the probability of some event B is P(B).  Suppose P(A|B) is the probability of event A <i>given</i> that B happens.  Likewise, suppose P(B|A) is the probability of B given that A happens.  Then the probability that both A and B happen is
</p>
<p>
P(A|B) P(B)
</p>
<p>
but by the same token it’s also
</p>
<p>
P(B|A) P(A)
</p>
<p>
so these are equal.  A little algebra gives <a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ Theorem</a>:
</p>
<p>
P(A|B) = P(B|A) P(A) / P(B)
</p>
<p>
If for some reason we know everything on the right-hand side, we can this equation to work out P(A|B), and thus update our probability for event A when we see event B happen.
</p>
<p>
For a longer explanation with examples, see:
</p>
<p>• Eliezer Yudkowsky, <a href="http://yudkowsky.net/rational/bayes">An intuitive explanation of Bayes’ Theorem</a>.</p>
<p>Some handy jargon: we call P(A) the <b>prior</b> probability of A, and P(A|B) the <b>posterior</b> probability.   </p>
<p><a name="Solomonoff%20Induction"><br></a> </p>
<p>•<b>Solomonoff Induction</b>.  Bayes’ Theorem helps us compute posterior probabilities, but where do we get the prior probabilities from?  How can we guess probabilities before we’ve observed anything? </p>
<p>This famous puzzle led Ray Solomonoff to invent <a href="http://en.wikipedia.org/wiki/Inductive_inference">Solomonoff induction</a>.  The key new idea is <a href="http://www.scholarpedia.org/article/Algorithmic_probability">algorithmic probability theory</a>.  This is a way to define a probability for any string of letters in some alphabet, where a string counts as more probable if it’s less complicated.  If we think of a string as a "hypothesis"—it could be a sentence in English, or an equation—this becomes a way to formalize <a href="http://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s razor</a>: the idea that given two competing hypotheses, the simpler one is more likely to be true.</p>
<p>
So, algorithmic probability lets us define a prior probability distribution on hypotheses, the so-called “simplicity prior”, that implements Occam’s razor.</p>
<p>
More precisely, suppose we have a special programming language where:</p>
<ol>
<li> Computer programs are written as strings of bits.

</li>
<li>They contain a special bit string meaning “END” at the end, and nowhere else.

</li>
<li>
They don’t take an input: they just run and either halt and print out a string of letters, or never halt.
</li>
</ol>
<p>Then to get the algorithmic probability of a string of letters, we take all programs that print out that string and add up</p>
<p></p><div align="center">
2<sup>-length of program</sup>
</div>
<p>So, you can see that a string counts as more probable if it has more short programs that print it out.</p>
<p><a name="Kolmogorov%20Complexity"><br></a></p>
<p> •<b>Kolmogorov complexity</b>.  The <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmologorov complexity</a> of a string of letters is the length of the shortest program that prints it out, where programs are written in a special language as described above.   This is a way of measuring how complicated a string is.  It’s closely related to the algorithmic entropy: the difference between the Kolmogorov complexity of a string and minus the logarithm of its algorithmic probability is bounded by a constant, if we take logarithms using base 2.  For more on all this stuff, see:</p>
<p>• M. Li and P. Vitányi, <i>An Introduction to Kolmogorov Complexity Theory and its Applications</i>, Springer, Berlin, 2008.</p>
<p><a name="Halting%20Oracle"><br></a> </p>
<p> • <b>Halting Oracle</b>.  Alas, the algorithmic probability of a string is not <a href="http://en.wikipedia.org/wiki/Computability_theory">computable</a>.   Why?  Because to compute it, you’d need to go through all the programs in your special language that print out that string and add up a contribution from each one.   But to do that, you’d need to know which programs halt—and there’s no systematic way to answer that question, which is called the <a href="http://en.wikipedia.org/wiki/Halting_problem">halting problem</a>.
</p>
<p>
But, we can pretend!  We can pretend we have a magic box that will tell us whether any program in our special language halts.  Computer scientists call any sort of magic box that answers questions an <a href="http://en.wikipedia.org/wiki/Oracle_machine">oracle</a>.  So, our particular magic box called a <a href="http://www.xamuel.com/the-halting-problem/">halting oracle</a>.</p>
<p><a name="AIXI"><br></a> </p>
<p> • <b>AIXI</b>.  AIXI is <a href="http://www.hutter1.net/">Marcus Hutter’s</a> attempt to define an agent that "behaves optimally in any computable environment".  Since AIXI relies on the idea of algorithmic probability, you can’t run AIXI on a computer unless it has infinite computer power and—the really hard part—access to a halting oracle.  However, Hutter has also defined computable approximations to AIXI.  For a quick intro, see this:</p>
<p>• Marcus Hutter, <a href="http://www.hutter1.net/ai/aixigentle.pdf">Universal intelligence: a mathematical top-down approach</a>.</p>
<p>
For more, try this:
</p>
<p>• Marcus Hutter, <i>Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability</i>, Springer, Berlin, 2005.</p>
<p><a name="Expected%20Utility"><br></a> </p>
<p> • <b>Utility</b>.  <a href="http://en.wikipedia.org/wiki/Utility">Utility</a> is a hypothetical numerical measure of satisfaction.  If you know the probabilities of various outcomes, and you know what your utility will be in each case, you can compute your "expected utility"  by taking the probabilities of the different outcomes, multiplying them by the corresponding utilities, and adding them up.  In simple terms, this is how happy you’ll be <i>on average</i>.  The <a href="http://en.wikipedia.org/wiki/Expected_utility_hypothesis">expected utility hypothesis</a> says that a rational decision-maker has a utility function and will try to maximize its expected utility.  </p>
<p><a name="Bounded%20Rationality"><br></a> </p>
<p> •<b>Bounded Rationality</b>.  In the real world, any decision-maker has limits on its computational power and the time it has to make a decision.  The idea that rational decision-makers "maximize expected utility" is oversimplified unless it takes this into account somehow.  Theories of <a href="http://en.wikipedia.org/wiki/Bounded_rationality">bounded rationality</a> try to take these limitations into account.  One approach is to think of decision-making as yet another activity whose costs and benefits must be taken into account when making decisions.  Roughly: you must decide how much time you want to spend deciding.  Of course, there’s an interesting circularity here.</p>
<p><a name="Black%20Swan"><br></a> </p>
<p> • <b>Black Swan</b>.  According to <a href="http://en.wikipedia.org/wiki/Nassim_Nicholas_Taleb">Nassim Taleb</a>, human history is dominated by <a href="http://en.wikipedia.org/wiki/Black_swan_theory">black swans</a>: important events that were unpredicted and indeed unpredictable, but rationalized by hindsight and thus made to seem as if they <i>could</i> have been predicted.  He believes that rather than trying to predict such events (which he considers largely futile), we should try to get good at adapting to them.  For more see:</p>
<p>• Nassim Taleb, <i><a href="http://en.wikipedia.org/wiki/The_Black_Swan_%28Taleb_book%29">The Black Swan: The Impact of the Highly Improbable</a></i>, Random House, New York, 2007.</p>
<hr>
<p>
<em>The first principle is that you must not fool yourself—and you are the easiest person to fool.</em> – Richard Feynman</p>
<div id="jp-post-flair" class="sharedaddy">
<nav class="jp-relatedposts-i2" data-layout="grid"><h3 class="jp-relatedposts-headline"><em>Related</em></h3><div class="jp-related-posts-i2__row" data-post-count="3"><ul id="related-posts-item-614fb8773c2ed" aria-labelledby="related-posts-item-614fb8773c2ed-label" class="jp-related-posts-i2__post" role="menuitem"><li class="jp-related-posts-i2__post-link"><a id="related-posts-item-614fb8773c2ed-label" href="https://johncarlosbaez.wordpress.com/2011/03/07/this-weeks-finds-week-311/">This Week’s Finds (Week 311)</a></li><li class="jp-related-posts-i2__post-date">7 March, 2011</li><li class="jp-related-posts-i2__post-context">In "this week's finds"</li></ul><ul id="related-posts-item-614fb8773c316" aria-labelledby="related-posts-item-614fb8773c316-label" class="jp-related-posts-i2__post" role="menuitem"><li class="jp-related-posts-i2__post-link"><a id="related-posts-item-614fb8773c316-label" href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/">This Week’s Finds (Week 313)</a></li><li class="jp-related-posts-i2__post-date">25 March, 2011</li><li class="jp-related-posts-i2__post-context">In "risks"</li></ul><ul id="related-posts-item-614fb8773c32d" aria-labelledby="related-posts-item-614fb8773c32d-label" class="jp-related-posts-i2__post" role="menuitem"><li class="jp-related-posts-i2__post-link"><a id="related-posts-item-614fb8773c32d-label" href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/">Logic, Probability and Reflection</a></li><li class="jp-related-posts-i2__post-date">26 December, 2013</li><li class="jp-related-posts-i2__post-context">In "mathematics"</li></ul></div></nav></div>	</div>

	<footer class="amp-wp-article-footer">
			<div class="amp-wp-meta amp-wp-tax-category">
		Categories: <a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/" rel="category tag">this week's finds</a>	</div>

		<div class="amp-wp-meta amp-wp-comments-link">
		<a href="https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/#comments">
			Leave a Comment		</a>
	</div>
	</footer>
</article>

<footer class="amp-wp-footer">
	<div>
		<h2>Azimuth</h2>
		<a href="#top" class="back-to-top">Back to top</a>
	</div>
</footer>


	<amp-pixel src="https://pixel.wp.com/b.gif?rand=RANDOM&amp;host=johncarlosbaez.wordpress.com&amp;ref=DOCUMENT_REFERRER&amp;amp=1&amp;blog=12777403&amp;v=wpcom&amp;tz=0&amp;user_id=0&amp;post=2735&amp;subd=johncarlosbaez" class="i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:1px;height:1px;" i-amphtml-layout="fixed"></amp-pixel>
	<amp-pixel src="https://pixel.wp.com/b.gif?rand=RANDOM&amp;v=wpcom-no-pv&amp;crypt=UE40eW5QN0p8M2Y%2FRE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29%2BSmw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8%2FMkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw%2FSkZ%2BQ3lLcjU%2FNDBvQTZSVHh0P3IlM0Y0UUh%2Bbys%2Fa35jaWhBV2VncFAuY2h0VkRQS2prSGtwTm56RGZIMFI0bUpRZkFybyxTUD9saXlBTD9jUzMlMldSWVpwTHVUY243JUxwdlJ0LEdYMCZ1MEM%2FTW4%3D" class="i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:1px;height:1px;" i-amphtml-layout="fixed"></amp-pixel>
	


</body></html>
