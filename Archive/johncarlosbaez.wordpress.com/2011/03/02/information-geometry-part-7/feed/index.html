<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information Geometry (Part 7)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/</link>
	<description></description>
	<lastBuildDate>Sat, 14 Jul 2012 03:12:17 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16763</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 14 Jul 2012 03:12:17 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16763</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16753&quot;&gt;Linas Vepstas&lt;/a&gt;.

Very intriguing stuff!  I&#039;m glad you&#039;re trying to improve the Wikipedia article on Fisher information by adding some of these known but obscure facts.  Someday some kid will read this stuff, put all the puzzle pieces together, and unify classical mechanics, quantum mechanics and probability theory in a new way.   Unless we get there first, of course.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16753">Linas Vepstas</a>.</p>
<p>Very intriguing stuff!  I&#8217;m glad you&#8217;re trying to improve the Wikipedia article on Fisher information by adding some of these known but obscure facts.  Someday some kid will read this stuff, put all the puzzle pieces together, and unify classical mechanics, quantum mechanics and probability theory in a new way.   Unless we get there first, of course.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Linas Vepstas		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16754</link>

		<dc:creator><![CDATA[Linas Vepstas]]></dc:creator>
		<pubDate>Fri, 13 Jul 2012 21:36:20 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16754</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16673&quot;&gt;John Baez&lt;/a&gt;.

The other part that is weird is that log p materializes as if it were a vector in a tangent space.  This suggests that the shannon entropy has some geometric interpretation, but I can&#039;t tell what it is.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16673">John Baez</a>.</p>
<p>The other part that is weird is that log p materializes as if it were a vector in a tangent space.  This suggests that the shannon entropy has some geometric interpretation, but I can&#8217;t tell what it is.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Linas Vepstas		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16753</link>

		<dc:creator><![CDATA[Linas Vepstas]]></dc:creator>
		<pubDate>Fri, 13 Jul 2012 21:19:20 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16753</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16673&quot;&gt;John Baez&lt;/a&gt;.

Turns out that trick you provide above leads directly to the Fubini-Study metric. Set the phase part of the complex wave-function to zero, apply the Fubini-Study metric, and you get (four times) the Fisher information metric. The Bures metric is identical to the Fubini-Study metric, except that its normally written for mixed states, while FB is normally expressed w/ pure states. The differences in notation obscures a lot. Wikipedia knows all, including references.

The part that awes me is that log p and the wave-function phase alpha put together gives a symplectic form.  This is surely somehow important, but I don&#039;t know why.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16673">John Baez</a>.</p>
<p>Turns out that trick you provide above leads directly to the Fubini-Study metric. Set the phase part of the complex wave-function to zero, apply the Fubini-Study metric, and you get (four times) the Fisher information metric. The Bures metric is identical to the Fubini-Study metric, except that its normally written for mixed states, while FB is normally expressed w/ pure states. The differences in notation obscures a lot. Wikipedia knows all, including references.</p>
<p>The part that awes me is that log p and the wave-function phase alpha put together gives a symplectic form.  This is surely somehow important, but I don&#8217;t know why.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Linas Vepstas		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16724</link>

		<dc:creator><![CDATA[Linas Vepstas]]></dc:creator>
		<pubDate>Fri, 13 Jul 2012 03:51:53 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16724</guid>

					<description><![CDATA[Huh. Will ponder. If I manage to clean this up, will insert into the WP article :-)  A quickie google search for &#039;fisher information metric&#039; and &#039;curvature&#039; bring up papers complicated enough to suggest that few are aware of of this: I guess a brute-force attack must get mired.]]></description>
			<content:encoded><![CDATA[<p>Huh. Will ponder. If I manage to clean this up, will insert into the WP article :-)  A quickie google search for &#8216;fisher information metric&#8217; and &#8216;curvature&#8217; bring up papers complicated enough to suggest that few are aware of of this: I guess a brute-force attack must get mired.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Linas Vepstas		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16723</link>

		<dc:creator><![CDATA[Linas Vepstas]]></dc:creator>
		<pubDate>Fri, 13 Jul 2012 03:29:25 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16723</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16672&quot;&gt;John Baez&lt;/a&gt;.

Perhaps Gromov is hinting at the random gaussian unitary ensemble?  I&#039;ve only gotten 3-4 page into his paper. 

I imagine there should be all sorts of &#039;secret links&#039;, not just for simplexes and hilbert spaces, but for generic homogenous spaces. At the risk of stating &#039;obvious&#039; things you understand better than I ... Whenever one&#039;s got some collection of operators acting on some space, you can ask what happens if you start picking out operators &#039;at random&#039; (after specifying a measure that defines what &#039;random&#039; means). By starting with something that&#039;s homogenous, you get gobs of symmetry, so measures and metrics are ... um, ahem, cough cough &#039;sphere-like&#039;. Generalizations of legendre polynomials and clebsch-gordon coefficients of good-ol su(2) to various wild hypergeometric series. There&#039;s a vast ocean of neato results coming out of this: there&#039;s also gobs of hyperbolic behaviour, lots of ergodic trajectories, and so analogs of the Riemann zeta that magically seem to obey the Riemann hypothesis (anything ergodic/hyperbolic seems to generalize something or other from number theory, this seems to be a general rule). I&#039;m sure Gromov is quite aware of these things, given his history...

... wish there were more hours in the day, it feels like I&#039;m in a candy store, sometimes...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16672">John Baez</a>.</p>
<p>Perhaps Gromov is hinting at the random gaussian unitary ensemble?  I&#8217;ve only gotten 3-4 page into his paper. </p>
<p>I imagine there should be all sorts of &#8216;secret links&#8217;, not just for simplexes and hilbert spaces, but for generic homogenous spaces. At the risk of stating &#8216;obvious&#8217; things you understand better than I &#8230; Whenever one&#8217;s got some collection of operators acting on some space, you can ask what happens if you start picking out operators &#8216;at random&#8217; (after specifying a measure that defines what &#8216;random&#8217; means). By starting with something that&#8217;s homogenous, you get gobs of symmetry, so measures and metrics are &#8230; um, ahem, cough cough &#8216;sphere-like&#8217;. Generalizations of legendre polynomials and clebsch-gordon coefficients of good-ol su(2) to various wild hypergeometric series. There&#8217;s a vast ocean of neato results coming out of this: there&#8217;s also gobs of hyperbolic behaviour, lots of ergodic trajectories, and so analogs of the Riemann zeta that magically seem to obey the Riemann hypothesis (anything ergodic/hyperbolic seems to generalize something or other from number theory, this seems to be a general rule). I&#8217;m sure Gromov is quite aware of these things, given his history&#8230;</p>
<p>&#8230; wish there were more hours in the day, it feels like I&#8217;m in a candy store, sometimes&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Linas Vepstas		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16721</link>

		<dc:creator><![CDATA[Linas Vepstas]]></dc:creator>
		<pubDate>Fri, 13 Jul 2012 02:55:51 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16721</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16671&quot;&gt;John Baez&lt;/a&gt;.

Yellow caution alert: Over the last few days, I quadrupled the size of the wikipedia article on the Fisher information metric, synthesizing from the Crooks paper, some of your posts, and other bits and pieces culled via google.  i.e. you&#039;re reading what I wrote...

(Dislaimer: I learn things by expanding WP articles. They&#039;re not peer-reviewed. Mistakes can creep in. They&#039;re sometimes sloppy and disorganized; it doesn&#039;t pay to be a perfectionist on WP).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16671">John Baez</a>.</p>
<p>Yellow caution alert: Over the last few days, I quadrupled the size of the wikipedia article on the Fisher information metric, synthesizing from the Crooks paper, some of your posts, and other bits and pieces culled via google.  i.e. you&#8217;re reading what I wrote&#8230;</p>
<p>(Dislaimer: I learn things by expanding WP articles. They&#8217;re not peer-reviewed. Mistakes can creep in. They&#8217;re sometimes sloppy and disorganized; it doesn&#8217;t pay to be a perfectionist on WP).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16673</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 12 Jul 2012 05:09:17 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16673</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668&quot;&gt;Linas Vepstas&lt;/a&gt;.

Sometime jokes turn out to be true.

I got motivated again to cheat and &lt;a href=&quot;http://www.stat.cmu.edu/~kass/papers/geometry.pdf&quot; rel=&quot;nofollow&quot;&gt;look up the answer&lt;/a&gt;, and it&#039;s very fascinating!  The trick is to think of an amplitude as being like &lt;i&gt;the square root&lt;/i&gt; of a probability---as usual in quantum mechanics!

We can map a probability distribution

$latex (p_1 , \dots, p_n) $

to a point on the sphere of radius 1, namely

$latex (p_1^{1/2}, \dots, p_n^{1/2})$

And then the Fisher information metric on the simplex gets sent to the usual round metric on a portion of the sphere... &lt;i&gt;up to a constant factor&lt;/i&gt;.

If you want the metrics to match exactly, you should map the simplex to the sphere of radius 2, via

$latex (p_1 , \dots, p_n) \mapsto 2  (p_1^{1/2}, \dots, p_n^{1/2})$

I&#039;m not sure how important that is: perhaps it simply means the Fisher information metric was defined slightly wrong, and should include a factor of 1/2 out front.

But what&#039;s the deep inner meaning of this relation between probability distributions and points on the sphere?   It seems to be saying that the &#039;right&#039; way to measure distances between probability distributions is to treat them as coming from quantum states, and measure the distance between those!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668">Linas Vepstas</a>.</p>
<p>Sometime jokes turn out to be true.</p>
<p>I got motivated again to cheat and <a href="http://www.stat.cmu.edu/~kass/papers/geometry.pdf" rel="nofollow">look up the answer</a>, and it&#8217;s very fascinating!  The trick is to think of an amplitude as being like <i>the square root</i> of a probability&#8212;as usual in quantum mechanics!</p>
<p>We can map a probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p_1+%2C+%5Cdots%2C+p_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1 , &#92;dots, p_n) " class="latex" /></p>
<p>to a point on the sphere of radius 1, namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p_1%5E%7B1%2F2%7D%2C+%5Cdots%2C+p_n%5E%7B1%2F2%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1^{1/2}, &#92;dots, p_n^{1/2})" class="latex" /></p>
<p>And then the Fisher information metric on the simplex gets sent to the usual round metric on a portion of the sphere&#8230; <i>up to a constant factor</i>.</p>
<p>If you want the metrics to match exactly, you should map the simplex to the sphere of radius 2, via</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p_1+%2C+%5Cdots%2C+p_n%29+%5Cmapsto+2++%28p_1%5E%7B1%2F2%7D%2C+%5Cdots%2C+p_n%5E%7B1%2F2%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1 , &#92;dots, p_n) &#92;mapsto 2  (p_1^{1/2}, &#92;dots, p_n^{1/2})" class="latex" /></p>
<p>I&#8217;m not sure how important that is: perhaps it simply means the Fisher information metric was defined slightly wrong, and should include a factor of 1/2 out front.</p>
<p>But what&#8217;s the deep inner meaning of this relation between probability distributions and points on the sphere?   It seems to be saying that the &#8216;right&#8217; way to measure distances between probability distributions is to treat them as coming from quantum states, and measure the distance between those!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16672</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 12 Jul 2012 04:47:32 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16672</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668&quot;&gt;Linas Vepstas&lt;/a&gt;.

Linas wrote:

&lt;blockquote&gt;
Next question: in principle, one could calculate the Levi-Civita symbols, the curvatute tensor, the Ricci tensor, “simply” by plugging in the expression of the Fisher metric, and turning the crank. My question then is: does one get lucky, do these expressions simplify, or do they remain a big mess? 
&lt;/blockquote&gt;

In the end it&#039;s all incredibly beautiful.  I think the right path was &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-4340&quot; rel=&quot;nofollow&quot;&gt;sketched earlier here&lt;/a&gt;.  First, observe that the  the space of probability distributions on an $latex n$-element set is an $latex (n-1)$-simplex.  Then, show tha  the Fisher information metric makes this simplex isometric to a portion of a round $latex (n-1)$-sphere!

Once we know this, it&#039;s clear that the curvature will be very nice and simple.

But last week Simon Willerton and I were trying to check that the simplex with its Fisher information metric really &lt;i&gt;is&lt;/i&gt; isometric to a portion of a sphere.  We got stuck, mainly because the &#039;obvious&#039; map from the simplex to the sphere, namely the one that sends a point

$latex (p_1 , \dots, p_n)$

such that $latex p_i \ge 0$ and $latex p_1 + \cdots p_n = 1$ to the point

$latex (p_1 , \dots, p_n) /\sqrt{p_1^2 + \cdots + p_n^2}$

on the unit sphere, didn&#039;t seem to work.  We couldn&#039;t guess the right one, and we couldn&#039;t find a mistake in our calculation. I tried to cheat by looking up the answer, but I couldn&#039;t quickly find it, and then I got distracted.  

It&#039;s all well-known stuff, to those who know it well.  In a &lt;a href=&quot;www.ihes.fr/~gromov/PDF/structre-serch-entropy-july5-2012.pdf&quot; rel=&quot;nofollow&quot;&gt;recent paper&lt;/a&gt;, Gromov hints that this relation between the Fisher information metric on the simplex and the round metric on the sphere is secretly a link between probability theory and quantum theory (where states lie on a sphere, but in a &lt;i&gt;complex&lt;/i&gt; Hilbert space).  But he doesn&#039;t elaborate, and he might have been joking.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668">Linas Vepstas</a>.</p>
<p>Linas wrote:</p>
<blockquote><p>
Next question: in principle, one could calculate the Levi-Civita symbols, the curvatute tensor, the Ricci tensor, “simply” by plugging in the expression of the Fisher metric, and turning the crank. My question then is: does one get lucky, do these expressions simplify, or do they remain a big mess?
</p></blockquote>
<p>In the end it&#8217;s all incredibly beautiful.  I think the right path was <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-4340" rel="nofollow">sketched earlier here</a>.  First, observe that the  the space of probability distributions on an <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-element set is an <img src="https://s0.wp.com/latex.php?latex=%28n-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(n-1)" class="latex" />-simplex.  Then, show tha  the Fisher information metric makes this simplex isometric to a portion of a round <img src="https://s0.wp.com/latex.php?latex=%28n-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(n-1)" class="latex" />-sphere!</p>
<p>Once we know this, it&#8217;s clear that the curvature will be very nice and simple.</p>
<p>But last week Simon Willerton and I were trying to check that the simplex with its Fisher information metric really <i>is</i> isometric to a portion of a sphere.  We got stuck, mainly because the &#8216;obvious&#8217; map from the simplex to the sphere, namely the one that sends a point</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p_1+%2C+%5Cdots%2C+p_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1 , &#92;dots, p_n)" class="latex" /></p>
<p>such that <img src="https://s0.wp.com/latex.php?latex=p_i+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;ge 0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_1+%2B+%5Ccdots+p_n+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 + &#92;cdots p_n = 1" class="latex" /> to the point</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p_1+%2C+%5Cdots%2C+p_n%29+%2F%5Csqrt%7Bp_1%5E2+%2B+%5Ccdots+%2B+p_n%5E2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p_1 , &#92;dots, p_n) /&#92;sqrt{p_1^2 + &#92;cdots + p_n^2}" class="latex" /></p>
<p>on the unit sphere, didn&#8217;t seem to work.  We couldn&#8217;t guess the right one, and we couldn&#8217;t find a mistake in our calculation. I tried to cheat by looking up the answer, but I couldn&#8217;t quickly find it, and then I got distracted.  </p>
<p>It&#8217;s all well-known stuff, to those who know it well.  In a <a href="www.ihes.fr/~gromov/PDF/structre-serch-entropy-july5-2012.pdf" rel="nofollow">recent paper</a>, Gromov hints that this relation between the Fisher information metric on the simplex and the round metric on the sphere is secretly a link between probability theory and quantum theory (where states lie on a sphere, but in a <i>complex</i> Hilbert space).  But he doesn&#8217;t elaborate, and he might have been joking.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16671</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 12 Jul 2012 04:24:32 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16671</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668&quot;&gt;Linas Vepstas&lt;/a&gt;.

Hi, Linas!  Good to see you here!  You wrote:

&lt;blockquote&gt;
The geodesic length is the square root of the Jensen--Shannon divergence, as pointed out by Gavin Crooks in the quoted article. 
&lt;/blockquote&gt;

Since I forget what the Jensen--Shannon divergence is, I imagine many other readers will too, so let&#039;s remind ourselves by peeking at the &lt;a href=&quot;http://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&quot; rel=&quot;nofollow&quot;&gt;Wikipedia article&lt;/a&gt;...

Oh, okay.  It&#039;s a well-known way of making the relative entropy (also called the &#039;Kullback-Leibler divergence&#039; by people who enjoy obscure technical terms) into an actual metric on the space of probability distributions.  

Given two probability distributions $latex p$ and $latex q$ on a measure space $latex (\Omega, \omega)$ the &lt;b&gt;entropy of $latex p$ relative to $latex q$&lt;/b&gt; is

$latex \displaystyle{ S(p,q) = \int_\Omega \; p\, \ln(\frac{p}{q}) \, d \omega }$

To make this symmetrical, we define a new probability distribution 

$latex  m = \displaystyle{ \frac{1}{2}(p+q)} $ 

that&#039;s the midpoint of $latex p$ and $latex q$ in some obvious naive sense, and define the &lt;b&gt;Jensen--Shannon distance&lt;/b&gt; to be

$latex \displaystyle{ JSD(p,q) = \frac{1}{2} \left( S(p,m) + S(m,d) \right) }$

Unlike the relative entropy, this is obviously symmetric:

$latex JSD(p,q) = JSD(q,p) $

And unlike the relative entropy, but less obviously, it also obeys the triangle inequality!

$latex JDS(p,r) \le JSD(p,q) + JSD(q,r)$

Nonetheless, I always thought this idea was a bit of a &#039;trick&#039;, and thus not worth studying.

For a second you made me think the Jensen--Shannon distance was just the square of the geodesic distance as measured by the Fisher information metric.  And that would mean it&#039;s &lt;i&gt;not&lt;/i&gt; just a trick!

But unfortunately that&#039;s not true... and probably not even what you were trying to say.  The truth is &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher_information_metric#Relation_to_the_Jensen-Shannon_divergence&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, and it&#039;s less pretty.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668">Linas Vepstas</a>.</p>
<p>Hi, Linas!  Good to see you here!  You wrote:</p>
<blockquote><p>
The geodesic length is the square root of the Jensen&#8211;Shannon divergence, as pointed out by Gavin Crooks in the quoted article.
</p></blockquote>
<p>Since I forget what the Jensen&#8211;Shannon divergence is, I imagine many other readers will too, so let&#8217;s remind ourselves by peeking at the <a href="http://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" rel="nofollow">Wikipedia article</a>&#8230;</p>
<p>Oh, okay.  It&#8217;s a well-known way of making the relative entropy (also called the &#8216;Kullback-Leibler divergence&#8217; by people who enjoy obscure technical terms) into an actual metric on the space of probability distributions.  </p>
<p>Given two probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on a measure space <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega, &#92;omega)" class="latex" /> the <b>entropy of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /></b> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28p%2Cq%29+%3D+%5Cint_%5COmega+%5C%3B+p%5C%2C+%5Cln%28%5Cfrac%7Bp%7D%7Bq%7D%29+%5C%2C+d+%5Comega+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(p,q) = &#92;int_&#92;Omega &#92;; p&#92;, &#92;ln(&#92;frac{p}{q}) &#92;, d &#92;omega }" class="latex" /></p>
<p>To make this symmetrical, we define a new probability distribution </p>
<p><img src="https://s0.wp.com/latex.php?latex=m+%3D+%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7B2%7D%28p%2Bq%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m = &#92;displaystyle{ &#92;frac{1}{2}(p+q)} " class="latex" /> </p>
<p>that&#8217;s the midpoint of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in some obvious naive sense, and define the <b>Jensen&#8211;Shannon distance</b> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+JSD%28p%2Cq%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+S%28p%2Cm%29+%2B+S%28m%2Cd%29+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ JSD(p,q) = &#92;frac{1}{2} &#92;left( S(p,m) + S(m,d) &#92;right) }" class="latex" /></p>
<p>Unlike the relative entropy, this is obviously symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=JSD%28p%2Cq%29+%3D+JSD%28q%2Cp%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="JSD(p,q) = JSD(q,p) " class="latex" /></p>
<p>And unlike the relative entropy, but less obviously, it also obeys the triangle inequality!</p>
<p><img src="https://s0.wp.com/latex.php?latex=JDS%28p%2Cr%29+%5Cle+JSD%28p%2Cq%29+%2B+JSD%28q%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="JDS(p,r) &#92;le JSD(p,q) + JSD(q,r)" class="latex" /></p>
<p>Nonetheless, I always thought this idea was a bit of a &#8216;trick&#8217;, and thus not worth studying.</p>
<p>For a second you made me think the Jensen&#8211;Shannon distance was just the square of the geodesic distance as measured by the Fisher information metric.  And that would mean it&#8217;s <i>not</i> just a trick!</p>
<p>But unfortunately that&#8217;s not true&#8230; and probably not even what you were trying to say.  The truth is <a href="http://en.wikipedia.org/wiki/Fisher_information_metric#Relation_to_the_Jensen-Shannon_divergence" rel="nofollow">here</a>, and it&#8217;s less pretty.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Linas Vepstas		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16669</link>

		<dc:creator><![CDATA[Linas Vepstas]]></dc:creator>
		<pubDate>Thu, 12 Jul 2012 02:45:56 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2576#comment-16669</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668&quot;&gt;Linas Vepstas&lt;/a&gt;.

Oh, hmm, right. Silly me. The geodesic length is the square root of the Jensen-Shannon divergence, as pointed out by Gavin Crooks in the quoted article.  I&#039;m not masochistic enough to try to verify this explicitly, although it would be a good exercise...

Next question: in principle, one could calculate the Levi-civita symbols, the curvatute tensor, the ricci tensor, &quot;simply&quot; by plugging in the expression of the fisher metric, and turning the crank.  My question then is: does one get lucky, do these expressions simplify, or do they remain a big mess? 

Again, since Jensen-Shannon does follow the geodesic, and since its a symmetrized form of Kullback-Leibler, I guess this mens that KL somehow follows a path that just starts in the wrong direction, and then curves around? Well, of course it does, but intuitively, its..?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comment-16668">Linas Vepstas</a>.</p>
<p>Oh, hmm, right. Silly me. The geodesic length is the square root of the Jensen-Shannon divergence, as pointed out by Gavin Crooks in the quoted article.  I&#8217;m not masochistic enough to try to verify this explicitly, although it would be a good exercise&#8230;</p>
<p>Next question: in principle, one could calculate the Levi-civita symbols, the curvatute tensor, the ricci tensor, &#8220;simply&#8221; by plugging in the expression of the fisher metric, and turning the crank.  My question then is: does one get lucky, do these expressions simplify, or do they remain a big mess? </p>
<p>Again, since Jensen-Shannon does follow the geodesic, and since its a symmetrized form of Kullback-Leibler, I guess this mens that KL somehow follows a path that just starts in the wrong direction, and then curves around? Well, of course it does, but intuitively, its..?</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
