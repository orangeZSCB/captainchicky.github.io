<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: This Week&#8217;s Finds (Week 313)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/</link>
	<description></description>
	<lastBuildDate>Thu, 26 Dec 2013 22:47:00 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Logic, Probability and Reflection &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-34841</link>

		<dc:creator><![CDATA[Logic, Probability and Reflection &#124; Azimuth]]></dc:creator>
		<pubDate>Thu, 26 Dec 2013 22:47:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-34841</guid>

					<description><![CDATA[[...] Last week I attended the &lt;a href=&quot;http://intelligence.org/&quot; rel=&quot;nofollow&quot;&gt;Machine Intelligence Research Institute&#039;s&lt;/a&gt; sixth &lt;a href=&quot;http://intelligence.org/2013/07/24/miris-december-2013-workshop/&quot; rel=&quot;nofollow&quot;&gt;Workshop on Logic, Probability, and Reflection&lt;/a&gt;.  You may know this institute under their previous name: the Singularity Institute.  It seems to be the brainchild of Eliezer Yudkowsky, a well-known advocate of &#039;friendly artificial intelligence&#039;, whom I interviewed in &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/07/this-weeks-finds-week-311/&quot; rel=&quot;nofollow&quot;&gt;week311&lt;/a&gt;, &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/&quot; rel=&quot;nofollow&quot;&gt;week312&lt;/a&gt; and &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/&quot; rel=&quot;nofollow&quot;&gt;week313&lt;/a&gt; of &lt;i&gt;This Week&#039;s Finds&lt;/i&gt;. [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] Last week I attended the <a href="http://intelligence.org/" rel="nofollow">Machine Intelligence Research Institute&#8217;s</a> sixth <a href="http://intelligence.org/2013/07/24/miris-december-2013-workshop/" rel="nofollow">Workshop on Logic, Probability, and Reflection</a>.  You may know this institute under their previous name: the Singularity Institute.  It seems to be the brainchild of Eliezer Yudkowsky, a well-known advocate of &#8216;friendly artificial intelligence&#8217;, whom I interviewed in <a href="https://johncarlosbaez.wordpress.com/2011/03/07/this-weeks-finds-week-311/" rel="nofollow">week311</a>, <a href="https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/" rel="nofollow">week312</a> and <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/" rel="nofollow">week313</a> of <i>This Week&#8217;s Finds</i>. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-10940</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 28 Nov 2011 00:22:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-10940</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-4903&quot;&gt;John Baez&lt;/a&gt;.

Sebastien wrote:

&lt;blockquote&gt;
From someone with this problem, how did you solve it?
&lt;/blockquote&gt;

I spent a lot of time every day reading about math, thinking about math, and talking to people about math.  I spent a lot of time taking math classes and working hard in them.  I spent a lot of time in good university libraries, trying to read all the math books: most of them were too hard at first, but I didn&#039;t give up. I kept (and keep) a series of notebooks in which I wrote down all my thoughts and calculations.

I spent a lot of time trying to think of math as a unified whole.  It&#039;s crucial to focus on tiny details a lot of the time, but it&#039;s also crucial to think a bit more vaguely about the &#039;big picture&#039; a lot of the time, without getting bogged down in details.  

Eventually, after a decade or two, math started making more and more sense, and a lot of things became obvious... including things that weren&#039;t obvious to other people!   The &#039;tao of mathematics&#039; started becoming clear.

I did a lot of this before the internet existed... but now if I were trying to get up to speed I&#039;d do all this &lt;i&gt;and&lt;/i&gt; read and discuss math a lot on math blogs and Mathoverflow, trying to always learn more than I argue.  Also, I&#039;d read lots of papers on the arXiv.

But while the internet is wonderful, it is still no substitute for spending hours in a good university math library, trying to understand all the books, sometimes reading one in detail, sometimes skimming a dozen in an hour.

It&#039;s also great to work on problems and brainstorm new ideas with a good, smart friend.  Two heads are better than one.

Good luck!  Just as water can slowly carve the landscape into fantastic rock formations, prolonged intelligent thought can do amazing things.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-4903">John Baez</a>.</p>
<p>Sebastien wrote:</p>
<blockquote><p>
From someone with this problem, how did you solve it?
</p></blockquote>
<p>I spent a lot of time every day reading about math, thinking about math, and talking to people about math.  I spent a lot of time taking math classes and working hard in them.  I spent a lot of time in good university libraries, trying to read all the math books: most of them were too hard at first, but I didn&#8217;t give up. I kept (and keep) a series of notebooks in which I wrote down all my thoughts and calculations.</p>
<p>I spent a lot of time trying to think of math as a unified whole.  It&#8217;s crucial to focus on tiny details a lot of the time, but it&#8217;s also crucial to think a bit more vaguely about the &#8216;big picture&#8217; a lot of the time, without getting bogged down in details.  </p>
<p>Eventually, after a decade or two, math started making more and more sense, and a lot of things became obvious&#8230; including things that weren&#8217;t obvious to other people!   The &#8216;tao of mathematics&#8217; started becoming clear.</p>
<p>I did a lot of this before the internet existed&#8230; but now if I were trying to get up to speed I&#8217;d do all this <i>and</i> read and discuss math a lot on math blogs and Mathoverflow, trying to always learn more than I argue.  Also, I&#8217;d read lots of papers on the arXiv.</p>
<p>But while the internet is wonderful, it is still no substitute for spending hours in a good university math library, trying to understand all the books, sometimes reading one in detail, sometimes skimming a dozen in an hour.</p>
<p>It&#8217;s also great to work on problems and brainstorm new ideas with a good, smart friend.  Two heads are better than one.</p>
<p>Good luck!  Just as water can slowly carve the landscape into fantastic rock formations, prolonged intelligent thought can do amazing things.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Sebastien		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-10917</link>

		<dc:creator><![CDATA[Sebastien]]></dc:creator>
		<pubDate>Sun, 27 Nov 2011 15:28:43 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-10917</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-4903&quot;&gt;John Baez&lt;/a&gt;.

&lt;blockquote&gt;Second, I wasn’t good enough at math. I could learn stuff pretty quickly, but I wasn’t yet able to invent good ideas of my own — and that was pretty frustrating.

Later on I solved both those problems, and I stopped being bored.&lt;/blockquote&gt;

From someone with this problem, how did you solve it?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-4903">John Baez</a>.</p>
<blockquote><p>Second, I wasn’t good enough at math. I could learn stuff pretty quickly, but I wasn’t yet able to invent good ideas of my own — and that was pretty frustrating.</p>
<p>Later on I solved both those problems, and I stopped being bored.</p></blockquote>
<p>From someone with this problem, how did you solve it?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: DavidTweed		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5575</link>

		<dc:creator><![CDATA[DavidTweed]]></dc:creator>
		<pubDate>Tue, 26 Apr 2011 21:49:27 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-5575</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5569&quot;&gt;Arrow&lt;/a&gt;.

I&#039;ll disagree with you about the triviality of this: it took a lot of work to get to this point. Likewise, I&#039;m not remotely convinced that &quot;Evaluating fitness of a sentient agent who is to live in a complex real world environment filled with other sentient beings whose actions cannot be predicted is next to impossible&quot;. (I&#039;m sure I&#039;ve read somewhere about a viewpoint where everything is either &quot;trivial&quot; or &quot;impossible&quot;, and things mysteriously move between the two as evidence is pointed out.) It&#039;s certainly hard, but for those who actually roll up their sleeves and work on the problem my opinion is that it&#039;ll turn out to be perfectly feasible to do well enough to do a got enough job that a reasonable amount of learning is possible. I really think that if you took the logic you&#039;re using and try to apply it to various things that are already happening in the world, you&#039;d see things are happening that you claim should be next to impossible.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5569">Arrow</a>.</p>
<p>I&#8217;ll disagree with you about the triviality of this: it took a lot of work to get to this point. Likewise, I&#8217;m not remotely convinced that &#8220;Evaluating fitness of a sentient agent who is to live in a complex real world environment filled with other sentient beings whose actions cannot be predicted is next to impossible&#8221;. (I&#8217;m sure I&#8217;ve read somewhere about a viewpoint where everything is either &#8220;trivial&#8221; or &#8220;impossible&#8221;, and things mysteriously move between the two as evidence is pointed out.) It&#8217;s certainly hard, but for those who actually roll up their sleeves and work on the problem my opinion is that it&#8217;ll turn out to be perfectly feasible to do well enough to do a got enough job that a reasonable amount of learning is possible. I really think that if you took the logic you&#8217;re using and try to apply it to various things that are already happening in the world, you&#8217;d see things are happening that you claim should be next to impossible.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Arrow		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5569</link>

		<dc:creator><![CDATA[Arrow]]></dc:creator>
		<pubDate>Tue, 26 Apr 2011 12:59:14 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-5569</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5565&quot;&gt;DavidTweed&lt;/a&gt;.

Seriously. Evaluating fitness for purpose of such a kinect classifier is trivial. 

Evaluating fitness of a sentient agent who is to live in a complex real world environment filled with other sentient beings whose actions cannot be predicted is next to impossible, it can only be attempted after the fact and even then the best you get is a rough estimate since random chance plays a large part.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5565">DavidTweed</a>.</p>
<p>Seriously. Evaluating fitness for purpose of such a kinect classifier is trivial. </p>
<p>Evaluating fitness of a sentient agent who is to live in a complex real world environment filled with other sentient beings whose actions cannot be predicted is next to impossible, it can only be attempted after the fact and even then the best you get is a rough estimate since random chance plays a large part.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: DavidTweed		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5565</link>

		<dc:creator><![CDATA[DavidTweed]]></dc:creator>
		<pubDate>Tue, 26 Apr 2011 11:29:12 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-5565</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5563&quot;&gt;Arrow&lt;/a&gt;.

I think your arguments are heuristic to the point of being erroneous. To pick one of your points, you say

&lt;blockquote&gt;

evolution requires millions of independent agents testing different modifications, each with a full access to the environment for which its fitness is to improve, and in this case it means each agent with full control over real world assets I mentioned before: ...

&lt;/blockquote&gt;

To pick an incontrovertaile real-world example, consider the Microsoft Kinect. The goal there was to produce a simple-enough-to-mass-produce a classifier for human body parts from a scanner. By your argument, that would require each &quot;proposed modification&quot; to have access to a human being doing movements in a front of a scanner. However, &lt;a href=&quot;http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; points out that it was possible to obtain the desired goal, namely an optimised classifier, using a cluster of machines with access to training and test data. (This is a much, much, much simpler task than refining an artificial general intelligence, but it shows that you can&#039;t rely on vague high-level reasoning to figure out what&#039;s possible.) &lt;em&gt;This is not some theoretical argument; this is bread-and-butter stuff people are doing today&lt;/em&gt;.

Similar actual examples could be produced to contradict some of your other reasoning. AI might prove to not be possible, but if so I suspect it will due to creating the initial AI being beyond the reach of even technology augmented humans.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5563">Arrow</a>.</p>
<p>I think your arguments are heuristic to the point of being erroneous. To pick one of your points, you say</p>
<blockquote>
<p>evolution requires millions of independent agents testing different modifications, each with a full access to the environment for which its fitness is to improve, and in this case it means each agent with full control over real world assets I mentioned before: &#8230;</p>
</blockquote>
<p>To pick an incontrovertaile real-world example, consider the Microsoft Kinect. The goal there was to produce a simple-enough-to-mass-produce a classifier for human body parts from a scanner. By your argument, that would require each &#8220;proposed modification&#8221; to have access to a human being doing movements in a front of a scanner. However, <a href="http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf" rel="nofollow">this paper</a> points out that it was possible to obtain the desired goal, namely an optimised classifier, using a cluster of machines with access to training and test data. (This is a much, much, much simpler task than refining an artificial general intelligence, but it shows that you can&#8217;t rely on vague high-level reasoning to figure out what&#8217;s possible.) <em>This is not some theoretical argument; this is bread-and-butter stuff people are doing today</em>.</p>
<p>Similar actual examples could be produced to contradict some of your other reasoning. AI might prove to not be possible, but if so I suspect it will due to creating the initial AI being beyond the reach of even technology augmented humans.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Arrow		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5563</link>

		<dc:creator><![CDATA[Arrow]]></dc:creator>
		<pubDate>Tue, 26 Apr 2011 09:35:44 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-5563</guid>

					<description><![CDATA[I find the scenario of runaway self-improving AI rather naive. 

Intelligence, artificial or otherwise, in itself is not dangerous, it&#039;s neither good or evil, friendly or hostile. It&#039;s just a tool, an ability to solve problems. It can be &quot;dangerous&quot; to some humans when placed in hands of other humans but that danger originates in humans themselves, not the intelligence.

For the runaway self-improving AI scenario to work the AI would have to be equipped with the equivalence of human survival instinct which has nothing to do with intelligence itself, it is simply hardwired into our biology.

It is far from obvious that such a drive can be imparted onto AI in any permanent way if this AI is going to be capable of self-modification. What will stop it from erasing this part of the code? Or for that matter what will stop it from erasing itself? This is an example of the more general problem with self-modifying agents - to achieve anything constructive there has to be some external framework which is off-limits and which sets the course, otherwise all you get is garbage. For us our biological instincts serve this purpose. But imagine for a moment what would happen if we could freely modify them.

But even assuming that a survival instinct equivalent can be somehow imparted to create a &quot;motivated&quot; AI, such an AI wouldn&#039;t pose any real threat to humanity until it were able to control all the real world aspects of its functioning: resource extraction, energy generation, manufacturing, defense and so on. It&#039;s certainly not technologically feasible now or in the near future but even if it were it is extremely unlikely humans would allow things to get that far out of control.

But the biggest problem with this scenario lies with self-improvement. For a &quot;motivated&quot; AI to self improve it would have to have ways of either testing or predicting the fitness of next iteration. Prediction is only possible if the next iteration is to be simpler, but simplification can only go so far. The viable long-term improvement would require rising the complexity, but a simpler AI cannot simulate more complex AI and therefore cannot predict its fitness. The only way to improve fitness while rising complexity is by trial and error. But there is a problem - most changes decrease fitness. So a single AI cannot just evolve itself, evolution requires millions of independent agents testing different modifications, each with a full access to the environment for which its fitness is to improve, and in this case it means each agent with full control over real world assets I mentioned before: resource extraction, energy generation, manufacturing, defense and so on. This is pure science fiction now and for the foreseeable future.]]></description>
			<content:encoded><![CDATA[<p>I find the scenario of runaway self-improving AI rather naive. </p>
<p>Intelligence, artificial or otherwise, in itself is not dangerous, it&#8217;s neither good or evil, friendly or hostile. It&#8217;s just a tool, an ability to solve problems. It can be &#8220;dangerous&#8221; to some humans when placed in hands of other humans but that danger originates in humans themselves, not the intelligence.</p>
<p>For the runaway self-improving AI scenario to work the AI would have to be equipped with the equivalence of human survival instinct which has nothing to do with intelligence itself, it is simply hardwired into our biology.</p>
<p>It is far from obvious that such a drive can be imparted onto AI in any permanent way if this AI is going to be capable of self-modification. What will stop it from erasing this part of the code? Or for that matter what will stop it from erasing itself? This is an example of the more general problem with self-modifying agents &#8211; to achieve anything constructive there has to be some external framework which is off-limits and which sets the course, otherwise all you get is garbage. For us our biological instincts serve this purpose. But imagine for a moment what would happen if we could freely modify them.</p>
<p>But even assuming that a survival instinct equivalent can be somehow imparted to create a &#8220;motivated&#8221; AI, such an AI wouldn&#8217;t pose any real threat to humanity until it were able to control all the real world aspects of its functioning: resource extraction, energy generation, manufacturing, defense and so on. It&#8217;s certainly not technologically feasible now or in the near future but even if it were it is extremely unlikely humans would allow things to get that far out of control.</p>
<p>But the biggest problem with this scenario lies with self-improvement. For a &#8220;motivated&#8221; AI to self improve it would have to have ways of either testing or predicting the fitness of next iteration. Prediction is only possible if the next iteration is to be simpler, but simplification can only go so far. The viable long-term improvement would require rising the complexity, but a simpler AI cannot simulate more complex AI and therefore cannot predict its fitness. The only way to improve fitness while rising complexity is by trial and error. But there is a problem &#8211; most changes decrease fitness. So a single AI cannot just evolve itself, evolution requires millions of independent agents testing different modifications, each with a full access to the environment for which its fitness is to improve, and in this case it means each agent with full control over real world assets I mentioned before: resource extraction, energy generation, manufacturing, defense and so on. This is pure science fiction now and for the foreseeable future.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: What To Do? « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5503</link>

		<dc:creator><![CDATA[What To Do? « Azimuth]]></dc:creator>
		<pubDate>Sun, 24 Apr 2011 14:46:32 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-5503</guid>

					<description><![CDATA[In a comment on my last interview with Yudkowsky, Eric Jordan wrote: &quot;John, it would be great if you could follow up at some point with your thoughts and responses to what Eliezer said....&quot;]]></description>
			<content:encoded><![CDATA[<p>In a comment on my last interview with Yudkowsky, Eric Jordan wrote: &#8220;John, it would be great if you could follow up at some point with your thoughts and responses to what Eliezer said&#8230;.&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5429</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 21 Apr 2011 05:31:31 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-5429</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5420&quot;&gt;Eric Jordan&lt;/a&gt;.

Okay, great.  I&#039;ve got a blog post I want to do now but yours will be the next one.  You raised an issue that&#039;s been on my mind a lot, so the only problem is organizing my thoughts and saying something coherent.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5420">Eric Jordan</a>.</p>
<p>Okay, great.  I&#8217;ve got a blog post I want to do now but yours will be the next one.  You raised an issue that&#8217;s been on my mind a lot, so the only problem is organizing my thoughts and saying something coherent.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phil Henshaw		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5425</link>

		<dc:creator><![CDATA[Phil Henshaw]]></dc:creator>
		<pubDate>Wed, 20 Apr 2011 21:52:51 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2830#comment-5425</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5383&quot;&gt;john&lt;/a&gt;.

P.S.  Just to suggest where I would go with that, as I did not leave it clear, I&#039;d ask if we&#039;ve exhausted the traditional relationship between man and tools.  Maybe the achievable end for AI is not to program robots to understand deep questions and give insightful and honest answers, replacing those jobs only humans are known for so far.  Maybe in the traditional way the technology could be an aid rather than a replacement for people, with it&#039;s task being to enhance human learning.  

A robot might direct human attention to where the robot observes emergent systems developing, and ask the human if that would add new dimensions to the problems they are concerned with, for example.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5383">john</a>.</p>
<p>P.S.  Just to suggest where I would go with that, as I did not leave it clear, I&#8217;d ask if we&#8217;ve exhausted the traditional relationship between man and tools.  Maybe the achievable end for AI is not to program robots to understand deep questions and give insightful and honest answers, replacing those jobs only humans are known for so far.  Maybe in the traditional way the technology could be an aid rather than a replacement for people, with it&#8217;s task being to enhance human learning.  </p>
<p>A robot might direct human attention to where the robot observes emergent systems developing, and ask the human if that would add new dimensions to the problems they are concerned with, for example.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
