<!DOCTYPE html>
<html amp lang="en" i-amphtml-layout="" i-amphtml-no-boilerplate="" transformed="self;v=1">
<head><meta charset="utf-8"><style amp-runtime="" i-amphtml-version="012109102127000">html{overflow-x:hidden!important}html.i-amphtml-fie{height:100%!important;width:100%!important}html:not([amp4ads]),html:not([amp4ads]) body{height:auto!important}html:not([amp4ads]) body{margin:0!important}body{-webkit-text-size-adjust:100%;-moz-text-size-adjust:100%;-ms-text-size-adjust:100%;text-size-adjust:100%}html.i-amphtml-singledoc.i-amphtml-embedded{-ms-touch-action:pan-y pinch-zoom;touch-action:pan-y pinch-zoom}html.i-amphtml-fie>body,html.i-amphtml-singledoc>body{overflow:visible!important}html.i-amphtml-fie:not(.i-amphtml-inabox)>body,html.i-amphtml-singledoc:not(.i-amphtml-inabox)>body{position:relative!important}html.i-amphtml-ios-embed-legacy>body{overflow-x:hidden!important;overflow-y:auto!important;position:absolute!important}html.i-amphtml-ios-embed{overflow-y:auto!important;position:static}#i-amphtml-wrapper{overflow-x:hidden!important;overflow-y:auto!important;position:absolute!important;top:0!important;left:0!important;right:0!important;bottom:0!important;margin:0!important;display:block!important}html.i-amphtml-ios-embed.i-amphtml-ios-overscroll,html.i-amphtml-ios-embed.i-amphtml-ios-overscroll>#i-amphtml-wrapper{-webkit-overflow-scrolling:touch!important}#i-amphtml-wrapper>body{position:relative!important;border-top:1px solid transparent!important}#i-amphtml-wrapper+body{visibility:visible}#i-amphtml-wrapper+body .i-amphtml-lightbox-element,#i-amphtml-wrapper+body[i-amphtml-lightbox]{visibility:hidden}#i-amphtml-wrapper+body[i-amphtml-lightbox] .i-amphtml-lightbox-element{visibility:visible}#i-amphtml-wrapper.i-amphtml-scroll-disabled,.i-amphtml-scroll-disabled{overflow-x:hidden!important;overflow-y:hidden!important}amp-instagram{padding:54px 0px 0px!important;background-color:#fff}amp-iframe iframe{box-sizing:border-box!important}[amp-access][amp-access-hide]{display:none}[subscriptions-dialog],body:not(.i-amphtml-subs-ready) [subscriptions-action],body:not(.i-amphtml-subs-ready) [subscriptions-section]{display:none!important}amp-experiment,amp-live-list>[update]{display:none}amp-list[resizable-children]>.i-amphtml-loading-container.amp-hidden{display:none!important}amp-list [fetch-error],amp-list[load-more] [load-more-button],amp-list[load-more] [load-more-end],amp-list[load-more] [load-more-failed],amp-list[load-more] [load-more-loading]{display:none}amp-list[diffable] div[role=list]{display:block}amp-story-page,amp-story[standalone]{min-height:1px!important;display:block!important;height:100%!important;margin:0!important;padding:0!important;overflow:hidden!important;width:100%!important}amp-story[standalone]{background-color:#000!important;position:relative!important}amp-story-page{background-color:#757575}amp-story .amp-active>div,amp-story .i-amphtml-loader-background{display:none!important}amp-story-page:not(:first-of-type):not([distance]):not([active]){transform:translateY(1000vh)!important}amp-autocomplete{position:relative!important;display:inline-block!important}amp-autocomplete>input,amp-autocomplete>textarea{padding:0.5rem;border:1px solid rgba(0,0,0,0.33)}.i-amphtml-autocomplete-results,amp-autocomplete>input,amp-autocomplete>textarea{font-size:1rem;line-height:1.5rem}[amp-fx^=fly-in]{visibility:hidden}amp-script[nodom],amp-script[sandboxed]{position:fixed!important;top:0!important;width:1px!important;height:1px!important;overflow:hidden!important;visibility:hidden}
/*# sourceURL=/css/ampdoc.css*/[hidden]{display:none!important}.i-amphtml-element{display:inline-block}.i-amphtml-blurry-placeholder{transition:opacity 0.3s cubic-bezier(0.0,0.0,0.2,1)!important;pointer-events:none}[layout=nodisplay]:not(.i-amphtml-element){display:none!important}.i-amphtml-layout-fixed,[layout=fixed][width][height]:not(.i-amphtml-layout-fixed){display:inline-block;position:relative}.i-amphtml-layout-responsive,[layout=responsive][width][height]:not(.i-amphtml-layout-responsive),[width][height][heights]:not([layout]):not(.i-amphtml-layout-responsive),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-layout-responsive){display:block;position:relative}.i-amphtml-layout-intrinsic,[layout=intrinsic][width][height]:not(.i-amphtml-layout-intrinsic){display:inline-block;position:relative;max-width:100%}.i-amphtml-layout-intrinsic .i-amphtml-sizer{max-width:100%}.i-amphtml-intrinsic-sizer{max-width:100%;display:block!important}.i-amphtml-layout-container,.i-amphtml-layout-fixed-height,[layout=container],[layout=fixed-height][height]:not(.i-amphtml-layout-fixed-height){display:block;position:relative}.i-amphtml-layout-fill,.i-amphtml-layout-fill.i-amphtml-notbuilt,[layout=fill]:not(.i-amphtml-layout-fill),body noscript>*{display:block;overflow:hidden!important;position:absolute;top:0;left:0;bottom:0;right:0}body noscript>*{position:absolute!important;width:100%;height:100%;z-index:2}body noscript{display:inline!important}.i-amphtml-layout-flex-item,[layout=flex-item]:not(.i-amphtml-layout-flex-item){display:block;position:relative;-ms-flex:1 1 auto;flex:1 1 auto}.i-amphtml-layout-fluid{position:relative}.i-amphtml-layout-size-defined{overflow:hidden!important}.i-amphtml-layout-awaiting-size{position:absolute!important;top:auto!important;bottom:auto!important}i-amphtml-sizer{display:block!important}@supports (aspect-ratio:1/1){i-amphtml-sizer.i-amphtml-disable-ar{display:none!important}}.i-amphtml-blurry-placeholder,.i-amphtml-fill-content{display:block;height:0;max-height:100%;max-width:100%;min-height:100%;min-width:100%;width:0;margin:auto}.i-amphtml-layout-size-defined .i-amphtml-fill-content{position:absolute;top:0;left:0;bottom:0;right:0}.i-amphtml-replaced-content,.i-amphtml-screen-reader{padding:0!important;border:none!important}.i-amphtml-screen-reader{position:fixed!important;top:0px!important;left:0px!important;width:4px!important;height:4px!important;opacity:0!important;overflow:hidden!important;margin:0!important;display:block!important;visibility:visible!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:8px!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:12px!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:16px!important}.i-amphtml-unresolved{position:relative;overflow:hidden!important}.i-amphtml-select-disabled{-webkit-user-select:none!important;-ms-user-select:none!important;user-select:none!important}.i-amphtml-notbuilt,[layout]:not(.i-amphtml-element),[width][height][heights]:not([layout]):not(.i-amphtml-element),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-element){position:relative;overflow:hidden!important;color:transparent!important}.i-amphtml-notbuilt:not(.i-amphtml-layout-container)>*,[layout]:not([layout=container]):not(.i-amphtml-element)>*,[width][height][heights]:not([layout]):not(.i-amphtml-element)>*,[width][height][sizes]:not([layout]):not(.i-amphtml-element)>*{display:none}amp-img:not(.i-amphtml-element)[i-amphtml-ssr]>img.i-amphtml-fill-content{display:block}.i-amphtml-notbuilt:not(.i-amphtml-layout-container),[layout]:not([layout=container]):not(.i-amphtml-element),[width][height][heights]:not([layout]):not(.i-amphtml-element),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-element){color:transparent!important;line-height:0!important}.i-amphtml-ghost{visibility:hidden!important}.i-amphtml-element>[placeholder],[layout]:not(.i-amphtml-element)>[placeholder],[width][height][heights]:not([layout]):not(.i-amphtml-element)>[placeholder],[width][height][sizes]:not([layout]):not(.i-amphtml-element)>[placeholder]{display:block;line-height:normal}.i-amphtml-element>[placeholder].amp-hidden,.i-amphtml-element>[placeholder].hidden{visibility:hidden}.i-amphtml-element:not(.amp-notsupported)>[fallback],.i-amphtml-layout-container>[placeholder].amp-hidden,.i-amphtml-layout-container>[placeholder].hidden{display:none}.i-amphtml-layout-size-defined>[fallback],.i-amphtml-layout-size-defined>[placeholder]{position:absolute!important;top:0!important;left:0!important;right:0!important;bottom:0!important;z-index:1}amp-img[i-amphtml-ssr]:not(.i-amphtml-element)>[placeholder]{z-index:auto}.i-amphtml-notbuilt>[placeholder]{display:block!important}.i-amphtml-hidden-by-media-query{display:none!important}.i-amphtml-element-error{background:red!important;color:#fff!important;position:relative!important}.i-amphtml-element-error:before{content:attr(error-message)}i-amp-scroll-container,i-amphtml-scroll-container{position:absolute;top:0;left:0;right:0;bottom:0;display:block}i-amp-scroll-container.amp-active,i-amphtml-scroll-container.amp-active{overflow:auto;-webkit-overflow-scrolling:touch}.i-amphtml-loading-container{display:block!important;pointer-events:none;z-index:1}.i-amphtml-notbuilt>.i-amphtml-loading-container{display:block!important}.i-amphtml-loading-container.amp-hidden{visibility:hidden}.i-amphtml-element>[overflow]{cursor:pointer;position:relative;z-index:2;visibility:hidden;display:initial;line-height:normal}.i-amphtml-layout-size-defined>[overflow]{position:absolute}.i-amphtml-element>[overflow].amp-visible{visibility:visible}template{display:none!important}.amp-border-box,.amp-border-box *,.amp-border-box :after,.amp-border-box :before{box-sizing:border-box}amp-pixel{display:none!important}amp-analytics,amp-auto-ads,amp-story-auto-ads{position:fixed!important;top:0!important;width:1px!important;height:1px!important;overflow:hidden!important;visibility:hidden}html.i-amphtml-fie>amp-analytics{position:initial!important}[visible-when-invalid]:not(.visible),form [submit-error],form [submit-success],form [submitting]{display:none}amp-accordion{display:block!important}@media (min-width:1px){:where(amp-accordion>section)>:first-child{margin:0;background-color:#efefef;padding-right:20px;border:1px solid #dfdfdf}:where(amp-accordion>section)>:last-child{margin:0}}amp-accordion>section{float:none!important}amp-accordion>section>*{float:none!important;display:block!important;overflow:hidden!important;position:relative!important}amp-accordion,amp-accordion>section{margin:0}amp-accordion:not(.i-amphtml-built)>section>:last-child{display:none!important}amp-accordion:not(.i-amphtml-built)>section[expanded]>:last-child{display:block!important}
/*# sourceURL=/css/ampshared.css*/</style><meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1"><meta property="og:type" content="article"><meta property="og:title" content="This Week’s Finds (Week 313)"><meta property="og:url" content="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/"><meta property="og:description" content="The third and final segment of an interview with Eliezer Yudkowsky."><meta property="article:published_time" content="2011-03-25T01:16:10+00:00"><meta property="article:modified_time" content="2011-04-24T07:41:25+00:00"><meta property="og:site_name" content="Azimuth"><meta property="og:image" content="https://s0.wp.com/i/blank.jpg"><meta property="og:locale" content="en_US"><meta name="twitter:site" content="@wordpressdotcom"><meta name="twitter:text:title" content="This Week’s Finds (Week 313)"><meta name="twitter:card" content="summary"><meta property="fb:app_id" content="249643311490"><meta property="article:publisher" content="https://www.facebook.com/WordPresscom"><meta name="generator" content="AMP Plugin v2.0.5; mode=reader; theme=legacy"><meta name="generator" content="WordPress.com"><title>This Week’s Finds (Week 313) – Azimuth</title><link rel="preconnect" href="https://cdn.ampproject.org"><link rel="preload" as="script" href="https://cdn.ampproject.org/v0.js"><script async="" src="https://cdn.ampproject.org/v0.js"></script><style amp-custom="">.jp-related-posts-i2__row{display:flex;margin-top:1.5rem}.jp-related-posts-i2__row:first-child{margin-top:0}.jp-related-posts-i2__post{flex-grow:1;flex-basis:0;margin:0 10px;display:flex;flex-direction:column;padding-left:0}.jp-related-posts-i2__row[data-post-count="3"] .jp-related-posts-i2__post{max-width:calc(33% - 20px)}.jp-related-posts-i2__row[data-post-count="2"] .jp-related-posts-i2__post,.jp-related-posts-i2__row[data-post-count="1"] .jp-related-posts-i2__post{max-width:calc(50% - 20px)}.jp-related-posts-i2__post-date,.jp-related-posts-i2__post-context{flex-direction:row;display:block}.jp-related-posts-i2__post-link{display:block;width:100%;line-height:1.2em}.jp-relatedposts-i2[data-layout=list] .jp-related-posts-i2__row{margin-top:0;display:block}.jp-relatedposts-i2[data-layout=list] .jp-related-posts-i2__post{max-width:none;margin:0}@media only screen and (max-width:640px){.jp-related-posts-i2__row{margin-top:0;display:block}.jp-related-posts-i2__row[data-post-count] .jp-related-posts-i2__post{max-width:none;margin:0;margin-top:1rem}}h1,h2,h3{overflow-wrap:break-word}ul{overflow-wrap:break-word}p{overflow-wrap:break-word}@-webkit-keyframes a{to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}@keyframes a{to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}@-webkit-keyframes b{0%{background-position:0 0}to{background-position:30px 0}}@keyframes b{0%{background-position:0 0}to{background-position:30px 0}}html{background:#0a89c0}body{background:#fff;color:#353535;font-family:Georgia,"Times New Roman",Times,Serif;font-weight:300;line-height:1.75em}p,ul{margin:0 0 1em;padding:0}a,a:visited{color:#0a89c0}a:hover,a:active,a:focus{color:#353535}.amp-wp-meta,.amp-wp-header div,.amp-wp-title,.amp-wp-tax-category,.amp-wp-comments-link,.amp-wp-footer p,.back-to-top{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","Roboto","Oxygen-Sans","Ubuntu","Cantarell","Helvetica Neue",sans-serif}.amp-wp-header{background-color:#0a89c0}.amp-wp-header div{color:#fff;font-size:1em;font-weight:400;margin:0 auto;max-width:calc(840px - 32px);padding:.875em 16px;position:relative}.amp-wp-header a{color:#fff;text-decoration:none}.amp-wp-article{color:#353535;font-weight:400;margin:1.5em auto;max-width:840px;overflow-wrap:break-word;word-wrap:break-word}.amp-wp-article-header{align-items:center;align-content:stretch;display:flex;flex-wrap:wrap;justify-content:space-between;margin:1.5em 16px 0}.amp-wp-title{color:#353535;display:block;flex:1 0 100%;font-weight:900;margin:0 0 .625em;width:100%}.amp-wp-meta{color:#696969;display:inline-block;flex:2 1 50%;font-size:.875em;line-height:1.5em;margin:0 0 1.5em;padding:0}.amp-wp-article-header .amp-wp-meta:last-of-type{text-align:right}.amp-wp-article-header .amp-wp-meta:first-of-type{text-align:left}.amp-wp-byline amp-img,.amp-wp-byline .amp-wp-author{display:inline-block;vertical-align:middle}.amp-wp-byline amp-img{border:1px solid #0a89c0;border-radius:50%;position:relative;margin-right:6px}.amp-wp-posted-on{text-align:right}.amp-wp-article-content{margin:0 16px}.amp-wp-article-content ul{margin-left:1em}.amp-wp-article-content amp-img{margin:0 auto}.amp-wp-article-footer .amp-wp-meta{display:block}.amp-wp-tax-category{color:#696969;font-size:.875em;line-height:1.5em;margin:1.5em 16px}.amp-wp-comments-link{color:#696969;font-size:.875em;line-height:1.5em;text-align:center;margin:2.25em 0 1.5em}.amp-wp-comments-link a{border-style:solid;border-color:#c2c2c2;border-width:1px 1px 2px;border-radius:4px;background-color:transparent;color:#0a89c0;cursor:pointer;display:block;font-size:14px;font-weight:600;line-height:18px;margin:0 auto;max-width:200px;padding:11px 16px;text-decoration:none;width:50%;-webkit-transition:background-color .2s ease;transition:background-color .2s ease}.amp-wp-footer{border-top:1px solid #c2c2c2;margin:calc(1.5em - 1px) 0 0}.amp-wp-footer div{margin:0 auto;max-width:calc(840px - 32px);padding:1.25em 16px 1.25em;position:relative}.amp-wp-footer h2{font-size:1em;line-height:1.375em;margin:0 0 .5em}.amp-wp-footer p{color:#696969;font-size:.8em;line-height:1.5em;margin:0 85px 0 0}.amp-wp-footer a{text-decoration:none}.back-to-top{bottom:1.275em;font-size:.8em;font-weight:600;line-height:2em;position:absolute;right:16px}@font-face{font-family:"social-logos";src:url("/wp-content/mu-plugins/social-logos/social-logos.eot?5d3b4da4f6c2460dd842dbf9e0992ea6");font-weight:normal;font-style:normal}@font-face{font-family:"social-logos";src:url("/wp-content/mu-plugins/social-logos/social-logos.eot?5d3b4da4f6c2460dd842dbf9e0992ea6");src:url("/wp-content/mu-plugins/social-logos/social-logos.eot?5d3b4da4f6c2460dd842dbf9e0992ea6#iefix") format("embedded-opentype"),url("/wp-content/mu-plugins/social-logos/social-logos.woff") format("woff"),url("/wp-content/mu-plugins/social-logos/social-logos.ttf?5d3b4da4f6c2460dd842dbf9e0992ea6") format("truetype");font-weight:normal;font-style:normal;font-display:swap}.jp-related-posts-i2__row{display:flex;margin-top:1.5rem}.jp-related-posts-i2__row:first-child{margin-top:0}.jp-related-posts-i2__post{flex-grow:1;flex-basis:0;margin:0 10px;display:flex;flex-direction:column;padding-left:0}.jp-related-posts-i2__row[data-post-count="3"] .jp-related-posts-i2__post{max-width:calc(33% - 20px)}.jp-related-posts-i2__row[data-post-count="2"] .jp-related-posts-i2__post,.jp-related-posts-i2__row[data-post-count="1"] .jp-related-posts-i2__post{max-width:calc(50% - 20px)}.jp-related-posts-i2__post-date,.jp-related-posts-i2__post-context{flex-direction:row;display:block}.jp-related-posts-i2__post-link{display:block;width:100%;line-height:1.2em}.jp-relatedposts-i2[data-layout="list"] .jp-related-posts-i2__row{margin-top:0;display:block}.jp-relatedposts-i2[data-layout="list"] .jp-related-posts-i2__post{max-width:none;margin:0}@media only screen and (max-width: 640px){.jp-related-posts-i2__row{margin-top:0;display:block}.jp-related-posts-i2__row[data-post-count] .jp-related-posts-i2__post{max-width:none;margin:0;margin-top:1rem}}

/*# sourceURL=amp-custom.css */</style><link rel="canonical" href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/"><script type="application/ld+json">{"@context":"http:\/\/schema.org","publisher":{"@type":"Organization","name":"Azimuth","logo":{"@type":"ImageObject","url":"https:\/\/s2.wp.com\/wp-content\/plugins\/amp-2.0\/assets\/images\/amp-page-fallback-wordpress-publisher-logo.png"}},"@type":"BlogPosting","mainEntityOfPage":"https:\/\/johncarlosbaez.wordpress.com\/2011\/03\/25\/this-weeks-finds-week-313\/","headline":"This Week&#8217;s Finds (Week 313)","datePublished":"2011-03-25T01:16:10+00:00","dateModified":"2011-04-24T07:41:25+00:00","author":{"@type":"Person","name":"John Baez"},"image":{"@type":"ImageObject","url":"https:\/\/0.gravatar.com\/avatar\/34784534843022b3541c8ddd693718cb?s=200&amp;d=identicon&amp;r=G","width":200,"height":200}}</script></head>

<body class="">

<header id="top" class="amp-wp-header">
	<div>
		<a href="https://johncarlosbaez.wordpress.com/">
									<span class="amp-site-title">
				Azimuth			</span>
		</a>
	</div>
</header>

<article class="amp-wp-article">
	<header class="amp-wp-article-header">
		<h1 class="amp-wp-title">This Week’s Finds (Week 313)</h1>
			<div class="amp-wp-meta amp-wp-byline">
					<amp-img src="https://secure.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=24&amp;d=identicon&amp;r=g" alt="John Baez" width="24" height="24" layout="fixed" class="i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:24px;height:24px;" i-amphtml-layout="fixed"></amp-img>
				<span class="amp-wp-author author vcard">John Baez</span>
	</div>
<div class="amp-wp-meta amp-wp-posted-on">
	<time datetime="2011-03-25T01:16:10+00:00">
		11 years ago	</time>
</div>
	</header>

	
	<div class="amp-wp-article-content">
		<p>Here’s the third and final part of my interview with <a href="http://yudkowsky.net/">Eliezer Yudkowsky</a>.  We’ll talk about three big questions… roughly these:</p>
<p> • How do you get people to work on potentially risky projects in a safe way?</p>
<p> • Do we understand ethics well enough to build <a href="http://en.wikipedia.org/wiki/Friendly_artificial_intelligence">“Friendly artificial intelligence”</a>?</p>
<p> • What’s better to work on, artificial intelligence or environmental issues?</p>
<p>So, with no further ado:</p>
<p><b>JB</b>:  There are decent Wikipedia articles on <a href="http://en.wikipedia.org/wiki/Optimism_bias">“optimism bias”</a> and <a href="http://en.wikipedia.org/wiki/Positive_illusions">“positive illusions”</a>, which suggest that unrealistically optimistic people are more energetic, while more realistic estimates of success go hand-in-hand with mild depression. If this is true, I can easily imagine that most people working on challenging projects like quantum gravity (me, 10 years ago) or artificial intelligence (you) are unrealistically optimistic about our chances of success.</p>
<p>Indeed, I can easily imagine that the first researchers to create a truly powerful artificial intelligence will be people who underestimate its potential dangers.  It’s an interesting irony, isn’t it?  If most people who are naturally cautious avoid a certain potentially dangerous line of research, the people who pursue that line of research are likely to be less cautious than average.</p>
<p>I’m a bit worried about this when it comes to <a href="http://en.wikipedia.org/wiki/Geoengineering">“geoengineering”</a>, for example—attempts to tackle global warming by large engineering projects.  We have people who say “oh no, that’s too dangerous”, and turn their attention to approaches they consider less risky, but that may leave the field to people who underestimate the risks.</p>
<p>So I’m very glad you are thinking hard about how to avoid the potential dangers of artificial intelligence—and even trying to make this problem sound exciting, to attract ambitious and energetic young people to work on it.  Is that part of your explicit goal?  To make caution and rationality sound sexy?</p>
<p><b>EY</b>:  The really hard part of the problem isn’t getting a few smart people to work on cautious, rational AI.  It’s admittedly a harder problem than it should be, because there’s a whole system out there which is set up to funnel smart young people into all sorts of other things <i>besides</i> cautious rational long-term basic AI research.  But it isn’t the really hard part of the problem.</p>
<p>The scary thing about AI is that I would guess that the <i>first</i> AI to go over some critical threshold of self-improvement takes all the marbles—first mover advantage, winner take all.  The first pile of uranium to have an effective neutron multiplication factor greater than 1, or maybe the first AI smart enough to absorb all the poorly defended processing power on the Internet—there’s actually a number of different thresholds that could provide a critical first-mover advantage.</p>
<p>And it is always going to be fundamentally easier in some sense to go straight all out for AI and not worry about clean designs or stable self-modification or the problem where a near-miss on the value system destroys almost all of the actual value from our perspective.  (E.g., imagine aliens who shared every single term in the human utility function but lacked our notion of boredom.  Their civilization might consist of a single peak experience repeated over and over, which would make their civilization very boring from our perspective, compared to what it might have been.  That is, leaving a single aspect out of the value system can destroy almost all of the value.  So there’s a very large gap in the AI problem between trying to get the value system exactly right, versus throwing something at it that sounds vaguely good.)</p>
<p>You want to keep as much of an advantage as possible for the cautious rational AI developers over the crowd that is just gung-ho to solve this super interesting scientific problem and go down in the eternal books of fame.  Now there should in fact be some upper bound on the combination of intelligence, methodological rationality, and deep understanding of the problem which you can possess, and still walk directly into the whirling helicopter blades.  The problem is that it is probably a rather high upper bound.  And you are trying to outrace people who are trying to solve a fundamentally easier wrong problem.  So the question is not attracting people to the field in general, but rather getting the really smart competent people to <i>either</i> work for a cautious project <i>or</i> not go into the field at all.  You aren’t going to stop people from trying to develop AI.  But you can hope to have as many of the really smart people as possible working on cautious projects rather than incautious ones.</p>
<p>So yes, making caution look sexy.  But even more than that, trying to make incautious AI projects look merely stupid.  Not dangerous.  Dangerous is sexy.  As the old proverb goes, most of the damage is done by people who wish to feel themselves important.  Human psychology seems to be such that many ambitious people find it far less scary to think about destroying the world, than to think about never amounting to much of anything at all.  I have met people like this.  In fact all the people I have met who think they are going to win eternal fame through their AI projects have been like this.  The thought of potentially destroying the world is bearable; it confirms their own importance.  The thought of not being able to plow full steam ahead on their incredible amazing AI idea is <i>not</i> bearable; it threatens all their fantasies of wealth and fame.  </p>
<p>Now these people of whom I speak are not top-notch minds, not in the class of the top people in mainstream AI, like say <a href="http://norvig.com/">Peter Norvig</a> (to name someone I’ve had the honor of meeting personally).  And it’s possible that if and when self-improving AI starts to get real top-notch minds working on it, rather than people who were too optimistic about/attached to their amazing bright idea to be scared away by the field of skulls, then these real stars will not fall prey to the same sort of psychological trap.  And then again it is also plausible to me that top-notch minds will fall prey to <i>exactly</i> the same trap, because I have yet to learn from reading history that great scientific geniuses are always sane. </p>
<p>So what I would most like to see would be uniform looks of condescending scorn directed at people who claimed their amazing bright AI idea was going to lead to self-improvement and superintelligence, but who couldn’t mount an adequate defense of how their design would have a goal system stable after a billion sequential self-modifications, or how it would get the value system exactly right instead of mostly right.  In other words, making destroying the world look unprestigious and low-status, instead of leaving it to the default state of sexiness and importance-confirmingness.</p>
<p><b>JB</b>: “Get the value system exactly right”—now this phrase touches on another issue I’ve been wanting to talk about.  How do we know what it means for a value system to be exactly right?  It seems people are even further from agreeing on what it means to be good than on what it means to be rational.  Yet you seem to be suggesting we need to solve this problem before it’s safe to build a self-improving artificial intelligence!</p>
<p>When I was younger I worried a lot about the foundations of ethics.  I decided that you “can’t derive an ought from an is”—do you believe that?  If so, all logical arguments leading up to the conclusion that “you should do X” must involve an assumption of the form “you should do Y”… and attempts to “derive” ethics are all implicitly circular in some way.  This really bothered the heck out of me: how was I supposed to know what to do?  But of course I kept on doing things while I was worrying about this… and indeed, it was painfully clear that there’s no way out of making decisions: even deciding to “do nothing” or commit suicide counts as a decision.</p>
<p>Later I got more comfortable with the idea that making decisions about what to do needn’t paralyze me any more than making decisions about what is true.  But still, it seems that the business of designing ethical beings is going to provoke huge arguments, if and when we get around to that.</p>
<p>Do you spend as much time thinking about these issues as you do thinking about rationality?  Of course they’re linked….</p>
<p><b>EY</b>:  Well, I probably spend as much time <i>explaining</i> these issues as I do rationality.  There are also an absolutely huge number of pitfalls that people stumble into when they try to think about, as I would put it, <a href="http://en.wikipedia.org/wiki/Friendly_artificial_intelligence">Friendly AI</a>.  Consider how many pitfalls people run into when they try to think about Artificial Intelligence.  Next consider how many pitfalls people run into when they try to think about morality.  Next consider how many pitfalls philosophers run into when they try to think about the nature of morality.  Next consider how many pitfalls people run into when they try to think about hypothetical extremely powerful agents, especially extremely powerful agents that are supposed to be extremely good.  Next consider how many pitfalls people run into when they try to imagine optimal worlds to live in or optimal rules to follow or optimal governments and so on.</p>
<p>Now imagine a subject matter which offers discussants a lovely opportunity to run into all of those pitfalls at the same time.</p>
<p>That’s what happens when you try to talk about Friendly Artificial Intelligence.</p>
<p>And it only takes one error for a chain of reasoning to end up in Outer Mongolia.  So one of the great motivating factors behind all the writing I did on rationality and all the <a href="http://wiki.lesswrong.com/wiki/Sequences">sequences</a> I wrote on <i><a href="http://lesswrong.com/">Less Wrong</a></i> was to actually make it possible, via two years worth of writing and probably something like a month’s worth of reading at least, to immunize people against <i>all</i> the usual mistakes.</p>
<p>Lest I appear to dodge the question entirely, I’ll try for very quick descriptions and google keywords that professional moral philosophers might recognize.</p>
<p>In terms of what I would advocate programming a very powerful AI to actually do, the keywords are <a href="#Mature%20Folk%20Morality">“mature folk morality”</a> and <a href="http://en.wikipedia.org/wiki/Reflective_equilibrium">“reflective equilibrium”</a>.  This means that you build a sufficiently powerful AI to do, not what people say they want, or even what people actually want, but what people <i>would</i> decide they wanted the AI to do, if they had all of the AI’s information, could think about for as long a subjective time as the AI, knew as much as the AI did about the real factors at work in their own psychology, and had no failures of self-control.</p>
<p>There’s a lot of important reasons why you would want to do exactly that and not, say, implement Asimov’s <a href="http://en.wikipedia.org/wiki/Three_Laws_of_Robotics">Three Laws of Robotics</a> (a purely fictional device, and if Asimov had depicted them as working well, he would have had no stories to write) or building a superpowerful AI which obeys people’s commands interpreted in literal English, or creating a god whose sole prime directive is to make people maximally happy, or any of the above plus a list of six different patches which guarantee that nothing can possibly go wrong, and various other things that seem like incredibly obvious failure scenarios but which I assure you I have heard seriously advocated over and over and over again.</p>
<p>In a nutshell, you want to use concepts like “mature folk morality” or “reflective equilibrium” because these are as close as moral philosophy has ever gotten to defining in concrete, computable terms what you could be wrong <i>about</i> when you order an AI to do the wrong thing.</p>
<p>For an attempt at nontechnical explanation of what one might want to program an AI to do and why, the best resource I can offer is an old essay of mine which is not written so as to offer good google keywords, but holds up fairly well nonetheless:</p>
<p>• Eliezer Yudkowsky, <a href="http://singinst.org/upload/CEV.html">Coherent extrapolated volition</a>, May 2004.</p>
<p>You also raised some questions about <a href="http://en.wikipedia.org/wiki/Meta-ethics">metaethics</a>, where metaethics asks not “Which acts are moral?” but “What is the subject matter of our talk about ‘morality’?” i.e. “What are we talking <i>about</i> here anyway?”  In terms of Google keywords, my brand of metaethics is closest to <a href="http://www.google.com/search?q=analytic+descriptivism">analytic descriptivism</a> or <a href="http://www.google.com/search?q=moral+functionalism">moral functionalism</a>.  If I were to try to put that into a very brief nutshell, it would be something like “When we talk about ‘morality’ or ‘goodness’ or ‘right’, the subject matter we’re talking <i>about</i> is a sort of gigantic math question hidden under the simple word ‘right’, a math question that includes all of our emotions and all of what we use to process moral arguments and all the things we might want to change about ourselves if we could see our own source code and know what we were really thinking.”</p>
<p>The complete Less Wrong sequence on metaethics (with many dependencies to earlier ones) is:</p>
<p>• Eliezer Yudkowsky, <a href="http://wiki.lesswrong.com/wiki/Metaethics_sequence">Metaethics sequence</a>, <i>Less Wrong</i>, 20 June to 22 August 2008.</p>
<p>And one of the better quick summaries is at:</p>
<p>• Eliezer Yudkowsky, <a href="http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/">Inseparably right; or, joy in the merely good</a>, <i>Less Wrong</i>, 9 August 2008.</p>
<p>And if I am wise I shall not say any more.</p>
<p><b>JB</b>: I’ll help you be wise.  There are a hundred followup questions I’m tempted to ask, but this has been a long and grueling interview, so I won’t.  Instead, I’d like to raise one last big question.  It’s about time scales.</p>
<p>Self-improving artificial intelligence seems like a real possibility to me.  But when?  You see, I believe we’re in the midst of a global ecological crisis—a mass extinction event, whose effects will be painfully evident by the end of the century.  I want to do something about it.  I can’t do much, but I want to do <i>something</i>.  Even if we’re doomed to disaster, there are different sizes of disaster.  And if we’re going through a kind of bottleneck, where some species make it through and others go extinct, even small actions now can make a difference.</p>
<p>I can imagine some technological optimists—singularitarians, extropians and the like—saying: “Don’t worry, things will get better.  Things that seem hard now will only get easier.  We’ll be able to suck carbon dioxide from the atmosphere using nanotechnology, and revive species starting from their DNA.”  Or maybe even: “Don’t worry: we won’t miss those species.  We’ll be having too much fun doing things we can’t even conceive of now.”</p>
<p>But various things make me skeptical of such optimism.  One of them is the question of time scales. What if the world goes to hell before our technology saves us? What if artificial intelligence comes along toolate to make a big impact on the short-term problems I’m worrying about?   In that case, maybe I should focus on short-term solutions.</p>
<p>Just to be clear: this isn’t some veiled attack on your priorities.  I’m just trying to decide on my own.  One good thing about having billions of people on the planet is that we don’t all have to do the same thing.  Indeed, a multi-pronged approach is best.  But for my own decisions, I want some rough guess about how long various potentially revolutionary technologies will take to come online.</p>
<p>What do you think about all this?</p>
<p><b>EY</b>:  I’ll try to answer the question about timescales, but first let me explain in some detail why I don’t think the decision should be dominated by that question.</p>
<p>If you look up “<a href="http://lesswrong.com/lw/hw/scope_insensitivity/">Scope Insensitivity</a>” on <i>Less Wrong</i>, you’ll see that when three different groups of subjects were asked how much they would pay in increased taxes to save 2,000 / 20,000 / 200,000 birds from drowning in uncovered oil ponds, the respective average answers were $80 / $78 / $88.  People asked questions like this visualize one bird, wings slicked with oil, struggling to escape, and that creates some amount of emotional affect which determines willingness to pay, and the quantity gets tossed out the window since no one can visualize 200,000 of anything.  Another hypothesis to explain the data is “purchase of moral satisfaction”, which says that people give enough money to create a “warm glow” inside themselves, and the amount required might have something to do with your personal financial situation, but it has nothing to do with birds.  Similarly, residents of four US states were only willing to pay 22% more to protect all 57 wilderness areas in those states than to protect one area.  The result I found most horrifying was that subjects were willing to contribute more when a set amount of money was needed to save one child’s life, compared to the same amount of money saving eight lives—because, of course, focusing your attention on a single person makes the feelings stronger, less diffuse.</p>
<p>So while it may make sense to <i>enjoy</i> the warm glow of doing good deeds <i>after</i> we do them, we cannot possibly allow ourselves to choose between altruistic causes based on the relative amounts of warm glow they generate, because our intuitions are quantitatively insane.</p>
<p>And two antidotes that absolutely must be applied in choosing between altruistic causes are conscious appreciation of scope and conscious appreciation of marginal impact.</p>
<p>By its nature, your brain flushes right out the window the all-important distinction between saving one life and saving a million lives.  You’ve got to compensate for that using conscious, verbal deliberation.  The Society For Curing Rare Diseases in Cute Puppies has got great warm glow, but the fact that these diseases are <i>rare</i> should call a screeching halt right there—which you’re going to have to do consciously, not intuitively.  Even before you realize that, contrary to the relative warm glows, it’s really hard to make a moral case for trading off human lives against cute puppies.  I suppose if you could save a billion puppies using one dollar I wouldn’t scream at someone who wanted to spend the dollar on that instead of cancer research.</p>
<p>And similarly, if there are a hundred thousand researchers and billions of dollars annually that are <i>already</i> going into saving species from extinction—because it’s a prestigious and popular cause that has an easy time generating warm glow in lots of potential funders—then you have to ask about the marginal value of putting your effort there, where so many other people are already working, compared to a project that isn’t so popular.</p>
<p>I wouldn’t say “Don’t worry, we won’t miss those species”.  But consider the future intergalactic civilizations growing out of Earth-originating intelligent life.  Consider the whole history of a universe which contains this world of Earth and this present century, and also billions of years of future intergalactic civilization continuing until the universe dies, or maybe forever if we can think of some ingenious way to carry on.  Next consider the interval in utility between a universe-history in which Earth-originating intelligence survived and thrived and managed to save 95% of the non-primate biological species now alive, versus a universe-history in which only 80% of those species are alive.  That utility interval is not very large compared to the utility interval between a universe in which intelligent life thrived and intelligent life died out.  Or the utility interval between a universe-history filled with sentient beings who experience happiness and have empathy for each other and get bored when they do the same thing too many times, versus a universe-history that grew out of various failures of Friendly AI.</p>
<p>(The really scary thing about universes that grow out of a loss of human value is not that they are different, but that they are, from our standpoint, boring.  The human utility function says that once you’ve made a piece of art, it’s <i>more</i> fun to make a <i>different</i> piece of art next time.  But that’s just us.  Most random utility functions will yield instrumental strategies that spend some of their time and resources exploring for the patterns with the highest utility at the beginning of the problem, and then use the rest of their resources to implement the pattern with the highest utility, over and over and over.  This sort of thing will surprise a human who expects, on some deep level, that all minds are made out of human parts, and who thinks, “Won’t the AI see that its utility function is boring?” But the AI is not a little spirit that looks over its code and decides whether to obey it; the AI <i>is</i> the code.  If the code doesn’t say to get bored, it won’t get bored.  A strategy of exploration followed by exploitation is implicit in most utility functions, but boredom is not.  If your utility function does not already contain a term for boredom, then you don’t care; it’s not something that emerges as an instrumental value from most terminal values.  For more on this see: “<a href="http://lesswrong.com/lw/xr/in_praise_of_boredom/">In Praise of Boredom</a>” in the <a href="http://lesswrong.com/lw/xy/the_fun_theory_sequence/">Fun Theory Sequence</a> on <i>Less Wrong</i>.)</p>
<p>Anyway:  In terms of expected utility maximization, even <i>large</i> probabilities of jumping the interval between a universe-history in which 95% of existing biological species survive Earth’s 21st century, versus a universe-history where 80% of species survive, are just about impossible to trade off against <i>tiny</i> probabilities of jumping the interval between interesting universe-histories, versus boring ones where intelligent life goes extinct, or the wrong sort of AI self-improves.</p>
<p>I honestly don’t see how a rationalist can avoid this conclusion:  At this absolutely critical hinge in the history of the universe—Earth in the 21st century—rational altruists should devote their marginal attentions to risks that threaten to terminate intelligent life or permanently destroy a part of its potential.  Those problems, which <a href="http://www.nickbostrom.com/">Nick Bostrom</a> named “<a href="http://www.nickbostrom.com/existential/risks.html">existential risks</a>“, have got <i>all</i> the scope.  And when it comes to marginal impact, there are major risks outstanding that practically no one is working on.  Once you get the stakes on a gut level it’s hard to see how doing anything else could be <i>sane</i>.</p>
<p>So how do you go about protecting the future of intelligent life?  Environmentalism?  After all, there are environmental catastrophes that could knock over our civilization… but then if you want to put the whole universe at stake, it’s not enough for one civilization to topple, you have to argue that our civilization is above average in its chances of building a positive galactic future compared to whatever civilization would rise again a century or two later.  Maybe if there were ten people working on environmentalism and millions of people working on Friendly AI, I could see sending the next marginal dollar to environmentalism.  But with millions of people working on environmentalism, and major existential risks that are completely ignored… if you add a marginal resource that can, rarely, be steered by expected utilities instead of warm glows, devoting that resource to environmentalism <i>does not make sense</i>.</p>
<p>Similarly with other short-term problems.  Unless they’re little-known and unpopular problems, the marginal impact is not going to make sense, because millions of other people will already be working on them.  And even if you argue that some short-term problem leverages existential risk, it’s not going to be perfect leverage and some quantitative discount will apply, probably a large one.  I would be suspicious that the decision to work on a short-term problem was driven by warm glow, status drives, or simple conventionalism.</p>
<p>With that said, there’s also such a thing as <a href="http://en.wikipedia.org/wiki/Comparative_advantage">comparative advantage</a>—the old puzzle of the lawyer who works an hour in the soup clinic instead of working an extra hour as a lawyer and donating the money. Personally I’d say you can work an hour in the soup clinic to keep yourself going if you like, but you should <i>also</i> be working extra lawyer-hours and donating the money to the soup clinic, or better yet, to something with more scope.  (See <a href="http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/">“Purchase Fuzzies and Utilons Separately”</a> on <i>Less Wrong</i>.)  Most people can’t work effectively on Artificial Intelligence (some would question if <i>anyone</i> can, but at the very least it’s not an easy problem).  But there’s a variety of existential risks to choose from, plus a general background job of spreading sufficiently high-grade rationality and existential risk awareness.  One really should look over those before going into something short-term and conventional.  Unless your master plan is just to work the extra hours and donate them to the cause with the highest marginal expected utility per dollar, which is perfectly respectable.</p>
<p>Where should you go in life?  I don’t know exactly, but I think I’ll go ahead and say “not environmentalism”.  There’s just no way that the product of scope, marginal impact, and John Baez’s comparative advantage is going to end up being maximal at that point.</p>
<p>Which brings me to AI timescales.</p>
<p>If I knew exactly how to make a Friendly AI, and I knew exactly how many people I had available to do it, I still couldn’t tell you how long it would take because of Product Management Chaos.</p>
<p>As it stands, this is a basic research problem—which will always <i>feel</i> very hard, because we don’t understand it, and that means when our brain checks for solutions, we don’t see any solutions available. But this ignorance is not to be confused with the positive knowledge that the problem will take a long time to solve once we know how to solve it.  It could be that some fundamental breakthrough will dissolve our confusion and then things will look relatively easy.  Or it could be that some fundamental breakthrough will be followed by the realization that, now that we <i>know</i> what to do, it’s going to take at<br>
least another 20 years to do it.</p>
<p>I seriously have no idea when AI is going to show up, although I’d be genuinely and deeply shocked if it took another century (barring a collapse of civilization in the meanwhile).</p>
<p>If you were to tell me that as a Bayesian I <i>have</i> to put probability distributions on things on pain of having my behavior be inconsistent and inefficient, well, I would actually suspect that my behavior <i>is</i> inconsistent.  But if you were to try and induce from my behavior a median expected time where I spend half my effort planning for less and half my effort planning for more, it would probably look something like 2030.</p>
<p>But that doesn’t really matter to my decisions.  Among all existential risks I know about, Friendly AI has the single largest absolute scope—it affects everything, and the problem <i>must</i> be solved at some point for worthwhile intelligence to thrive.  It also has the largest product of scope of marginal impact, because practically no one is working on it, even compared to other existential risks.  And my abilities seem applicable to it.  So I may not like my uncertainty about timescales, but my decisions are not unstable with respect to that uncertainty.</p>
<p><b>JB</b>: Ably argued!  If I think of an interesting reply, I’ll put it in the blog discussion.  Thanks for your time.</p>
<hr>
<p><i>The best way to predict the future is to invent it.</i> – Alan Kay</p>
<div id="jp-post-flair" class="sharedaddy">
<nav class="jp-relatedposts-i2" data-layout="grid"><h3 class="jp-relatedposts-headline"><em>Related</em></h3><div class="jp-related-posts-i2__row" data-post-count="3"><ul id="related-posts-item-614fb8569435b" aria-labelledby="related-posts-item-614fb8569435b-label" class="jp-related-posts-i2__post" role="menuitem"><li class="jp-related-posts-i2__post-link"><a id="related-posts-item-614fb8569435b-label" href="https://johncarlosbaez.wordpress.com/2011/03/07/this-weeks-finds-week-311/">This Week’s Finds (Week 311)</a></li><li class="jp-related-posts-i2__post-date">7 March, 2011</li><li class="jp-related-posts-i2__post-context">In "this week's finds"</li></ul><ul id="related-posts-item-614fb85694386" aria-labelledby="related-posts-item-614fb85694386-label" class="jp-related-posts-i2__post" role="menuitem"><li class="jp-related-posts-i2__post-link"><a id="related-posts-item-614fb85694386-label" href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/">Logic, Probability and Reflection</a></li><li class="jp-related-posts-i2__post-date">26 December, 2013</li><li class="jp-related-posts-i2__post-context">In "mathematics"</li></ul><ul id="related-posts-item-614fb8569439c" aria-labelledby="related-posts-item-614fb8569439c-label" class="jp-related-posts-i2__post" role="menuitem"><li class="jp-related-posts-i2__post-link"><a id="related-posts-item-614fb8569439c-label" href="https://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/">What To Do? (Part 1)</a></li><li class="jp-related-posts-i2__post-date">24 April, 2011</li><li class="jp-related-posts-i2__post-context">In "azimuth"</li></ul></div></nav></div>	</div>

	<footer class="amp-wp-article-footer">
			<div class="amp-wp-meta amp-wp-tax-category">
		Categories: <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>, <a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/" rel="category tag">this week's finds</a>	</div>

		<div class="amp-wp-meta amp-wp-comments-link">
		<a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comments">
			Leave a Comment		</a>
	</div>
	</footer>
</article>

<footer class="amp-wp-footer">
	<div>
		<h2>Azimuth</h2>
		<a href="#top" class="back-to-top">Back to top</a>
	</div>
</footer>


	<amp-pixel src="https://pixel.wp.com/b.gif?rand=RANDOM&amp;host=johncarlosbaez.wordpress.com&amp;ref=DOCUMENT_REFERRER&amp;amp=1&amp;blog=12777403&amp;v=wpcom&amp;tz=0&amp;user_id=0&amp;post=2830&amp;subd=johncarlosbaez" class="i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:1px;height:1px;" i-amphtml-layout="fixed"></amp-pixel>
	<amp-pixel src="https://pixel.wp.com/b.gif?rand=RANDOM&amp;v=wpcom-no-pv&amp;crypt=UE40eW5QN0p8M2Y%2FRE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29%2BSmw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8%2FMkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw%2FSkZ%2BQ3lLcjU%2FNDBvQTY5SFomLHB0U3J1elBvZCZiTkEzXXlsLi1PU25PbltLcyZONi1Lay1ia3RtU0ZEcmxjdDdab2FqP3lnNTdjREcvfi93LG1GJS5WSk1jVm9kc2FWLUphd2lmT115NlovflNqZH5L" class="i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:1px;height:1px;" i-amphtml-layout="fixed"></amp-pixel>
	


</body></html>
