<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: A Characterization of Entropy	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/</link>
	<description></description>
	<lastBuildDate>Tue, 27 Jun 2017 15:29:06 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Martin Taylor		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-64760</link>

		<dc:creator><![CDATA[Martin Taylor]]></dc:creator>
		<pubDate>Sat, 07 Mar 2015 14:03:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-64760</guid>

					<description><![CDATA[Coming from a quite different and quite ancient background, the idea sounds to me to be pretty much Boltzmann&#039;s approach to entropy, but maybe I missed something in the mathematical style, which is unfamiliar to me.

Boltzmann&#039;s idea (translated into Shannon&#039;s terms) was that the world of possibility could be divided into regions within which changes made no difference to the meaning, but between which changes did matter (microstates within macrostates). For Shannon, information is change in entropy, which means a selection among possible macrostates.

As Shannon pointed out in the part of the book on continuous systems, although total entropy is not well specified, information is, provided the coordinate system remains constant. The entropy-information partition can be stated as: information from a message (observation) is what you learn about which macrostate the system being observed is in, entropy is what you don&#039;t know about the system as a whole. The latter can be further subdifided into potential information obtainable by further observation (refinement of the macrostate probability distribution) and the possibly infinite remaining unobservable differences in the total system.

Incidentally, one of the above commenters said that Weaver said entropy had nothing to do with meaning. That&#039;s not what Shannon said. He said that in calculating the capacity of a noisy channel (observation method) one should not consider the meaning, because the meaning of a message (observation) to the receiver (observer) depended entirely on the prior state of the receiver (observer). As does the quantity of information the receiver (observer) could get from the message (observation).

But maybe I have misinterpreted the message in the original post.]]></description>
			<content:encoded><![CDATA[<p>Coming from a quite different and quite ancient background, the idea sounds to me to be pretty much Boltzmann&#8217;s approach to entropy, but maybe I missed something in the mathematical style, which is unfamiliar to me.</p>
<p>Boltzmann&#8217;s idea (translated into Shannon&#8217;s terms) was that the world of possibility could be divided into regions within which changes made no difference to the meaning, but between which changes did matter (microstates within macrostates). For Shannon, information is change in entropy, which means a selection among possible macrostates.</p>
<p>As Shannon pointed out in the part of the book on continuous systems, although total entropy is not well specified, information is, provided the coordinate system remains constant. The entropy-information partition can be stated as: information from a message (observation) is what you learn about which macrostate the system being observed is in, entropy is what you don&#8217;t know about the system as a whole. The latter can be further subdifided into potential information obtainable by further observation (refinement of the macrostate probability distribution) and the possibly infinite remaining unobservable differences in the total system.</p>
<p>Incidentally, one of the above commenters said that Weaver said entropy had nothing to do with meaning. That&#8217;s not what Shannon said. He said that in calculating the capacity of a noisy channel (observation method) one should not consider the meaning, because the meaning of a message (observation) to the receiver (observer) depended entirely on the prior state of the receiver (observer). As does the quantity of information the receiver (observer) could get from the message (observation).</p>
<p>But maybe I have misinterpreted the message in the original post.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Categories in Control &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-36001</link>

		<dc:creator><![CDATA[Categories in Control &#124; Azimuth]]></dc:creator>
		<pubDate>Thu, 06 Feb 2014 07:38:22 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-36001</guid>

					<description><![CDATA[Tobias Fritz, a postdoc at the Perimeter Institute, is working with me on category-theoretic aspects of information theory. We published a paper on entropy with Tom Leinster, and we&#8217;ve got a followup on relative entropy that&#8217;s almost done. I should be working on it right this instant!  But for now, read the series of posts here on Azimuth: Relative Entropy Part 1, Part 2 and Part 3. [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>Tobias Fritz, a postdoc at the Perimeter Institute, is working with me on category-theoretic aspects of information theory. We published a paper on entropy with Tom Leinster, and we&#8217;ve got a followup on relative entropy that&#8217;s almost done. I should be working on it right this instant!  But for now, read the series of posts here on Azimuth: Relative Entropy Part 1, Part 2 and Part 3. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Relative Entropy (Part 1) &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-30857</link>

		<dc:creator><![CDATA[Relative Entropy (Part 1) &#124; Azimuth]]></dc:creator>
		<pubDate>Thu, 20 Jun 2013 05:44:51 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-30857</guid>

					<description><![CDATA[I’m trying to finish off a paper that Tobias Fritz and I have been working on, which gives a category-theoretic (and Bayesian!) characterization of relative entropy.  It&#8217;s a kind of sequel to our paper with Tom Leinster, in which we characterized entropy.]]></description>
			<content:encoded><![CDATA[<p>I’m trying to finish off a paper that Tobias Fritz and I have been working on, which gives a category-theoretic (and Bayesian!) characterization of relative entropy.  It&#8217;s a kind of sequel to our paper with Tom Leinster, in which we characterized entropy.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: More Second Laws of Thermodynamics &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-18737</link>

		<dc:creator><![CDATA[More Second Laws of Thermodynamics &#171; Azimuth]]></dc:creator>
		<pubDate>Fri, 24 Aug 2012 14:14:32 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-18737</guid>

					<description><![CDATA[[...] By the way, Nielsen&#8217;s paper contains another very nice result about majorization! Suppose you have states  and  of a 2-part quantum system. You can trace out one part and get density matrices describing mixed states of the other part, say  and . Then Nielsen shows you can get from  to  using &#8216;local operations and classical communication&#8217; if and only if  majorizes . Note that things are going backwards here compared to how they&#8217;ve been going in the rest of this post: if we can get from  to , then all forms of entropy go down when we go from  to ! This &#8216;anti-second-law&#8217; behavior is confusing at first, but familiar to me by now. [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] By the way, Nielsen&#8217;s paper contains another very nice result about majorization! Suppose you have states  and  of a 2-part quantum system. You can trace out one part and get density matrices describing mixed states of the other part, say  and . Then Nielsen shows you can get from  to  using &#8216;local operations and classical communication&#8217; if and only if  majorizes . Note that things are going backwards here compared to how they&#8217;ve been going in the rest of this post: if we can get from  to , then all forms of entropy go down when we go from  to ! This &#8216;anti-second-law&#8217; behavior is confusing at first, but familiar to me by now. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-10937</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 27 Nov 2011 23:59:38 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-10937</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-10846&quot;&gt;Ady&lt;/a&gt;.

Tobias wrote: 

&lt;blockquote&gt;
In my understanding, what John means by “information” is the amount by which your “missing information” changes when he tells you something about his number. Think of it as the amount of information contained in his message to you.
&lt;/blockquote&gt;

Yes.  It&#039;s very easy to get mixed up about minus signs when studying entropy and information.  I won&#039;t be ultra-precise here because it seems that Ady and you and I all understand and agree about what&#039;s going on.

The entropy of a probability distribution is the expected amount of information you gain when you learn the value of a random variable distributed according to that probability distribution.  

So, it&#039;s the expected amount of information you gain when someone sends you a message telling you the value of that random variable. 

Or, you could say it&#039;s the amount of information you&#039;re &lt;i&gt;missing&lt;/i&gt;, &lt;i&gt;before&lt;/i&gt; you know that random variable.  

Since I&#039;ve thought about this a lot and have become pretty relaxed about it, I don&#039;t mind saying &#039;entropy is information&#039;.  But that can be confusing if you haven&#039;t thought about this subject a lot.  

It&#039;s a bit like how the sign of &#039;work&#039; in physics can get very confusing if you don&#039;t keep track of whether you mean &#039;work done on the system&#039; or &#039;work the system has done&#039;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-10846">Ady</a>.</p>
<p>Tobias wrote: </p>
<blockquote><p>
In my understanding, what John means by “information” is the amount by which your “missing information” changes when he tells you something about his number. Think of it as the amount of information contained in his message to you.
</p></blockquote>
<p>Yes.  It&#8217;s very easy to get mixed up about minus signs when studying entropy and information.  I won&#8217;t be ultra-precise here because it seems that Ady and you and I all understand and agree about what&#8217;s going on.</p>
<p>The entropy of a probability distribution is the expected amount of information you gain when you learn the value of a random variable distributed according to that probability distribution.  </p>
<p>So, it&#8217;s the expected amount of information you gain when someone sends you a message telling you the value of that random variable. </p>
<p>Or, you could say it&#8217;s the amount of information you&#8217;re <i>missing</i>, <i>before</i> you know that random variable.  </p>
<p>Since I&#8217;ve thought about this a lot and have become pretty relaxed about it, I don&#8217;t mind saying &#8216;entropy is information&#8217;.  But that can be confusing if you haven&#8217;t thought about this subject a lot.  </p>
<p>It&#8217;s a bit like how the sign of &#8216;work&#8217; in physics can get very confusing if you don&#8217;t keep track of whether you mean &#8216;work done on the system&#8217; or &#8216;work the system has done&#8217;.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Tobias Fritz		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-10913</link>

		<dc:creator><![CDATA[Tobias Fritz]]></dc:creator>
		<pubDate>Sun, 27 Nov 2011 12:53:43 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-10913</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-10846&quot;&gt;Ady&lt;/a&gt;.

There might be several ways to answer this question. Here&#039;s my take on it.

John wrote:

&lt;blockquote&gt;It makes more intuitive sense if you think of entropy as information, and the function f as some kind of data processing that doesn’t introduce any additional randomness. Such a process can only decrease the amount of information. For example, squaring the number -5 gives the same answer as squaring 5, so if I tell you “this number squared is 25″, I’m giving you less information than if I said “this number is -5″.
&lt;/blockquote&gt;

In my understanding, what John means by &quot;information&quot; is the amount by which your &quot;missing information&quot; changes when he tells you something about his number. Think of it as the amount of information contained in his message to you.

Now consider the following two cases:

If he tells you the number itself, your missing information suddenly decreases from total ignorance of the value of the number to complete knowledge of the number.

On the other hand, if the tells you only the square of the number, your missing information will not decrease to 0, since you won&#039;t know the sign of the number. Your missing information decreases by a smaller amount than in the previous case.

The difference between these two cases is precisely our &quot;information loss&quot;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-10846">Ady</a>.</p>
<p>There might be several ways to answer this question. Here&#8217;s my take on it.</p>
<p>John wrote:</p>
<blockquote><p>It makes more intuitive sense if you think of entropy as information, and the function f as some kind of data processing that doesn’t introduce any additional randomness. Such a process can only decrease the amount of information. For example, squaring the number -5 gives the same answer as squaring 5, so if I tell you “this number squared is 25″, I’m giving you less information than if I said “this number is -5″.
</p></blockquote>
<p>In my understanding, what John means by &#8220;information&#8221; is the amount by which your &#8220;missing information&#8221; changes when he tells you something about his number. Think of it as the amount of information contained in his message to you.</p>
<p>Now consider the following two cases:</p>
<p>If he tells you the number itself, your missing information suddenly decreases from total ignorance of the value of the number to complete knowledge of the number.</p>
<p>On the other hand, if the tells you only the square of the number, your missing information will not decrease to 0, since you won&#8217;t know the sign of the number. Your missing information decreases by a smaller amount than in the previous case.</p>
<p>The difference between these two cases is precisely our &#8220;information loss&#8221;.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Ady		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-10846</link>

		<dc:creator><![CDATA[Ady]]></dc:creator>
		<pubDate>Sat, 26 Nov 2011 14:22:04 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-10846</guid>

					<description><![CDATA[&quot;A process of this sort always decreases the entropy&quot;

If the amount of information is decreased then the missing information is increased, so shouldn&#039;t the entropy have increased? (It&#039;s supposed to be a measure of the missing information - our ignorance about the system).]]></description>
			<content:encoded><![CDATA[<p>&#8220;A process of this sort always decreases the entropy&#8221;</p>
<p>If the amount of information is decreased then the missing information is increased, so shouldn&#8217;t the entropy have increased? (It&#8217;s supposed to be a measure of the missing information &#8211; our ignorance about the system).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Quora		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-6285</link>

		<dc:creator><![CDATA[Quora]]></dc:creator>
		<pubDate>Sat, 11 Jun 2011 06:28:39 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-6285</guid>

					<description><![CDATA[&lt;strong&gt;Why would you multiply a function back by its natural log? Or do something in the form of [x ln(x)] / [y ln(y)]?...&lt;/strong&gt;

One reason would be in order to measure entropy, so x or y has to be a probability. -xlnx is then the entropy. Or, I should say entropy is the negative sum over all your possible x&#039;s of that value. So if the random variable X can equal 0 (tails) or 1 ...]]></description>
			<content:encoded><![CDATA[<p><strong>Why would you multiply a function back by its natural log? Or do something in the form of [x ln(x)] / [y ln(y)]?&#8230;</strong></p>
<p>One reason would be in order to measure entropy, so x or y has to be a probability. -xlnx is then the entropy. Or, I should say entropy is the negative sum over all your possible x&#8217;s of that value. So if the random variable X can equal 0 (tails) or 1 &#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Weekly Picks « Mathblogging.org — the Blog		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-6249</link>

		<dc:creator><![CDATA[Weekly Picks « Mathblogging.org — the Blog]]></dc:creator>
		<pubDate>Wed, 08 Jun 2011 03:06:28 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-6249</guid>

					<description><![CDATA[Last but not least, Azimuth described the experiment of writing a paper on a blog. [...]]]></description>
			<content:encoded><![CDATA[<p>Last but not least, Azimuth described the experiment of writing a paper on a blog. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Don Foster		</title>
		<link>https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comment-6243</link>

		<dc:creator><![CDATA[Don Foster]]></dc:creator>
		<pubDate>Tue, 07 Jun 2011 16:05:38 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=3832#comment-6243</guid>

					<description><![CDATA[Phil, thanks. And yes, I believe Weaver warns that their H theorem does not deal with “meaning” on about the first page of his little book. Perhaps this is equivalent to statistical entropy being a path independent measure. 
The baseball example was meant to show that while the Shannon entropy of the black box signal (the pitcher throwing perfect strikes) recorded low information content, the internal energetic process of the black box, the greater metabolism of the pitcher, was in fact highly negentropic, far from a normal distribution. (This is not saying that it contravenes the second law, clearly a greater portion of energy goes to ground in the process)  At this point I am not sure what that proves, but I am unsatisfied with the notion of an identity between information and entropy.
The H theorem is a measure with a particular angle of incidence that, like a conic section, reveals one aspect of information but not the whole. 
Of course the genus of information theoretic measures is a living testament to evolution, there are new species at every turn. I have a middling grasp of the Shannon’s more fundamental theorems and it is likely that, unbeknownst to me, there is formal recognition that information is related to, but not identical to entropy.
And, having come this far, unfortunately there is more. I regret being both flatfootedly declarative and perhaps ultimately unclear, but for me this is a germinal notion. Perhaps it is the season for a shift of paradigms and perhaps I am actually coming late to this understanding.
Would things sort out more satisfactorily if we shift information upward in the cosmological hierarchy, grant it a more prominent, costarring role in nature’s grand opera? 
I have come to view it in this way. Information is energy’s counterpoise and correspondent constraint. Information keeps energy from running directly off the page, gives it pause and time to doodle things like planets and plants. Path is emergent in the interaction of energy and information and path is the salient feature in life process. And information theoretic measures lend themselves to the understanding of path.
That’s probably more than enough. Regards.]]></description>
			<content:encoded><![CDATA[<p>Phil, thanks. And yes, I believe Weaver warns that their H theorem does not deal with “meaning” on about the first page of his little book. Perhaps this is equivalent to statistical entropy being a path independent measure.<br />
The baseball example was meant to show that while the Shannon entropy of the black box signal (the pitcher throwing perfect strikes) recorded low information content, the internal energetic process of the black box, the greater metabolism of the pitcher, was in fact highly negentropic, far from a normal distribution. (This is not saying that it contravenes the second law, clearly a greater portion of energy goes to ground in the process)  At this point I am not sure what that proves, but I am unsatisfied with the notion of an identity between information and entropy.<br />
The H theorem is a measure with a particular angle of incidence that, like a conic section, reveals one aspect of information but not the whole.<br />
Of course the genus of information theoretic measures is a living testament to evolution, there are new species at every turn. I have a middling grasp of the Shannon’s more fundamental theorems and it is likely that, unbeknownst to me, there is formal recognition that information is related to, but not identical to entropy.<br />
And, having come this far, unfortunately there is more. I regret being both flatfootedly declarative and perhaps ultimately unclear, but for me this is a germinal notion. Perhaps it is the season for a shift of paradigms and perhaps I am actually coming late to this understanding.<br />
Would things sort out more satisfactorily if we shift information upward in the cosmological hierarchy, grant it a more prominent, costarring role in nature’s grand opera?<br />
I have come to view it in this way. Information is energy’s counterpoise and correspondent constraint. Information keeps energy from running directly off the page, gives it pause and time to doodle things like planets and plants. Path is emergent in the interaction of energy and information and path is the salient feature in life process. And information theoretic measures lend themselves to the understanding of path.<br />
That’s probably more than enough. Regards.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
