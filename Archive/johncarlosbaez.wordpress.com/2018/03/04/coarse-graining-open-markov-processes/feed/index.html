<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Coarse-Graining Open Markov Processes	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/</link>
	<description></description>
	<lastBuildDate>Sat, 15 Aug 2020 00:49:10 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Open Systems: A Double Categorical Perspective (Part 1) &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-164231</link>

		<dc:creator><![CDATA[Open Systems: A Double Categorical Perspective (Part 1) &#124; Azimuth]]></dc:creator>
		<pubDate>Sat, 15 Aug 2020 00:49:10 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-164231</guid>

					<description><![CDATA[My student &lt;a href=&quot;https://www.coursicle.com/ucr/professors/Kenny+Courser/&quot; rel=&quot;nofollow ugc&quot;&gt;Kenny Courser&lt;/a&gt;&#039;s thesis has hit the arXiv:

• Kenny Courser, &lt;em&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.02394&quot; rel=&quot;nofollow ugc&quot;&gt;Open Systems: A Double Categorical Perspective&lt;/a&gt;&lt;/em&gt;, Ph.D. thesis, U. C. Riverside, 2020.]]></description>
			<content:encoded><![CDATA[<p>My student <a href="https://www.coursicle.com/ucr/professors/Kenny+Courser/" rel="nofollow ugc">Kenny Courser</a>&#8216;s thesis has hit the arXiv:</p>
<p>• Kenny Courser, <em><a href="https://arxiv.org/abs/2008.02394" rel="nofollow ugc">Open Systems: A Double Categorical Perspective</a></em>, Ph.D. thesis, U. C. Riverside, 2020.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Open Markov Processes &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-162834</link>

		<dc:creator><![CDATA[Open Markov Processes &#124; Azimuth]]></dc:creator>
		<pubDate>Sat, 04 Jul 2020 00:09:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-162834</guid>

					<description><![CDATA[I&#039;m giving a talk on work I did with Kenny Courser.  It&#039;s part of the &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/06/27/act2020-program/&quot;&gt;ACT2020&lt;/a&gt; conference, and it&#039;s happening on 20:40 UTC on Tuesday July 7th, 2020.  (That&#039;s 1:40 pm here in California, just so I don&#039;t forget.)

Like all talks at this conference, you can watch it live &lt;a href=&quot;https://mit.zoom.us/j/7055345747&quot; rel=&quot;nofollow ugc&quot;&gt;on Zoom&lt;/a&gt; or &lt;a href=&quot;https://www.youtube.com/playlist?list=PLCOXjXDLt3pZDHGYOIqtg1m1lLOURjl1Q&quot; rel=&quot;nofollow ugc&quot;&gt;on YouTube&lt;/a&gt;.  It&#039;ll also be recorded, so you can watch it later on YouTube too, somewhere &lt;a href=&quot;https://www.youtube.com/playlist?list=PLCOXjXDLt3pYot9VNdLlZqGajHyZUywdI&quot; rel=&quot;nofollow ugc&quot;&gt;here&lt;/a&gt;.

You can see my slides now, and read some other helpful things:

• &lt;a href=&quot;http://math.ucr.edu/home/baez/open_markov/open_markov.pdf&quot; rel=&quot;nofollow ugc&quot;&gt;Coarse-graining open Markov processes&lt;/a&gt;.

&lt;blockquote&gt;
  &lt;strong&gt;Abstract.&lt;/strong&gt; We illustrate some new paradigms in applied category theory with the example of coarse-graining open Markov processes. Coarse-graining is a standard method of extracting a simpler Markov process from a more complicated one by identifying states. Here we extend coarse-graining to &#039;open&#039; Markov processes: that is, those where probability can flow in or out of certain states called &#039;inputs&#039; and &#039;outputs&#039;. One can build up an ordinary Markov process from smaller open pieces in two basic ways: composition, where we identify the outputs of one open Markov process with the inputs of another, and tensoring, where we set two open Markov processes side by side. These constructions make open Markov processes into the morphisms of a symmetric monoidal category. But we can go further and construct a symmetric monoidal double category where the 2-morphisms include ways of coarse-graining open Markov processes. We can describe the behavior of open Markov processes using double functors out of this double category.
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>I&#8217;m giving a talk on work I did with Kenny Courser.  It&#8217;s part of the <a href="https://johncarlosbaez.wordpress.com/2020/06/27/act2020-program/">ACT2020</a> conference, and it&#8217;s happening on 20:40 UTC on Tuesday July 7th, 2020.  (That&#8217;s 1:40 pm here in California, just so I don&#8217;t forget.)</p>
<p>Like all talks at this conference, you can watch it live <a href="https://mit.zoom.us/j/7055345747" rel="nofollow ugc">on Zoom</a> or <a href="https://www.youtube.com/playlist?list=PLCOXjXDLt3pZDHGYOIqtg1m1lLOURjl1Q" rel="nofollow ugc">on YouTube</a>.  It&#8217;ll also be recorded, so you can watch it later on YouTube too, somewhere <a href="https://www.youtube.com/playlist?list=PLCOXjXDLt3pYot9VNdLlZqGajHyZUywdI" rel="nofollow ugc">here</a>.</p>
<p>You can see my slides now, and read some other helpful things:</p>
<p>• <a href="http://math.ucr.edu/home/baez/open_markov/open_markov.pdf" rel="nofollow ugc">Coarse-graining open Markov processes</a>.</p>
<blockquote><p>
  <strong>Abstract.</strong> We illustrate some new paradigms in applied category theory with the example of coarse-graining open Markov processes. Coarse-graining is a standard method of extracting a simpler Markov process from a more complicated one by identifying states. Here we extend coarse-graining to &#8216;open&#8217; Markov processes: that is, those where probability can flow in or out of certain states called &#8216;inputs&#8217; and &#8216;outputs&#8217;. One can build up an ordinary Markov process from smaller open pieces in two basic ways: composition, where we identify the outputs of one open Markov process with the inputs of another, and tensoring, where we set two open Markov processes side by side. These constructions make open Markov processes into the morphisms of a symmetric monoidal category. But we can go further and construct a symmetric monoidal double category where the 2-morphisms include ways of coarse-graining open Markov processes. We can describe the behavior of open Markov processes using double functors out of this double category.
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Open Petri Nets &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-126729</link>

		<dc:creator><![CDATA[Open Petri Nets &#124; Azimuth]]></dc:creator>
		<pubDate>Wed, 15 Aug 2018 15:24:31 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-126729</guid>

					<description><![CDATA[Double categories were introduced in the 1960s by Charles Ehresmann.  More recently they have found their way into applied mathematics.  They been used to study various things, including open dynamical systems:

&#8226;  Eugene Lerman and David Spivak, &lt;a href=&quot;http://arxiv.org/abs/1602.01017&quot; rel=&quot;nofollow&quot;&gt;An algebra of open continuous time dynamical systems and networks&lt;/a&gt;.

open electrical circuits and chemical reaction networks:

&#8226; Kenny Courser, &lt;a href=&quot;https://arxiv.org/abs/1605.08100&quot; rel=&quot;nofollow&quot;&gt;A bicategory of decorated cospans&lt;/a&gt;, &lt;i&gt;Theory and Applications of Categories&lt;/i&gt; &lt;b&gt;32&lt;/b&gt; (2017), 995&#8211;1027.

open discrete-time Markov chains:

&#8226; Florence Clerc, Harrison Humphrey and P. Panangaden, &lt;a href=&quot;http://www.cs.mcgill.ca/~prakash/Pubs/bicats_final.pdf&quot; rel=&quot;nofollow&quot;&gt;Bicategories of Markov processes&lt;/a&gt;, in &lt;i&gt;Models, Algorithms, Logics and Tools&lt;/i&gt;, Lecture Notes in Computer Science &lt;b&gt;10460&lt;/b&gt;, Springer, Berlin, 2017, pp. 112&#8211;124.

and coarse-graining for open continuous-time Markov chains:

&#8226; John Baez and Kenny Courser, &lt;a href=&quot;https://arxiv.org/abs/1710.11343&quot; rel=&quot;nofollow&quot;&gt;Coarse-graining open Markov processes&lt;/a&gt;.  (Blog article &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/&quot;&gt;here&lt;/a&gt;.)

As noted by Shulman, the easiest way to get a symmetric monoidal bicategory is often to first construct a symmetric monoidal double category:

&#8226; Mike Shulman, &lt;a href=&quot;https://arxiv.org/abs/1004.0993&quot; rel=&quot;nofollow&quot;&gt;Constructing symmetric monoidal bicategories&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>Double categories were introduced in the 1960s by Charles Ehresmann.  More recently they have found their way into applied mathematics.  They been used to study various things, including open dynamical systems:</p>
<p>&bull;  Eugene Lerman and David Spivak, <a href="http://arxiv.org/abs/1602.01017" rel="nofollow">An algebra of open continuous time dynamical systems and networks</a>.</p>
<p>open electrical circuits and chemical reaction networks:</p>
<p>&bull; Kenny Courser, <a href="https://arxiv.org/abs/1605.08100" rel="nofollow">A bicategory of decorated cospans</a>, <i>Theory and Applications of Categories</i> <b>32</b> (2017), 995&ndash;1027.</p>
<p>open discrete-time Markov chains:</p>
<p>&bull; Florence Clerc, Harrison Humphrey and P. Panangaden, <a href="http://www.cs.mcgill.ca/~prakash/Pubs/bicats_final.pdf" rel="nofollow">Bicategories of Markov processes</a>, in <i>Models, Algorithms, Logics and Tools</i>, Lecture Notes in Computer Science <b>10460</b>, Springer, Berlin, 2017, pp. 112&ndash;124.</p>
<p>and coarse-graining for open continuous-time Markov chains:</p>
<p>&bull; John Baez and Kenny Courser, <a href="https://arxiv.org/abs/1710.11343" rel="nofollow">Coarse-graining open Markov processes</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/">here</a>.)</p>
<p>As noted by Shulman, the easiest way to get a symmetric monoidal bicategory is often to first construct a symmetric monoidal double category:</p>
<p>&bull; Mike Shulman, <a href="https://arxiv.org/abs/1004.0993" rel="nofollow">Constructing symmetric monoidal bicategories</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112225</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 22 Mar 2018 22:03:46 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-112225</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112216&quot;&gt;nad&lt;/a&gt;.

Thanks for being so publicly confused about our paper; having worked on it for a year the meaning of the formalism has become obvious to Kenny and me, but you&#039;re reminding me that a few sentences here and there could help explain its meaning.

Lemma 5.3 is about what happens when we apply the 2-functor $latex \blacksquare$, which is &lt;em&gt;black-boxing&lt;/em&gt;, to 2-morphisms in Mark, which are &lt;em&gt;morphisms between open Markov processes&lt;/em&gt;.  The simplest morphisms between open Markov processes are &lt;em&gt;coarse-grainings&lt;/em&gt;.  So, the Lemma says what happens when we black-box a coarse-graining.

The simplest case is when the maps $latex f$ and $latex g$ in the diagram are identity functions.  This means that we&#039;re leaving the input and output states alone.  In this case, Lemma 5.3 says that the relation between input and output probabilities and flows in steady state are &lt;em&gt;the same&lt;/em&gt; for the original open Markov process and the coarse-grained open Markov process.

I thought that might be what you were trying to say here:

&lt;blockquote&gt;
  I could have imagined e.g. that an internal lumpability might for example not change the relations between input and outputs of a Markov network.
&lt;/blockquote&gt;

This is true if by &quot;the relations between inputs and outputs&quot; you mean &quot;the relation between input and output probabilities and flows in steady state&quot;.   This relation is what black-boxing gives us.

The double category formalism, with its supposedly cryptic diagrams, is just a way of drawing open Markov processes and the morphisms between them, e.g. coarse-grainings.  With this style of picture, the coarse-grainings look like rectangles.  The basic operations on coarse-grainings look like ways of sticking together rectangles to get bigger rectangles. The rules these operations obey turn into intuitive rules about sticking together rectangles.

That&#039;s the whole point of double categories.  Double categories are &quot;the algebra of rectangles&quot;, just as categories are &quot;the algebra of arrows&quot;.

We don&#039;t explain this, because this paper was written for people who are comfortable with double categories.  Explaining double categories would double the length of this paper.

I said morphisms between open Markov processes look like rectangles.  But the simplest morphisms between open Markov processes are coarse-grainings, so let me talk just about coarse-grainings.  The two main operations on these are:

1) We can compose coarse-grainings of open Markov processes &quot;horizontally&quot; by attaching the outputs of one to the inputs of another.

2) We can compose coarse-grainings of open Markov processes &quot;vertically&quot; by doing first one and then another: coarse-graining and then coarse-graining some more.

We can stick together two rectangles either horizontally or vertically, and that&#039;s how we draw operations 1) and 2).  Some nontrivial laws governing these operations then drawn as obvious-looking laws about sticking rectangles together.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112216">nad</a>.</p>
<p>Thanks for being so publicly confused about our paper; having worked on it for a year the meaning of the formalism has become obvious to Kenny and me, but you&#8217;re reminding me that a few sentences here and there could help explain its meaning.</p>
<p>Lemma 5.3 is about what happens when we apply the 2-functor <img src="https://s0.wp.com/latex.php?latex=%5Cblacksquare&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;blacksquare" class="latex" />, which is <em>black-boxing</em>, to 2-morphisms in Mark, which are <em>morphisms between open Markov processes</em>.  The simplest morphisms between open Markov processes are <em>coarse-grainings</em>.  So, the Lemma says what happens when we black-box a coarse-graining.</p>
<p>The simplest case is when the maps <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> in the diagram are identity functions.  This means that we&#8217;re leaving the input and output states alone.  In this case, Lemma 5.3 says that the relation between input and output probabilities and flows in steady state are <em>the same</em> for the original open Markov process and the coarse-grained open Markov process.</p>
<p>I thought that might be what you were trying to say here:</p>
<blockquote><p>
  I could have imagined e.g. that an internal lumpability might for example not change the relations between input and outputs of a Markov network.
</p></blockquote>
<p>This is true if by &#8220;the relations between inputs and outputs&#8221; you mean &#8220;the relation between input and output probabilities and flows in steady state&#8221;.   This relation is what black-boxing gives us.</p>
<p>The double category formalism, with its supposedly cryptic diagrams, is just a way of drawing open Markov processes and the morphisms between them, e.g. coarse-grainings.  With this style of picture, the coarse-grainings look like rectangles.  The basic operations on coarse-grainings look like ways of sticking together rectangles to get bigger rectangles. The rules these operations obey turn into intuitive rules about sticking together rectangles.</p>
<p>That&#8217;s the whole point of double categories.  Double categories are &#8220;the algebra of rectangles&#8221;, just as categories are &#8220;the algebra of arrows&#8221;.</p>
<p>We don&#8217;t explain this, because this paper was written for people who are comfortable with double categories.  Explaining double categories would double the length of this paper.</p>
<p>I said morphisms between open Markov processes look like rectangles.  But the simplest morphisms between open Markov processes are coarse-grainings, so let me talk just about coarse-grainings.  The two main operations on these are:</p>
<p>1) We can compose coarse-grainings of open Markov processes &#8220;horizontally&#8221; by attaching the outputs of one to the inputs of another.</p>
<p>2) We can compose coarse-grainings of open Markov processes &#8220;vertically&#8221; by doing first one and then another: coarse-graining and then coarse-graining some more.</p>
<p>We can stick together two rectangles either horizontally or vertically, and that&#8217;s how we draw operations 1) and 2).  Some nontrivial laws governing these operations then drawn as obvious-looking laws about sticking rectangles together.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112216</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Thu, 22 Mar 2018 20:59:20 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-112216</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111889&quot;&gt;Kenny&lt;/a&gt;.

John wrote:

&lt;blockquote&gt;It sounds like you’re talking about Lemma 5.3, which says that coarse-graining is compatible with black-boxing. &lt;/blockquote&gt;

Well, as said I was looking for a remark like that, but I didn&#039;t and unfortunately still don&#039;t see that  &quot;coarse-graining is compatible with black-boxing&quot; is explained in Lemma 5.3:

&lt;blockquote&gt;Lemma 5.3: Given a 2-morphism [DIAGRAM]  in Mark, there exists a unique 2-morphism [DIAGRAM] in LinRel.&lt;/blockquote&gt;

That is even if you tell me that &quot;build the equation $latex p_* H = H&#039; p_*$ into the definition of ‘morphism between open Markov processes&quot; I don&#039;t see why Lemma 5.3. says &quot;coarse-graining is compatible with black-boxing&quot; and in particular what you mean with &quot;compatible.&quot; This seems to be hidden in those cryptic diagrams. But anyways I got now a rough idea of what you were doing here.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111889">Kenny</a>.</p>
<p>John wrote:</p>
<blockquote><p>It sounds like you’re talking about Lemma 5.3, which says that coarse-graining is compatible with black-boxing. </p></blockquote>
<p>Well, as said I was looking for a remark like that, but I didn&#8217;t and unfortunately still don&#8217;t see that  &#8220;coarse-graining is compatible with black-boxing&#8221; is explained in Lemma 5.3:</p>
<blockquote><p>Lemma 5.3: Given a 2-morphism [DIAGRAM]  in Mark, there exists a unique 2-morphism [DIAGRAM] in LinRel.</p></blockquote>
<p>That is even if you tell me that &#8220;build the equation <img src="https://s0.wp.com/latex.php?latex=p_%2A+H+%3D+H%27+p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_* H = H&#039; p_*" class="latex" /> into the definition of ‘morphism between open Markov processes&#8221; I don&#8217;t see why Lemma 5.3. says &#8220;coarse-graining is compatible with black-boxing&#8221; and in particular what you mean with &#8220;compatible.&#8221; This seems to be hidden in those cryptic diagrams. But anyways I got now a rough idea of what you were doing here.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112183</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 22 Mar 2018 16:11:38 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-112183</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112083&quot;&gt;nad&lt;/a&gt;.

Nad wrote:

&lt;blockquote&gt;
  I could have imagined e.g. that an internal lumpability might for example not change the relations between input and outputs of a Markov network, but I haven’t found a remark similar to that.
&lt;/blockquote&gt;

It sounds like you&#039;re talking about Lemma 5.3, which says that coarse-graining is compatible with black-boxing.  You&#039;ll note that the proof uses the equation

$latex p_* H = H&#039; p_*$

In our paper we define &#039;lumpability&#039; in Definition 3.4.   Theorem 3.10 gives 3 equivalent conditions for lumpability.  The best one is condition (ii):

$latex p_* H = H&#039; p_*$

This is clearly equivalent to

$latex p_* \exp(tH) = \exp(tH&#039;) p_*  \textrm{ for all } t \in [0,\infty) $

This condition says that two things are the same:

1) evolving a state in time using our original Markov process with Hamiltonian $latex H$ and then coarse-graining it with $latex p_*$,

2) coarse-graining a state with $latex p_*$ and then evolving it in time using the coarse-grained Markov process with Hamiltonian $latex H&#039;$.

Since this is so nice, we build the equation $latex p_* H = H&#039; p_*$ into the definition of &#039;morphism between open Markov processes&#039; in Definition 3.4.  In the rest of the paper we study these morphisms and never mention the word &#039;lump&#039;.

We need the condition $latex p_* H = H&#039; p_*$ to prove most of the interesting results in the paper.  For example, Lemma 5.3.

The literature on Markov processes talks about lumpability, but they usually use the equivalent condition (iii) as their definition of lumpability.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112083">nad</a>.</p>
<p>Nad wrote:</p>
<blockquote><p>
  I could have imagined e.g. that an internal lumpability might for example not change the relations between input and outputs of a Markov network, but I haven’t found a remark similar to that.
</p></blockquote>
<p>It sounds like you&#8217;re talking about Lemma 5.3, which says that coarse-graining is compatible with black-boxing.  You&#8217;ll note that the proof uses the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_%2A+H+%3D+H%27+p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_* H = H&#039; p_*" class="latex" /></p>
<p>In our paper we define &#8216;lumpability&#8217; in Definition 3.4.   Theorem 3.10 gives 3 equivalent conditions for lumpability.  The best one is condition (ii):</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_%2A+H+%3D+H%27+p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_* H = H&#039; p_*" class="latex" /></p>
<p>This is clearly equivalent to</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_%2A+%5Cexp%28tH%29+%3D+%5Cexp%28tH%27%29+p_%2A++%5Ctextrm%7B+for+all+%7D+t+%5Cin+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_* &#92;exp(tH) = &#92;exp(tH&#039;) p_*  &#92;textrm{ for all } t &#92;in [0,&#92;infty) " class="latex" /></p>
<p>This condition says that two things are the same:</p>
<p>1) evolving a state in time using our original Markov process with Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> and then coarse-graining it with <img src="https://s0.wp.com/latex.php?latex=p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_*" class="latex" />,</p>
<p>2) coarse-graining a state with <img src="https://s0.wp.com/latex.php?latex=p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_*" class="latex" /> and then evolving it in time using the coarse-grained Markov process with Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039;" class="latex" />.</p>
<p>Since this is so nice, we build the equation <img src="https://s0.wp.com/latex.php?latex=p_%2A+H+%3D+H%27+p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_* H = H&#039; p_*" class="latex" /> into the definition of &#8216;morphism between open Markov processes&#8217; in Definition 3.4.  In the rest of the paper we study these morphisms and never mention the word &#8216;lump&#8217;.</p>
<p>We need the condition <img src="https://s0.wp.com/latex.php?latex=p_%2A+H+%3D+H%27+p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_* H = H&#039; p_*" class="latex" /> to prove most of the interesting results in the paper.  For example, Lemma 5.3.</p>
<p>The literature on Markov processes talks about lumpability, but they usually use the equivalent condition (iii) as their definition of lumpability.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-112083</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Thu, 22 Mar 2018 07:26:39 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-112083</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111889&quot;&gt;Kenny&lt;/a&gt;.

Hello Kenny

thanks for replying. You wrote:

&lt;blockquote&gt;Later on, for category theoretic reasons, we decided to only work with Markov processes that were lumpable, since lumpability is precisely the condition needed for the Hamiltonian of the new Markov process to be independent of the choice of stochastic section s. &lt;/blockquote&gt;

Frankly I have problems to see why this is a category theoretic question. In particular I haven&#039;t sofar understood what this lumpability is needed for. I could have imagined e.g. that an internal lumpability might for example not change the relations between input and outputs of a Markov network, but I haven&#039;t found a remark similar to that.

I searched the text for all occasions of the word &quot;lump&quot;, but apart from the reference it didn&#039;t appear in the text after theorem 3.10.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111889">Kenny</a>.</p>
<p>Hello Kenny</p>
<p>thanks for replying. You wrote:</p>
<blockquote><p>Later on, for category theoretic reasons, we decided to only work with Markov processes that were lumpable, since lumpability is precisely the condition needed for the Hamiltonian of the new Markov process to be independent of the choice of stochastic section s. </p></blockquote>
<p>Frankly I have problems to see why this is a category theoretic question. In particular I haven&#8217;t sofar understood what this lumpability is needed for. I could have imagined e.g. that an internal lumpability might for example not change the relations between input and outputs of a Markov network, but I haven&#8217;t found a remark similar to that.</p>
<p>I searched the text for all occasions of the word &#8220;lump&#8221;, but apart from the reference it didn&#8217;t appear in the text after theorem 3.10.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Kenny		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111889</link>

		<dc:creator><![CDATA[Kenny]]></dc:creator>
		<pubDate>Wed, 21 Mar 2018 07:33:11 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-111889</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-110923&quot;&gt;nad&lt;/a&gt;.

Hi Nad,

The image in the tweet was taken from back when we were originally considering more general coarse-grainings of Markov processes. Later on, for category theoretic reasons, we decided to only work with Markov processes that were lumpable, since lumpability is precisely the condition needed for the Hamiltonian of the new Markov process to be independent of the choice of stochastic section s. As you noticed, the image in the tweet isn&#039;t lumpable (at least not in the manner that is being visually suggested) since the rates of the edges going from both of the states w1 and w2 to the state y are not the same. They are in the example in the paper.

It&#039;s also perhaps worth noting that the edge with rate 4 going from the state w2 to the state w1 plays no role in the lumped Markov process, which intuitively makes sense: anything in either of the states w1 or w2 will transition to state y at the same rate in the case where the original Markov process is lumpable, and transitioning between states w1 and w2 will not affect this.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-110923">nad</a>.</p>
<p>Hi Nad,</p>
<p>The image in the tweet was taken from back when we were originally considering more general coarse-grainings of Markov processes. Later on, for category theoretic reasons, we decided to only work with Markov processes that were lumpable, since lumpability is precisely the condition needed for the Hamiltonian of the new Markov process to be independent of the choice of stochastic section s. As you noticed, the image in the tweet isn&#8217;t lumpable (at least not in the manner that is being visually suggested) since the rates of the edges going from both of the states w1 and w2 to the state y are not the same. They are in the example in the paper.</p>
<p>It&#8217;s also perhaps worth noting that the edge with rate 4 going from the state w2 to the state w1 plays no role in the lumped Markov process, which intuitively makes sense: anything in either of the states w1 or w2 will transition to state y at the same rate in the case where the original Markov process is lumpable, and transitioning between states w1 and w2 will not affect this.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111866</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 20 Mar 2018 05:06:33 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-111866</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111849&quot;&gt;nad&lt;/a&gt;.

I doubt Kenny is reading the comments here.  I&#039;ll tell him about these comments.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111849">nad</a>.</p>
<p>I doubt Kenny is reading the comments here.  I&#8217;ll tell him about these comments.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-111849</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Mon, 19 Mar 2018 08:56:19 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=24586#comment-111849</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-110962&quot;&gt;John Baez&lt;/a&gt;.

@John
OK thanks. Now I am more or less sure that I understand what you mean with the notation $latex p_*$. By the way is Kenny reading the comments to this  blog post?
As said I might have a different approach to derive a Hamiltonian from a diagram, but with the approach I sofar used, the Hamiltonian  &lt;a href=&quot;https://twitter.com/johncarlosbaez/status/971836740607475712/photo/1&quot; rel=&quot;nofollow&quot;&gt;as derived with my approach&lt;/a&gt; would not be lumpable:
$latex H = \left( \begin{array}{ccccc}
-5&#038;0&#038;0&#038;0&#038;0\\
5&#038;-11&#038;0&#038;0&#038;0\\
0&#038;8&#038;-9&#038;4&#038;0\\
0&#038;3&#038;0&#038;-10&#038;0\\
0&#038;0&#038;9&#038;6&#038;0 \end{array}\right)$

@Paul
I don&#039;t see what this have to do with stiffness problems, but if 4 gets big then with the above hamiltonian the w to y transition would converge to 9 only for a specific section.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2018/03/04/coarse-graining-open-markov-processes/#comment-110962">John Baez</a>.</p>
<p>@John<br />
OK thanks. Now I am more or less sure that I understand what you mean with the notation <img src="https://s0.wp.com/latex.php?latex=p_%2A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_*" class="latex" />. By the way is Kenny reading the comments to this  blog post?<br />
As said I might have a different approach to derive a Hamiltonian from a diagram, but with the approach I sofar used, the Hamiltonian  <a href="https://twitter.com/johncarlosbaez/status/971836740607475712/photo/1" rel="nofollow">as derived with my approach</a> would not be lumpable:<br />
<img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cleft%28+%5Cbegin%7Barray%7D%7Bccccc%7D+-5%260%260%260%260%5C%5C+5%26-11%260%260%260%5C%5C+0%268%26-9%264%260%5C%5C+0%263%260%26-10%260%5C%5C+0%260%269%266%260+%5Cend%7Barray%7D%5Cright%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;left( &#92;begin{array}{ccccc} -5&amp;0&amp;0&amp;0&amp;0&#92;&#92; 5&amp;-11&amp;0&amp;0&amp;0&#92;&#92; 0&amp;8&amp;-9&amp;4&amp;0&#92;&#92; 0&amp;3&amp;0&amp;-10&amp;0&#92;&#92; 0&amp;0&amp;9&amp;6&amp;0 &#92;end{array}&#92;right)" class="latex" /></p>
<p>@Paul<br />
I don&#8217;t see what this have to do with stiffness problems, but if 4 gets big then with the above hamiltonian the w to y transition would converge to 9 only for a specific section.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
