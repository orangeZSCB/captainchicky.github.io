<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Thermodynamics with Continuous Information Flow	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/</link>
	<description></description>
	<lastBuildDate>Sat, 30 May 2015 16:47:04 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-66945</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 30 May 2015 16:47:04 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-66945</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-66930&quot;&gt;Blake Pollard&lt;/a&gt;.

It will fit into a nice picture---and your job is to figure out as much as possible about this picture and write it up in your thesis, starting today!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-66930">Blake Pollard</a>.</p>
<p>It will fit into a nice picture&#8212;and your job is to figure out as much as possible about this picture and write it up in your thesis, starting today!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Blake Pollard		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-66930</link>

		<dc:creator><![CDATA[Blake Pollard]]></dc:creator>
		<pubDate>Fri, 29 May 2015 21:46:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-66930</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65230&quot;&gt;linasv&lt;/a&gt;.

That is great question/comment! I&#039;m still learning a lot of this stuff myself.  Part of my goal on my dissertation journey is to understand all the different &#039;min/max entropy&#039; and &#039;min/max entropy production&#039; principles and how they are related to one another. Ecologists, molecular biologists, stochastic thermodynamicists, and others provide a slew of interesting examples to look at. Lots of them use diagrams or networks to represent the systems they are studying so hopefully it all fits together as part of a nice picture!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65230">linasv</a>.</p>
<p>That is great question/comment! I&#8217;m still learning a lot of this stuff myself.  Part of my goal on my dissertation journey is to understand all the different &#8216;min/max entropy&#8217; and &#8216;min/max entropy production&#8217; principles and how they are related to one another. Ecologists, molecular biologists, stochastic thermodynamicists, and others provide a slew of interesting examples to look at. Lots of them use diagrams or networks to represent the systems they are studying so hopefully it all fits together as part of a nice picture!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: linasv		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65392</link>

		<dc:creator><![CDATA[linasv]]></dc:creator>
		<pubDate>Mon, 30 Mar 2015 01:50:57 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65392</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65377&quot;&gt;Blake Pollard&lt;/a&gt;.

Thanks, Blake!  I can&#039;t help but see another question: when you ask &quot;what is the time integral of (expression)&quot;, it seems to me that the time-integral will be history-dependent: i.e. it will depend on the specific values that the $latex p_i(t) $ take over time; different paths would seem to yield different values for that integral.   I don&#039;t see how it couldn&#039;t be history-dependent.

So, if I wander into &lt;em&gt;that&lt;/em&gt; rabbit-hole ... to actually obtain $latex S_e $ from that expression, you would have to perform a path integral, summing over all paths.  I&#039;m going to take a wild stab here, at the path integral; it might look like this:

$latex  S_e(T ) =  \int [dp] \int_0^T dt {\dot S_e} (p_i(t)) $

where the $latex  \int [dp] $ says &quot;average over all possible paths that each $latex p_i(t) $ can take&quot;. Although the above does not really look correct; it doesn&#039;t take the canonical form of path integral; the canonical form is exp-trace-log. So really, it should  resemble something like this:

$latex Z =  \int [dp] \exp ( - \int_0^T dt \log (H_{ij} / H_{ji} ) (p_i (t))) $

I probably wrote that wrong. Z is then the partition function, so that $latex S_e = \log Z $, more or less, up to whatever errors I made in writing this all down.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65377">Blake Pollard</a>.</p>
<p>Thanks, Blake!  I can&#8217;t help but see another question: when you ask &#8220;what is the time integral of (expression)&#8221;, it seems to me that the time-integral will be history-dependent: i.e. it will depend on the specific values that the <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) " class="latex" /> take over time; different paths would seem to yield different values for that integral.   I don&#8217;t see how it couldn&#8217;t be history-dependent.</p>
<p>So, if I wander into <em>that</em> rabbit-hole &#8230; to actually obtain <img src="https://s0.wp.com/latex.php?latex=S_e+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_e " class="latex" /> from that expression, you would have to perform a path integral, summing over all paths.  I&#8217;m going to take a wild stab here, at the path integral; it might look like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S_e%28T+%29+%3D++%5Cint+%5Bdp%5D+%5Cint_0%5ET+dt+%7B%5Cdot+S_e%7D+%28p_i%28t%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_e(T ) =  &#92;int [dp] &#92;int_0^T dt {&#92;dot S_e} (p_i(t)) " class="latex" /></p>
<p>where the <img src="https://s0.wp.com/latex.php?latex=%5Cint+%5Bdp%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int [dp] " class="latex" /> says &#8220;average over all possible paths that each <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) " class="latex" /> can take&#8221;. Although the above does not really look correct; it doesn&#8217;t take the canonical form of path integral; the canonical form is exp-trace-log. So really, it should  resemble something like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D++%5Cint+%5Bdp%5D+%5Cexp+%28+-+%5Cint_0%5ET+dt+%5Clog+%28H_%7Bij%7D+%2F+H_%7Bji%7D+%29+%28p_i+%28t%29%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z =  &#92;int [dp] &#92;exp ( - &#92;int_0^T dt &#92;log (H_{ij} / H_{ji} ) (p_i (t))) " class="latex" /></p>
<p>I probably wrote that wrong. Z is then the partition function, so that <img src="https://s0.wp.com/latex.php?latex=S_e+%3D+%5Clog+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_e = &#92;log Z " class="latex" />, more or less, up to whatever errors I made in writing this all down.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Blake Pollard		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65377</link>

		<dc:creator><![CDATA[Blake Pollard]]></dc:creator>
		<pubDate>Sun, 29 Mar 2015 18:35:20 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65377</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65368&quot;&gt;linasv&lt;/a&gt;.

Right. I agree that

$latex \displaystyle{ \frac{dS_e}{dt} }$

has a physical interpretation as rate of change of entropy due to heat flow between the system and the environment, while $latex S_e$ itself has a less clear form and interpretation. Nonetheless you can ask the question what is the time integral of

$latex \sum_{i,j} (H_{ij}p_j - H_{ji}p_i ) \ln \left( \frac{H_{ij} }{H_{ji} } \right)$

and try to make some sense of it. That is all I was doing.

Schnakenberg wrote down the formula for entropy production as one-half the sum of currents

$latex J_{ij} = H_{ij}p_j - H_{ji}p_i $

times the affinities

$latex A_{ij}  = \ln \left( \frac{H_{ij}p_j }{H_{ji} p_i} \right)$

but only part of this quantity is the time derivative of an entropy. He also showed that you can write this using a cycle basis as a sum over cycles instead of summing over all pairs of states.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65368">linasv</a>.</p>
<p>Right. I agree that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_e%7D%7Bdt%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_e}{dt} }" class="latex" /></p>
<p>has a physical interpretation as rate of change of entropy due to heat flow between the system and the environment, while <img src="https://s0.wp.com/latex.php?latex=S_e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_e" class="latex" /> itself has a less clear form and interpretation. Nonetheless you can ask the question what is the time integral of</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%2Cj%7D+%28H_%7Bij%7Dp_j+-+H_%7Bji%7Dp_i+%29+%5Cln+%5Cleft%28+%5Cfrac%7BH_%7Bij%7D+%7D%7BH_%7Bji%7D+%7D+%5Cright%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i,j} (H_{ij}p_j - H_{ji}p_i ) &#92;ln &#92;left( &#92;frac{H_{ij} }{H_{ji} } &#92;right)" class="latex" /></p>
<p>and try to make some sense of it. That is all I was doing.</p>
<p>Schnakenberg wrote down the formula for entropy production as one-half the sum of currents</p>
<p><img src="https://s0.wp.com/latex.php?latex=J_%7Bij%7D+%3D+H_%7Bij%7Dp_j+-+H_%7Bji%7Dp_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="J_{ij} = H_{ij}p_j - H_{ji}p_i " class="latex" /></p>
<p>times the affinities</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D++%3D+%5Cln+%5Cleft%28+%5Cfrac%7BH_%7Bij%7Dp_j+%7D%7BH_%7Bji%7D+p_i%7D+%5Cright%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}  = &#92;ln &#92;left( &#92;frac{H_{ij}p_j }{H_{ji} p_i} &#92;right)" class="latex" /></p>
<p>but only part of this quantity is the time derivative of an entropy. He also showed that you can write this using a cycle basis as a sum over cycles instead of summing over all pairs of states.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: linasv		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65368</link>

		<dc:creator><![CDATA[linasv]]></dc:creator>
		<pubDate>Sun, 29 Mar 2015 06:39:06 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65368</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65362&quot;&gt;Blake S. Pollard&lt;/a&gt;.

Blake, I&#039;m confused by some of your remarks; John and I discussed this, up above, and obtained what I thought was an entirely satisfactory answer: the system as a whole is attached to a heat bath, and detailed balance &lt;strong&gt;with the heat bath&lt;/strong&gt; provides the environment term (and not the detailed balance for Markov chain all by itself). One cannot obtain $latex S_e$ this way, but one can obtain $latex \dot{S}_e$, as John sketches. The arxiv paper says as much, it just didn&#039;t quite say it in a way that I immediately recognized.

Its not really &quot;physical&quot; to try to reconstruct $latex S_e$: it&#039;s a heat bath of some indeterminate size, you can&#039;t know what it&#039;s entropy is; you can only know how it changes.

An interesting generalization to this problem would be to attach X and Y to heat baths of different temperatures.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65362">Blake S. Pollard</a>.</p>
<p>Blake, I&#8217;m confused by some of your remarks; John and I discussed this, up above, and obtained what I thought was an entirely satisfactory answer: the system as a whole is attached to a heat bath, and detailed balance <strong>with the heat bath</strong> provides the environment term (and not the detailed balance for Markov chain all by itself). One cannot obtain <img src="https://s0.wp.com/latex.php?latex=S_e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_e" class="latex" /> this way, but one can obtain <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7BS%7D_e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{S}_e" class="latex" />, as John sketches. The arxiv paper says as much, it just didn&#8217;t quite say it in a way that I immediately recognized.</p>
<p>Its not really &#8220;physical&#8221; to try to reconstruct <img src="https://s0.wp.com/latex.php?latex=S_e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_e" class="latex" />: it&#8217;s a heat bath of some indeterminate size, you can&#8217;t know what it&#8217;s entropy is; you can only know how it changes.</p>
<p>An interesting generalization to this problem would be to attach X and Y to heat baths of different temperatures.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Blake S. Pollard		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65363</link>

		<dc:creator><![CDATA[Blake S. Pollard]]></dc:creator>
		<pubDate>Sun, 29 Mar 2015 01:05:56 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65363</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65146&quot;&gt;Bradley Robinson&lt;/a&gt;.

Thanks for your comments. I&#039;ll have to take a look at the paper you mentioned by Ville Kaila and Arto Annila, sounds like interesting stuff related to the more recent post by Marc Harper here on Azimuth.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65146">Bradley Robinson</a>.</p>
<p>Thanks for your comments. I&#8217;ll have to take a look at the paper you mentioned by Ville Kaila and Arto Annila, sounds like interesting stuff related to the more recent post by Marc Harper here on Azimuth.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Blake S. Pollard		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65362</link>

		<dc:creator><![CDATA[Blake S. Pollard]]></dc:creator>
		<pubDate>Sun, 29 Mar 2015 01:02:36 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65362</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65227&quot;&gt;John Baez&lt;/a&gt;.

Whoops, I can see my first mistake! Most of the sums should be over $latex x \geq x^\prime$ and $latex y \geq y^\prime$ to avoid double counting.

The $latex S_{e}$ term is something that has confused me too. For starters,

$latex \displaystyle{ S_{XY} = \sum_{x,y} p(x,y) \ln ( p(x,y) ) }$

is the Shannon entropy of the joint system, and

$latex \displaystyle{ \frac{dS_{XY} }{dt} = \sum_{x \geq x&#039;,y \geq y&#039;} \left( H_{x,x&#039;}^{y,y&#039;} p(x&#039;,y&#039;) - H_{x&#039;,x}^{y&#039;,y}p(x,y) \right) \ln \left( \frac{p(x&#039;,y&#039;) }{p(x,y)} \right) } $

is its time derivative. The other term, $latex S_{e},$ is a little trickier. I’m not sure who first introduced it, but Schnakenberg already wrote the entropy production with this extra ‘environment’ term.

Your question about $latex S_e$ applies to any Markov process, not just bipartite ones. Therefore let me use the simpler notation where $latex H_{ij}$ denotes the transition rate from $latex j$ to $latex i$ and $latex p_i$ is the probability of being in the $latex i^{\text{th}}$ state. The Shannon entropy is then

$latex S = \sum_i p_i \ln(p_i) $

and

$latex \displaystyle{ \frac{dS}{dt} = \sum_{i,j} \frac{1}{2} ( H_{ij}p_j - H_{ji} p_i ) \ln \left( \frac{p_i}{p_j} \right) }$

To get something of the form

$latex (x-y)\ln \left( \frac{x}{y} \right)$ we need to get the $latex H_{ij}$s inside the logarithm, so we add in the additional

$latex \displaystyle{ \frac{dS_{e} }{dt} = \sum_{i,j} \frac{1}{2} \left( H_{ij} p_j - H_{ji} p_i \right) \; \ln \left( \frac{ H_{ij} }{ H_{ji} } \right) } $

by hand.

If the transition rates are constant, it is the time derivative of

$latex  \sum_{i,j} \frac{1}{2} p_i \ln \left( \frac{H_{ij} } {H_{ji} } \right) $

which looks like one half the expectation value of the ‘skewedness’ of transitioning between $latex j$ and $latex i$. What it doesn’t look like is any entropy I’ve ever seen since it involves the transition rates. If there exists an equilibrium distribution $latex p_{eq}$ satisfying detailed balance, then

$latex \displaystyle{ \ln \left( \frac{ H_{ij} } {H_{ji} } \right) = \ln \left( \frac{p_i}{p_j } \right) } $

Together this implies that the total entropy is

$latex S_{tot} = S + S_e = \displaystyle{ \sum_i p_i \ln(p_i) + \sum_{i,j} \frac{1}{2} p_i \ln \left( \frac{H_{ij} }{H_{ji} } \right) } $

There is probably a better way to write the second term.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65227">John Baez</a>.</p>
<p>Whoops, I can see my first mistake! Most of the sums should be over <img src="https://s0.wp.com/latex.php?latex=x+%5Cgeq+x%5E%5Cprime&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;geq x^&#92;prime" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y+%5Cgeq+y%5E%5Cprime&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;geq y^&#92;prime" class="latex" /> to avoid double counting.</p>
<p>The <img src="https://s0.wp.com/latex.php?latex=S_%7Be%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_{e}" class="latex" /> term is something that has confused me too. For starters,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S_%7BXY%7D+%3D+%5Csum_%7Bx%2Cy%7D+p%28x%2Cy%29+%5Cln+%28+p%28x%2Cy%29+%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S_{XY} = &#92;sum_{x,y} p(x,y) &#92;ln ( p(x,y) ) }" class="latex" /></p>
<p>is the Shannon entropy of the joint system, and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_%7BXY%7D+%7D%7Bdt%7D+%3D+%5Csum_%7Bx+%5Cgeq+x%27%2Cy+%5Cgeq+y%27%7D+%5Cleft%28+H_%7Bx%2Cx%27%7D%5E%7By%2Cy%27%7D+p%28x%27%2Cy%27%29+-+H_%7Bx%27%2Cx%7D%5E%7By%27%2Cy%7Dp%28x%2Cy%29+%5Cright%29+%5Cln+%5Cleft%28+%5Cfrac%7Bp%28x%27%2Cy%27%29+%7D%7Bp%28x%2Cy%29%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_{XY} }{dt} = &#92;sum_{x &#92;geq x&#039;,y &#92;geq y&#039;} &#92;left( H_{x,x&#039;}^{y,y&#039;} p(x&#039;,y&#039;) - H_{x&#039;,x}^{y&#039;,y}p(x,y) &#92;right) &#92;ln &#92;left( &#92;frac{p(x&#039;,y&#039;) }{p(x,y)} &#92;right) } " class="latex" /></p>
<p>is its time derivative. The other term, <img src="https://s0.wp.com/latex.php?latex=S_%7Be%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_{e}," class="latex" /> is a little trickier. I’m not sure who first introduced it, but Schnakenberg already wrote the entropy production with this extra ‘environment’ term.</p>
<p>Your question about <img src="https://s0.wp.com/latex.php?latex=S_e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_e" class="latex" /> applies to any Markov process, not just bipartite ones. Therefore let me use the simpler notation where <img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij}" class="latex" /> denotes the transition rate from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the probability of being in the <img src="https://s0.wp.com/latex.php?latex=i%5E%7B%5Ctext%7Bth%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i^{&#92;text{th}}" class="latex" /> state. The Shannon entropy is then</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+%5Csum_i+p_i+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = &#92;sum_i p_i &#92;ln(p_i) " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS%7D%7Bdt%7D+%3D+%5Csum_%7Bi%2Cj%7D+%5Cfrac%7B1%7D%7B2%7D+%28+H_%7Bij%7Dp_j+-+H_%7Bji%7D+p_i+%29+%5Cln+%5Cleft%28+%5Cfrac%7Bp_i%7D%7Bp_j%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS}{dt} = &#92;sum_{i,j} &#92;frac{1}{2} ( H_{ij}p_j - H_{ji} p_i ) &#92;ln &#92;left( &#92;frac{p_i}{p_j} &#92;right) }" class="latex" /></p>
<p>To get something of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28x-y%29%5Cln+%5Cleft%28+%5Cfrac%7Bx%7D%7By%7D+%5Cright%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x-y)&#92;ln &#92;left( &#92;frac{x}{y} &#92;right)" class="latex" /> we need to get the <img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij}" class="latex" />s inside the logarithm, so we add in the additional</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_%7Be%7D+%7D%7Bdt%7D+%3D+%5Csum_%7Bi%2Cj%7D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+H_%7Bij%7D+p_j+-+H_%7Bji%7D+p_i+%5Cright%29+%5C%3B+%5Cln+%5Cleft%28+%5Cfrac%7B+H_%7Bij%7D+%7D%7B+H_%7Bji%7D+%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_{e} }{dt} = &#92;sum_{i,j} &#92;frac{1}{2} &#92;left( H_{ij} p_j - H_{ji} p_i &#92;right) &#92;; &#92;ln &#92;left( &#92;frac{ H_{ij} }{ H_{ji} } &#92;right) } " class="latex" /></p>
<p>by hand.</p>
<p>If the transition rates are constant, it is the time derivative of</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%2Cj%7D+%5Cfrac%7B1%7D%7B2%7D+p_i+%5Cln+%5Cleft%28+%5Cfrac%7BH_%7Bij%7D+%7D+%7BH_%7Bji%7D+%7D+%5Cright%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i,j} &#92;frac{1}{2} p_i &#92;ln &#92;left( &#92;frac{H_{ij} } {H_{ji} } &#92;right) " class="latex" /></p>
<p>which looks like one half the expectation value of the ‘skewedness’ of transitioning between <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />. What it doesn’t look like is any entropy I’ve ever seen since it involves the transition rates. If there exists an equilibrium distribution <img src="https://s0.wp.com/latex.php?latex=p_%7Beq%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_{eq}" class="latex" /> satisfying detailed balance, then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cln+%5Cleft%28+%5Cfrac%7B+H_%7Bij%7D+%7D+%7BH_%7Bji%7D+%7D+%5Cright%29+%3D+%5Cln+%5Cleft%28+%5Cfrac%7Bp_i%7D%7Bp_j+%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;ln &#92;left( &#92;frac{ H_{ij} } {H_{ji} } &#92;right) = &#92;ln &#92;left( &#92;frac{p_i}{p_j } &#92;right) } " class="latex" /></p>
<p>Together this implies that the total entropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=S_%7Btot%7D+%3D+S+%2B+S_e+%3D+%5Cdisplaystyle%7B+%5Csum_i+p_i+%5Cln%28p_i%29+%2B+%5Csum_%7Bi%2Cj%7D+%5Cfrac%7B1%7D%7B2%7D+p_i+%5Cln+%5Cleft%28+%5Cfrac%7BH_%7Bij%7D+%7D%7BH_%7Bji%7D+%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_{tot} = S + S_e = &#92;displaystyle{ &#92;sum_i p_i &#92;ln(p_i) + &#92;sum_{i,j} &#92;frac{1}{2} p_i &#92;ln &#92;left( &#92;frac{H_{ij} }{H_{ji} } &#92;right) } " class="latex" /></p>
<p>There is probably a better way to write the second term.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: lee bloomquist		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65287</link>

		<dc:creator><![CDATA[lee bloomquist]]></dc:creator>
		<pubDate>Thu, 26 Mar 2015 23:23:48 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65287</guid>

					<description><![CDATA[I may have another example of what the authors of this paper call a &quot;bipartite&quot; system.  It&#039;s in a paper for a physics contest at FXQI.org titled &quot;Simple Math for Questions to Physicists.&quot; There it&#039;s called the &quot;Born Infomorphism.&quot;

http://fqxi.org/community/forum/topic/2420

The rigorous math supporting all the natural talk in this contest paper about &quot;informational flow&quot; is, at its foundations in the references, Category Theory. In these references, Category Theory has been applied by the authors to create Channel Theory.  We read about the &quot;information channels&quot; by virtue of which &quot;information flows&quot; in the world of Channel Theory.

(About Channel Theory-- as stated by the authors of Information Flow: The Logic of Distributed Systems (p 31):  &quot;In a more general setting, these infomorphisms are known in computer science as Chu transformations...So one could look at this book as an application of Chu spaces and Chu transformations to a theory of information.&quot; Chu spaces are a category.)

But in the paper &quot;Thermodynamics with Continuous Information Flow,&quot; the mathematical language is based on Shannon-like formulas and axioms from thermodynamics. This-- is a completely different mathematical language. However, it also supports talk about &quot;information flow.&quot;

Here we have two different kinds of mathematics, talking about the same words--

&quot;information flow.&quot;

Leading perhaps to a question that might be of interest:

Is one of these mathematical languages stronger than the other for talking about information? If so, which is stronger? Is it Channel Theory, as in the references for &quot;Simple Math for Questions to Physicists&quot;? Or is it Shannon&#039;s theory, combined with the laws of thermodynamics in &quot;Thermodynamics with Continuous Information Flow&quot;?

In Channel Theory an inquiry about the comparative strength of two mathematical languages would go something like this (p 31):

&quot;...let us think about the example of number theory considered as a part of set theory. Applying example 2.2, suppose that L1 is the language of arithmetic, with numerals 0 and 1 and additional nonlogical symbols like &#060;,+,x,=, and so on. By the tokens of L1 we mean any structure that satisfies the basic axioms PA of Peano arithmetic; the types are sentences formulated using the above symbols plus standard symbols from logic. Let L2 be the language of set theory, with only \in sign and = as nonlogical symbols. By the tokens of L2 we mean any structure that satisfies the usual axioms ZFC of Zermelo-Fraenkel set theory; again types are sentences formulated in terms of \in ,  =, and the basic symbols of logic.&quot;

&quot;One of the standard themes in any course on set theory is to show how to translate number theory into set theory using the finite von Neumann ordinals. Formally, what is going on is the development of an &quot;interpretation.&quot; One shows how to translate any sentence of number theory into a sentence of set theory.&quot;

&quot;At the level of structures, though, things go the other way. A model of number theory does not determine a unique model of set theory. Indeed, some models of number theory are not parts of any models of set theory at all, because set theory is much stronger than number theory. By contrast, any model of set theory does determine a unique model of number theory. The reversal of directions is quite important.&quot;

Given this example, do you think that the following steps could be a practical approach for finding out whether or not one of the above mathematical languages is &quot;stronger&quot; than the other for talking about information-- as, in the example, set theory is &quot;stronger&quot; than number theory?


If possible, map every equation from (a) &quot;Thermodynamics with Continuous Information Flow&quot; to (b) sentences in Channel Theory and Informationalism, as found in the references to &quot;Simple Math for Questions to Physicists.&quot;
If possible, map the particular model or structure supporting each of the above sentences (b) in Channel Theory and Informationalism to: the model or structure supporting its original equation in (a).


If it is possible to complete the informorphisms or translations from every equation in (a) to sentences in (b), as well as contra-wise the corresponding models-- but impossible the other way around-- then (b) is &quot;stronger&quot; than (a).

Or, it might go the other way. In that case (a) is &quot;stronger&quot; than (b).

Probably would not get a clean answer, of course. In that case, here is another question:

Are these two languages part of a single &quot;information channel&quot;? (p 76)]]></description>
			<content:encoded><![CDATA[<p>I may have another example of what the authors of this paper call a &#8220;bipartite&#8221; system.  It&#8217;s in a paper for a physics contest at FXQI.org titled &#8220;Simple Math for Questions to Physicists.&#8221; There it&#8217;s called the &#8220;Born Infomorphism.&#8221;</p>
<p><a href="http://fqxi.org/community/forum/topic/2420" rel="nofollow ugc">http://fqxi.org/community/forum/topic/2420</a></p>
<p>The rigorous math supporting all the natural talk in this contest paper about &#8220;informational flow&#8221; is, at its foundations in the references, Category Theory. In these references, Category Theory has been applied by the authors to create Channel Theory.  We read about the &#8220;information channels&#8221; by virtue of which &#8220;information flows&#8221; in the world of Channel Theory.</p>
<p>(About Channel Theory&#8211; as stated by the authors of Information Flow: The Logic of Distributed Systems (p 31):  &#8220;In a more general setting, these infomorphisms are known in computer science as Chu transformations&#8230;So one could look at this book as an application of Chu spaces and Chu transformations to a theory of information.&#8221; Chu spaces are a category.)</p>
<p>But in the paper &#8220;Thermodynamics with Continuous Information Flow,&#8221; the mathematical language is based on Shannon-like formulas and axioms from thermodynamics. This&#8211; is a completely different mathematical language. However, it also supports talk about &#8220;information flow.&#8221;</p>
<p>Here we have two different kinds of mathematics, talking about the same words&#8211;</p>
<p>&#8220;information flow.&#8221;</p>
<p>Leading perhaps to a question that might be of interest:</p>
<p>Is one of these mathematical languages stronger than the other for talking about information? If so, which is stronger? Is it Channel Theory, as in the references for &#8220;Simple Math for Questions to Physicists&#8221;? Or is it Shannon&#8217;s theory, combined with the laws of thermodynamics in &#8220;Thermodynamics with Continuous Information Flow&#8221;?</p>
<p>In Channel Theory an inquiry about the comparative strength of two mathematical languages would go something like this (p 31):</p>
<p>&#8220;&#8230;let us think about the example of number theory considered as a part of set theory. Applying example 2.2, suppose that L1 is the language of arithmetic, with numerals 0 and 1 and additional nonlogical symbols like &lt;,+,x,=, and so on. By the tokens of L1 we mean any structure that satisfies the basic axioms PA of Peano arithmetic; the types are sentences formulated using the above symbols plus standard symbols from logic. Let L2 be the language of set theory, with only \in sign and = as nonlogical symbols. By the tokens of L2 we mean any structure that satisfies the usual axioms ZFC of Zermelo-Fraenkel set theory; again types are sentences formulated in terms of \in ,  =, and the basic symbols of logic.&#8221;</p>
<p>&#8220;One of the standard themes in any course on set theory is to show how to translate number theory into set theory using the finite von Neumann ordinals. Formally, what is going on is the development of an &#8220;interpretation.&#8221; One shows how to translate any sentence of number theory into a sentence of set theory.&#8221;</p>
<p>&#8220;At the level of structures, though, things go the other way. A model of number theory does not determine a unique model of set theory. Indeed, some models of number theory are not parts of any models of set theory at all, because set theory is much stronger than number theory. By contrast, any model of set theory does determine a unique model of number theory. The reversal of directions is quite important.&#8221;</p>
<p>Given this example, do you think that the following steps could be a practical approach for finding out whether or not one of the above mathematical languages is &#8220;stronger&#8221; than the other for talking about information&#8211; as, in the example, set theory is &#8220;stronger&#8221; than number theory?</p>
<p>If possible, map every equation from (a) &#8220;Thermodynamics with Continuous Information Flow&#8221; to (b) sentences in Channel Theory and Informationalism, as found in the references to &#8220;Simple Math for Questions to Physicists.&#8221;<br />
If possible, map the particular model or structure supporting each of the above sentences (b) in Channel Theory and Informationalism to: the model or structure supporting its original equation in (a).</p>
<p>If it is possible to complete the informorphisms or translations from every equation in (a) to sentences in (b), as well as contra-wise the corresponding models&#8211; but impossible the other way around&#8211; then (b) is &#8220;stronger&#8221; than (a).</p>
<p>Or, it might go the other way. In that case (a) is &#8220;stronger&#8221; than (b).</p>
<p>Probably would not get a clean answer, of course. In that case, here is another question:</p>
<p>Are these two languages part of a single &#8220;information channel&#8221;? (p 76)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: linasv		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65268</link>

		<dc:creator><![CDATA[linasv]]></dc:creator>
		<pubDate>Thu, 26 Mar 2015 03:10:26 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65268</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65261&quot;&gt;John Baez&lt;/a&gt;.

Thanks again; I have a nasty habit of hitting the &quot;post&quot; button before I have finished thinking (or proof-reading). But of course, &quot;non-equilibrium Bayesian statistics&quot; is more-or-less the theory of Markov chains. Doh.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65261">John Baez</a>.</p>
<p>Thanks again; I have a nasty habit of hitting the &#8220;post&#8221; button before I have finished thinking (or proof-reading). But of course, &#8220;non-equilibrium Bayesian statistics&#8221; is more-or-less the theory of Markov chains. Doh.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: linasv		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65266</link>

		<dc:creator><![CDATA[linasv]]></dc:creator>
		<pubDate>Thu, 26 Mar 2015 03:01:39 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19395#comment-65266</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65261&quot;&gt;John Baez&lt;/a&gt;.

And one minor emphasis/clarification: the $latex p_i $ used in the derivation of the detailed balance formula is the equilibrium distribution, and not a stand-in for $latex p(x,y) $. Since both use the letter p, this can be a source of confusion.  The point being that, at equilibrium, the quantity of &quot;things&quot; transitioning into a state must equal the those leaving.

In writing these words, I just realized that the detailed balance formula looks just like Bayes law, but in disguise. Bayesians would say: P(i&#124;j) is the probability of state i given condition j, so that Bayes law is P(i&#124;j) P(j) = P(j&#124;i)P(i)  Now, of course, a conditional probability is not a transition rate, but it does suggest that, in some strange sense, the master equation describes &quot;non-equilibrium Bayesian statistics&quot;. Hmmm. Curious.  Surely others have noted the resemblance; can one deuce anything wise from it, or is it a gee-whiz thing?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comment-65261">John Baez</a>.</p>
<p>And one minor emphasis/clarification: the <img src="https://s0.wp.com/latex.php?latex=p_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i " class="latex" /> used in the derivation of the detailed balance formula is the equilibrium distribution, and not a stand-in for <img src="https://s0.wp.com/latex.php?latex=p%28x%2Cy%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x,y) " class="latex" />. Since both use the letter p, this can be a source of confusion.  The point being that, at equilibrium, the quantity of &#8220;things&#8221; transitioning into a state must equal the those leaving.</p>
<p>In writing these words, I just realized that the detailed balance formula looks just like Bayes law, but in disguise. Bayesians would say: P(i|j) is the probability of state i given condition j, so that Bayes law is P(i|j) P(j) = P(j|i)P(i)  Now, of course, a conditional probability is not a transition rate, but it does suggest that, in some strange sense, the master equation describes &#8220;non-equilibrium Bayesian statistics&#8221;. Hmmm. Curious.  Surely others have noted the resemblance; can one deuce anything wise from it, or is it a gee-whiz thing?</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
