<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information and Entropy in Biological Systems (Part 7)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/</link>
	<description></description>
	<lastBuildDate>Wed, 08 Jul 2015 13:21:16 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68748</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 08 Jul 2015 13:21:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-68748</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68747&quot;&gt;Berényi Péter&lt;/a&gt;.

I think it&#039;s fairly evident what this slide means.  Clearly natural selection punishes organisms with extremely high heat dissipation (or free energy consumption), but it doesn&#039;t seem to be pushing it down to the Landauer limit.  Maybe in some far future regime where free energy is extremely scarce, this will happen.  But it doesn&#039;t seem to be happening now.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68747">Berényi Péter</a>.</p>
<p>I think it&#8217;s fairly evident what this slide means.  Clearly natural selection punishes organisms with extremely high heat dissipation (or free energy consumption), but it doesn&#8217;t seem to be pushing it down to the Landauer limit.  Maybe in some far future regime where free energy is extremely scarce, this will happen.  But it doesn&#8217;t seem to be happening now.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Berényi Péter		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68747</link>

		<dc:creator><![CDATA[Berényi Péter]]></dc:creator>
		<pubDate>Wed, 08 Jul 2015 12:24:35 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-68747</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68218&quot;&gt;John Baez&lt;/a&gt;.

Okay. In that case what slide 14 is supposed to mean in David Wolpert&#039;s presentation?

&lt;pre&gt;         IMPLICATIONS FOR 
         DESIGN OF BRAINS
• P(xt) a dynamic process outside of a brain;
• Natural selection favors brains that: 
  • (generate vt’s that) predict future of x accurately;
         but ...
  • not generate heat that needs to be dissipated;
  • not require free energy from environment (need to create all that heat)
         
         Natural selection favors brains that:
 1) Accurately predict future (quantified with a fitness function);
 2) Using a prediction program with minimal thermo. cost&lt;/pre&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68218">John Baez</a>.</p>
<p>Okay. In that case what slide 14 is supposed to mean in David Wolpert&#8217;s presentation?</p>
<pre>         IMPLICATIONS FOR 
         DESIGN OF BRAINS
• P(xt) a dynamic process outside of a brain;
• Natural selection favors brains that: 
  • (generate vt’s that) predict future of x accurately;
         but ...
  • not generate heat that needs to be dissipated;
  • not require free energy from environment (need to create all that heat)
         
         Natural selection favors brains that:
 1) Accurately predict future (quantified with a fitness function);
 2) Using a prediction program with minimal thermo. cost</pre>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68229</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 30 Jun 2015 17:23:36 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-68229</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68220&quot;&gt;Theophanes Raptis&lt;/a&gt;.

Landauer&#039;s limit is not an &quot;axiom of nature&quot;, but it is a theorem about quite general physical systems, given suitable assumptions.  Not all theorems are &lt;em&gt;important&lt;/em&gt; in every situation.

I know about membrane computing.  It&#039;s a good idea, but I believe we still need more ideas.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68220">Theophanes Raptis</a>.</p>
<p>Landauer&#8217;s limit is not an &#8220;axiom of nature&#8221;, but it is a theorem about quite general physical systems, given suitable assumptions.  Not all theorems are <em>important</em> in every situation.</p>
<p>I know about membrane computing.  It&#8217;s a good idea, but I believe we still need more ideas.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Theophanes Raptis		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68220</link>

		<dc:creator><![CDATA[Theophanes Raptis]]></dc:creator>
		<pubDate>Tue, 30 Jun 2015 13:46:55 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-68220</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68218&quot;&gt;John Baez&lt;/a&gt;.

Such models already exist...
https://en.wikipedia.org/wiki/Membrane_computing
https://en.wikipedia.org/wiki/X-machine#Analog_X_Machine_.28AXM.29

The true question though is, if this is so for biology why do people in physics try to generalize Landauer as if it was an axiom of nature and why do people neglect other&#039;s works using this as an argument as Dr Motls did in the below post...
http://motls.blogspot.gr/2012/12/exorcising-maxwells-daemons.html]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68218">John Baez</a>.</p>
<p>Such models already exist&#8230;<br />
<a href="https://en.wikipedia.org/wiki/Membrane_computing" rel="nofollow ugc">https://en.wikipedia.org/wiki/Membrane_computing</a><br />
<a href="https://en.wikipedia.org/wiki/X-machine#Analog_X_Machine_.28AXM.29" rel="nofollow ugc">https://en.wikipedia.org/wiki/X-machine#Analog_X_Machine_.28AXM.29</a></p>
<p>The true question though is, if this is so for biology why do people in physics try to generalize Landauer as if it was an axiom of nature and why do people neglect other&#8217;s works using this as an argument as Dr Motls did in the below post&#8230;<br />
<a href="http://motls.blogspot.gr/2012/12/exorcising-maxwells-daemons.html" rel="nofollow ugc">http://motls.blogspot.gr/2012/12/exorcising-maxwells-daemons.html</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68218</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 30 Jun 2015 13:26:36 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-68218</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68047&quot;&gt;Berényi Péter&lt;/a&gt;.

I believe Landauer&#039;s limit is largely irrelevant to biological information processing, because organisms don&#039;t come close to reaching this bound.  It&#039;s nice to know that this bound exists, and as a question of physics it&#039;s important to clarify exactly what this bound says.  But for biology, we need other ideas.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68047">Berényi Péter</a>.</p>
<p>I believe Landauer&#8217;s limit is largely irrelevant to biological information processing, because organisms don&#8217;t come close to reaching this bound.  It&#8217;s nice to know that this bound exists, and as a question of physics it&#8217;s important to clarify exactly what this bound says.  But for biology, we need other ideas.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Berényi Péter		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-68047</link>

		<dc:creator><![CDATA[Berényi Péter]]></dc:creator>
		<pubDate>Fri, 26 Jun 2015 23:17:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-68047</guid>

					<description><![CDATA[With a 25 W power consumption for the human brain, Landauer&#039;s limit gives us 8.4×10²¹ irreversible operations per second at 310 K.

If there was some evolutionary pressure to utilize reversible computing to save power, it translates to orders of magnitude more logical operations in each second.

That&#039;s a million to a billion times more computational capacity than what&#039;s derived from standard neuro-synaptic brain models.

Therefore either Landauer&#039;s limit is irrelevant in biological information processing or there is a layer of extremely powerful high frequency molecular computational architecture below the neuro-synaptic level, completely hidden from observations so far.]]></description>
			<content:encoded><![CDATA[<p>With a 25 W power consumption for the human brain, Landauer&#8217;s limit gives us 8.4×10²¹ irreversible operations per second at 310 K.</p>
<p>If there was some evolutionary pressure to utilize reversible computing to save power, it translates to orders of magnitude more logical operations in each second.</p>
<p>That&#8217;s a million to a billion times more computational capacity than what&#8217;s derived from standard neuro-synaptic brain models.</p>
<p>Therefore either Landauer&#8217;s limit is irrelevant in biological information processing or there is a layer of extremely powerful high frequency molecular computational architecture below the neuro-synaptic level, completely hidden from observations so far.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67376</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 16 Jun 2015 06:57:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-67376</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67258&quot;&gt;nad&lt;/a&gt;.

Nad wrote:

&lt;blockquote&gt;
  So I was already wondering a bit why Matteo had a time-dependent $latex \gamma_{ab}(t)$ but I hadn’t found the time to investigate that.
&lt;/blockquote&gt;

Allowing Markov processes with a time-dependent Hamiltonian---I&#039;ll call $latex \gamma_{ab}$ the &lt;b&gt;Hamiltonian&lt;/b&gt;---allows us to study a wider variety of interesting problems.

For example, we can start with by holding $latex \gamma_{ab}$ constant for some interval of time, start with an equilibrium probability distribution for this $latex \gamma_{ab}$, then &#039;drive the system out of equilibrium&#039; by changing $latex \gamma_{ab}$ for a while, then bring $latex \gamma_{ab}$ back to its original value, and study how entropy changes as the system goes back toward equilibrium.

However, there are also plenty of interesting questions to ask about time-independent Hamiltonians!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67258">nad</a>.</p>
<p>Nad wrote:</p>
<blockquote><p>
  So I was already wondering a bit why Matteo had a time-dependent <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bab%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{ab}(t)" class="latex" /> but I hadn’t found the time to investigate that.
</p></blockquote>
<p>Allowing Markov processes with a time-dependent Hamiltonian&#8212;I&#8217;ll call <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bab%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{ab}" class="latex" /> the <b>Hamiltonian</b>&#8212;allows us to study a wider variety of interesting problems.</p>
<p>For example, we can start with by holding <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bab%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{ab}" class="latex" /> constant for some interval of time, start with an equilibrium probability distribution for this <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bab%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{ab}" class="latex" />, then &#8216;drive the system out of equilibrium&#8217; by changing <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bab%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{ab}" class="latex" /> for a while, then bring <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bab%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{ab}" class="latex" /> back to its original value, and study how entropy changes as the system goes back toward equilibrium.</p>
<p>However, there are also plenty of interesting questions to ask about time-independent Hamiltonians!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67374</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Tue, 16 Jun 2015 06:42:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-67374</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67265&quot;&gt;Blake Pollard&lt;/a&gt;.

John wrote:

&lt;blockquote&gt;Not quite; the correct formula is at the bottom of Matteo’s post.&lt;/blockquote&gt;

OK. right. I wrote:

&lt;blockquote&gt;So there a meaningful probability distribution for the path would be probably the product over the matrix elements corresponding to that chain of states.&lt;/blockquote&gt;

I guess I would have liked to write:

&lt;blockquote&gt;So there a meaningful probability distribution for the path would be probably the product over the matrix elements corresponding to that chain of states times the initial probability&lt;/blockquote&gt;

Thanks for pointing this line out to me. As I said I skipped the proof and so I didn&#039;t see it. Now I understand what Matteo means with jumping. I was also a bit mislead because &lt;a href=&quot;http://math.ucr.edu/home/baez/networks/networks_20.html&quot; rel=&quot;nofollow&quot;&gt;Jake called something master equation &lt;/a&gt; where the time dependence was trivial. So I was already wondering a bit why Matteo had a time-dependent $latex \gamma_{ab}(t)$ but I hadn&#039;t found the time to investigate that.

Yes and you are right with the &quot;not quite&quot;. That is apart from my forgetting of the initial probability this what Matteo writes with respect to  $latex \phi_{a_{j-1}}(\tau_{j-1},\tau_j)$ at the bottom is conceptionally quite similar to what I said above, the $latex \gamma_{a_{j-1},a_j}$ are though giving me headaches, since there the columns don&#039;t sum to one but to zero. Anyways I am again late for work.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67265">Blake Pollard</a>.</p>
<p>John wrote:</p>
<blockquote><p>Not quite; the correct formula is at the bottom of Matteo’s post.</p></blockquote>
<p>OK. right. I wrote:</p>
<blockquote><p>So there a meaningful probability distribution for the path would be probably the product over the matrix elements corresponding to that chain of states.</p></blockquote>
<p>I guess I would have liked to write:</p>
<blockquote><p>So there a meaningful probability distribution for the path would be probably the product over the matrix elements corresponding to that chain of states times the initial probability</p></blockquote>
<p>Thanks for pointing this line out to me. As I said I skipped the proof and so I didn&#8217;t see it. Now I understand what Matteo means with jumping. I was also a bit mislead because <a href="http://math.ucr.edu/home/baez/networks/networks_20.html" rel="nofollow">Jake called something master equation </a> where the time dependence was trivial. So I was already wondering a bit why Matteo had a time-dependent <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bab%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{ab}(t)" class="latex" /> but I hadn&#8217;t found the time to investigate that.</p>
<p>Yes and you are right with the &#8220;not quite&#8221;. That is apart from my forgetting of the initial probability this what Matteo writes with respect to  <img src="https://s0.wp.com/latex.php?latex=%5Cphi_%7Ba_%7Bj-1%7D%7D%28%5Ctau_%7Bj-1%7D%2C%5Ctau_j%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi_{a_{j-1}}(&#92;tau_{j-1},&#92;tau_j)" class="latex" /> at the bottom is conceptionally quite similar to what I said above, the <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Ba_%7Bj-1%7D%2Ca_j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{a_{j-1},a_j}" class="latex" /> are though giving me headaches, since there the columns don&#8217;t sum to one but to zero. Anyways I am again late for work.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67366</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 16 Jun 2015 01:11:34 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-67366</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67354&quot;&gt;nad&lt;/a&gt;.

Nad wrote:

&lt;blockquote&gt;
  Is that what is taken as a probability distributions for the paths?
&lt;/blockquote&gt;

Not quite; the correct formula is at the bottom of Matteo&#039;s post.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67354">nad</a>.</p>
<p>Nad wrote:</p>
<blockquote><p>
  Is that what is taken as a probability distributions for the paths?
</p></blockquote>
<p>Not quite; the correct formula is at the bottom of Matteo&#8217;s post.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67354</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Mon, 15 Jun 2015 17:55:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=19837#comment-67354</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67265&quot;&gt;Blake Pollard&lt;/a&gt;.

&lt;blockquote&gt;
  The relevant paths are constant except at finitely many times $latex \tau_j$; these are “the times $latex \tau_{j}$ at which the transitions $latex a_{j-1} \longrightarrow a_{j} $occur”.
&lt;/blockquote&gt;

I see something that could remotely be interpreted as transitions at different times for the case of a  &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain&quot; rel=&quot;nofollow&quot;&gt;Markov chain&lt;/a&gt;. There you have if I understood correctly the conditional probabilities as transition matrix elements. So there a meaningful probability distribution for the path would be probably  the product over the matrix elements corresponding to that chain of states. Is that what is taken as a probability distributions for the paths?

But Matteo talks about an infinitesimal stochastic matrix, so this is a continuous process so there things would get a little more messy, but then I haven&#039;t even seen yet where you need the paths for Matteos averaging procedure.

By the way I just noticed that I mistakingly added the self-adjointness property to the infinitesimal stochastic matrix property, &lt;a href=&quot;http://math.ucr.edu/home/baez/networks/networks_20.html&quot; rel=&quot;nofollow&quot;&gt;because I didn&#039;t read the line above the definition&lt;/a&gt;. So H could be unsymmetric and not necessarily symmetric as in my &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67253&quot; rel=&quot;nofollow&quot;&gt;above comment&lt;/a&gt;. That may be important for the skewness, but it doesn&#039;t change the questions and the arguments I had in my last comment, in particular I unfortunately still don&#039;t understand those &quot;jumpy&quot; transitions.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67265">Blake Pollard</a>.</p>
<blockquote><p>
  The relevant paths are constant except at finitely many times <img src="https://s0.wp.com/latex.php?latex=%5Ctau_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau_j" class="latex" />; these are “the times <img src="https://s0.wp.com/latex.php?latex=%5Ctau_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau_{j}" class="latex" /> at which the transitions <img src="https://s0.wp.com/latex.php?latex=a_%7Bj-1%7D+%5Clongrightarrow+a_%7Bj%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j-1} &#92;longrightarrow a_{j} " class="latex" />occur”.
</p></blockquote>
<p>I see something that could remotely be interpreted as transitions at different times for the case of a  <a href="https://en.wikipedia.org/wiki/Markov_chain" rel="nofollow">Markov chain</a>. There you have if I understood correctly the conditional probabilities as transition matrix elements. So there a meaningful probability distribution for the path would be probably  the product over the matrix elements corresponding to that chain of states. Is that what is taken as a probability distributions for the paths?</p>
<p>But Matteo talks about an infinitesimal stochastic matrix, so this is a continuous process so there things would get a little more messy, but then I haven&#8217;t even seen yet where you need the paths for Matteos averaging procedure.</p>
<p>By the way I just noticed that I mistakingly added the self-adjointness property to the infinitesimal stochastic matrix property, <a href="http://math.ucr.edu/home/baez/networks/networks_20.html" rel="nofollow">because I didn&#8217;t read the line above the definition</a>. So H could be unsymmetric and not necessarily symmetric as in my <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comment-67253" rel="nofollow">above comment</a>. That may be important for the skewness, but it doesn&#8217;t change the questions and the arguments I had in my last comment, in particular I unfortunately still don&#8217;t understand those &#8220;jumpy&#8221; transitions.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
