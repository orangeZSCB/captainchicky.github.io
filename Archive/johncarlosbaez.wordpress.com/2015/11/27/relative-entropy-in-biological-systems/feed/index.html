<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Relative Entropy in Biological Systems	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/</link>
	<description></description>
	<lastBuildDate>Sat, 06 Feb 2016 03:06:12 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Information Geometry (Part 15) &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-76082</link>

		<dc:creator><![CDATA[Information Geometry (Part 15) &#124; Azimuth]]></dc:creator>
		<pubDate>Mon, 11 Jan 2016 06:00:47 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-76082</guid>

					<description><![CDATA[It&#039;s been a long time since you&#039;ve seen an installment of the &lt;a href=&quot;http://math.ucr.edu/home/baez/information/&quot; rel=&quot;nofollow&quot;&gt;information geometry&lt;/a&gt; series on this blog!  Before I took a long break, I was explaining relative entropy and how it changes in evolutionary games.  Much of what I said is summarized and carried further here:

&#8226; John Baez and Blake Pollard, &lt;a href=&quot;http://arxiv.org/abs/1512.02742&quot; rel=&quot;nofollow&quot;&gt;Relative entropy in biological systems&lt;/a&gt;.  (Blog article &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.)

But now Blake Pollard has a new paper, and I want to talk about that:

&#8226; Blake Pollard, &lt;a href=&quot;http://arxiv.org/abs/1601.00711&quot; rel=&quot;nofollow&quot;&gt;Open Markov processes: A compositional perspective on non-equilibrium steady states in biology&lt;/a&gt;.

I&#039;ll focus on just one aspect: the principle of minimum entropy production.]]></description>
			<content:encoded><![CDATA[<p>It&#8217;s been a long time since you&#8217;ve seen an installment of the <a href="http://math.ucr.edu/home/baez/information/" rel="nofollow">information geometry</a> series on this blog!  Before I took a long break, I was explaining relative entropy and how it changes in evolutionary games.  Much of what I said is summarized and carried further here:</p>
<p>&bull; John Baez and Blake Pollard, <a href="http://arxiv.org/abs/1512.02742" rel="nofollow">Relative entropy in biological systems</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/" rel="nofollow">here</a>.)</p>
<p>But now Blake Pollard has a new paper, and I want to talk about that:</p>
<p>&bull; Blake Pollard, <a href="http://arxiv.org/abs/1601.00711" rel="nofollow">Open Markov processes: A compositional perspective on non-equilibrium steady states in biology</a>.</p>
<p>I&#8217;ll focus on just one aspect: the principle of minimum entropy production.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Glycolysis &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-75841</link>

		<dc:creator><![CDATA[Glycolysis &#124; Azimuth]]></dc:creator>
		<pubDate>Fri, 08 Jan 2016 01:00:46 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-75841</guid>

					<description><![CDATA[[&#8230;]  There&#039;s a lot more to say here, but I just want to add that free energy can also be interpreted as &#039;relative information&#039;, a purely information-theoretic concept.   For an explanation, see Section 4 of this paper:

&#8226; John Baez and Blake Pollard, &lt;a href=&quot;http://arxiv.org/abs/1512.02742&quot; rel=&quot;nofollow&quot;&gt;Relative entropy in biological systems&lt;/a&gt;.  (Blog article &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.)

Since I like abstract generalities, this information-theoretic way of understanding free energy appeals to me.

And of course free energy is &lt;i&gt;useful&lt;/i&gt;, so an organism should care about it---and we should be able to track what an organism actually does with it.  This is one of my main goals: understanding better what it means for a system to &#039;do something with free energy&#039;.

In glycolysis, some of the free energy of glucose gets transferred to ATP.  ATP is a bit like &#039;money&#039;: it carries free energy in a way that the cell can easily &#039;spend&#039; to do interesting things.  So, at some point I want to look at an example of how the cell actually spends this money.  But for now I want to think about glycolysis---which may be more like &#039;cashing a check and getting money&#039;. [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;]  There&#8217;s a lot more to say here, but I just want to add that free energy can also be interpreted as &#8216;relative information&#8217;, a purely information-theoretic concept.   For an explanation, see Section 4 of this paper:</p>
<p>&bull; John Baez and Blake Pollard, <a href="http://arxiv.org/abs/1512.02742" rel="nofollow">Relative entropy in biological systems</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/" rel="nofollow">here</a>.)</p>
<p>Since I like abstract generalities, this information-theoretic way of understanding free energy appeals to me.</p>
<p>And of course free energy is <i>useful</i>, so an organism should care about it&#8212;and we should be able to track what an organism actually does with it.  This is one of my main goals: understanding better what it means for a system to &#8216;do something with free energy&#8217;.</p>
<p>In glycolysis, some of the free energy of glucose gets transferred to ATP.  ATP is a bit like &#8216;money&#8217;: it carries free energy in a way that the cell can easily &#8216;spend&#8217; to do interesting things.  So, at some point I want to look at an example of how the cell actually spends this money.  But for now I want to think about glycolysis&#8212;which may be more like &#8216;cashing a check and getting money&#8217;. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-74615</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 11 Dec 2015 00:09:54 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-74615</guid>

					<description><![CDATA[We fixed up the paper and put it &lt;a href=&quot;http://arxiv.org/abs/1512.02742&quot; rel=&quot;nofollow&quot;&gt;on the arXiv&lt;/a&gt;, though there are at least a few typos in the current arXiv version which I&#039;ll fix later.  We also submitted the paper to &lt;em&gt;Entropy&lt;/em&gt;.]]></description>
			<content:encoded><![CDATA[<p>We fixed up the paper and put it <a href="http://arxiv.org/abs/1512.02742" rel="nofollow">on the arXiv</a>, though there are at least a few typos in the current arXiv version which I&#8217;ll fix later.  We also submitted the paper to <em>Entropy</em>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: domenico		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-74086</link>

		<dc:creator><![CDATA[domenico]]></dc:creator>
		<pubDate>Tue, 01 Dec 2015 11:47:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-74086</guid>

					<description><![CDATA[It is very interesting.
I am thinking that the Landauer&#039;s principle can be used to prove the entropy increase using heat flow (graph) from a chemical reaction, so that each chemical reaction can give ordered phases, and life=consciousness, for high reduction of the inner energy (and the self-replication like a usual manner to obtain order, in long times).
I am thinking that if the information is a bit string, and if each program is a number of memory unit, then it could be possible to generate computer programs using free Helmholtz free energy like a reservoir of bit change, and the environment like a reservoir of bit change: the graphs coding the flow of the energy like an adaptive program (thermodynamic computer science).
If there exists an upper limit of the entropy (for the third law of thermodynamic), then the Landauer&#039;s principle is not true ever, and there is a quantum restriction: there is an information that is not stored in the thermodynamic system because all the increase of the quantum levels have an energy greater of the Landauer limit (so that a principle in thermodynamic could be a quantum principle).]]></description>
			<content:encoded><![CDATA[<p>It is very interesting.<br />
I am thinking that the Landauer&#8217;s principle can be used to prove the entropy increase using heat flow (graph) from a chemical reaction, so that each chemical reaction can give ordered phases, and life=consciousness, for high reduction of the inner energy (and the self-replication like a usual manner to obtain order, in long times).<br />
I am thinking that if the information is a bit string, and if each program is a number of memory unit, then it could be possible to generate computer programs using free Helmholtz free energy like a reservoir of bit change, and the environment like a reservoir of bit change: the graphs coding the flow of the energy like an adaptive program (thermodynamic computer science).<br />
If there exists an upper limit of the entropy (for the third law of thermodynamic), then the Landauer&#8217;s principle is not true ever, and there is a quantum restriction: there is an information that is not stored in the thermodynamic system because all the increase of the quantum levels have an energy greater of the Landauer limit (so that a principle in thermodynamic could be a quantum principle).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Blake Stacey		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73998</link>

		<dc:creator><![CDATA[Blake Stacey]]></dc:creator>
		<pubDate>Sat, 28 Nov 2015 20:31:15 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-73998</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73996&quot;&gt;Blake Stacey&lt;/a&gt;.

p. 11: &quot;This is a version of the Second Law of Thermodynamics: it says that To prove this, note that&quot;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73996">Blake Stacey</a>.</p>
<p>p. 11: &#8220;This is a version of the Second Law of Thermodynamics: it says that To prove this, note that&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Blake Stacey		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73997</link>

		<dc:creator><![CDATA[Blake Stacey]]></dc:creator>
		<pubDate>Sat, 28 Nov 2015 20:29:30 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-73997</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73996&quot;&gt;Blake Stacey&lt;/a&gt;.

And a couple more on p. 9: &quot;any two probability distribution&quot;

&quot;alwasy infinitesimal stochastic&quot;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73996">Blake Stacey</a>.</p>
<p>And a couple more on p. 9: &#8220;any two probability distribution&#8221;</p>
<p>&#8220;alwasy infinitesimal stochastic&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Blake Stacey		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73996</link>

		<dc:creator><![CDATA[Blake Stacey]]></dc:creator>
		<pubDate>Sat, 28 Nov 2015 20:27:35 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-73996</guid>

					<description><![CDATA[Typo on p. 5: &quot;At this, point Marc Harper&quot;]]></description>
			<content:encoded><![CDATA[<p>Typo on p. 5: &#8220;At this, point Marc Harper&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73950</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 28 Nov 2015 00:22:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-73950</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73948&quot;&gt;Bruce Smith&lt;/a&gt;.

I say a lot about other divergences in Section 6, and I don&#039;t think the introduction (quoted here) is the place to dive into that.   But I&#039;ll just tell you something in there:

The &lt;strong&gt;Jensen--Shannon divergence&lt;/strong&gt; of two probability distributions is defined in terms of relative information by

$latex  \displaystyle{  JS(p\&#124;q) = \frac{I(p\&#124;m) + I(m\&#124;p)}{2}  } $

where $latex m$ is the arithmetic mean of the probability distributions $latex p$ and $latex q$:

$latex   \displaystyle{   m_i = \frac{p_i + q_i}{2} } $

The Jensen--Shannon divergence is obviously symmetric in its arguments.  More interesting is that its square root is actually a metric on the space of probability distributions!  In particular, it obeys the triangle inequality.  Even better, it is nonincreasing whenever $latex p$ and $latex q$ evolve in time via a Markov process.  Without some property like this, a metric on probability distributions is not very interesting to me.

Markov processes are linear.   For nonlinear systems like the replicator equation or the rate equation of a chemical reaction network, it seems hard to find any metric where distances between populations decrease as time passes.  There are however nice theorems about how relative information decreases, as summarized in this introduction.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73948">Bruce Smith</a>.</p>
<p>I say a lot about other divergences in Section 6, and I don&#8217;t think the introduction (quoted here) is the place to dive into that.   But I&#8217;ll just tell you something in there:</p>
<p>The <strong>Jensen&#8211;Shannon divergence</strong> of two probability distributions is defined in terms of relative information by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++JS%28p%5C%7Cq%29+%3D+%5Cfrac%7BI%28p%5C%7Cm%29+%2B+I%28m%5C%7Cp%29%7D%7B2%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  JS(p&#92;|q) = &#92;frac{I(p&#92;|m) + I(m&#92;|p)}{2}  } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m" class="latex" /> is the arithmetic mean of the probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++m_i+%3D+%5Cfrac%7Bp_i+%2B+q_i%7D%7B2%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   m_i = &#92;frac{p_i + q_i}{2} } " class="latex" /></p>
<p>The Jensen&#8211;Shannon divergence is obviously symmetric in its arguments.  More interesting is that its square root is actually a metric on the space of probability distributions!  In particular, it obeys the triangle inequality.  Even better, it is nonincreasing whenever <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> evolve in time via a Markov process.  Without some property like this, a metric on probability distributions is not very interesting to me.</p>
<p>Markov processes are linear.   For nonlinear systems like the replicator equation or the rate equation of a chemical reaction network, it seems hard to find any metric where distances between populations decrease as time passes.  There are however nice theorems about how relative information decreases, as summarized in this introduction.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bruce Smith		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73948</link>

		<dc:creator><![CDATA[Bruce Smith]]></dc:creator>
		<pubDate>Sat, 28 Nov 2015 00:08:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-73948</guid>

					<description><![CDATA[The part I read sounds good (basic idea and first two examples.) I have no suggestion for title.

If 1-2 sentences could do it, you might add whether or not any of the &quot;other divergences&quot; are more like metrics, and if not, what that means, and if so, why they&#039;re not &quot;better&quot;. (If some divergence is symmetric, maybe nonlinearly rescaling it could also fix the triangle inequality -- if not, it would be interesting to understand why.)]]></description>
			<content:encoded><![CDATA[<p>The part I read sounds good (basic idea and first two examples.) I have no suggestion for title.</p>
<p>If 1-2 sentences could do it, you might add whether or not any of the &#8220;other divergences&#8221; are more like metrics, and if not, what that means, and if so, why they&#8217;re not &#8220;better&#8221;. (If some divergence is symmetric, maybe nonlinearly rescaling it could also fix the triangle inequality &#8212; if not, it would be interesting to understand why.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73946</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 27 Nov 2015 23:20:47 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=20189#comment-73946</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73939&quot;&gt;Simon Burton&lt;/a&gt;.

I don&#039;t know about &#039;causal entropic forces&#039;, but you might like this blog article:

&#8226; John Baez, &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/02/01/entropic-forces/&quot; rel=&quot;nofollow&quot;&gt;Entropic forces&lt;/a&gt;, &lt;em&gt;Azimuth&lt;/em&gt;, 1 February 2012.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comment-73939">Simon Burton</a>.</p>
<p>I don&#8217;t know about &#8216;causal entropic forces&#8217;, but you might like this blog article:</p>
<p>&bull; John Baez, <a href="https://johncarlosbaez.wordpress.com/2012/02/01/entropic-forces/" rel="nofollow">Entropic forces</a>, <em>Azimuth</em>, 1 February 2012.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
