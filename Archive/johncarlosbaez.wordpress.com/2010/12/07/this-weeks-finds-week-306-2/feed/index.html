<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: This Week&#8217;s Finds (Week 306)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/</link>
	<description></description>
	<lastBuildDate>Mon, 06 Jun 2011 05:13:55 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3401</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 08 Jan 2011 06:38:51 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3401</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3372&quot;&gt;John Baez&lt;/a&gt;.

Bill wrote:

&lt;blockquote&gt;
I don’t think you or Tim Palmer discussed infinite velocity and vorticity.
&lt;/blockquote&gt;

No.  He probably understands how that&#039;s related to what he said.  I don&#039;t.  

In fact, it&#039;s quite possible that a lot of what I&#039;ve said so far is somewhat inaccurate &#8212; so if you &lt;i&gt;really&lt;/i&gt; want to understand this stuff, take everything I say with a mongo grain of salt!  

I wish someone working on fluid dynamics would join this conversation and help us out.  But barring that, I suggest taking a look at this:

&#8226; Terry Tao, &lt;a href=&quot;http://terrytao.wordpress.com/2007/03/18/why-global-regularity-for-navier-stokes-is-hard/&quot; rel=&quot;nofollow&quot;&gt;Why global regularity for Navier-Stokes is hard&lt;/a&gt;.

You&#039;ll see that he makes crucial use of a certain scale-invariance property of the Navier-Stokes equations.  This is presumably somehow related to the multi-scale analysis I was sketching. 

However, I was talking about how unknown short-scale information causes unpredictability at large distance scales: e.g., how the flap of a butterflies wings today can cause a typhoon next year.  

He, on the other hand, is talking about how energy spread over large distance scales can concentrate down to shorter and shorter distance scales, potentially causing a singularity at some point!   This is what he wants to rule out, to prove global existence for Navier-Stokes:

&lt;blockquote&gt;
In order to prevent blowup, therefore, we must arrest this motion of energy from coarse scales (or low frequencies) to fine scales (or high frequencies).
&lt;/blockquote&gt;

So maybe I&#039;m upside down and backwards &#8212; or quite possibly both effects are very important, but in different ways.  

&lt;blockquote&gt;
Both you &#038; Tim say that when you discretize at a 2&lt;sup&gt;-n&lt;/sup&gt; timescale, you get an ODE.
&lt;/blockquote&gt;

No!  We&#039;re saying that if you discretize at any fixed &lt;i&gt;spatial&lt;/i&gt; distance scale, you get an ODE.  

For example, if you chop a unit cube up into 8&lt;sup&gt;n&lt;/sup&gt; little cubes, and keep track of the average velocity, average pressure and average density in all these little cubes, that&#039;s 5 &#215; 8&lt;sup&gt;n&lt;/sup&gt; variables to keep track of.  Think of all these variables as components of a single vector 

$latex x \in \mathbb{R}^{5 \times 8^n}$

Then, you can discretize the Navier-Stokes equations to obtain a multivariable ODE, sort of like this: 

$latex \frac{d x}{d t} = F(x) $

where &lt;i&gt;F&lt;/i&gt; is some horribly complicated function.

I&#039;m not an expert on this, so I could be making even more mistakes, but I&#039;m pretty sure this is the right general idea.  

I should warn you, however, that doing this discretization is not a straightforward business: it requires intelligent choices, which basically amount to guesses about what&#039;s going on at length scales shorter than the shortest length scale we&#039;re explicitly considering!

I should also add that there&#039;s nothing sacred about the number 8&lt;sup&gt;n&lt;/sup&gt; here; I&#039;m using it merely because in my original story I imagined repeatedly chopping the cubes we had into 8 smaller cubes, to study shorter and shorter length scales.

]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3372">John Baez</a>.</p>
<p>Bill wrote:</p>
<blockquote><p>
I don’t think you or Tim Palmer discussed infinite velocity and vorticity.
</p></blockquote>
<p>No.  He probably understands how that&#8217;s related to what he said.  I don&#8217;t.  </p>
<p>In fact, it&#8217;s quite possible that a lot of what I&#8217;ve said so far is somewhat inaccurate &mdash; so if you <i>really</i> want to understand this stuff, take everything I say with a mongo grain of salt!  </p>
<p>I wish someone working on fluid dynamics would join this conversation and help us out.  But barring that, I suggest taking a look at this:</p>
<p>&bull; Terry Tao, <a href="http://terrytao.wordpress.com/2007/03/18/why-global-regularity-for-navier-stokes-is-hard/" rel="nofollow">Why global regularity for Navier-Stokes is hard</a>.</p>
<p>You&#8217;ll see that he makes crucial use of a certain scale-invariance property of the Navier-Stokes equations.  This is presumably somehow related to the multi-scale analysis I was sketching. </p>
<p>However, I was talking about how unknown short-scale information causes unpredictability at large distance scales: e.g., how the flap of a butterflies wings today can cause a typhoon next year.  </p>
<p>He, on the other hand, is talking about how energy spread over large distance scales can concentrate down to shorter and shorter distance scales, potentially causing a singularity at some point!   This is what he wants to rule out, to prove global existence for Navier-Stokes:</p>
<blockquote><p>
In order to prevent blowup, therefore, we must arrest this motion of energy from coarse scales (or low frequencies) to fine scales (or high frequencies).
</p></blockquote>
<p>So maybe I&#8217;m upside down and backwards &mdash; or quite possibly both effects are very important, but in different ways.  </p>
<blockquote><p>
Both you &amp; Tim say that when you discretize at a 2<sup>-n</sup> timescale, you get an ODE.
</p></blockquote>
<p>No!  We&#8217;re saying that if you discretize at any fixed <i>spatial</i> distance scale, you get an ODE.  </p>
<p>For example, if you chop a unit cube up into 8<sup>n</sup> little cubes, and keep track of the average velocity, average pressure and average density in all these little cubes, that&#8217;s 5 &times; 8<sup>n</sup> variables to keep track of.  Think of all these variables as components of a single vector </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5E%7B5+%5Ctimes+8%5En%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;mathbb{R}^{5 &#92;times 8^n}" class="latex" /></p>
<p>Then, you can discretize the Navier-Stokes equations to obtain a multivariable ODE, sort of like this: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+F%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d x}{d t} = F(x) " class="latex" /></p>
<p>where <i>F</i> is some horribly complicated function.</p>
<p>I&#8217;m not an expert on this, so I could be making even more mistakes, but I&#8217;m pretty sure this is the right general idea.  </p>
<p>I should warn you, however, that doing this discretization is not a straightforward business: it requires intelligent choices, which basically amount to guesses about what&#8217;s going on at length scales shorter than the shortest length scale we&#8217;re explicitly considering!</p>
<p>I should also add that there&#8217;s nothing sacred about the number 8<sup>n</sup> here; I&#8217;m using it merely because in my original story I imagined repeatedly chopping the cubes we had into 8 smaller cubes, to study shorter and shorter length scales.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bill Richter		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3384</link>

		<dc:creator><![CDATA[Bill Richter]]></dc:creator>
		<pubDate>Fri, 07 Jan 2011 16:37:53 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3384</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3372&quot;&gt;John Baez&lt;/a&gt;.

Thanks, John! I hope your trip to Vietnam went well.  I have a few simple questions, but first a retraction: Charles Feffererman &lt;a href=&quot;http://www.claymath.org/millennium/Navier-Stokes_Equations/navierstokes.pdf&quot; rel=&quot;nofollow&quot;&gt;says&lt;/a&gt; they have proved short-term existence for NS. Fefferman says the long-term solution problem (top p. 3) is that we would get infinite velocity and vorticity at the blowup time T.  I don&#039;t think you or Tim Palmer discussed infinite velocity and vorticity.
Both you &#038; Tim say that when you discretize at a 2&lt;sup&gt;-n&lt;/sup&gt; timescale, you get an ODE.  That&#039;s probably pretty basic, but I don&#039;t understand that.

I liked your description of the problem if T&lt;sub&gt;n&lt;/sub&gt; get shorter.  That really seems to mean you couldn&#039;t predict the weather on a computer.  Is that Tim&#039;s nonlinear interaction between the timescales?  He said &quot;that there may be situations (in 3 dimensional turbulence)&#039;&#039; where we&#039;re in real trouble.  Can you argue that in Tim&#039;s turbulence situation, your T&lt;sub&gt;n&lt;/sub&gt; do get shorter?  BTW I don&#039;t understand Tim&#039;s convergence &#038; divergence at all.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3372">John Baez</a>.</p>
<p>Thanks, John! I hope your trip to Vietnam went well.  I have a few simple questions, but first a retraction: Charles Feffererman <a href="http://www.claymath.org/millennium/Navier-Stokes_Equations/navierstokes.pdf" rel="nofollow">says</a> they have proved short-term existence for NS. Fefferman says the long-term solution problem (top p. 3) is that we would get infinite velocity and vorticity at the blowup time T.  I don&#8217;t think you or Tim Palmer discussed infinite velocity and vorticity.<br />
Both you &amp; Tim say that when you discretize at a 2<sup>-n</sup> timescale, you get an ODE.  That&#8217;s probably pretty basic, but I don&#8217;t understand that.</p>
<p>I liked your description of the problem if T<sub>n</sub> get shorter.  That really seems to mean you couldn&#8217;t predict the weather on a computer.  Is that Tim&#8217;s nonlinear interaction between the timescales?  He said &#8220;that there may be situations (in 3 dimensional turbulence)&#8221; where we&#8217;re in real trouble.  Can you argue that in Tim&#8217;s turbulence situation, your T<sub>n</sub> do get shorter?  BTW I don&#8217;t understand Tim&#8217;s convergence &amp; divergence at all.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3372</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 07 Jan 2011 10:19:47 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3372</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3340&quot;&gt;Bill Richter&lt;/a&gt;.

Hi, Bill!

It&#039;s hard for me to explain this stuff with crystalline clarity, for three reasons:

1) I&#039;m not a real expert on it.  (That&#039;s actually a sufficient reason, but I&#039;ll give two more.)

2) As far as I can tell, nobody knows that we need an infinite amount of information about the initial data to compute a solution of the Navier-Stokes equation to within a given desired accuracy past a certain time.

3) Of course nobody knows if the Navier-Stokes equations &lt;i&gt;have&lt;/i&gt; solutions past a certain time, given some smooth initial data.  This is the million-dollar Clay Prize puzzle.

But the idea is that that issues 2) and 3) are related.

Of course they&#039;re related in an obvious way.  If given some initial data the Navier-Stokes don&#039;t have a solution past a certain time, it&#039;s silly to ask about predicting what the solution will be after that time.  

But there&#039;s also a more interesting relation.  Very roughly, I think it goes like this. 

Say we have a box of fluid that&#039;s one meter across.  We can describe the velocity vector field of this  follows.  First, say the average velocity of the entire box.  Second, divide the box into 2&lt;sup&gt;3&lt;/sup&gt; little boxes, and for each of these, give the average velocity in the little box, minus the average in the bigger box.    Third, divide each of these little boxes into 2&lt;sup&gt;3&lt;/sup&gt; smaller boxes and repeat the procedure.  Fourth... etcetera.

This is a simple way to take our information about the velocity vector field and break it up into pieces corresponding to different length scales: the one-meter scale, the half-meter scale, the quarter-meter scale, and so on.  

But now suppose we&#039;re trying to &lt;i&gt;approximately&lt;/i&gt; solve a &lt;i&gt;discretized&lt;/i&gt; version of the Navier-Stokes equation.  Then we give up after the nth step: for example, we take our cube and chop it up into lots of little cubes that are each 2&lt;sup&gt;-n&lt;/sup&gt; meters across, and hope that&#039;s good enough.  

If we do this, we can approximate the Navier-Stokes equations by an ordinary differential equation that says how the velocities at length scales above 2&lt;sup&gt;-n&lt;/sup&gt; evolve in time. 

However, this ordinary differential equation could be chaotic.  

Let&#039;s suppose it is.  Then it has some Lyapunov time T&lt;sub&gt;n&lt;/sub&gt;, meaning roughly that any error in our initial data will double after time T&lt;sub&gt;n&lt;/sub&gt;.  

Even worse, it could be true that T&lt;sub&gt;n&lt;/sub&gt; gets shorter as n increases!  

This might not be so bad if information at short distance scales didn&#039;t affect the information at long distance scales very much.  You could say &quot;I only care about the wind velocity at the 1-meter scale, so I don&#039;t care if it&#039;s extremely hard to predict the velocity at the 1/1024-meter scale after a very short time.&quot;

However, it might be true that the information at short distance scales affects the distance at longer distance scales quite a lot, quite soon.  

Then we could be in real trouble.  As we subdivide our cube more finely to describe our velocity vector field more accurately, we&#039;re punished by a shorter Lyapunov time.  Even worse, any error in our initial data at the shortest distance scale we consider quickly affects what&#039;s going on at the 1-meter scale.  

I hope you can see that in this situation, it could be impossible to use a discretization to compute solutions of the Navier-Stokes equation to within a desired accuracy past a certain short time.  

And if you can&#039;t figure out a way to &lt;i&gt;compute&lt;/i&gt; the solution, you should worry that maybe the solution doesn&#039;t even &lt;i&gt;exist!&lt;/i&gt;  After all, a great way to prove the existence of a solution would be to construct it as a limit of better and better approximate solutions.  But here that strategy seems to be failing. 

Whew.  This took a long time to write, and there are still lots of places where it could be improved.   Please read it three times before asking questions!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3340">Bill Richter</a>.</p>
<p>Hi, Bill!</p>
<p>It&#8217;s hard for me to explain this stuff with crystalline clarity, for three reasons:</p>
<p>1) I&#8217;m not a real expert on it.  (That&#8217;s actually a sufficient reason, but I&#8217;ll give two more.)</p>
<p>2) As far as I can tell, nobody knows that we need an infinite amount of information about the initial data to compute a solution of the Navier-Stokes equation to within a given desired accuracy past a certain time.</p>
<p>3) Of course nobody knows if the Navier-Stokes equations <i>have</i> solutions past a certain time, given some smooth initial data.  This is the million-dollar Clay Prize puzzle.</p>
<p>But the idea is that that issues 2) and 3) are related.</p>
<p>Of course they&#8217;re related in an obvious way.  If given some initial data the Navier-Stokes don&#8217;t have a solution past a certain time, it&#8217;s silly to ask about predicting what the solution will be after that time.  </p>
<p>But there&#8217;s also a more interesting relation.  Very roughly, I think it goes like this. </p>
<p>Say we have a box of fluid that&#8217;s one meter across.  We can describe the velocity vector field of this  follows.  First, say the average velocity of the entire box.  Second, divide the box into 2<sup>3</sup> little boxes, and for each of these, give the average velocity in the little box, minus the average in the bigger box.    Third, divide each of these little boxes into 2<sup>3</sup> smaller boxes and repeat the procedure.  Fourth&#8230; etcetera.</p>
<p>This is a simple way to take our information about the velocity vector field and break it up into pieces corresponding to different length scales: the one-meter scale, the half-meter scale, the quarter-meter scale, and so on.  </p>
<p>But now suppose we&#8217;re trying to <i>approximately</i> solve a <i>discretized</i> version of the Navier-Stokes equation.  Then we give up after the nth step: for example, we take our cube and chop it up into lots of little cubes that are each 2<sup>-n</sup> meters across, and hope that&#8217;s good enough.  </p>
<p>If we do this, we can approximate the Navier-Stokes equations by an ordinary differential equation that says how the velocities at length scales above 2<sup>-n</sup> evolve in time. </p>
<p>However, this ordinary differential equation could be chaotic.  </p>
<p>Let&#8217;s suppose it is.  Then it has some Lyapunov time T<sub>n</sub>, meaning roughly that any error in our initial data will double after time T<sub>n</sub>.  </p>
<p>Even worse, it could be true that T<sub>n</sub> gets shorter as n increases!  </p>
<p>This might not be so bad if information at short distance scales didn&#8217;t affect the information at long distance scales very much.  You could say &#8220;I only care about the wind velocity at the 1-meter scale, so I don&#8217;t care if it&#8217;s extremely hard to predict the velocity at the 1/1024-meter scale after a very short time.&#8221;</p>
<p>However, it might be true that the information at short distance scales affects the distance at longer distance scales quite a lot, quite soon.  </p>
<p>Then we could be in real trouble.  As we subdivide our cube more finely to describe our velocity vector field more accurately, we&#8217;re punished by a shorter Lyapunov time.  Even worse, any error in our initial data at the shortest distance scale we consider quickly affects what&#8217;s going on at the 1-meter scale.  </p>
<p>I hope you can see that in this situation, it could be impossible to use a discretization to compute solutions of the Navier-Stokes equation to within a desired accuracy past a certain short time.  </p>
<p>And if you can&#8217;t figure out a way to <i>compute</i> the solution, you should worry that maybe the solution doesn&#8217;t even <i>exist!</i>  After all, a great way to prove the existence of a solution would be to construct it as a limit of better and better approximate solutions.  But here that strategy seems to be failing. </p>
<p>Whew.  This took a long time to write, and there are still lots of places where it could be improved.   Please read it three times before asking questions!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bill Richter		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3340</link>

		<dc:creator><![CDATA[Bill Richter]]></dc:creator>
		<pubDate>Thu, 06 Jan 2011 16:06:49 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3340</guid>

					<description><![CDATA[John, it sounds like you &#038; Tim Palmer are relating two different-sounding things: the unpredictability of the weather and the unproved short-time existence and uniqueness for solutions of the Navier-Stokes equations.  That sounds cool, but I didn&#039;t understand the reasoning (involving summing up all the Lyapunov timescales).  I understood this much from you about weather unpredictability:
&lt;blockquote&gt;we need an essentially infinite amount of information to predict the future past a certain amount of time.&lt;/blockquote&gt;
Can you explain how this (your too much &quot;information percolating up from short distance scales&#039;&#039;) makes NS existence &#038; uniqueness hard to prove?  I don&#039;t suppose this is related to chaos in the n-body problem, which I think is caused by possible singularities caused by collisions.]]></description>
			<content:encoded><![CDATA[<p>John, it sounds like you &amp; Tim Palmer are relating two different-sounding things: the unpredictability of the weather and the unproved short-time existence and uniqueness for solutions of the Navier-Stokes equations.  That sounds cool, but I didn&#8217;t understand the reasoning (involving summing up all the Lyapunov timescales).  I understood this much from you about weather unpredictability:</p>
<blockquote><p>we need an essentially infinite amount of information to predict the future past a certain amount of time.</p></blockquote>
<p>Can you explain how this (your too much &#8220;information percolating up from short distance scales&#8221;) makes NS existence &amp; uniqueness hard to prove?  I don&#8217;t suppose this is related to chaos in the n-body problem, which I think is caused by possible singularities caused by collisions.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: AGU talk: Tim Palmer on Building Probabilistic Climate Models &#124; Serendipity		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3204</link>

		<dc:creator><![CDATA[AGU talk: Tim Palmer on Building Probabilistic Climate Models &#124; Serendipity]]></dc:creator>
		<pubDate>Tue, 21 Dec 2010 21:37:14 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3204</guid>

					<description><![CDATA[[...] John Baez has a great in-depth interview with Tim over at [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] John Baez has a great in-depth interview with Tim over at [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Tim van Beek		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3198</link>

		<dc:creator><![CDATA[Tim van Beek]]></dc:creator>
		<pubDate>Tue, 21 Dec 2010 06:51:07 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3198</guid>

					<description><![CDATA[Heads up: Steve Easterbrook&#039;s latest post on serendipity is about Tim Palmer&#039;s talk at the AGU conference: &lt;a href=&quot;http://www.easterbrook.ca/steve/?p=2110&quot; rel=&quot;nofollow&quot;&gt;Tim Palmer on Building Probabilistic Climate Models&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>Heads up: Steve Easterbrook&#8217;s latest post on serendipity is about Tim Palmer&#8217;s talk at the AGU conference: <a href="http://www.easterbrook.ca/steve/?p=2110" rel="nofollow">Tim Palmer on Building Probabilistic Climate Models</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Giampiero Campa		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3128</link>

		<dc:creator><![CDATA[Giampiero Campa]]></dc:creator>
		<pubDate>Sat, 18 Dec 2010 18:26:30 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3128</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3024&quot;&gt;John Baez&lt;/a&gt;.

Hi John, yes i can see the fact that it would take infinite information to predict the future within a certain accuracy. 

If this is the difference between the &quot;real&quot; butterfly effect and the other one, then OK i get it.

I understand less the fact that you can assign lypapunov exponents to scales, but i think it is just a different way of expressing them.

If the most prominent characteristic of the &quot;real&quot; butterfly effect is that the uncertainty propagates from small scales to larger ones, then it seems to me that the same would happen if you have N objects interacting e.g. gravitationally (with N not even too large) distributed in the 3D space. Or in a large undamped 3D structure with many links. 

But maybe i am mistaken ...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3024">John Baez</a>.</p>
<p>Hi John, yes i can see the fact that it would take infinite information to predict the future within a certain accuracy. </p>
<p>If this is the difference between the &#8220;real&#8221; butterfly effect and the other one, then OK i get it.</p>
<p>I understand less the fact that you can assign lypapunov exponents to scales, but i think it is just a different way of expressing them.</p>
<p>If the most prominent characteristic of the &#8220;real&#8221; butterfly effect is that the uncertainty propagates from small scales to larger ones, then it seems to me that the same would happen if you have N objects interacting e.g. gravitationally (with N not even too large) distributed in the 3D space. Or in a large undamped 3D structure with many links. </p>
<p>But maybe i am mistaken &#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phil Henshaw		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3077</link>

		<dc:creator><![CDATA[Phil Henshaw]]></dc:creator>
		<pubDate>Thu, 16 Dec 2010 18:00:20 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3077</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3072&quot;&gt;Phil Henshaw&lt;/a&gt;.

Thanks, that helps clarify that.  Then the system with observable properties beyond the ability of your actual or potential information to describe, then cant&#039;t even be considered to be an information construct at all.   

That&#039;s what I look at, these intense concentrations of complexly organized systems that fairly clearly exceed to potential of information to describe.  So I just construct information definitions for the functional boundaries that uniquely locate them, and more or less wait for them to &quot;misbehave&quot; so I can study them.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3072">Phil Henshaw</a>.</p>
<p>Thanks, that helps clarify that.  Then the system with observable properties beyond the ability of your actual or potential information to describe, then cant&#8217;t even be considered to be an information construct at all.   </p>
<p>That&#8217;s what I look at, these intense concentrations of complexly organized systems that fairly clearly exceed to potential of information to describe.  So I just construct information definitions for the functional boundaries that uniquely locate them, and more or less wait for them to &#8220;misbehave&#8221; so I can study them.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3076</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 16 Dec 2010 06:43:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3076</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3062&quot;&gt;Graham&lt;/a&gt;.

That&#039;s a helpful bit of jargon, Graham.

I don&#039;t find that explanation quite as crystalline in its clarity as I would desire.  But I guess the idea is that if we try to predict the weather using a discretization where our variables are quantities (e.g. wind velocities) averaged over kilometer-sized cubes, we find that it&#039;s hard to predict what these variables will do in the future without knowing averaged quantities in smaller cubes &#8212; say, meter-sized cubes.  And if we decide to work with quantities averaged over meter-sized cubes, we find that &lt;i&gt;those&lt;/i&gt; are hard to predict without knowing averaged quantities in even &lt;i&gt;smaller&lt;/i&gt; cubes.  And so on, at least down to the Kolmogorov length scale, which is unfortunately very small, less than a centimeter &#8212; leaving us with an impractically large number of cubes. 

So, we throw up our hands in disgust, work with the smallest cubes our computer can handle, and make some guess about what&#039;s going on at smaller scales: a &quot;closure assumption&quot;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3062">Graham</a>.</p>
<p>That&#8217;s a helpful bit of jargon, Graham.</p>
<p>I don&#8217;t find that explanation quite as crystalline in its clarity as I would desire.  But I guess the idea is that if we try to predict the weather using a discretization where our variables are quantities (e.g. wind velocities) averaged over kilometer-sized cubes, we find that it&#8217;s hard to predict what these variables will do in the future without knowing averaged quantities in smaller cubes &mdash; say, meter-sized cubes.  And if we decide to work with quantities averaged over meter-sized cubes, we find that <i>those</i> are hard to predict without knowing averaged quantities in even <i>smaller</i> cubes.  And so on, at least down to the Kolmogorov length scale, which is unfortunately very small, less than a centimeter &mdash; leaving us with an impractically large number of cubes. </p>
<p>So, we throw up our hands in disgust, work with the smallest cubes our computer can handle, and make some guess about what&#8217;s going on at smaller scales: a &#8220;closure assumption&#8221;.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Frederik De Roo		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3073</link>

		<dc:creator><![CDATA[Frederik De Roo]]></dc:creator>
		<pubDate>Wed, 15 Dec 2010 17:50:21 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1971#comment-3073</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3072&quot;&gt;Phil Henshaw&lt;/a&gt;.

Comparing Tim&#039;s and Phil&#039;s posts has actually clarified for me the relation between Kolmogorov complexity and Shannon information. The message that provides the most new information is indeed the most complex ;-)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/07/this-weeks-finds-week-306-2/#comment-3072">Phil Henshaw</a>.</p>
<p>Comparing Tim&#8217;s and Phil&#8217;s posts has actually clarified for me the relation between Kolmogorov complexity and Shannon information. The message that provides the most new information is indeed the most complex ;-)</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
