<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: This Week&#8217;s Finds (Week 308)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/</link>
	<description></description>
	<lastBuildDate>Tue, 29 Jul 2014 01:56:31 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: lee bloomquist		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-54351</link>

		<dc:creator><![CDATA[lee bloomquist]]></dc:creator>
		<pubDate>Tue, 29 Jul 2014 01:56:31 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-54351</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-54332&quot;&gt;davetweed&lt;/a&gt;.

Dave, when you say &quot; ... &quot;null hypothesis” more as a shorthand for “base model”, in the sense that whenever you create a model which you know will not be a full, correct model of the phenomenon (which philosophically might be the only thing science does, but that’s a tangential debate)&quot;--

I do agree. Not being a scientist but an engineer (ret.) I admit to bringing up something that is perhaps tangential. From an equation that explained to me a well-studied phenomenon in psychology called &lt;a href=&quot;//johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/#comment-33945”&quot; rel=&quot;nofollow&quot;&gt;probability learning&lt;/a&gt;, I think I see a way to apply this equation for the lab animals&#039; behavior to guide investors&#039; or governments&#039; spending if and when ENSOs get worse and worse. Actually, my application of the equation wouldn&#039;t care at all about the details of particular models. It doesn&#039;t answer which model explains historical data better. The idea is just to give investors&#039; and governments&#039; some guidance for their spending behavior in real time-- as a future unfolds in which the probability of ENSOs might well change over comparatively short periods of time.

Although I do think it would be interesting to use the equation in order to compare a number of differently initialized Hopf bifurcation models with 1/f noise against what is going to happen in the future. The algorithm might pick one or the other of the differently initialized Hopf models as time goes on. And in a separate running, their performance as the future unfolds could be compared to the same application of the equation to the models of the &quot;big guys.&quot; To me it looks like a competition between simple and complex models.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-54332">davetweed</a>.</p>
<p>Dave, when you say &#8221; &#8230; &#8220;null hypothesis” more as a shorthand for “base model”, in the sense that whenever you create a model which you know will not be a full, correct model of the phenomenon (which philosophically might be the only thing science does, but that’s a tangential debate)&#8221;&#8211;</p>
<p>I do agree. Not being a scientist but an engineer (ret.) I admit to bringing up something that is perhaps tangential. From an equation that explained to me a well-studied phenomenon in psychology called <a href="//johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/#comment-33945”" rel="nofollow">probability learning</a>, I think I see a way to apply this equation for the lab animals&#8217; behavior to guide investors&#8217; or governments&#8217; spending if and when ENSOs get worse and worse. Actually, my application of the equation wouldn&#8217;t care at all about the details of particular models. It doesn&#8217;t answer which model explains historical data better. The idea is just to give investors&#8217; and governments&#8217; some guidance for their spending behavior in real time&#8211; as a future unfolds in which the probability of ENSOs might well change over comparatively short periods of time.</p>
<p>Although I do think it would be interesting to use the equation in order to compare a number of differently initialized Hopf bifurcation models with 1/f noise against what is going to happen in the future. The algorithm might pick one or the other of the differently initialized Hopf models as time goes on. And in a separate running, their performance as the future unfolds could be compared to the same application of the equation to the models of the &#8220;big guys.&#8221; To me it looks like a competition between simple and complex models.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davetweed		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-54332</link>

		<dc:creator><![CDATA[davetweed]]></dc:creator>
		<pubDate>Mon, 28 Jul 2014 14:22:29 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-54332</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-54314&quot;&gt;lee bloomquist&lt;/a&gt;.

I believe that Tim was using &quot;null hypothesis&quot; more as a shorthand for &quot;base model&quot;, in the sense that whenever you create a model which you &lt;em&gt;know&lt;/em&gt; will not be a &lt;em&gt;full, correct&lt;/em&gt; model of the phenomenon (which philosophically might be the only thing science does, but that&#039;s a tangential debate) you generally want to see if it performs better than your base model. For example, in a binary detection classifier one can compare against the classifiers &quot;always true&quot; and &quot;always false&quot;: if your model performs &lt;em&gt;worse&lt;/em&gt; than that then it&#039;s clearly no use and should (in its current incarnation) be abandoned.

As such, I think the only time you&#039;d use them is when you&#039;re coming up with a new classifier technique, and you compare against the base model (on your &quot;validation set&quot;) and discard one of the two models. (Actually, if you care about the &lt;em&gt;kinds&lt;/em&gt; of error (false positive, false negative, etc) you might find different models work better for different regimes and you&#039;re off doing ROC analysis as briefly discussed a little &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/07/23/el-nino-project-part-6/#comment-54141&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. However, in that case you&#039;d still only have one active model for each combination of error values.)

That&#039;s related to what I think Tim was getting at; I&#039;ll have to think some more about what you proposed as an idea in its own right.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-54314">lee bloomquist</a>.</p>
<p>I believe that Tim was using &#8220;null hypothesis&#8221; more as a shorthand for &#8220;base model&#8221;, in the sense that whenever you create a model which you <em>know</em> will not be a <em>full, correct</em> model of the phenomenon (which philosophically might be the only thing science does, but that&#8217;s a tangential debate) you generally want to see if it performs better than your base model. For example, in a binary detection classifier one can compare against the classifiers &#8220;always true&#8221; and &#8220;always false&#8221;: if your model performs <em>worse</em> than that then it&#8217;s clearly no use and should (in its current incarnation) be abandoned.</p>
<p>As such, I think the only time you&#8217;d use them is when you&#8217;re coming up with a new classifier technique, and you compare against the base model (on your &#8220;validation set&#8221;) and discard one of the two models. (Actually, if you care about the <em>kinds</em> of error (false positive, false negative, etc) you might find different models work better for different regimes and you&#8217;re off doing ROC analysis as briefly discussed a little <a href="https://johncarlosbaez.wordpress.com/2014/07/23/el-nino-project-part-6/#comment-54141" rel="nofollow">here</a>. However, in that case you&#8217;d still only have one active model for each combination of error values.)</p>
<p>That&#8217;s related to what I think Tim was getting at; I&#8217;ll have to think some more about what you proposed as an idea in its own right.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: lee bloomquist		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-54314</link>

		<dc:creator><![CDATA[lee bloomquist]]></dc:creator>
		<pubDate>Mon, 28 Jul 2014 03:35:01 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-54314</guid>

					<description><![CDATA[--  John wrote: &quot;...there are infinitely many different kinds of noise, so building a model involving noise requires that you make assumptions about the nature of the noise involved.&quot;

Since &lt;a href=&quot;http://en.m.wikipedia.org/wiki/Pink_noise&quot; rel=&quot;nofollow&quot;&gt;1/f noise&lt;/a&gt; is found in some types of meteorological data, has anybody thought about using 1/f noise to model ENSO with Hopf bifurcation and noise? Since 1/f noise is said to model &quot;long term memory effect&quot; could it also be relevant to the current discussion in the El Nino project, part 6, between Steve Wenner and HyperG about Bayesian and frequentist interpretations of probability?


-- In the above post Tim van Beek wrote &quot;a model can have different uses...[for example] as a null hypothesis for other, more sophisticated models.&quot;

Instead of a single null hypothesis I wonder if an algorithm based on multiple null hypotheses would be useful. Don&#039;t know if that&#039;s standard but here goes.

This is an idea about an optimal capital budgeting process based on software prediction of ENSOs (using an algorithm based on multiple null hypotheses). 

Why this kind of approach to software specification? Because ENSOs have big &lt;a href=&quot;http://www.publicaffairs.noaa.gov/worldsummit/pdfs/improving.pdfr&quot; rel=&quot;nofollow&quot;&gt;economic costs&lt;/a&gt; and so we might be able to cost-justify developing some software for this kind of optimal budgeting process.

Banks should be interested because, on behalf of their shareholders, they would be able to bet their money on better ENSO predictions. Governments should be interested because, on behalf of the people under their governance, they would be able to make better expenditures e.g. on appropriate corn storage and so on.

The idea comes from looking at Steve&#039;s spreadsheet, specifically the table for &quot;Training.&quot; And then, thinking of all those models from the &quot;big guys,&quot; that John showed us, as actually, null hypotheses. For what follows I assume that each null hypothesis from the list either predicts or does not predict an upcoming ENSO each year.

To make it easier (for me anyway), imagine a cartoon about a bank with a group of investment bankers, in three acts. Each investment banker is given just one dollar per year from a finite amount of dollars to invest based on predictions of an ENSO. After all it&#039;s a just a cartoon.

Act one focuses on one of the investment bankers. As the curtain opens it is capital budgeting time and he is considering all of the null hypotheses. Scene one: he focuses on one of the null hypotheses. It is predicting that next year there will be an ENSO. The banker bets his one dollar on shorting a company that will be hurt by the predicted ENSO. Scene two: The predicted ENSO does not occur. The banker puts a mark on a blackboard for this particular null hypothesis labelled &quot;buyer&#039;s regret.&quot; Because he regrets having listened to the prediction by this particular null hypothesis. Scene three: Time passes. Once again we see the banker focusing on the very same null hypothesis. It predicts an ENSO for next year. The banker goes somewhere else with his dollar. Scene four: As predicted, the ENSO occurs. The banker puts a mark on another blackboard next to the buyer&#039;s regret blackboard for this null hypothesis. It is labelled &quot;regret at lost opportunity.&quot;  Scene five: Time moves on like pages turning in a book. We see the banker putting more and more marks on each blackboard for the null hypothesis, keeping track of his two kinds of regret associated to the null hypothesis in question. Final scene of act one: the narrator tells us how the banker has balanced the counts. Through his choices the banker has evenly balanced his counts on the two blackboards assigned to the null hypothesis in question. Buyers regret balances regret from lost opportunity. Each mark of regret on betting this null hypothesis would be correct pushes the investment banker away from it. But each mark of regret on having lost an opportunity to invest in this null hypothesis pushes the banker toward it. He is pushed toward and away from the null hypothesis in question by these two opposing entropic forces. Equlibrium occurs for the null hypothesis in question when these two balance each other. Note: the bankers in this cartoon bet just on whether or not a particular null hypothesis is right in its prediction, regardless of whether the prediction is for or against an ENSO . (This would be the infamous probability learning algorithm if all the null hypothesis worked like the feeding spots in the probability learning experiment. Of course they don&#039;t, but for now this is just a cartoon.)

Act two. Scene one: We see a the entire group of investors for the bank,each given a dollar bill per budgeting period, each balancing these two forces of regret for each null hypothesis in the collection that John gave us. That is, each banker has two blackboards for each of the null hypotheses. It&#039;s a lot of blackboards. Scene two: In  flurry of activity the bankers run around from one kind of blackboard to the other for each null hypothesis. Finally, some form emerges and we see that things settle out to there being for each null hypothesis a group of bankers, each group of varying size in proportion to the prediction accuracy of each null hypothesis. (It&#039;s like the school of fish settling into a Nash equilibrium in a previous comment on biology.) Scene three: We focus on the original soliatry banker and see the cartoon bubble of thoughts in his mind. &quot;I&#039;d like to try out another one of those hypotheses, but I&#039;m afraid of leaving my group here. It&#039;s stable. Who knows, if I leave it they might not let me back in. And those other groups might not let me in. Think I&#039;ll just stay here. I&#039;m afraid to do anything else.&quot;

Act three: the narrator speaks. &quot;Socrates left us with the idea that self-knowledge is the basis of all other knowledge. We may never be able to know the perfect model, but we do know how all of these null hypotheses can make us experience regret. The self-knowledge in this case is knowing our own experience of regret as a kind of thirst or hunger not satisfied. It&#039;s the basis of the algorithm.&quot;

(curtain closes)]]></description>
			<content:encoded><![CDATA[<p>&#8212;  John wrote: &#8220;&#8230;there are infinitely many different kinds of noise, so building a model involving noise requires that you make assumptions about the nature of the noise involved.&#8221;</p>
<p>Since <a href="http://en.m.wikipedia.org/wiki/Pink_noise" rel="nofollow">1/f noise</a> is found in some types of meteorological data, has anybody thought about using 1/f noise to model ENSO with Hopf bifurcation and noise? Since 1/f noise is said to model &#8220;long term memory effect&#8221; could it also be relevant to the current discussion in the El Nino project, part 6, between Steve Wenner and HyperG about Bayesian and frequentist interpretations of probability?</p>
<p>&#8212; In the above post Tim van Beek wrote &#8220;a model can have different uses&#8230;[for example] as a null hypothesis for other, more sophisticated models.&#8221;</p>
<p>Instead of a single null hypothesis I wonder if an algorithm based on multiple null hypotheses would be useful. Don&#8217;t know if that&#8217;s standard but here goes.</p>
<p>This is an idea about an optimal capital budgeting process based on software prediction of ENSOs (using an algorithm based on multiple null hypotheses). </p>
<p>Why this kind of approach to software specification? Because ENSOs have big <a href="http://www.publicaffairs.noaa.gov/worldsummit/pdfs/improving.pdfr" rel="nofollow">economic costs</a> and so we might be able to cost-justify developing some software for this kind of optimal budgeting process.</p>
<p>Banks should be interested because, on behalf of their shareholders, they would be able to bet their money on better ENSO predictions. Governments should be interested because, on behalf of the people under their governance, they would be able to make better expenditures e.g. on appropriate corn storage and so on.</p>
<p>The idea comes from looking at Steve&#8217;s spreadsheet, specifically the table for &#8220;Training.&#8221; And then, thinking of all those models from the &#8220;big guys,&#8221; that John showed us, as actually, null hypotheses. For what follows I assume that each null hypothesis from the list either predicts or does not predict an upcoming ENSO each year.</p>
<p>To make it easier (for me anyway), imagine a cartoon about a bank with a group of investment bankers, in three acts. Each investment banker is given just one dollar per year from a finite amount of dollars to invest based on predictions of an ENSO. After all it&#8217;s a just a cartoon.</p>
<p>Act one focuses on one of the investment bankers. As the curtain opens it is capital budgeting time and he is considering all of the null hypotheses. Scene one: he focuses on one of the null hypotheses. It is predicting that next year there will be an ENSO. The banker bets his one dollar on shorting a company that will be hurt by the predicted ENSO. Scene two: The predicted ENSO does not occur. The banker puts a mark on a blackboard for this particular null hypothesis labelled &#8220;buyer&#8217;s regret.&#8221; Because he regrets having listened to the prediction by this particular null hypothesis. Scene three: Time passes. Once again we see the banker focusing on the very same null hypothesis. It predicts an ENSO for next year. The banker goes somewhere else with his dollar. Scene four: As predicted, the ENSO occurs. The banker puts a mark on another blackboard next to the buyer&#8217;s regret blackboard for this null hypothesis. It is labelled &#8220;regret at lost opportunity.&#8221;  Scene five: Time moves on like pages turning in a book. We see the banker putting more and more marks on each blackboard for the null hypothesis, keeping track of his two kinds of regret associated to the null hypothesis in question. Final scene of act one: the narrator tells us how the banker has balanced the counts. Through his choices the banker has evenly balanced his counts on the two blackboards assigned to the null hypothesis in question. Buyers regret balances regret from lost opportunity. Each mark of regret on betting this null hypothesis would be correct pushes the investment banker away from it. But each mark of regret on having lost an opportunity to invest in this null hypothesis pushes the banker toward it. He is pushed toward and away from the null hypothesis in question by these two opposing entropic forces. Equlibrium occurs for the null hypothesis in question when these two balance each other. Note: the bankers in this cartoon bet just on whether or not a particular null hypothesis is right in its prediction, regardless of whether the prediction is for or against an ENSO . (This would be the infamous probability learning algorithm if all the null hypothesis worked like the feeding spots in the probability learning experiment. Of course they don&#8217;t, but for now this is just a cartoon.)</p>
<p>Act two. Scene one: We see a the entire group of investors for the bank,each given a dollar bill per budgeting period, each balancing these two forces of regret for each null hypothesis in the collection that John gave us. That is, each banker has two blackboards for each of the null hypotheses. It&#8217;s a lot of blackboards. Scene two: In  flurry of activity the bankers run around from one kind of blackboard to the other for each null hypothesis. Finally, some form emerges and we see that things settle out to there being for each null hypothesis a group of bankers, each group of varying size in proportion to the prediction accuracy of each null hypothesis. (It&#8217;s like the school of fish settling into a Nash equilibrium in a previous comment on biology.) Scene three: We focus on the original soliatry banker and see the cartoon bubble of thoughts in his mind. &#8220;I&#8217;d like to try out another one of those hypotheses, but I&#8217;m afraid of leaving my group here. It&#8217;s stable. Who knows, if I leave it they might not let me back in. And those other groups might not let me in. Think I&#8217;ll just stay here. I&#8217;m afraid to do anything else.&#8221;</p>
<p>Act three: the narrator speaks. &#8220;Socrates left us with the idea that self-knowledge is the basis of all other knowledge. We may never be able to know the perfect model, but we do know how all of these null hypotheses can make us experience regret. The self-knowledge in this case is knowing our own experience of regret as a kind of thirst or hunger not satisfied. It&#8217;s the basis of the algorithm.&#8221;</p>
<p>(curtain closes)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Increasing the Signal-to-Noise Ratio With More Noise « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-17337</link>

		<dc:creator><![CDATA[Increasing the Signal-to-Noise Ratio With More Noise « Azimuth]]></dc:creator>
		<pubDate>Mon, 30 Jul 2012 07:34:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-17337</guid>

					<description><![CDATA[In &lt;i&gt;This Weeks Finds&lt;/i&gt;, back in &#8216;week308&#8242;, we had a look at a model with &#8216;noise&#8217;. A lot of systems can be described by ordinary [...]]]></description>
			<content:encoded><![CDATA[<p>In <i>This Weeks Finds</i>, back in &#8216;week308&#8242;, we had a look at a model with &#8216;noise&#8217;. A lot of systems can be described by ordinary [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Azimuth Project News &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3823</link>

		<dc:creator><![CDATA[Azimuth Project News &#171; Azimuth]]></dc:creator>
		<pubDate>Mon, 24 Jan 2011 06:22:17 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-3823</guid>

					<description><![CDATA[[...] Software for investigating the Hopf bifurcation and its stochastic version: see week308 of This Week&#8217;s [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] Software for investigating the Hopf bifurcation and its stochastic version: see week308 of This Week&#8217;s [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Joris Vankerschaver		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3681</link>

		<dc:creator><![CDATA[Joris Vankerschaver]]></dc:creator>
		<pubDate>Mon, 17 Jan 2011 10:07:39 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-3681</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3661&quot;&gt;John Baez&lt;/a&gt;.

Thanks for pointing this out!  It seems that we have stumbled upon a limitation within Sage: apparently, to see the interactive elements in a worksheet one needs to be logged in.  Rather than asking you all to create an account with Sage, I noticed that there are some patches lying around to enable this, and once I get back from this conference I&#039;m attending, I&#039;ll try to set up my own server running this worksheet.

Sorry for this unexpected setback!  You can still see the niftyness by logging in though...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3661">John Baez</a>.</p>
<p>Thanks for pointing this out!  It seems that we have stumbled upon a limitation within Sage: apparently, to see the interactive elements in a worksheet one needs to be logged in.  Rather than asking you all to create an account with Sage, I noticed that there are some patches lying around to enable this, and once I get back from this conference I&#8217;m attending, I&#8217;ll try to set up my own server running this worksheet.</p>
<p>Sorry for this unexpected setback!  You can still see the niftyness by logging in though&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Tim van Beek		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3671</link>

		<dc:creator><![CDATA[Tim van Beek]]></dc:creator>
		<pubDate>Sun, 16 Jan 2011 16:21:25 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-3671</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3661&quot;&gt;John Baez&lt;/a&gt;.

Unfortunately it does not work for me, either, I have the same problems as John.
Over at the Azimuth forum Staffan Liljegren already pointed out Sage but I haven&#039;t taken a closer look at it yet. The motivation of the authors of Sage is very similar to the motivation of the Azimuth project, so I do hope that we can use and maybe extend Sage for our purposes.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3661">John Baez</a>.</p>
<p>Unfortunately it does not work for me, either, I have the same problems as John.<br />
Over at the Azimuth forum Staffan Liljegren already pointed out Sage but I haven&#8217;t taken a closer look at it yet. The motivation of the authors of Sage is very similar to the motivation of the Azimuth project, so I do hope that we can use and maybe extend Sage for our purposes.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nathan Urban		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3668</link>

		<dc:creator><![CDATA[Nathan Urban]]></dc:creator>
		<pubDate>Sun, 16 Jan 2011 13:48:30 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-3668</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3661&quot;&gt;John Baez&lt;/a&gt;.

I have the same problems as John (using Mac Firefox 3.6.1 or Safari 5.0.3).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3661">John Baez</a>.</p>
<p>I have the same problems as John (using Mac Firefox 3.6.1 or Safari 5.0.3).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3661</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 16 Jan 2011 04:24:29 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-3661</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3333&quot;&gt;Joris Vankerschaver&lt;/a&gt;.

Joris wrote:

&lt;blockquote&gt;

Just for fun I created an interactive Sage version of this phenomenon: you can play with the model at http://www.sagenb.org/home/pub/2630.

&lt;/blockquote&gt;

For some reason that doesn&#039;t work for me.  I see some sliders at the bottom, and I can slide them, but I don&#039;t see a plot of the solution.  

Also, for some mysterious reason the jsmath doesn&#039;t work for me either.  

Does this stuff work for anyone else reading this?  Maybe it&#039;s just me.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3333">Joris Vankerschaver</a>.</p>
<p>Joris wrote:</p>
<blockquote>
<p>Just for fun I created an interactive Sage version of this phenomenon: you can play with the model at <a href="http://www.sagenb.org/home/pub/2630" rel="nofollow ugc">http://www.sagenb.org/home/pub/2630</a>.</p>
</blockquote>
<p>For some reason that doesn&#8217;t work for me.  I see some sliders at the bottom, and I can slide them, but I don&#8217;t see a plot of the solution.  </p>
<p>Also, for some mysterious reason the jsmath doesn&#8217;t work for me either.  </p>
<p>Does this stuff work for anyone else reading this?  Maybe it&#8217;s just me.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: peter waaben		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/#comment-3462</link>

		<dc:creator><![CDATA[peter waaben]]></dc:creator>
		<pubDate>Sun, 09 Jan 2011 15:42:43 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=2091#comment-3462</guid>

					<description><![CDATA[The Poincaré–Bendixson theorem states that any orbit that stays in a compact region of the state space of a 2-dimensional planar continuous dynamical system, all of whose fixed points are isolated, approaches its ω-limit set, which is either a fixed point, a periodic orbit, or is a connected set composed of a finite number of fixed points together with homoclinic and heteroclinic orbits connecting these. Thus chaotic behaviour can only arise in continuous dynamical systems whose phase space has three or more dimensions. However the theorem does not apply to discrete dynamical systems, where chaotic behaviour can arise in two or even one dimensional systems.

A weaker version of the theorem was originally conceived by Henri Poincaré, although he lacked a complete proof. Ivar Bendixson (1901) gave a rigorous proof of the full theorem.

I.e., this is a proof that discrete and continous systems are not-isomorphic thus care seems appropriate in the development of models etc.]]></description>
			<content:encoded><![CDATA[<p>The Poincaré–Bendixson theorem states that any orbit that stays in a compact region of the state space of a 2-dimensional planar continuous dynamical system, all of whose fixed points are isolated, approaches its ω-limit set, which is either a fixed point, a periodic orbit, or is a connected set composed of a finite number of fixed points together with homoclinic and heteroclinic orbits connecting these. Thus chaotic behaviour can only arise in continuous dynamical systems whose phase space has three or more dimensions. However the theorem does not apply to discrete dynamical systems, where chaotic behaviour can arise in two or even one dimensional systems.</p>
<p>A weaker version of the theorem was originally conceived by Henri Poincaré, although he lacked a complete proof. Ivar Bendixson (1901) gave a rigorous proof of the full theorem.</p>
<p>I.e., this is a proof that discrete and continous systems are not-isomorphic thus care seems appropriate in the development of models etc.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
