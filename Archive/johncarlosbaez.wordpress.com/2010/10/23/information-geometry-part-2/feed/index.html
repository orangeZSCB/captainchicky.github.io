<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information Geometry (Part 2)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/</link>
	<description></description>
	<lastBuildDate>Sun, 05 May 2013 20:24:08 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Connection between Fisher metric and the relative entropy &#124; Question and Answer		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-28242</link>

		<dc:creator><![CDATA[Connection between Fisher metric and the relative entropy &#124; Question and Answer]]></dc:creator>
		<pubDate>Sun, 05 May 2013 20:24:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-28242</guid>

					<description><![CDATA[[...] found the above in the nice blog of John Baez where Vasileios Anagnostopoulos says about that in the [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] found the above in the nice blog of John Baez where Vasileios Anagnostopoulos says about that in the [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 6) &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-3770</link>

		<dc:creator><![CDATA[Information Geometry (Part 6) &#171; Azimuth]]></dc:creator>
		<pubDate>Fri, 21 Jan 2011 08:02:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-3770</guid>

					<description><![CDATA[[...] Part 1 &#160; &#160; • Part 2 &#160; &#160; • Part 3 &#160; &#160; • Part 4 &#160; &#160; • Part [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] Part 1 &nbsp; &nbsp; • Part 2 &nbsp; &nbsp; • Part 3 &nbsp; &nbsp; • Part 4 &nbsp; &nbsp; • Part [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Vasileios Anagnostopoulos		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2295</link>

		<dc:creator><![CDATA[Vasileios Anagnostopoulos]]></dc:creator>
		<pubDate>Mon, 01 Nov 2010 11:15:42 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2295</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2294&quot;&gt;Vasileios Anagnostopoulos&lt;/a&gt;.

You can also see it as a differential of a Kullback-Leibler divergence between the same parametric probability model:

$latex D( p(\cdot , a+da) \&#124; p(.,a) ) = g_{i,j} da^{i} da^{j} + (O( \&#124;da\&#124;^{3})$]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2294">Vasileios Anagnostopoulos</a>.</p>
<p>You can also see it as a differential of a Kullback-Leibler divergence between the same parametric probability model:</p>
<p><img src="https://s0.wp.com/latex.php?latex=D%28+p%28%5Ccdot+%2C+a%2Bda%29+%5C%7C+p%28.%2Ca%29+%29+%3D+g_%7Bi%2Cj%7D+da%5E%7Bi%7D+da%5E%7Bj%7D+%2B+%28O%28+%5C%7Cda%5C%7C%5E%7B3%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D( p(&#92;cdot , a+da) &#92;| p(.,a) ) = g_{i,j} da^{i} da^{j} + (O( &#92;|da&#92;|^{3})" class="latex" /></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Vasileios Anagnostopoulos		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2294</link>

		<dc:creator><![CDATA[Vasileios Anagnostopoulos]]></dc:creator>
		<pubDate>Mon, 01 Nov 2010 11:12:50 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2294</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2126&quot;&gt;John Baez&lt;/a&gt;.

It is not mysterious: see

www.lti.cs.cmu.edu/Research/Thesis/GuyLebanon05.pdf

and especially the wonderful:

http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.2901

Check out the proofs.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2126">John Baez</a>.</p>
<p>It is not mysterious: see</p>
<p><a href="http://www.lti.cs.cmu.edu/Research/Thesis/GuyLebanon05.pdf" rel="nofollow ugc">http://www.lti.cs.cmu.edu/Research/Thesis/GuyLebanon05.pdf</a></p>
<p>and especially the wonderful:</p>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.2901" rel="nofollow ugc">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.2901</a></p>
<p>Check out the proofs.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 4) &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2242</link>

		<dc:creator><![CDATA[Information Geometry (Part 4) &#171; Azimuth]]></dc:creator>
		<pubDate>Fri, 29 Oct 2010 08:45:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2242</guid>

					<description><![CDATA[[...] suppose we&#8217;re working in the special case discussed in Part 2, where our manifold is an open subset of , and  at the point  is the Gibbs state with . Then all [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] suppose we&#8217;re working in the special case discussed in Part 2, where our manifold is an open subset of , and  at the point  is the Gibbs state with . Then all [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: phorgyphynance		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2179</link>

		<dc:creator><![CDATA[phorgyphynance]]></dc:creator>
		<pubDate>Tue, 26 Oct 2010 10:20:26 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2179</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2136&quot;&gt;David Corfield&lt;/a&gt;.

&lt;blockquote&gt;If it worked, it would be nice. This would also be a way of sidestepping Eric Forgy’s desire for a Lorentzian metric. I don’t see any way to get a Lorentzian metric from a Fisher information metric: the latter is always Riemannian (i.e., positive definite).&lt;/blockquote&gt;

For what its worth, I never suggested you could (or that you should even try to) get a Lorentzian metric from the Fisher information metric. What I suggested is what I put forward &lt;a href=&quot;http://phorgyphynance.wordpress.com/2009/10/11/einstein-meets-markowitz-relativity-theory-of-risk-return/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Nothing more nothing less.

The Fisher information metric defines the spatial component of your Lorentzian metric where &quot;space&quot; is the probabilistic space whose length is standard deviation and cosines of angles are correlations.

The &quot;time&quot; component corresponds to the deterministic part of your observable where length corresponds to the mean. It is most easily seen as a differential

$latex dX = \sigma dW + \mu dt,$

where $latex \sigma$ is standard deviation and $latex \mu$ is mean.

The Fisher information metric sees the &quot;spatial&quot; component $latex g(\sigma dW,\sigma dW) = \sigma^2$. The &quot;temporal&quot; component of the metric sees the deterministic part $latex g(\mu dt,\mu dt) = -\frac{\mu^2}{c^2}$.

The full inner product of your observable is then

$latex g(dX,dX) = \sigma^2 - \frac{\mu^2}{c^2},$

where $latex c$ is a parameter that relates to &quot;risk aversion&quot; in the financial interpretation.

The mathematics is very neat. The algebra gives meaningful results that have clear statistical meaning.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2136">David Corfield</a>.</p>
<blockquote><p>If it worked, it would be nice. This would also be a way of sidestepping Eric Forgy’s desire for a Lorentzian metric. I don’t see any way to get a Lorentzian metric from a Fisher information metric: the latter is always Riemannian (i.e., positive definite).</p></blockquote>
<p>For what its worth, I never suggested you could (or that you should even try to) get a Lorentzian metric from the Fisher information metric. What I suggested is what I put forward <a href="http://phorgyphynance.wordpress.com/2009/10/11/einstein-meets-markowitz-relativity-theory-of-risk-return/" rel="nofollow">here</a>. Nothing more nothing less.</p>
<p>The Fisher information metric defines the spatial component of your Lorentzian metric where &#8220;space&#8221; is the probabilistic space whose length is standard deviation and cosines of angles are correlations.</p>
<p>The &#8220;time&#8221; component corresponds to the deterministic part of your observable where length corresponds to the mean. It is most easily seen as a differential</p>
<p><img src="https://s0.wp.com/latex.php?latex=dX+%3D+%5Csigma+dW+%2B+%5Cmu+dt%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dX = &#92;sigma dW + &#92;mu dt," class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma" class="latex" /> is standard deviation and <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> is mean.</p>
<p>The Fisher information metric sees the &#8220;spatial&#8221; component <img src="https://s0.wp.com/latex.php?latex=g%28%5Csigma+dW%2C%5Csigma+dW%29+%3D+%5Csigma%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(&#92;sigma dW,&#92;sigma dW) = &#92;sigma^2" class="latex" />. The &#8220;temporal&#8221; component of the metric sees the deterministic part <img src="https://s0.wp.com/latex.php?latex=g%28%5Cmu+dt%2C%5Cmu+dt%29+%3D+-%5Cfrac%7B%5Cmu%5E2%7D%7Bc%5E2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(&#92;mu dt,&#92;mu dt) = -&#92;frac{&#92;mu^2}{c^2}" class="latex" />.</p>
<p>The full inner product of your observable is then</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28dX%2CdX%29+%3D+%5Csigma%5E2+-+%5Cfrac%7B%5Cmu%5E2%7D%7Bc%5E2%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(dX,dX) = &#92;sigma^2 - &#92;frac{&#92;mu^2}{c^2}," class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> is a parameter that relates to &#8220;risk aversion&#8221; in the financial interpretation.</p>
<p>The mathematics is very neat. The algebra gives meaningful results that have clear statistical meaning.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2159</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 26 Oct 2010 00:08:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2159</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2145&quot;&gt;Michael Eilenberg&lt;/a&gt;.

I don&#039;t know!  I see what you mean about this problem.  Maybe someone else knows more about solutions.

For &lt;i&gt;This Week&#039;s Finds&lt;/i&gt;, you&#039;ll probably find it easier to print &lt;a href=&quot;http://math.ucr.edu/home/baez/this.week.html&quot; rel=&quot;nofollow&quot;&gt; the version on my website&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2145">Michael Eilenberg</a>.</p>
<p>I don&#8217;t know!  I see what you mean about this problem.  Maybe someone else knows more about solutions.</p>
<p>For <i>This Week&#8217;s Finds</i>, you&#8217;ll probably find it easier to print <a href="http://math.ucr.edu/home/baez/this.week.html" rel="nofollow"> the version on my website</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Michael Eilenberg		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2145</link>

		<dc:creator><![CDATA[Michael Eilenberg]]></dc:creator>
		<pubDate>Mon, 25 Oct 2010 14:34:01 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2145</guid>

					<description><![CDATA[Often, I would like to print posts from this site, but unfortunately the result is always disastrous; at least in Firefox. Typically the output is a union of the first page of {page header, article, comments, page footer}.

I solve it by copy&#038;paste into OpenOffice and then print. Is there a better way?

Michael]]></description>
			<content:encoded><![CDATA[<p>Often, I would like to print posts from this site, but unfortunately the result is always disastrous; at least in Firefox. Typically the output is a union of the first page of {page header, article, comments, page footer}.</p>
<p>I solve it by copy&amp;paste into OpenOffice and then print. Is there a better way?</p>
<p>Michael</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2142</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 25 Oct 2010 13:00:42 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2142</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2136&quot;&gt;David Corfield&lt;/a&gt;.

When someone says something like “The laws of physics could be mere rules for processing information about nature&quot;, the only controversial bit is the word &quot;mere&quot;, which suggests all sorts of deep mysteries, but doesn&#039;t actually explain them.

Caticha&#039;s paper looks interesting, but it&#039;s hard to tell how interesting.  It seeks to describe points in space as &quot;merely&quot; labels for probability distributions, getting a Riemannian metric on space which is &quot;merely&quot; the Fisher information metric.

The fun starts when Caticha cooks up a dynamics that describes how the geometry of space evolves in time.  This is reminiscent of Wheeler&#039;s &quot;geometrodynamics&quot;: a formulation of Einstein&#039;s equation of general relativity that focuses on how the geometry of space changes with time.  Caticha points this out... but alas, leaves open the question of whether his dynamics matches the usual dynamics in general relativity!

If it worked, it would be nice.  This would also be a way of sidestepping Eric Forgy&#039;s desire for a &lt;i&gt;Lorentzian&lt;/i&gt; metric.  I don&#039;t see any way to get a Lorentzian metric from a Fisher information metric: the latter is always Riemannian (i.e., positive definite).

But I think this whole program of getting general relativity from information theory is a bit over-speculative... until someone gets it to work.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2136">David Corfield</a>.</p>
<p>When someone says something like “The laws of physics could be mere rules for processing information about nature&#8221;, the only controversial bit is the word &#8220;mere&#8221;, which suggests all sorts of deep mysteries, but doesn&#8217;t actually explain them.</p>
<p>Caticha&#8217;s paper looks interesting, but it&#8217;s hard to tell how interesting.  It seeks to describe points in space as &#8220;merely&#8221; labels for probability distributions, getting a Riemannian metric on space which is &#8220;merely&#8221; the Fisher information metric.</p>
<p>The fun starts when Caticha cooks up a dynamics that describes how the geometry of space evolves in time.  This is reminiscent of Wheeler&#8217;s &#8220;geometrodynamics&#8221;: a formulation of Einstein&#8217;s equation of general relativity that focuses on how the geometry of space changes with time.  Caticha points this out&#8230; but alas, leaves open the question of whether his dynamics matches the usual dynamics in general relativity!</p>
<p>If it worked, it would be nice.  This would also be a way of sidestepping Eric Forgy&#8217;s desire for a <i>Lorentzian</i> metric.  I don&#8217;t see any way to get a Lorentzian metric from a Fisher information metric: the latter is always Riemannian (i.e., positive definite).</p>
<p>But I think this whole program of getting general relativity from information theory is a bit over-speculative&#8230; until someone gets it to work.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: David Corfield		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comment-2136</link>

		<dc:creator><![CDATA[David Corfield]]></dc:creator>
		<pubDate>Mon, 25 Oct 2010 08:38:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1372#comment-2136</guid>

					<description><![CDATA[With the appearance of informational geometric notions in quantum statistics and machine learning, how should that lead us to answer the question raised in this &lt;a href=&quot;http://golem.ph.utexas.edu/category/2006/12/common_applications.html&quot; rel=&quot;nofollow&quot;&gt;post&lt;/a&gt; as to how much of physical theory is inference? Ariel Caticha, in &lt;a href=&quot;http://arxiv.org/abs/gr-qc/0508108&quot; rel=&quot;nofollow&quot;&gt;The Information Geometry of Space and Time&lt;/a&gt;, goes so far as &quot;The laws of physics could be mere rules for processing information about nature.&quot;]]></description>
			<content:encoded><![CDATA[<p>With the appearance of informational geometric notions in quantum statistics and machine learning, how should that lead us to answer the question raised in this <a href="http://golem.ph.utexas.edu/category/2006/12/common_applications.html" rel="nofollow">post</a> as to how much of physical theory is inference? Ariel Caticha, in <a href="http://arxiv.org/abs/gr-qc/0508108" rel="nofollow">The Information Geometry of Space and Time</a>, goes so far as &#8220;The laws of physics could be mere rules for processing information about nature.&#8221;</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
