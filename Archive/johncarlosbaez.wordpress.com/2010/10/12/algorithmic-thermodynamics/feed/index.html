<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Algorithmic Thermodynamics (Part 1)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/</link>
	<description></description>
	<lastBuildDate>Thu, 08 Aug 2019 04:07:12 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Algorithmic Thermodynamics (Part 3) &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-84989</link>

		<dc:creator><![CDATA[Algorithmic Thermodynamics (Part 3) &#124; Azimuth]]></dc:creator>
		<pubDate>Tue, 15 Nov 2016 23:33:59 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-84989</guid>

					<description><![CDATA[This is my talk for the Santa Fe Institute workshop on &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2016/11/07/information-processing-and-biology/&quot;&gt;Statistical Mechanics, Information Processing and Biology&lt;/a&gt;:

• &lt;a href=&quot;http://math.ucr.edu/home/baez/thermo/thermo_nopause.pdf&quot; rel=&quot;nofollow&quot;&gt;Algorithmic thermodynamics.&lt;/a&gt;

It&#039;s about the link between computation and entropy.  I take the idea of a Turing machine for granted, but starting with that I explain recursive functions, the Church-Turing thesis, Kolomogorov complexity, the relation between Kolmogorov complexity and Shannon entropy, the uncomputability of Kolmogorov complexity, the &#039;complexity barrier&#039;, Levin&#039;s computable version of complexity, and finally my work with Mike Stay on algorithmic thermodynamics.

For more details, read our paper:

• John Baez and Mike Stay, &lt;a href=&quot;http://arxiv.org/abs/1010.2067&quot; rel=&quot;nofollow&quot;&gt;Algorithmic thermodynamics&lt;/a&gt;, &lt;i&gt;Math. Struct. Comp. Sci.&lt;/i&gt; &lt;b&gt;22&lt;/b&gt; (2012), 771-787.

or these blog articles:

•  &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/&quot;&gt;Algorithmic thermodynamics (part 1)&lt;/a&gt;.

• &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/&quot;&gt;Algorithmic thermodynamics (part 2)&lt;/a&gt;.

They all emphasize slightly different aspects!]]></description>
			<content:encoded><![CDATA[<p>This is my talk for the Santa Fe Institute workshop on <a href="https://johncarlosbaez.wordpress.com/2016/11/07/information-processing-and-biology/">Statistical Mechanics, Information Processing and Biology</a>:</p>
<p>• <a href="http://math.ucr.edu/home/baez/thermo/thermo_nopause.pdf" rel="nofollow">Algorithmic thermodynamics.</a></p>
<p>It&#8217;s about the link between computation and entropy.  I take the idea of a Turing machine for granted, but starting with that I explain recursive functions, the Church-Turing thesis, Kolomogorov complexity, the relation between Kolmogorov complexity and Shannon entropy, the uncomputability of Kolmogorov complexity, the &#8216;complexity barrier&#8217;, Levin&#8217;s computable version of complexity, and finally my work with Mike Stay on algorithmic thermodynamics.</p>
<p>For more details, read our paper:</p>
<p>• John Baez and Mike Stay, <a href="http://arxiv.org/abs/1010.2067" rel="nofollow">Algorithmic thermodynamics</a>, <i>Math. Struct. Comp. Sci.</i> <b>22</b> (2012), 771-787.</p>
<p>or these blog articles:</p>
<p>•  <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">Algorithmic thermodynamics (part 1)</a>.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/">Algorithmic thermodynamics (part 2)</a>.</p>
<p>They all emphasize slightly different aspects!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Algorithmic Thermodynamics (Part 2) &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-3336</link>

		<dc:creator><![CDATA[Algorithmic Thermodynamics (Part 2) &#171; Azimuth]]></dc:creator>
		<pubDate>Thu, 06 Jan 2011 10:14:46 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-3336</guid>

					<description><![CDATA[[...] are my lecture notes for a talk at the CQT. You can think of this as a followup to my first post on this subject, though it overlaps a lot with that previous [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] are my lecture notes for a talk at the CQT. You can think of this as a followup to my first post on this subject, though it overlaps a lot with that previous [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 2) &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2118</link>

		<dc:creator><![CDATA[Information Geometry (Part 2) &#171; Azimuth]]></dc:creator>
		<pubDate>Sat, 23 Oct 2010 12:59:47 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-2118</guid>

					<description><![CDATA[[...] This shouldn&#8217;t be surprising: after all, we&#8217;re talking about maximum entropy, but entropy is related to information. But I want to gradually make this idea more [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] This shouldn&#8217;t be surprising: after all, we&#8217;re talking about maximum entropy, but entropy is related to information. But I want to gradually make this idea more [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2100</link>

		<dc:creator><![CDATA[Information Geometry &#171; Azimuth]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 10:37:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-2100</guid>

					<description><![CDATA[[...] where we take the logarithm of a positive operator just by taking the log of each of its eigenvalues, while keeping the same eigenvectors. This formula for entropy should remind you of the one that Gibbs and Shannon used &#8212; the one I explained a while back. [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] where we take the logarithm of a positive operator just by taking the log of each of its eigenvalues, while keeping the same eigenvectors. This formula for entropy should remind you of the one that Gibbs and Shannon used &mdash; the one I explained a while back. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: David Ellerman		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2044</link>

		<dc:creator><![CDATA[David Ellerman]]></dc:creator>
		<pubDate>Tue, 19 Oct 2010 16:30:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-2044</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2034&quot;&gt;John Baez&lt;/a&gt;.

I like the quote and wish it was directly from Shannon! The idea that information is essentially about differences is developed in the simple notion of &quot;logical entropy&quot; or &quot;logical information content&quot; of a partition. One can develop an extended analogy between the notion of &quot;elements of a subset&quot; and &quot;distinctions of a partition&quot; where a &quot;distinction&quot; is a pair of elements from distinct blocks of a partition. Then just as finite probability theory was started by taking the probability assigned to a subset as the normalized number of elements, so one can take the logical entropy of a partition on a finite set as the relative (or normalized) number of distinctions of the partition. The logical entropy formula: $latex h(\pi) = \sum_{B \in \pi}p_{B}(1-p_{B}) = 1-\sum_{B \in \pi}p_{B}^{2}$ has a long history summarized in my blog at: http://www.mathblog.ellerman.org/2010/02/history-of-the-logical-entropy-formula/ . The longer Synthese paper giving the precise connections with Shannon&#039;s notion of entropy is available at: http://www.ellerman.org/Davids-Stuff/Maths/Counting-Dits-reprint.pdf. Also Rao defined a notion of quadratic entropy which depends on some &quot;distance function&quot; and when one defines the logical distance between two points as being 0 or 1 depending on them being in the same or different blocks of a partition, then the quadratic entropy of the partition is the logical entropy.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2034">John Baez</a>.</p>
<p>I like the quote and wish it was directly from Shannon! The idea that information is essentially about differences is developed in the simple notion of &#8220;logical entropy&#8221; or &#8220;logical information content&#8221; of a partition. One can develop an extended analogy between the notion of &#8220;elements of a subset&#8221; and &#8220;distinctions of a partition&#8221; where a &#8220;distinction&#8221; is a pair of elements from distinct blocks of a partition. Then just as finite probability theory was started by taking the probability assigned to a subset as the normalized number of elements, so one can take the logical entropy of a partition on a finite set as the relative (or normalized) number of distinctions of the partition. The logical entropy formula: <img src="https://s0.wp.com/latex.php?latex=h%28%5Cpi%29+%3D+%5Csum_%7BB+%5Cin+%5Cpi%7Dp_%7BB%7D%281-p_%7BB%7D%29+%3D+1-%5Csum_%7BB+%5Cin+%5Cpi%7Dp_%7BB%7D%5E%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h(&#92;pi) = &#92;sum_{B &#92;in &#92;pi}p_{B}(1-p_{B}) = 1-&#92;sum_{B &#92;in &#92;pi}p_{B}^{2}" class="latex" /> has a long history summarized in my blog at: <a href="http://www.mathblog.ellerman.org/2010/02/history-of-the-logical-entropy-formula/" rel="nofollow ugc">http://www.mathblog.ellerman.org/2010/02/history-of-the-logical-entropy-formula/</a> . The longer Synthese paper giving the precise connections with Shannon&#8217;s notion of entropy is available at: <a href="http://www.ellerman.org/Davids-Stuff/Maths/Counting-Dits-reprint.pdf" rel="nofollow ugc">http://www.ellerman.org/Davids-Stuff/Maths/Counting-Dits-reprint.pdf</a>. Also Rao defined a notion of quadratic entropy which depends on some &#8220;distance function&#8221; and when one defines the logical distance between two points as being 0 or 1 depending on them being in the same or different blocks of a partition, then the quadratic entropy of the partition is the logical entropy.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Entropy and Uncertainty &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2035</link>

		<dc:creator><![CDATA[Entropy and Uncertainty &#171; Azimuth]]></dc:creator>
		<pubDate>Tue, 19 Oct 2010 07:17:22 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-2035</guid>

					<description><![CDATA[[...] explained entropy back here, so let me say a word about the uncertainty principle. It&#8217;s a limitation on how accurately [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] explained entropy back here, so let me say a word about the uncertainty principle. It&#8217;s a limitation on how accurately [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2034</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 19 Oct 2010 05:44:40 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-2034</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2031&quot;&gt;David Ellerman&lt;/a&gt;.

Hi, David!  If you click on that picture of Shannon above, you&#039;ll see my source for that quote.  But it&#039;s not a very scholarly source... and indeed, a quick Google tour suggests the quote may be due to Bateson.

So, I&#039;ll remove that quote, and hope the other one is really due to Shannon.  Certainly Shannon had some ideas along these general lines...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2031">David Ellerman</a>.</p>
<p>Hi, David!  If you click on that picture of Shannon above, you&#8217;ll see my source for that quote.  But it&#8217;s not a very scholarly source&#8230; and indeed, a quick Google tour suggests the quote may be due to Bateson.</p>
<p>So, I&#8217;ll remove that quote, and hope the other one is really due to Shannon.  Certainly Shannon had some ideas along these general lines&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: David Ellerman		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-2031</link>

		<dc:creator><![CDATA[David Ellerman]]></dc:creator>
		<pubDate>Mon, 18 Oct 2010 23:07:36 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-2031</guid>

					<description><![CDATA[&quot;Information is any difference that makes a difference.&quot;
This is referred to as a quote from Shannon. I would be glad to find it in Shannon&#039;s writings but I think it actually comes from Gregory Bateson. For instance, in Bateson, Gregory 1972. Steps to an Ecology of Mind. New York: Ballantine, he says: &quot;A &#039;bit&#039; of information is defined as a difference which makes a difference.&quot; (p. 315), or in Bateson, Gregory 1979. Mind and nature: A necessary unity. New York: Dutton, he says: &quot;Information consists of differences that make a difference.&quot; (p. 110).]]></description>
			<content:encoded><![CDATA[<p>&#8220;Information is any difference that makes a difference.&#8221;<br />
This is referred to as a quote from Shannon. I would be glad to find it in Shannon&#8217;s writings but I think it actually comes from Gregory Bateson. For instance, in Bateson, Gregory 1972. Steps to an Ecology of Mind. New York: Ballantine, he says: &#8220;A &#8216;bit&#8217; of information is defined as a difference which makes a difference.&#8221; (p. 315), or in Bateson, Gregory 1979. Mind and nature: A necessary unity. New York: Dutton, he says: &#8220;Information consists of differences that make a difference.&#8221; (p. 110).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-1916</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 14 Oct 2010 04:08:21 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-1916</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-1898&quot;&gt;John Baez&lt;/a&gt;.

Tim wrote:

&lt;blockquote&gt;
Maybe, but last time I did not understand what a “prefix-free Turing machine” is :-)
&lt;/blockquote&gt;

The definition is in the paper; I bet you just didn&#039;t know why it&#039;s interesting. And it&#039;s actually not very interesting  &#8212; it&#039;s just a useful trick.

There are lots of ways to list programs.  We say such a way is &quot;prefix-free&quot; if the name of one program that halts never appears as the initial part of the name of another one that halts.  For example, you can&#039;t have a program that halts named

01010

if you also have one named

01010110

It&#039;s easy to accomplish this: just require that every program ends with a string that means &quot;END&quot;, say for example 1111, and require that this string only shows up at the end of the program.  So, you might have a program named

0010101111

but then you can&#039;t have one named

0010101111001111

None of this stuff is very important or profound.  It&#039;s just a standard trick for defining a &quot;probability&quot; for programs!  Suppose the probability of a program is 2&lt;sup&gt;&lt;/sup&gt;&lt;sup&gt;-N&lt;/sup&gt; when its name has N bits.  Then &lt;i&gt;if the naming system is prefix-free&lt;/i&gt;, the sum of the probabilities of all programs that halt is &#8804; 1.  You can imagine flipping a coin until the bit string it generates is the name of a program, and the chance you got that sequence of coin flips is the measure of your program.  

(There may also be cases where you keep flipping the coin and never get a name of a program that halts.)

(You&#039;ll note that sometimes I say &quot;program that halts&quot; and sometimes I say &quot;program&quot;.  Again, this is just technical baloney.)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-1898">John Baez</a>.</p>
<p>Tim wrote:</p>
<blockquote><p>
Maybe, but last time I did not understand what a “prefix-free Turing machine” is :-)
</p></blockquote>
<p>The definition is in the paper; I bet you just didn&#8217;t know why it&#8217;s interesting. And it&#8217;s actually not very interesting  &mdash; it&#8217;s just a useful trick.</p>
<p>There are lots of ways to list programs.  We say such a way is &#8220;prefix-free&#8221; if the name of one program that halts never appears as the initial part of the name of another one that halts.  For example, you can&#8217;t have a program that halts named</p>
<p>01010</p>
<p>if you also have one named</p>
<p>01010110</p>
<p>It&#8217;s easy to accomplish this: just require that every program ends with a string that means &#8220;END&#8221;, say for example 1111, and require that this string only shows up at the end of the program.  So, you might have a program named</p>
<p>0010101111</p>
<p>but then you can&#8217;t have one named</p>
<p>0010101111001111</p>
<p>None of this stuff is very important or profound.  It&#8217;s just a standard trick for defining a &#8220;probability&#8221; for programs!  Suppose the probability of a program is 2<sup></sup><sup>-N</sup> when its name has N bits.  Then <i>if the naming system is prefix-free</i>, the sum of the probabilities of all programs that halt is &le; 1.  You can imagine flipping a coin until the bit string it generates is the name of a program, and the chance you got that sequence of coin flips is the measure of your program.  </p>
<p>(There may also be cases where you keep flipping the coin and never get a name of a program that halts.)</p>
<p>(You&#8217;ll note that sometimes I say &#8220;program that halts&#8221; and sometimes I say &#8220;program&#8221;.  Again, this is just technical baloney.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Tim van Beek		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-1906</link>

		<dc:creator><![CDATA[Tim van Beek]]></dc:creator>
		<pubDate>Wed, 13 Oct 2010 09:01:17 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1238#comment-1906</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-1898&quot;&gt;John Baez&lt;/a&gt;.

&lt;blockquote&gt;

Perhaps part of the problem is that I’m trying to explain these concepts in everyday language, instead of using jargon like “prefix-free Turing machines”.

&lt;/blockquote&gt;
Maybe, but &lt;a href=&quot;http://golem.ph.utexas.edu/category/2010/02/algorithmic_thermodynamics.html#c031771&quot; rel=&quot;nofollow&quot;&gt;last time&lt;/a&gt; I did not understand what a &quot;prefix-free Turing machine&quot; is :-)

But this paper of Mike Stay helped me a lot in this regard:
&lt;a href=&quot;http://arxiv.org/abs/cs/0508056&quot; rel=&quot;nofollow&quot;&gt;Very Simple Chaitin Machines for Concrete AIT&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comment-1898">John Baez</a>.</p>
<blockquote>
<p>Perhaps part of the problem is that I’m trying to explain these concepts in everyday language, instead of using jargon like “prefix-free Turing machines”.</p>
</blockquote>
<p>Maybe, but <a href="http://golem.ph.utexas.edu/category/2010/02/algorithmic_thermodynamics.html#c031771" rel="nofollow">last time</a> I did not understand what a &#8220;prefix-free Turing machine&#8221; is :-)</p>
<p>But this paper of Mike Stay helped me a lot in this regard:<br />
<a href="http://arxiv.org/abs/cs/0508056" rel="nofollow">Very Simple Chaitin Machines for Concrete AIT</a>.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
