<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Entropy and Uncertainty	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/</link>
	<description></description>
	<lastBuildDate>Sat, 06 Nov 2010 08:17:40 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: john e gray		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2413</link>

		<dc:creator><![CDATA[john e gray]]></dc:creator>
		<pubDate>Sat, 06 Nov 2010 08:17:40 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2413</guid>

					<description><![CDATA[This misses reference to pioneering work by Weinhold that I first read about when I started reading Physics Today, found in the introductory article in Physics Today: 

&#8226; F. Weinhold, Geometry and Thermodynamics, Physics Today 29(3), 23-30 (1976). 

The pioneering insight was the usage of concavity found in the  property equations of state surface in a geometric fashion. Riemann was introduced by Gilmore (Drexel Univ) extended this to Reimannian geometry in the 80&#039;s. A number of papers can be found on his website:

http://www.physics.drexel.edu/~bob/Thermodynamics_pgm.html

People interested in this subject can find much of interest on this webpage.]]></description>
			<content:encoded><![CDATA[<p>This misses reference to pioneering work by Weinhold that I first read about when I started reading Physics Today, found in the introductory article in Physics Today: </p>
<p>&bull; F. Weinhold, Geometry and Thermodynamics, Physics Today 29(3), 23-30 (1976). </p>
<p>The pioneering insight was the usage of concavity found in the  property equations of state surface in a geometric fashion. Riemann was introduced by Gilmore (Drexel Univ) extended this to Reimannian geometry in the 80&#8217;s. A number of papers can be found on his website:</p>
<p><a href="http://www.physics.drexel.edu/~bob/Thermodynamics_pgm.html" rel="nofollow ugc">http://www.physics.drexel.edu/~bob/Thermodynamics_pgm.html</a></p>
<p>People interested in this subject can find much of interest on this webpage.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Giampiero Campa		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2121</link>

		<dc:creator><![CDATA[Giampiero Campa]]></dc:creator>
		<pubDate>Sat, 23 Oct 2010 19:25:03 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2121</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2057&quot;&gt;John Baez&lt;/a&gt;.

I think that from either a conceptual or physical standpoint unknown states like U an X are neither states nor values, just a way or representing uncertainty, so i am not sure that they should be treated as equal to 0 or 1.

Similarly, the don&#039;t care value, -, is a characteristic of a given function or truth table, so it definitely does not belong there.

Distinguishing Z from W is really a subtlety, while H and L might actually be useful, even if i still believe that grouping HiZ states under Z is fine for most purposes.

Anyway, Z is probably hard to detect, so it&#039;s not practical to use Z to either store or transmit information, and therefore we are &quot;stuck&quot; with binary logic for practical purposes.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2057">John Baez</a>.</p>
<p>I think that from either a conceptual or physical standpoint unknown states like U an X are neither states nor values, just a way or representing uncertainty, so i am not sure that they should be treated as equal to 0 or 1.</p>
<p>Similarly, the don&#8217;t care value, -, is a characteristic of a given function or truth table, so it definitely does not belong there.</p>
<p>Distinguishing Z from W is really a subtlety, while H and L might actually be useful, even if i still believe that grouping HiZ states under Z is fine for most purposes.</p>
<p>Anyway, Z is probably hard to detect, so it&#8217;s not practical to use Z to either store or transmit information, and therefore we are &#8220;stuck&#8221; with binary logic for practical purposes.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2112</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 23 Oct 2010 01:06:27 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2112</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2097&quot;&gt;Tim van Beek&lt;/a&gt;.

As for the last two sentence in the text you quote: the author is apparently defining a metric space to be &lt;b&gt;intrinsic&lt;/b&gt; if for any two points a and c there exists a &lt;b&gt;midpoint&lt;/b&gt;, meaning a point b with

 d(a, b) + d(b, c) = d(a, c).

And then he&#039;s claiming that given any metric space with metric d, we can define a new metric d&#039; that is intrinsic, by letting d&#039;(a,b) be the infimum of the &quot;arclengths&quot; of continuous paths from a to b.  I&#039;m not sure, but I think I see a reasonable way to define the &quot;arclength&quot; of a continuous path in any metric space (which could, of course, be infinite).  

And I think he&#039;s also hinting that for an intrinsic metric space, the metric d&#039; is the same as the metric d.

In other words: for an intrinsic metric space, the distance between two points equals the infimum of the arclengths of all continuous paths from one point to another.  This is pretty easy to see: just take your two points and repeatedly take choose midpoints to create, in the limit, a continuous path from one point to the other, whose arclength is the distance between these points!

He is saying all this a fairly concise way. &lt;img src=&quot;http://math.ucr.edu/home/baez/emoticons/rolleyes.gif&quot; alt=&quot;&quot; /&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2097">Tim van Beek</a>.</p>
<p>As for the last two sentence in the text you quote: the author is apparently defining a metric space to be <b>intrinsic</b> if for any two points a and c there exists a <b>midpoint</b>, meaning a point b with</p>
<p> d(a, b) + d(b, c) = d(a, c).</p>
<p>And then he&#8217;s claiming that given any metric space with metric d, we can define a new metric d&#8217; that is intrinsic, by letting d'(a,b) be the infimum of the &#8220;arclengths&#8221; of continuous paths from a to b.  I&#8217;m not sure, but I think I see a reasonable way to define the &#8220;arclength&#8221; of a continuous path in any metric space (which could, of course, be infinite).  </p>
<p>And I think he&#8217;s also hinting that for an intrinsic metric space, the metric d&#8217; is the same as the metric d.</p>
<p>In other words: for an intrinsic metric space, the distance between two points equals the infimum of the arclengths of all continuous paths from one point to another.  This is pretty easy to see: just take your two points and repeatedly take choose midpoints to create, in the limit, a continuous path from one point to the other, whose arclength is the distance between these points!</p>
<p>He is saying all this a fairly concise way. <img src="https://i2.wp.com/math.ucr.edu/home/baez/emoticons/rolleyes.gif" alt="" /></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John F		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2106</link>

		<dc:creator><![CDATA[John F]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 15:14:40 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2106</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092&quot;&gt;John F&lt;/a&gt;.

Tim,

&lt;i&gt;numerically&lt;/i&gt; a divergence is simply a monotonic nonnegative measure of (usually highly multidimensional) difference, and can&#039;t necessarily be massaged into a norm or anything. Divergences are mostly commonly used as class separability indicators, for instance in hyperspectral geospatial imaging applications. The most useful divergence implemented in most GIS software is called Transform Divergence, which despite its vague name is fast to calculate. The square root is &lt;i&gt;numerically&lt;/i&gt; the square root.

The Jensen-Shannon metric is a metric that doesn&#039;t require coordinatizing to define. I think the last sentence is saying that it could be coordinatized if you wanted to.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092">John F</a>.</p>
<p>Tim,</p>
<p><i>numerically</i> a divergence is simply a monotonic nonnegative measure of (usually highly multidimensional) difference, and can&#8217;t necessarily be massaged into a norm or anything. Divergences are mostly commonly used as class separability indicators, for instance in hyperspectral geospatial imaging applications. The most useful divergence implemented in most GIS software is called Transform Divergence, which despite its vague name is fast to calculate. The square root is <i>numerically</i> the square root.</p>
<p>The Jensen-Shannon metric is a metric that doesn&#8217;t require coordinatizing to define. I think the last sentence is saying that it could be coordinatized if you wanted to.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Sniffnoy		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2104</link>

		<dc:creator><![CDATA[Sniffnoy]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 13:43:23 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2104</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2102&quot;&gt;Sniffnoy&lt;/a&gt;.

Oops, I missed the &quot;radix economy&quot; note.  Which has of course the same reason - 3 being the integer maximizing $latex n^{1/n}$, or minimizing n/(log n)...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2102">Sniffnoy</a>.</p>
<p>Oops, I missed the &#8220;radix economy&#8221; note.  Which has of course the same reason &#8211; 3 being the integer maximizing <img src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2Fn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n^{1/n}" class="latex" />, or minimizing n/(log n)&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Sniffnoy		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2102</link>

		<dc:creator><![CDATA[Sniffnoy]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 13:03:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2102</guid>

					<description><![CDATA[Regarding ternary: I&#039;ve been doing some stuff with a kind of terrible sequence known as &lt;a href=&quot;http://www.research.att.com/~njas/sequences/A005245&quot; rel=&quot;nofollow&quot;&gt;integer complexity&lt;/a&gt; and the fact that 3 is closer to e than 2 comes up there.  I&#039;ll use &#124;&#124;n&#124;&#124; to denote the complexity of n, the smallest number of 1s needed to write n using addition and multiplication - well, you can go see the Sloane entry for yourself.  This fact isn&#039;t relevant for finding an upper bound - the upper bound based on binary representation is better than the one based on ternary.  But if you kind of invert the question and ask, I have k ones, what&#039;s the largest number I can make with them using addition and multiplication?  Well ideally you&#039;d group them into groups of e and multiply those together to get e^(k/e), but seeing as that doesn&#039;t make a lot of sense, the actual maximum is 3^(k/3).  (If k is divisible by 3, anyway; otherwise you have a group of 2 or 4 left over.)  So you get &#124;&#124;n&#124;&#124;&#062;=3log_3(n).  And this fact leads to 3 being nice in other ways with respect to this sequence, which results in you ending up looking at ternary representations after all for a bunch of things...]]></description>
			<content:encoded><![CDATA[<p>Regarding ternary: I&#8217;ve been doing some stuff with a kind of terrible sequence known as <a href="http://www.research.att.com/~njas/sequences/A005245" rel="nofollow">integer complexity</a> and the fact that 3 is closer to e than 2 comes up there.  I&#8217;ll use ||n|| to denote the complexity of n, the smallest number of 1s needed to write n using addition and multiplication &#8211; well, you can go see the Sloane entry for yourself.  This fact isn&#8217;t relevant for finding an upper bound &#8211; the upper bound based on binary representation is better than the one based on ternary.  But if you kind of invert the question and ask, I have k ones, what&#8217;s the largest number I can make with them using addition and multiplication?  Well ideally you&#8217;d group them into groups of e and multiply those together to get e^(k/e), but seeing as that doesn&#8217;t make a lot of sense, the actual maximum is 3^(k/3).  (If k is divisible by 3, anyway; otherwise you have a group of 2 or 4 left over.)  So you get ||n||&gt;=3log_3(n).  And this fact leads to 3 being nice in other ways with respect to this sequence, which results in you ending up looking at ternary representations after all for a bunch of things&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry &#171; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2099</link>

		<dc:creator><![CDATA[Information Geometry &#171; Azimuth]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 10:37:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2099</guid>

					<description><![CDATA[[...] was pointed out by John F in our discussion of entropy and [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] was pointed out by John F in our discussion of entropy and [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Tim van Beek		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2097</link>

		<dc:creator><![CDATA[Tim van Beek]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 07:52:29 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2097</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092&quot;&gt;John F&lt;/a&gt;.

Aha!

(What&#039;s &quot;fractional distillation&quot;?)

But I&#039;m still getting confused by statements like this one:

&lt;blockquote&gt;

The square root of the
Jensen-Shannon divergence is a metric between probability
distributions [25]. However, unlike a Riemannian
metric, the Jensen-Shannon metric space is not an intrinsic
length space. There may not be a mid point b between
points a and c such that d(a, b) + d(b, c) = d(a, c)
and consequentially we cannot naturally measure path
lengths. However, on any metric space we can define a
new intrinsic metric by measuring the distance along continuous
paths.

&lt;/blockquote&gt;
How does the author define &quot;divergence&quot; and &quot;square root of a divergence&quot;?

And what do the last two sentences mean?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092">John F</a>.</p>
<p>Aha!</p>
<p>(What&#8217;s &#8220;fractional distillation&#8221;?)</p>
<p>But I&#8217;m still getting confused by statements like this one:</p>
<blockquote>
<p>The square root of the<br />
Jensen-Shannon divergence is a metric between probability<br />
distributions [25]. However, unlike a Riemannian<br />
metric, the Jensen-Shannon metric space is not an intrinsic<br />
length space. There may not be a mid point b between<br />
points a and c such that d(a, b) + d(b, c) = d(a, c)<br />
and consequentially we cannot naturally measure path<br />
lengths. However, on any metric space we can define a<br />
new intrinsic metric by measuring the distance along continuous<br />
paths.</p>
</blockquote>
<p>How does the author define &#8220;divergence&#8221; and &#8220;square root of a divergence&#8221;?</p>
<p>And what do the last two sentences mean?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: westy31		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2096</link>

		<dc:creator><![CDATA[westy31]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 07:39:21 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2096</guid>

					<description><![CDATA[John wote:

&lt;blockquote&gt;

are there some applications where ternary digits would theoretically be better than binary ones?

&lt;/blockquote&gt;

One is the most efficient way to weigh something with a balance.

If you put the object to be weighed on one side, and then weights on the other side, then binary weights (1,2,4,8...) are the most efficient, they use the least weights. But if you use both sides of the balance, subtracting the weights put together with the weighed objects, then ternary weights (1,3,9,27,...) are the most efficient. You can weigh anything upto 13 with just 3 weights:

    * 1=1
    * 2=3-1
    * 3=3
    * 4=3+1
    * 5=9-3-1
    * 6=9-3
    * 7=9-3+1
    * 8 =9-1
    * 9=9
    * 10=9+1
    * 11=9+3-1
    * 12=9+3
    * 13=9+3+1

Similarly, coins of 1,3,8,27,.. are the most efficient way of paying if you allow the seller to give change. (Binary is the best if you don&#039;t allow change)

Gerard]]></description>
			<content:encoded><![CDATA[<p>John wote:</p>
<blockquote>
<p>are there some applications where ternary digits would theoretically be better than binary ones?</p>
</blockquote>
<p>One is the most efficient way to weigh something with a balance.</p>
<p>If you put the object to be weighed on one side, and then weights on the other side, then binary weights (1,2,4,8&#8230;) are the most efficient, they use the least weights. But if you use both sides of the balance, subtracting the weights put together with the weighed objects, then ternary weights (1,3,9,27,&#8230;) are the most efficient. You can weigh anything upto 13 with just 3 weights:</p>
<p>    * 1=1<br />
    * 2=3-1<br />
    * 3=3<br />
    * 4=3+1<br />
    * 5=9-3-1<br />
    * 6=9-3<br />
    * 7=9-3+1<br />
    * 8 =9-1<br />
    * 9=9<br />
    * 10=9+1<br />
    * 11=9+3-1<br />
    * 12=9+3<br />
    * 13=9+3+1</p>
<p>Similarly, coins of 1,3,8,27,.. are the most efficient way of paying if you allow the seller to give change. (Binary is the best if you don&#8217;t allow change)</p>
<p>Gerard</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2094</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 22 Oct 2010 00:53:50 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=1308#comment-2094</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092&quot;&gt;John F&lt;/a&gt;.

Aha!  

Very, very nice &#8212; thanks!  This helps immensely.

This article by Crooks should be readable to anyone who is comfortable with the usual statistical mechanics game of computing the means, variances, and higher moments of observables by repeatedly differentiating the logarithm of the partition function.  And you don&#039;t need a subscription to Phys. Rev. Lett. to read it!  It&#039;s free here:

&#8226; Gavin E. Crooks, &lt;a href=&quot;http://arxiv.org/abs/0706.0559&quot; rel=&quot;nofollow&quot;&gt;Measuring thermodynamic length&lt;/a&gt;, &lt;br /&gt; arXiv:0706.0559.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092">John F</a>.</p>
<p>Aha!  </p>
<p>Very, very nice &mdash; thanks!  This helps immensely.</p>
<p>This article by Crooks should be readable to anyone who is comfortable with the usual statistical mechanics game of computing the means, variances, and higher moments of observables by repeatedly differentiating the logarithm of the partition function.  And you don&#8217;t need a subscription to Phys. Rev. Lett. to read it!  It&#8217;s free here:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>, <br /> arXiv:0706.0559.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
