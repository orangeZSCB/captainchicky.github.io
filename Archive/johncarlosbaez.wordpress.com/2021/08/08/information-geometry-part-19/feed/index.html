<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information Geometry (Part 19)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/</link>
	<description></description>
	<lastBuildDate>Sun, 15 Aug 2021 18:54:21 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171574</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 15 Aug 2021 18:54:21 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171574</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171544&quot;&gt;Emmy Blumenthal&lt;/a&gt;.

One important thing you can compute from a probability distribution is its &lt;a href=&quot;https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy&quot; rel=&quot;nofollow ugc&quot;&gt;Rényi entropy&lt;/a&gt;: a generalization of Shannon entropy that depends on a real-valued parameter.  I suspect that if you know the Rényi entropy of a probability distribution $latex (q_1, \dots, q_n)$ for all values of thid parameter, you can recover the numbers $latex q_i$ and how many times each one appears in the list---but not the order of the list.

Maybe this is an example of the kind of thing you&#039;re curious about.  Computing the Rényi entropy of a probability distribution is a bit like taking the Fourier or Laplace transform of a function, but different.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171544">Emmy Blumenthal</a>.</p>
<p>One important thing you can compute from a probability distribution is its <a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy" rel="nofollow ugc">Rényi entropy</a>: a generalization of Shannon entropy that depends on a real-valued parameter.  I suspect that if you know the Rényi entropy of a probability distribution <img src="https://s0.wp.com/latex.php?latex=%28q_1%2C+%5Cdots%2C+q_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1, &#92;dots, q_n)" class="latex" /> for all values of thid parameter, you can recover the numbers <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and how many times each one appears in the list&#8212;but not the order of the list.</p>
<p>Maybe this is an example of the kind of thing you&#8217;re curious about.  Computing the Rényi entropy of a probability distribution is a bit like taking the Fourier or Laplace transform of a function, but different.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Emmy Blumenthal		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171544</link>

		<dc:creator><![CDATA[Emmy Blumenthal]]></dc:creator>
		<pubDate>Sat, 14 Aug 2021 16:43:49 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171544</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347&quot;&gt;Emmy B&lt;/a&gt;.

I am happy to clarify what I meant; my question was a bit imprecise and more conceptual, and I am not too fixated on the Fourier transform here. In this framework, a probability distribution on some finite set is described by some given sequence of numbers, $latex (q_1,q_2,\dots,q_n)$, but—as was noted in your lovely replies—the order of these particular numbers do not matter. That is to say that the same physical/statistical/mathematical information is expressed no matter the order. However, to compare two given permutations of lists of numbers supposedly specifying the same information/distribution, we must know the appropriate permutation to relate them and verify if they do indeed specify the same information. A similar argument can be made about many other ways of communicating and restructuring the list $latex (q_1,q_2\dots,q_n)$. As long as there is some sort of consistent and well-designed (very vague I know…) translation between the two ways of expressing the distribution, any two expressions of some distribution can appropriately be expressed in a myriad of forms. Philosophically, this reminds us that the list of numbers is not the distribution nor physical information in question but is a representation that is meaningful when put in mathematical context. In my original post, I tried to use the Fourier transform as an example of how this data may be expressed.

I point this out as a naive undergraduate because my intuition tells me there is some connection between these concepts specifically the question of uniting different physical perspectives and category theory. Specifically, the idea of translating between individual representations of the same data reminds me vaguely of category theory, and I’m curious if there is any reading that you recommend in order to better understand and explore such a connection.
Best,
Emmy Blumenthal (they/them)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347">Emmy B</a>.</p>
<p>I am happy to clarify what I meant; my question was a bit imprecise and more conceptual, and I am not too fixated on the Fourier transform here. In this framework, a probability distribution on some finite set is described by some given sequence of numbers, <img src="https://s0.wp.com/latex.php?latex=%28q_1%2Cq_2%2C%5Cdots%2Cq_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1,q_2,&#92;dots,q_n)" class="latex" />, but—as was noted in your lovely replies—the order of these particular numbers do not matter. That is to say that the same physical/statistical/mathematical information is expressed no matter the order. However, to compare two given permutations of lists of numbers supposedly specifying the same information/distribution, we must know the appropriate permutation to relate them and verify if they do indeed specify the same information. A similar argument can be made about many other ways of communicating and restructuring the list <img src="https://s0.wp.com/latex.php?latex=%28q_1%2Cq_2%5Cdots%2Cq_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1,q_2&#92;dots,q_n)" class="latex" />. As long as there is some sort of consistent and well-designed (very vague I know…) translation between the two ways of expressing the distribution, any two expressions of some distribution can appropriately be expressed in a myriad of forms. Philosophically, this reminds us that the list of numbers is not the distribution nor physical information in question but is a representation that is meaningful when put in mathematical context. In my original post, I tried to use the Fourier transform as an example of how this data may be expressed.</p>
<p>I point this out as a naive undergraduate because my intuition tells me there is some connection between these concepts specifically the question of uniting different physical perspectives and category theory. Specifically, the idea of translating between individual representations of the same data reminds me vaguely of category theory, and I’m curious if there is any reading that you recommend in order to better understand and explore such a connection.<br />
Best,<br />
Emmy Blumenthal (they/them)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171525</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 13 Aug 2021 23:06:10 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171525</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171514&quot;&gt;John Baez&lt;/a&gt;.

That&#039;s not a dumb question.  Physicists act like the amplitude for each path has the same magnitude, and they rarely think about complex-valued Lagrangians.

On the other hand, there are great subtleties involved in defining the correct measure when you integrate over all paths---since it&#039;s rarely a finite or even countably infinite &lt;i&gt;sum&lt;/i&gt; over all paths.  Path integrals are often integrals over infinite-dimensional spaces, and we just barely understand what a measure means in this case: it&#039;s one of the main problems in quantum field theory.  Even in the finite-dimensional case, getting the &#039;right&#039; measure is nontrivial.

But changing the measure in your path integral is equivalent to changing the magnitude of the paths&#039; amplitudes!  And this in turn is equivalent to making the Lagrangian complex.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171514">John Baez</a>.</p>
<p>That&#8217;s not a dumb question.  Physicists act like the amplitude for each path has the same magnitude, and they rarely think about complex-valued Lagrangians.</p>
<p>On the other hand, there are great subtleties involved in defining the correct measure when you integrate over all paths&#8212;since it&#8217;s rarely a finite or even countably infinite <i>sum</i> over all paths.  Path integrals are often integrals over infinite-dimensional spaces, and we just barely understand what a measure means in this case: it&#8217;s one of the main problems in quantum field theory.  Even in the finite-dimensional case, getting the &#8216;right&#8217; measure is nontrivial.</p>
<p>But changing the measure in your path integral is equivalent to changing the magnitude of the paths&#8217; amplitudes!  And this in turn is equivalent to making the Lagrangian complex.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: geoenergymath		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171524</link>

		<dc:creator><![CDATA[geoenergymath]]></dc:creator>
		<pubDate>Fri, 13 Aug 2021 21:15:35 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171524</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347&quot;&gt;Emmy B&lt;/a&gt;.

OK, I thought I understood what Emmy was asking but if not I will let her clarify if she ever sees this comment thread again.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347">Emmy B</a>.</p>
<p>OK, I thought I understood what Emmy was asking but if not I will let her clarify if she ever sees this comment thread again.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: First Approximation		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171518</link>

		<dc:creator><![CDATA[First Approximation]]></dc:creator>
		<pubDate>Fri, 13 Aug 2021 06:44:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171518</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171514&quot;&gt;John Baez&lt;/a&gt;.

Possibly dumb question: in the path integral method the amplitude for each path has the same magnitude. Is there any good justification for that? Seems similar to the idea in thermodynamics that all accessible micro-states are equiprobable over the long run.

Dropping this requirement seems equivalent to letting the action be complex. I can&#039;t think of any reason to discount it either than that sounds weird.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171514">John Baez</a>.</p>
<p>Possibly dumb question: in the path integral method the amplitude for each path has the same magnitude. Is there any good justification for that? Seems similar to the idea in thermodynamics that all accessible micro-states are equiprobable over the long run.</p>
<p>Dropping this requirement seems equivalent to letting the action be complex. I can&#8217;t think of any reason to discount it either than that sounds weird.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171516</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 12 Aug 2021 20:08:34 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171516</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347&quot;&gt;Emmy B&lt;/a&gt;.

If you arbitrarily permute the numbers in the list $latex (q_1, \dots, q_n)$, their Fourier transform changes in a very messy way.  If these numbers are time series data they come in a particular order, but I&#039;m not assuming they&#039;re time series data.   They&#039;re just a probability distribution on a finite set.   I&#039;m calling this finite set $latex \{1,\dots,n\}$ to make my blog articles easier to read, but if I were talking to myself I&#039;d just call it $latex X,$ since I&#039;m never using the linear ordering.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347">Emmy B</a>.</p>
<p>If you arbitrarily permute the numbers in the list <img src="https://s0.wp.com/latex.php?latex=%28q_1%2C+%5Cdots%2C+q_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1, &#92;dots, q_n)" class="latex" />, their Fourier transform changes in a very messy way.  If these numbers are time series data they come in a particular order, but I&#8217;m not assuming they&#8217;re time series data.   They&#8217;re just a probability distribution on a finite set.   I&#8217;m calling this finite set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cdots%2Cn%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1,&#92;dots,n&#92;}" class="latex" /> to make my blog articles easier to read, but if I were talking to myself I&#8217;d just call it <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> since I&#8217;m never using the linear ordering.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171515</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 12 Aug 2021 20:03:43 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171515</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171351&quot;&gt;Frederic Barbaresco&lt;/a&gt;.

Thanks for all these links, Frederic!  I have been wanting to understand Souriau&#039;s ideas on this topic, but finding it hard to find something to read---in part because it&#039;s a lot of work for me to read French.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171351">Frederic Barbaresco</a>.</p>
<p>Thanks for all these links, Frederic!  I have been wanting to understand Souriau&#8217;s ideas on this topic, but finding it hard to find something to read&#8212;in part because it&#8217;s a lot of work for me to read French.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171514</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 12 Aug 2021 20:02:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171514</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171346&quot;&gt;Matt&lt;/a&gt;.

In the short run I&#039;m just trying to fit thermodynamics, statistical mechanics and probability theory into the same framework as classical mechanics, using the fact that extremal principles (least action, maximum entropy) are important in all these subjects.

I wrote about this idea back in &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/01/19/classical-mechanics-versus-thermodynamics-part-1/&quot;&gt;Classical mechanics versus thermodynamics (part 1)&lt;/a&gt;:

&lt;a name=&quot;the_big_picture&quot; rel=&quot;nofollow ugc&quot;&gt;

&lt;h4&gt; &lt;font color=&quot;black&quot;&gt;The big picture&lt;/font&gt; &lt;/h4&gt;

&lt;/a&gt;

Now let&#039;s step back and think about what&#039;s going on.

Lately I&#039;ve been trying to unify a bunch of &#039;extremal principles&#039;, including:

1) the &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_least_action&quot; rel=&quot;nofollow ugc&quot;&gt;principle of least action&lt;/a&gt;
2) the &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_minimum_energy&quot; rel=&quot;nofollow ugc&quot;&gt;principle of least energy&lt;/a&gt;
3) the &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_maximum_entropy&quot; rel=&quot;nofollow ugc&quot;&gt;principle of maximum entropy&lt;/a&gt;
4) the principle of maximum simplicity, or &lt;a href=&quot;http://en.wikipedia.org/wiki/Occam%27s_razor&quot; rel=&quot;nofollow ugc&quot;&gt;Occam&#039;s razor&lt;/a&gt;

In my post on &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/&quot;&gt;quantropy&lt;/a&gt; I explained how the first three principles fit into a single framework if we treat Planck&#039;s constant as an imaginary temperature.  The guiding principle of this framework is

&lt;div align=&quot;center&quot;&gt;
&lt;i&gt;&lt;b&gt;maximize entropy &lt;br /&gt; subject to the constraints imposed by what you believe&lt;/b&gt;&lt;/i&gt;&lt;/div&gt;

And that&#039;s nice, because E. T. Jaynes has made a &lt;a href=&quot;http://bayes.wustl.edu/etj/articles/stand.on.entropy.pdf&quot; rel=&quot;nofollow ugc&quot;&gt;powerful case&lt;/a&gt; for this principle.

However, when the temperature is imaginary, entropy is so different that it may deserves a new name: say, &#039;quantropy&#039;.  In particular, it&#039;s complex-valued, so instead of maximizing it we have to look for stationary points: places where its first derivative is zero.  But this isn&#039;t so bad.  Indeed, a lot of minimum and maximum principles are really &#039;stationary principles&#039; if you examine them carefully.

What about the fourth principle: Occam&#039;s razor?  We can formalize this using &lt;a href=&quot;http://www.scholarpedia.org/article/Algorithmic_probability&quot; rel=&quot;nofollow ugc&quot;&gt;algorithmic probability theory&lt;/a&gt;.  Occam&#039;s razor then becomes yet another special case of

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;&lt;b&gt;maximize entropy &lt;br /&gt; subject the constraints imposed by what you believe&lt;/b&gt;&lt;/i&gt;&lt;/div&gt;

once we realize that &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/&quot;&gt;algorithmic entropy is a special case of ordinary entropy&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171346">Matt</a>.</p>
<p>In the short run I&#8217;m just trying to fit thermodynamics, statistical mechanics and probability theory into the same framework as classical mechanics, using the fact that extremal principles (least action, maximum entropy) are important in all these subjects.</p>
<p>I wrote about this idea back in <a href="https://johncarlosbaez.wordpress.com/2012/01/19/classical-mechanics-versus-thermodynamics-part-1/">Classical mechanics versus thermodynamics (part 1)</a>:</p>
<p><a name="the_big_picture" rel="nofollow ugc"></p>
<h4> <font color="black">The big picture</font> </h4>
<p></a></p>
<p>Now let&#8217;s step back and think about what&#8217;s going on.</p>
<p>Lately I&#8217;ve been trying to unify a bunch of &#8216;extremal principles&#8217;, including:</p>
<p>1) the <a href="http://en.wikipedia.org/wiki/Principle_of_least_action" rel="nofollow ugc">principle of least action</a><br />
2) the <a href="http://en.wikipedia.org/wiki/Principle_of_minimum_energy" rel="nofollow ugc">principle of least energy</a><br />
3) the <a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy" rel="nofollow ugc">principle of maximum entropy</a><br />
4) the principle of maximum simplicity, or <a href="http://en.wikipedia.org/wiki/Occam%27s_razor" rel="nofollow ugc">Occam&#8217;s razor</a></p>
<p>In my post on <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/">quantropy</a> I explained how the first three principles fit into a single framework if we treat Planck&#8217;s constant as an imaginary temperature.  The guiding principle of this framework is</p>
<div align="center">
<i><b>maximize entropy <br /> subject to the constraints imposed by what you believe</b></i></div>
<p>And that&#8217;s nice, because E. T. Jaynes has made a <a href="http://bayes.wustl.edu/etj/articles/stand.on.entropy.pdf" rel="nofollow ugc">powerful case</a> for this principle.</p>
<p>However, when the temperature is imaginary, entropy is so different that it may deserves a new name: say, &#8216;quantropy&#8217;.  In particular, it&#8217;s complex-valued, so instead of maximizing it we have to look for stationary points: places where its first derivative is zero.  But this isn&#8217;t so bad.  Indeed, a lot of minimum and maximum principles are really &#8216;stationary principles&#8217; if you examine them carefully.</p>
<p>What about the fourth principle: Occam&#8217;s razor?  We can formalize this using <a href="http://www.scholarpedia.org/article/Algorithmic_probability" rel="nofollow ugc">algorithmic probability theory</a>.  Occam&#8217;s razor then becomes yet another special case of</p>
<div align="center"><i><b>maximize entropy <br /> subject the constraints imposed by what you believe</b></i></div>
<p>once we realize that <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/">algorithmic entropy is a special case of ordinary entropy</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: geoenergymath		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171479</link>

		<dc:creator><![CDATA[geoenergymath]]></dc:creator>
		<pubDate>Tue, 10 Aug 2021 18:31:43 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171479</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347&quot;&gt;Emmy B&lt;/a&gt;.

The order of values in the list doesn&#039;t matter if it&#039;s a Mach-Zehnder-like modulation. One can take the Fourier periodogram and apply it to some practical applications as in this recent post: https://geoenergymath.com/2021/05/17/inverting-non-autonomous-functions/]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347">Emmy B</a>.</p>
<p>The order of values in the list doesn&#8217;t matter if it&#8217;s a Mach-Zehnder-like modulation. One can take the Fourier periodogram and apply it to some practical applications as in this recent post: <a href="https://geoenergymath.com/2021/05/17/inverting-non-autonomous-functions/" rel="nofollow ugc">https://geoenergymath.com/2021/05/17/inverting-non-autonomous-functions/</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171437</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 09 Aug 2021 20:14:40 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31491#comment-171437</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347&quot;&gt;Emmy B&lt;/a&gt;.

Since the order of the list $latex (q_1, \dots, q_n)$ is completely arbitrary and unphysical, yet it affects the Fourier transform of this list (thought as a function on $latex \mathbb{Z}/n\mathbb{Z}$) in dramatic ways, I don&#039;t think taking the Fourier transform of this list is a good idea.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comment-171347">Emmy B</a>.</p>
<p>Since the order of the list <img src="https://s0.wp.com/latex.php?latex=%28q_1%2C+%5Cdots%2C+q_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1, &#92;dots, q_n)" class="latex" /> is completely arbitrary and unphysical, yet it affects the Fourier transform of this list (thought as a function on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D%2Fn%5Cmathbb%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{Z}/n&#92;mathbb{Z}" class="latex" />) in dramatic ways, I don&#8217;t think taking the Fourier transform of this list is a good idea.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
