<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information Geometry (Part 21)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/</link>
	<description></description>
	<lastBuildDate>Fri, 20 Aug 2021 20:27:47 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Keith Harbaugh		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171746</link>

		<dc:creator><![CDATA[Keith Harbaugh]]></dc:creator>
		<pubDate>Fri, 20 Aug 2021 20:27:47 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171746</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171745&quot;&gt;Keith Harbaugh&lt;/a&gt;.

Oh wait, I see it is ... The text 
Giaquinta, Mariano; Hildebrandt, Stefan (1996), Calculus of Variations 1. The Lagrangian Formalism.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171745">Keith Harbaugh</a>.</p>
<p>Oh wait, I see it is &#8230; The text<br />
Giaquinta, Mariano; Hildebrandt, Stefan (1996), Calculus of Variations 1. The Lagrangian Formalism.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Keith Harbaugh		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171745</link>

		<dc:creator><![CDATA[Keith Harbaugh]]></dc:creator>
		<pubDate>Fri, 20 Aug 2021 20:20:31 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171745</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171744&quot;&gt;John Baez&lt;/a&gt;.

Very interesting.

Courant and Hilbert? How classical can you get :-) Looks like the DWM haven&#039;t been made obsolete quite yet.

I&#039;m kind of amazed this isn&#039;t covered by a more modern, mainstream textbook from some place like Springer.   Maybe it is. Hopefully someone can make a suggestion.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171744">John Baez</a>.</p>
<p>Very interesting.</p>
<p>Courant and Hilbert? How classical can you get :-) Looks like the DWM haven&#8217;t been made obsolete quite yet.</p>
<p>I&#8217;m kind of amazed this isn&#8217;t covered by a more modern, mainstream textbook from some place like Springer.   Maybe it is. Hopefully someone can make a suggestion.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171744</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 20 Aug 2021 20:08:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171744</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171732&quot;&gt;Keith Harbaugh&lt;/a&gt;.

There must be a bunch, but I forget where I learned this stuff---probably here and there.   It can&#039;t hurt too much to start here:

&#8226; Wikipedia, &lt;a href=&quot;https://en.wikipedia.org/wiki/Functional_derivative&quot; rel=&quot;nofollow ugc&quot;&gt;Functional derivative&lt;/a&gt;.

They recommend a bunch of textbooks, starting with Courant and Hilbert and working on up to Gelfan&#039;d and Fomin, which is a Dover book---so fairly cheap, I imagine.  All four of these folks are famous.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171732">Keith Harbaugh</a>.</p>
<p>There must be a bunch, but I forget where I learned this stuff&#8212;probably here and there.   It can&#8217;t hurt too much to start here:</p>
<p>&bull; Wikipedia, <a href="https://en.wikipedia.org/wiki/Functional_derivative" rel="nofollow ugc">Functional derivative</a>.</p>
<p>They recommend a bunch of textbooks, starting with Courant and Hilbert and working on up to Gelfan&#8217;d and Fomin, which is a Dover book&#8212;so fairly cheap, I imagine.  All four of these folks are famous.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Keith Harbaugh		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171732</link>

		<dc:creator><![CDATA[Keith Harbaugh]]></dc:creator>
		<pubDate>Fri, 20 Aug 2021 15:07:55 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171732</guid>

					<description><![CDATA[Re your comment that

&lt;blockquote&gt;When I was a math major taking physics classes, the way physicists did variational derivatives seemed like black magic to me.  Then I spent months reading how mathematicians rigorously justified these techniques.&lt;/blockquote&gt;

Is there a half-way decent textbook that explains that?

Thanks.]]></description>
			<content:encoded><![CDATA[<p>Re your comment that</p>
<blockquote><p>When I was a math major taking physics classes, the way physicists did variational derivatives seemed like black magic to me.  Then I spent months reading how mathematicians rigorously justified these techniques.</p></blockquote>
<p>Is there a half-way decent textbook that explains that?</p>
<p>Thanks.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171655</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 18 Aug 2021 18:54:31 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171655</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171650&quot;&gt;John Baez&lt;/a&gt;.

Okay, thanks---that&#039;s interesting.  Horn and Jackson introduced it in chemistry in 1972 and called it the &quot;pseudo-Helmholtz function&quot;:

&#8226; F. Horn and R. Jackson, General mass action kinetics,
&lt;em&gt;Arch. Ration. Mech. An.&lt;/em&gt; &lt;strong&gt;47&lt;/strong&gt; (1972), 81–116.

It plays an important role as a Lyapunov function in chemical reaction networks that are &#039;complex balanced&#039;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171650">John Baez</a>.</p>
<p>Okay, thanks&#8212;that&#8217;s interesting.  Horn and Jackson introduced it in chemistry in 1972 and called it the &#8220;pseudo-Helmholtz function&#8221;:</p>
<p>&bull; F. Horn and R. Jackson, General mass action kinetics,<br />
<em>Arch. Ration. Mech. An.</em> <strong>47</strong> (1972), 81–116.</p>
<p>It plays an important role as a Lyapunov function in chemical reaction networks that are &#8216;complex balanced&#8217;.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: ab		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171654</link>

		<dc:creator><![CDATA[ab]]></dc:creator>
		<pubDate>Wed, 18 Aug 2021 18:53:58 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171654</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171650&quot;&gt;John Baez&lt;/a&gt;.

It occurs to me that this form of the KL-divergence is also what arises when you calculate the Bergman divergence of the usual entropy formula, but I can&#039;t say I really understand Bregman divergences either :).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171650">John Baez</a>.</p>
<p>It occurs to me that this form of the KL-divergence is also what arises when you calculate the Bergman divergence of the usual entropy formula, but I can&#8217;t say I really understand Bregman divergences either :).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: ab		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171652</link>

		<dc:creator><![CDATA[ab]]></dc:creator>
		<pubDate>Wed, 18 Aug 2021 18:30:01 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171652</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171650&quot;&gt;John Baez&lt;/a&gt;.

I first saw it referred to in some work by Csiszar. A little googling gives &lt;a href=&quot;https://scholar.google.com/scholar?hl=en&#038;as_sdt=0%2C21&#038;q=Why+least+squares+and+maximum+entropy%3F+An+axiomatic+approach+to+inference+for+linear+inverse+problems.&#038;btnG=&quot; rel=&quot;nofollow ugc&quot;&gt;this&lt;/a&gt; as maybe the origin, but I haven&#039;t read it yet.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171650">John Baez</a>.</p>
<p>I first saw it referred to in some work by Csiszar. A little googling gives <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C21&amp;q=Why+least+squares+and+maximum+entropy%3F+An+axiomatic+approach+to+inference+for+linear+inverse+problems.&amp;btnG=" rel="nofollow ugc">this</a> as maybe the origin, but I haven&#8217;t read it yet.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171650</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 18 Aug 2021 16:51:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171650</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171634&quot;&gt;ab&lt;/a&gt;.

I&#039;ve used this generalization of the Kullback--Leibler divergence myself---see equation (21) in this paper:

&#8226; John Baez and Blake Pollard, &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/&quot;&gt;Relative information in biological systems&lt;/a&gt;.

(I call the Kullback--Leibler divergence &#039;relative information&#039;.)

And yes, it helps!  But this generalization is still somewhat mysterious to me.   It&#039;s sometimes used in mathematical chemistry where instead of normalized probability distributions we have &#039;populations&#039;, e.g. numbers of molecules---I believe it was introduced there by Horn and Jackson.  Have you seen it somewhere else?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171634">ab</a>.</p>
<p>I&#8217;ve used this generalization of the Kullback&#8211;Leibler divergence myself&#8212;see equation (21) in this paper:</p>
<p>&bull; John Baez and Blake Pollard, <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/">Relative information in biological systems</a>.</p>
<p>(I call the Kullback&#8211;Leibler divergence &#8216;relative information&#8217;.)</p>
<p>And yes, it helps!  But this generalization is still somewhat mysterious to me.   It&#8217;s sometimes used in mathematical chemistry where instead of normalized probability distributions we have &#8216;populations&#8217;, e.g. numbers of molecules&#8212;I believe it was introduced there by Horn and Jackson.  Have you seen it somewhere else?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171644</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 18 Aug 2021 15:37:51 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171644</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171643&quot;&gt;Steve Huntsman&lt;/a&gt;.

I feel like maybe I didn&#039;t make myself clear.

There&#039;s no concept of &quot;time&quot; in what I&#039;m talking about, and no Markov process.   I&#039;ve just got a probability distribution maximizing entropy subject to constraints on expected values of some finite list of random variables (aka observables).

But this &lt;em&gt;subsumes&lt;/em&gt; the case where we call one those observables &quot;energy&quot;.  In this case the conjugate intensive variable is $latex 1/kT,$ where $latex T$ is the temperature and $latex k$ is Boltzmann&#039;s constant, and then $latex \mathrm -k T \ln Z$ is free energy.

So, I was trying to say we can easily specialize the framework described here to relate $latex \ln Z$ to free energy, but that $latex \ln Z$ deserves some other name when we&#039;re working at the level of generality described here.

I believe in quantum field theory people sometimes call $latex \ln Z$  &quot;free energy&quot; even in contexts where it deserves some other name.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171643">Steve Huntsman</a>.</p>
<p>I feel like maybe I didn&#8217;t make myself clear.</p>
<p>There&#8217;s no concept of &#8220;time&#8221; in what I&#8217;m talking about, and no Markov process.   I&#8217;ve just got a probability distribution maximizing entropy subject to constraints on expected values of some finite list of random variables (aka observables).</p>
<p>But this <em>subsumes</em> the case where we call one those observables &#8220;energy&#8221;.  In this case the conjugate intensive variable is <img src="https://s0.wp.com/latex.php?latex=1%2FkT%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/kT," class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the temperature and <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> is Boltzmann&#8217;s constant, and then <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm+-k+T+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm -k T &#92;ln Z" class="latex" /> is free energy.</p>
<p>So, I was trying to say we can easily specialize the framework described here to relate <img src="https://s0.wp.com/latex.php?latex=%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln Z" class="latex" /> to free energy, but that <img src="https://s0.wp.com/latex.php?latex=%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln Z" class="latex" /> deserves some other name when we&#8217;re working at the level of generality described here.</p>
<p>I believe in quantum field theory people sometimes call <img src="https://s0.wp.com/latex.php?latex=%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln Z" class="latex" />  &#8220;free energy&#8221; even in contexts where it deserves some other name.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Steve Huntsman		</title>
		<link>https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comment-171643</link>

		<dc:creator><![CDATA[Steve Huntsman]]></dc:creator>
		<pubDate>Wed, 18 Aug 2021 15:16:28 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=31635#comment-171643</guid>

					<description><![CDATA[If you also have a timescale measuring something like the growth rate of orbits or a mixing time to go along with the probability setup (say, in the case that the distribution arises from a Markov process), then you &lt;em&gt;do&lt;/em&gt; have energy at this level of generality: see section 3 of https://arxiv.org/abs/2104.00753. The caveat here is that the precise nature of the timescale to generically reproduce physics isn&#039;t completely nailed down, but it&#039;s very highly constrained. Moreover, a &quot;minimum channel capacity&quot; Ansatz in the case of a Markov process (unpublished, but I have a writeup) suggests a general principle for determining this timescale.]]></description>
			<content:encoded><![CDATA[<p>If you also have a timescale measuring something like the growth rate of orbits or a mixing time to go along with the probability setup (say, in the case that the distribution arises from a Markov process), then you <em>do</em> have energy at this level of generality: see section 3 of <a href="https://arxiv.org/abs/2104.00753" rel="nofollow ugc">https://arxiv.org/abs/2104.00753</a>. The caveat here is that the precise nature of the timescale to generically reproduce physics isn&#8217;t completely nailed down, but it&#8217;s very highly constrained. Moreover, a &#8220;minimum channel capacity&#8221; Ansatz in the case of a Markov process (unpublished, but I have a writeup) suggests a general principle for determining this timescale.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
