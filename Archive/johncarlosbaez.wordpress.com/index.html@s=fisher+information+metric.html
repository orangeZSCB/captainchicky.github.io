<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> fisher information metric | Search Results  | Azimuth</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;fisher information metric&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/fisher+information+metric/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F14%2Finformation-geometry-part-20%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"677eca7dd0","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"0272f16312\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/?s=fisher+information+metric","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F%3Fs%3Dfisher%2Binformation%2Bmetric","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,227 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F14%2Finformation-geometry-part-20%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;fisher information metric&#8221; &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-31552 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31552">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/" rel="bookmark">Information Geometry (Part&nbsp;20)</a></h2>
				<small>14 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Last time</a> we worked out an analogy between classical mechanics, thermodynamics and probability theory.  The latter two look suspiciously similar:</p>
<div align="center">
<table border="1">
<tbody>
<tr>
<td></td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;<i>q</i> &nbsp;</td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">probabilities</td>
</tr>
<tr>
<td>&nbsp;<i>p</i>&nbsp;</td>
<td style="text-align:center;"> momentum</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">surprisals</td>
</tr>
<tr>
<td>&nbsp;<i>S</i> &nbsp;</td>
<td style="text-align:center;"> action</td>
<td style="text-align:center;">entropy</td>
<td style="text-align:center;"> Shannon entropy</td>
</tr>
</tbody>
</table>
</div>
<p>This is no coincidence.  After all, in the subject of statistical mechanics we <i>explain</i> classical thermodynamics using probability theory&#8212;and entropy is revealed to be Shannon entropy (or its quantum analogue).</p>
<p>Now I want to make this precise.</p>
<p>To connect classical thermodynamics to probability theory, I&#8217;ll start by discussing &#8216;statistical manifolds&#8217;.  I introduced the idea of a statistical manifold in <a href="https://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>: it&#8217;s a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> equipped with a map sending each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> to a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> on some measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega." class="latex" />   Now I&#8217;ll say how these fit into the second column of the above chart.</p>
<p>Then I&#8217;ll talk about statistical manifolds of a special sort used in thermodynamics, which I&#8217;ll call &#8216;Gibbsian&#8217;, since they really go back to Josiah Willard Gibbs.</p>
<div align="center"><a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs"><img src="https://johncarlosbaez.files.wordpress.com/2021/08/josiah_willard_gibbs_-from_mms-.jpg?w=200" alt="" width="200" /></a></div>
<p>In a Gibbsian statistical manifold, for each <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a &#8216;Gibbs distribution&#8217;.   Physically, these Gibbs distributions describe <a href="https://en.wikipedia.org/wiki/Thermodynamic_equilibrium">thermodynamic equilibria</a>.  For example, if you specify the volume, energy and number of particles in a box of gas, there will be a Gibbs distribution describing what the particles do in thermodynamic equilibrium under these conditions.   Mathematically, Gibbs distributions <i>maximize entropy</i> subject to some constraints specified by the point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q." class="latex" /></p>
<p>More precisely: in a Gibbsian statistical manifold we have a list of observables <img src="https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots+%2C+A_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_1, &#92;dots , A_n" class="latex" /> whose expected values serve as coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> for points <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the probability distribution that maximizes entropy subject to the constraint that the expected value of <img src="https://s0.wp.com/latex.php?latex=A_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_i" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   We can derive most of the interesting formulas of thermodynamics starting from this!</p>
<h3> Statistical manifolds</h3>
<p>Let&#8217;s fix a measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu." class="latex" />  A <b>statistical manifold</b> is then a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> equipped with a smooth map <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> assigning to each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> a probability distribution on <img src="https://s0.wp.com/latex.php?latex=%5COmega%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega," class="latex" /> which I&#8217;ll call <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" />   So, <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a function on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cpi_q+%5C%2C+d%5Cmu+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;pi_q &#92;, d&#92;mu = 1 }" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%28x%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q(x) &#92;ge 0" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>The idea here is that the space of <i>all</i> probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> may be too huge to understand in as much detail as we&#8217;d like, so instead we describe <i>some</i> of these probability distributions&#8212;a family parametrized by points of some manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" />&#8212;using the map <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" />   This is the basic idea behind <a href="https://en.wikipedia.org/wiki/Parametric_statistics">parametric statistics</a>.</p>
<p>Information geometry is the geometry of statistical manifolds.  Any statistical manifold comes with a bunch of interesting geometrical structures.   One is the &#8216;Fisher information metric&#8217;, a Riemannian metric I explained in <a href="https://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>.  Another is a 1-parameter family of connections on the tangent bundle <img src="https://s0.wp.com/latex.php?latex=T+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T Q," class="latex" /> which is important in Amari&#8217;s approach to information geometry.   You can read about this here:</p>
<p>• Hiroshi Matsuzoe, <a href="https://projecteuclid.org/download/pdf_1/euclid.aspm/1543086326">Statistical manifolds and affine differential geometry</a>, in <i>Advanced Studies in Pure Mathematics 57</i>, pp. 303–321.</p>
<p>I don&#8217;t want to talk about it now&#8212;I just wanted to reassure you that I&#8217;m not completely ignorant of it!</p>
<p>I want to focus on the story I&#8217;ve been telling, which is about <i>entropy</i>.   Our statistical manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> comes with a smooth <b>entropy</b> function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++f%28q%29+%3D+-%5Cint_%5COmega+%5Cpi_q%28x%29+%5C%2C+%5Cln+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29++++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  f(q) = -&#92;int_&#92;Omega &#92;pi_q(x) &#92;, &#92;ln &#92;pi_q(x) &#92;, d&#92;mu(x)    } " class="latex" /></p>
<p>We can use this entropy function to do many of the things we usually do in thermodynamics!   For example, at any point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> where this function is differentiable, its differential gives a cotangent vector</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>which has an important physical meaning.   In coordinates we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>and we call <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> the <b>intensive variable conjugate to</b> <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   For example if <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> is energy, <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> will be &#8216;coolness&#8217;: the reciprocal of temperature.</p>
<p>Defining <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> this way gives a Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B+%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+x+%5Cin+M%2C+%5C%3B+p+%3D++%28df%29_x+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{ (q,p) &#92;in T^&#92;ast Q : &#92;; x &#92;in M, &#92;; p =  (df)_x &#92;} " class="latex" /></p>
<p>of the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   We can also get contact geometry into the game by defining a contact manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> and a Legendrian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B+%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+x+%5Cin+M%2C+%5C%3B+p+%3D++%28df%29_q+%2C+S+%3D+f%28q%29+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{ (q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; x &#92;in M, &#92;; p =  (df)_q , S = f(q) &#92;}" class="latex" /></p>
<p>But I&#8217;ve been talking about these ideas for the last three episodes, so I won&#8217;t say more just now!   Instead, I want to throw a new idea into the pot.</p>
<h3> Gibbsian statistical manifolds</h3>
<p>Thermodynamics, and statistical mechanics, spend a lot of time dealing with statistical manifold of a special sort I&#8217;ll call &#8216;Gibbsian&#8217;.  In these, each probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a &#8216;Gibbs distribution&#8217;, meaning that it maximizes entropy subject to certain constraints specified by the point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q." class="latex" /></p>
<p>How does this work?  For starters, an integrable function</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%5Ccolon+%5COmega+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;colon &#92;Omega &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is called a <b>random variable</b>, or in physics an <b>observable</b>.  The <b>expected value</b> of an observable is a smooth real-valued function on our statistical manifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+A+%5Crangle%28q%29+%3D+%5Cint_%5COmega+A%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle A &#92;rangle(q) = &#92;int_&#92;Omega A(x) &#92;pi_q(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>In other words, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle" class="latex" /> is a function whose value at at any point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> is the expected value of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> with respect to the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>Now, suppose our statistical manifold is <i>n</i>-dimensional and we have <i>n</i> observables <img src="https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots%2C+A_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_1, &#92;dots, A_n." class="latex" />   Their expected values will be smooth functions on our manifold&#8212;and sometimes these functions will be a coordinate system!</p>
<p>This may sound rather unlikely, but it&#8217;s really not so outlandish.  Indeed, if there&#8217;s a point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> such that the differentials of the functions <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle" class="latex" /> are linearly independent at this point, these functions will be a coordinate system in some neighborhood of this point, by the <a href="https://en.wikipedia.org/wiki/Inverse_function_theorem">inverse function theorem</a>.  So, we can take this neighborhood, use <i>it</i> as our statistical manifold, and the functions <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle" class="latex" /> will be coordinates.</p>
<p>So, let&#8217;s assume the expected values of our observables give a coordinate system on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   Let&#8217;s call these coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n," class="latex" /> so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle%28q%29+%3D+q_i++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle(q) = q_i  " class="latex" /></p>
<p>Now for the kicker: we say our statistical manifold is <b>Gibbsian</b> if for each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the probability distribution that <i>maximizes entropy subject to the above condition!</i></p>
<p>Which condition?   The condition saying that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+A_i%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega A_i(x) &#92;pi_q(x) &#92;, d&#92;mu(x) = q_i } " class="latex" /></p>
<p>for all <i>i</i>.   This is just the previous equation spelled out so that you can see it&#8217;s a condition on <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>This assumption of the entropy-maximizing nature of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a very powerful, because it implies a useful and nontrivial formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" />   It&#8217;s called the <b><a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs distribution</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cpi_q%28x%29+%3D+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;pi_q(x) = &#92;frac{1}{Z(q)} &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x)&#92;right) }" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the intensive variable conjugate to <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> while <img src="https://s0.wp.com/latex.php?latex=Z%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(q)" class="latex" /> is the <b>partition function</b>: the thing we must divide by to make sure <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> integrates to 1.  In other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Z%28q%29+%3D+%5Cint_%5COmega+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29+%5Cright%29+%5C%2C+d%5Cmu%28x%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Z(q) = &#92;int_&#92;Omega &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x) &#92;right) &#92;, d&#92;mu(x)   } " class="latex" /></p>
<p>By the way, this formula may look confusing at first, since the left side depends on the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in our statistical manifold, while there&#8217;s no <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> visible in the right side!  Do you see what&#8217;s going on?</p>
<p>I&#8217;ll tell you: the conjugate variable <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> sitting on the right side of the above formula, depends on <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   Remember, we got it by taking the partial derivative of the entropy in the <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> direction</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>and the evaluating this derivative at the point <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>But wait a minute!  <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> here is the entropy&#8212;but the entropy of <i>what?</i></p>
<p>The entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> of course!</p>
<p>So there&#8217;s something circular about our formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /> To know <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> you need to know the conjugate variables <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> but to compute these you need to know the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>This is actually okay.  While circular, the formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is still <i>true</i>.  It&#8217;s harder to work with than you might hope.  But it&#8217;s still extremely useful.</p>
<p>Next time I&#8217;ll prove that this formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is true, and do a few things with it.   All this material was discovered by Gibbs in the late 1800&#8217;s, and it&#8217;s lurking any good book on statistical mechanics&#8212;but not phrased in the language of statistical manifolds.  The physics textbooks usually consider special cases, like a box of gas where:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1" class="latex" /> is energy, <img src="https://s0.wp.com/latex.php?latex=p_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1" class="latex" /> is 1/temperature.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_2" class="latex" /> is volume, <img src="https://s0.wp.com/latex.php?latex=p_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_2" class="latex" /> is &ndash;pressure/temperature.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_3" class="latex" /> is the number of particles, <img src="https://s0.wp.com/latex.php?latex=p_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_3" class="latex" /> is chemical potential / pressure.</p>
<p>While these special cases are important and interesting, I&#8217;d rather be general!</p>
<h3> Technical comments</h3>
<p>I said &#8220;Any statistical manifold comes with a bunch of interesting geometrical structures&#8221;, but in fact some conditions are required.  For example, the Fisher information metric is only well-defined and nondegenerate under some conditions on the map <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" />   For example, if <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> maps every point of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> to the same probability distribution, the Fisher information metric will <i>vanish</i>.</p>
<p>Similarly, the entropy function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is only smooth under some conditions on <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" /></p>
<p>Furthermore, the integral</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29+%5Cright%29+%5C%2C+d%5Cmu%28x%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x) &#92;right) &#92;, d&#92;mu(x)   } " class="latex" /></p>
<p>may not converge for all values of the numbers <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cdots%2C+p_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;dots, p_n." class="latex" />   But in my discussion of Gibbsian statistical manifolds, I was assuming that an entropy-maximizing probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+A_i%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega A_i(x) &#92;pi_q(x) &#92;, d&#92;mu(x) = q_i } " class="latex" /></p>
<p>actually <i>exists</i>.   In this case the probability distribution is also unique (almost everywhere).</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p>• <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/#comments">16 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;20)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31321 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31321">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/" rel="bookmark">Information Geometry (Part&nbsp;18)</a></h2>
				<small>5 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">Last time</a> I sketched how two related forms of geometry, <a href="https://en.wikipedia.org/wiki/Symplectic_geometry">symplectic</a> and <a href="https://en.wikipedia.org/wiki/Contact_geometry">contact</a> geometry, show up in thermodynamics.  Today I want to explain how they show up in probability theory.</p>
<p>For some reason I haven&#8217;t seen much discussion of this!  But people should have looked into this.  After all, statistical mechanics explains thermodynamics in terms of probability theory, so if some mathematical structure shows up in thermodynamics it should appear in statistical mechanics&#8230; and thus ultimately in probability theory.</p>
<p>I just figured out how this works for symplectic and contact geometry.</p>
<p>Suppose a system has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> possible states.  We&#8217;ll call these <b><a href="https://en.wikipedia.org/wiki/Microstate_(statistical_mechanics)">microstates</a></b>, following the tradition in statistical mechanics.   If you don&#8217;t know what &#8216;microstate&#8217; means, don&#8217;t worry about it!  But the rough idea is that if you have a macroscopic system like a rock, the precise details of what its atoms are doing are described by a microstate, and many different microstates could be indistinguishable unless you look very carefully.</p>
<p>We&#8217;ll call the microstates <img src="https://s0.wp.com/latex.php?latex=1%2C+2%2C+%5Cdots%2C+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1, 2, &#92;dots, n." class="latex" />  So, if you don&#8217;t want to think about physics, when I say <b>microstate</b> I&#8217;ll just mean an integer from 1 to <em>n</em>.</p>
<p>Next, a <b>probability distribution</b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> assigns a real number <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> to each microstate, and these numbers must sum to 1 and be nonnegative.  So, we have <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathbb%7BR%7D%5En%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in &#92;mathbb{R}^n," class="latex" /> though not every vector in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> is a probability distribution.</p>
<p>I&#8217;m sure you&#8217;re wondering why I&#8217;m using <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> rather than <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to stand for an observable instead of a probability distribution.   Am I just trying to confuse you?</p>
<p>No: I&#8217;m trying to set up an analogy to physics!</p>
<p><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">Last time</a> I introduced symplectic geometry using classical mechanics.  The most important example of a symplectic manifold is the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> of a manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   A point of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is a pair <img src="https://s0.wp.com/latex.php?latex=%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q,p)" class="latex" /> consisting of a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> and a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+T%5E%5Cast_q+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in T^&#92;ast_q Q." class="latex" />  In classical mechanics the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> describes the position of some physical system, while <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> describes its momentum.</p>
<p>So, I&#8217;m going to set up an analogy like this:</p>
<div align="center">
<table border="1">
<tr>
<td> </td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;"><b>Probability Theory</b></td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q " class="latex" /> &nbsp; </td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">&nbsp; probability distribution &nbsp; </td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> &nbsp;</td>
<td style="text-align:center;">momentum</td>
<td style="text-align:center;">???</td>
</tr>
</table>
</div>
<p>But <i>what is to momentum as probability is to position?</i></p>
<p>A big clue is the appearance of symplectic geometry in thermodynamics, which I also outlined <a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">last time</a>.    We can use this to get some intuition about the analogue of momentum in probability theory.</p>
<p>In thermodynamics, a system has a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> of states.  (These are not the &#8216;microstates&#8217; I mentioned before: we&#8217;ll see the relation later.)  There is a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>describing the <a href="https://en.wikipedia.org/wiki/Entropy">entropy</a> of the system as a function of its state.   There is a law of thermodynamics saying that</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>This equation picks out a submanifold of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q," class="latex" /> namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{(q,p) &#92;in T^&#92;ast Q : &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>Moreover this submanifold is Lagrangian: the symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> vanishes when restricted to it:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Comega+%7C_%5CLambda+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;omega |_&#92;Lambda = 0 } " class="latex" /></p>
<p>This is very beautiful, but it goes by so fast you might almost miss it!   So let&#8217;s clutter it up a bit with coordinates.  We often use local coordinates on <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> and describe a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> using these coordinates, getting a point</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28q_1%2C+%5Cdots%2C+q_n%29+%5Cin+%5Cmathbb%7BR%7D%5En+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1, &#92;dots, q_n) &#92;in &#92;mathbb{R}^n " class="latex" /></p>
<p>They give rise to local coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n%2C+p_1%2C+%5Cdots%2C+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n, p_1, &#92;dots, p_n" class="latex" /> on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   The <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are called <b>extensive variables</b>, because they are typically things that you can measure only by totalling up something over the whole system, like the energy or volume of a cylinder of gas.   The <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are called <b>intensive variables</b>, because they are typically things that you can measure locally at any point, like temperature or pressure.</p>
<p>In these local coordinates, the symplectic structure on <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is the 2-form given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+dp_1+%5Cwedge+dq_1+%2B+%5Ccdots+%2B+dp_n+%5Cwedge+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = dp_1 &#92;wedge dq_1 + &#92;cdots + dp_n &#92;wedge dq_n " class="latex" /></p>
<p>The equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q" class="latex" /></p>
<p>serves as a law of physics that determines the intensive variables given the extensive ones when our system is in thermodynamic equilibrium.   Written out using coordinates, this law says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>It looks pretty bland here, but in fact it gives formulas for the temperature and pressure of a gas, and many other useful formulas in thermodynamics.</p>
<p>Now we are ready to see how all this plays out in probability theory!  We&#8217;ll get an analogy like this, which goes hand-in-hand with our earlier one:</p>
<div align="center">
<table border="1">
<tr>
<td> </td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q " class="latex" /> &nbsp; </td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">&nbsp; probability distribution &nbsp; </td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> &nbsp;</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">???</td>
</tr>
</table>
</div>
<p>This analogy is clearer than the last, because statistical mechanics reveals that the extensive variables in thermodynamics are really just summaries of probability distributions on microstates.  Furthermore, both thermodynamics and probability theory have a concept of <i>entropy</i>.</p>
<p>So, let&#8217;s take our manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> to consist of probability distributions on the set of microstates I was talking about before: the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}." class="latex" />   Actually, let&#8217;s use <i>nowhere vanishing</i> probability distributions:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+q_i+%3E+0%2C+%5C%3B+%5Csum_%7Bi%3D1%7D%5En+q_i+%3D+1+%5C%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = &#92;{ q &#92;in &#92;mathbb{R}^n : &#92;; q_i &gt; 0, &#92;; &#92;sum_{i=1}^n q_i = 1 &#92;} } " class="latex" /></p>
<p>I&#8217;m requiring <img src="https://s0.wp.com/latex.php?latex=q_i+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i &gt; 0" class="latex" /> to ensure <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a manifold, and also to make sure <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is differentiable: it ceases to be differentiable when one of the probabilities <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> hits zero.</p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a manifold, its cotangent bundle is a symplectic manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   And here&#8217;s the good news: we have a god-given entropy function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>namely the <b><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a></b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Csum_%7Bi+%3D+1%7D%5En+q_i+%5Cln+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;sum_{i = 1}^n q_i &#92;ln q_i } " class="latex" /></p>
<p>So, everything I just described about thermodynamics works in the setting of plain old probability theory!  Starting from our manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> and the entropy function, we get all the rest, leading up to the Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{(q,p) &#92;in T^&#92;ast Q : &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>that describes the relation between extensive and intensive variables.</p>
<p>For computations it helps to pick coordinates on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   Since the probabilities <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> sum to 1, they aren&#8217;t independent coordinates on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   So, we can either pick all but one of them as coordinates, or learn how to deal with non-independent coordinates, which are already completely standard in <a href="https://en.wikipedia.org/wiki/Homogeneous_coordinates">projective geometry</a>.  Let&#8217;s do the former, just to keep things simple.</p>
<p>These coordinates on <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> give rise in the usual way to coordinates <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   These play the role of extensive and intensive variables, respectively, and it should be very interesting to impose the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is the Shannon entropy.   This picks out a Lagrangian submanifold <img src="https://s0.wp.com/latex.php?latex=%5CLambda+%5Csubseteq+T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda &#92;subseteq T^&#92;ast Q." class="latex" /></p>
<p>So, the question becomes: what does this mean?  If this formula gives the analogue of momentum for probability theory, what does this analogue of momentum <i>mean?</i></p>
<p>Here&#8217;s a preliminary answer: <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how fast entropy increases as we increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> that our system is in the <i>i</i>th microstate.  So if we think of nature as &#8216;wanting&#8217; to maximize entropy, the quantity <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how eager it is to increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" /></p>
<p>Indeed, you can think of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> as a bit like <i>pressure</i>&#8212;one of the most famous intensive quantities in thermodynamics.   A gas &#8216;wants&#8217; to expand, and its pressure says precisely how eager it is to expand.   Similarly, a probability distribution &#8216;wants&#8217; to flatten out, to maximize entropy, and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how eager it is to increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> in order to do this.</p>
<p>But what can we <i>do</i> with this concept?  And what does symplectic geometry <i>do</i> for probability theory?</p>
<p>I will start tackling these questions next time.</p>
<p>One thing I&#8217;ll show is that when we reduce thermodynamics to probability theory using the ideas of statistical mechanics, the appearance of symplectic geometry in thermodynamics <i>follows</i> from its appearance in probability theory.</p>
<p>Another thing I want to investigate is how other geometrical structures on the space of probability distributions, like the <a href="https://math.ucr.edu/home/baez/information/information_geometry_1.html">Fisher information metric</a>, interact with the symplectic structure on its cotangent bundle.   This will integrate symplectic geometry and information geometry.</p>
<p>I also want to bring contact geometry into the picture.  It&#8217;s already easy to see from our work last time how this should go.  We treat the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as an independent variable, and replace <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> with a larger manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> having <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as an extra coordinate.  This is a contact manifold with contact form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_1+dq_i+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_1 dq_i + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>This contact manifold has a submanifold <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> where we remember that entropy is a function of the probability distribution <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> and define <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in terms of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as usual:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+S+%3D+f%28q%29%2C+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{(q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; S = f(q), &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>And as we saw last time, <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> is a Legendrian submanifold, meaning</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Calpha%7C_%7B%5CSigma%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;alpha|_{&#92;Sigma} = 0 } " class="latex" /></p>
<p>But again, we want to understand what these ideas from contact geometry really <i>do</i> for probability theory!</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p>• <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/#comments">2 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;18)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31205 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-probability" id="post-31205">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/" rel="bookmark">Fisher&#8217;s Fundamental Theorem (Part&nbsp;4)</a></h2>
				<small>13 July, 2021</small><br />


				<div class="entry">
					<p>I wrote a paper that summarizes my work connecting natural selection to information theory:</p>
<p>• John Baez, <a href="https://arxiv.org/abs/2107.05610">The fundamental theorem of natural selection</a>.</p>
<p>Check it out! If you have any questions or see any mistakes, please let me know.</p>
<p>Just for fun, here&#8217;s the abstract and introduction.</p>
<p><strong>Abstract.</strong> Suppose we have <i>n</i> different types of self-replicating entity, with the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of the <i>i</i>th type changing at a rate equal to <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> times the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> of that type. Suppose the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is any continuous function of all the populations <img src="https://s0.wp.com/latex.php?latex=P_1%2C+%5Cdots%2C+P_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_1, &#92;dots, P_n" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> be the fraction of replicators that are of the <i>i</i>th type. Then <img src="https://s0.wp.com/latex.php?latex=p+%3D+%28p_1%2C+%5Cdots%2C+p_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (p_1, &#92;dots, p_n)" class="latex" /> is a time-dependent probability distribution, and we prove that its speed as measured by the Fisher information metric equals the variance in fitness. In rough terms, this says that the speed at which information is updated through natural selection equals the variance in fitness. This result can be seen as a modified version of Fisher&#8217;s fundamental theorem of natural selection. We compare it to Fisher&#8217;s original result as interpreted by Price, Ewens and Edwards.</p>
<h4>Introduction</h4>
<p>In 1930, Fisher stated his &#8220;fundamental theorem of natural selection&#8221; as follows:</p>
<blockquote><p>The rate of increase in fitness of any organism at any time is equal to its genetic variance in fitness at that time</p></blockquote>
<p>Some tried to make this statement precise as follows:</p>
<blockquote><p>The time derivative of the mean fitness of a population equals the variance of its fitness.</p></blockquote>
<p>But this is only true under very restrictive conditions, so a controversy was ignited.</p>
<p>An interesting resolution was proposed by Price, and later amplified by Ewens and Edwards. We can formalize their idea as follows. Suppose we have <i>n</i> types of self-replicating entity, and idealize the population of the <i>i</i>th type as a real-valued function <img src="https://s0.wp.com/latex.php?latex=P_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t)" class="latex" />. Suppose</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+P_i%28t%29+%3D+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%2C+P_i%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} P_i(t) = f_i(P_1(t), &#92;dots, P_n(t)) &#92;, P_i(t) } " class="latex" /></p>
<p>where the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is a differentiable function of the populations of every type of replicator. The mean fitness at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Coverline%7Bf%7D%28t%29+%3D+%5Csum_%7Bi%3D1%7D%5En+p_i%28t%29+%5C%2C+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;overline{f}(t) = &#92;sum_{i=1}^n p_i(t) &#92;, f_i(P_1(t), &#92;dots, P_n(t)) } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t)" class="latex" /> is the fraction of replicators of the <i>i</i>th type:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Cphantom%7B%5CBig%7C%7D+%5Csum_%7Bj+%3D+1%7D%5En+P_j%28t%29+%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i(t) = &#92;frac{P_i(t)}{&#92;phantom{&#92;Big|} &#92;sum_{j = 1}^n P_j(t) } } " class="latex" /></p>
<p>By the product rule, the rate of change of the mean fitness is the sum of two terms:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+%5Coverline%7Bf%7D%28t%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cdot%7Bp%7D_i%28t%29+%5C%2C+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%3B+%2B+%5C%3B+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} &#92;overline{f}(t) = &#92;sum_{i=1}^n &#92;dot{p}_i(t) &#92;, f_i(P_1(t), &#92;dots, P_n(t)) &#92;; + &#92;; }" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i%28t%29+%5C%2C%5Cfrac%7Bd%7D%7Bdt%7D+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i(t) &#92;,&#92;frac{d}{dt} f_i(P_1(t), &#92;dots, P_n(t)) } " class="latex" /></p>
<p>The <i>first</i> of these two terms equals the variance of the fitness at time <i>t</i>. We give the easy proof in Theorem 1. Unfortunately, the conceptual significance of this first term is much less clear than that of the total rate of change of mean fitness. Ewens concluded that &#8220;the theorem does not provide the substantial biological statement that Fisher claimed&#8221;.</p>
<p>But there is another way out, based on an idea Fisher himself introduced in 1922: Fisher information. Fisher information gives rise to a Riemannian metric on the space of probability distributions on a finite set, called the &#8216;Fisher information metric&#8217;&#8212;or in the context of evolutionary game theory, the &#8216;Shahshahani metric&#8217;. Using this metric we can define the speed at which a time-dependent probability distribution changes with time. We call this its &#8216;Fisher speed&#8217;. Under just the assumptions already stated, we prove in Theorem 2 that the Fisher speed of the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28t%29+%3D+%28p_1%28t%29%2C+%5Cdots%2C+p_n%28t%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t) = (p_1(t), &#92;dots, p_n(t)) " class="latex" /></p>
<p>is the variance of the fitness at time <i>t</i>.</p>
<p>As explained by Harper, natural selection can be thought of as a learning process, and studied using ideas from information geometry&#8212;that is, the geometry of the space of probability distributions. As <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> changes with time, the rate at which information is updated is closely connected to its Fisher speed. Thus, our revised version of the fundamental theorem of natural selection can be loosely stated as follows:</p>
<blockquote><p>As a population changes with time, the rate at which information is updated equals the variance of fitness.</p></blockquote>
<p>The precise statement, with all the hypotheses, is in Theorem 2. But one lesson is this: variance in fitness may not cause &#8216;progress&#8217; in the sense of increased mean fitness, but it does cause change!</p>
<p>For more details in a user-friendly blog format, read the whole series:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-1/">Part 1</a>: the obscurity of Fisher&#8217;s original paper.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Part 2</a>: a precise statement of Fisher&#8217;s fundamental theorem of natural selection, and conditions under which it holds.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/">Part 3</a>: a modified version of the fundamental theorem of natural selection, which holds much more generally.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/">Part 4</a>: my paper on the fundamental theorem of natural selection.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/#comments">6 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/" rel="bookmark" title="Permanent Link to Fisher&#8217;s Fundamental Theorem (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-28874 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-probability" id="post-28874">
				<h2><a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/" rel="bookmark">Fisher&#8217;s Fundamental Theorem (Part&nbsp;3)</a></h2>
				<small>8 October, 2020</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Last time</a> we stated and proved a simple version of Fisher&#8217;s fundamental theorem of natural selection, which says that <i>under some conditions</i>, the rate of increase of the mean fitness equals the variance of the fitness.   But the conditions we gave were very restrictive: namely, that the fitness of each species of replicator is constant, not depending on how many of these replicators there are, or any other replicators.</p>
<p>To broaden the scope of Fisher&#8217;s fundamental theorem we need to do one of two things:</p>
<p>1) change the left side of the equation: talk about some other quantity other than rate of change of mean fitness.</p>
<p>2) change the right side of the question: talk about some other quantity than the variance in fitness.</p>
<p>Or we could do both!   <img src="https://i1.wp.com/math.ucr.edu/home/baez/emoticons/tongue2.gif" />  People have spent a lot of time generalizing Fisher&#8217;s fundamental theorem.   I don&#8217;t think there are, or should be, any hard rules on what counts as a generalization.</p>
<p>But today we&#8217;ll take alternative 1).  We&#8217;ll show the square of something called the &#8216;Fisher speed&#8217; <i>always</i> equals the variance in fitness.  One nice thing about this result is that we can drop the restrictive condition I mentioned.   Another nice thing is that the Fisher speed is a concept from information theory!  It&#8217;s defined using the <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">Fisher metric</a> on the space of probability distributions.</p>
<p>And yes&#8212;that metric is named after the same guy who proved Fisher&#8217;s fundamental theorem!  So, arguably, <i>Fisher</i> should have proved this generalization of Fisher&#8217;s fundamental theorem.  But in fact it seems that I was the first to prove it, around <a href="https://math.ucr.edu/home/baez/information/information_geometry_16.html">February 1st, 2017</a>.   Some similar results were already known, and I will discuss those someday.  But they&#8217;re a bit different.</p>
<p>A good way to think about the Fisher speed is that it&#8217;s &#8216;the rate at which information is being updated&#8217;.   A population of replicators of different species gives a probability distribution.  Like any probability distribution, this has information in it.   As the populations of our replicators change, the Fisher speed says the rate at which this information is being updated.  So, in simple terms, we&#8217;ll show</p>
<blockquote><p>
  The square of the rate at which information is updated is equal to the variance in fitness.</p></blockquote>
<p>This is quite a change from Fisher&#8217;s original idea, namely:</p>
<blockquote><p>
  The rate of increase of mean fitness is equal to the variance in fitness.</p></blockquote>
<p>But it has the advantage of always being true&#8230; as long the population dynamics are described by the general framework we introduced last time.  So let me remind you of the general setup, and then prove the result!</p>
<h3> The setup</h3>
<p>We start out with population functions <img src="https://s0.wp.com/latex.php?latex=P_i+%5Ccolon+%5Cmathbb%7BR%7D+%5Cto+%280%2C%5Cinfty%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i &#92;colon &#92;mathbb{R} &#92;to (0,&#92;infty)," class="latex" /> one for each species of replicator <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C%5Cdots%2Cn%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,&#92;dots,n," class="latex" /> obeying the <b>Lotka–Volterra equation</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) P_i } " class="latex" /></p>
<p>for some differentiable functions <img src="https://s0.wp.com/latex.php?latex=f_i+%5Ccolon+%280%2C%5Cinfty%29+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i &#92;colon (0,&#92;infty) &#92;to &#92;mathbb{R}" class="latex" /> called <b>fitness functions</b>.   The probability of a replicator being in the <i>i</i>th species is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Csum_j+P_j%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  p_i(t) = &#92;frac{P_i(t)}{&#92;sum_j P_j(t)} } " class="latex" /></p>
<p>Using the Lotka–Volterra equation we showed last time that these probabilities obey the <b>replicator equation</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5Cleft%28+f_i%28P%29+-+%5Coverline+f%28P%29+%5Cright%29++p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;left( f_i(P) - &#92;overline f(P) &#92;right)  p_i } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> is short for the whole list of populations <img src="https://s0.wp.com/latex.php?latex=%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(P_1(t), &#92;dots, P_n(t))," class="latex" /> and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Coverline+f%28P%29+%3D+%5Csum_j+f_j%28P%29+p_j++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;overline f(P) = &#92;sum_j f_j(P) p_j  } " class="latex" /></p>
<p>is the <b>mean fitness</b>.</p>
<h3>The Fisher metric</h3>
<p>The space of probability distributions on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}" class="latex" /> is called the <b>(n-1)-simplex</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D+%3D+%5C%7B+%28x_1%2C+%5Cdots%2C+x_n%29+%3A+%5C%3B+x_i+%5Cge+0%2C+%5C%3B+%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+x_i+%3D+1+%7D+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1} = &#92;{ (x_1, &#92;dots, x_n) : &#92;; x_i &#92;ge 0, &#92;; &#92;displaystyle{ &#92;sum_{i=1}^n x_i = 1 } &#92;} " class="latex" /></p>
<p>It&#8217;s called <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1}" class="latex" /> because it&#8217;s (n-1)-dimensional.  When <img src="https://s0.wp.com/latex.php?latex=n+%3D+3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 3" class="latex" /> it looks like the letter <img src="https://s0.wp.com/latex.php?latex=%5CDelta%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta:" class="latex" /></p>
<div align="center">
<a href="https://johncarlosbaez.files.wordpress.com/2020/10/800px-2d-simplex.svg_.png"><img src="https://johncarlosbaez.files.wordpress.com/2020/10/800px-2d-simplex.svg_.png?w=150" alt="" width="150" /></a></div>
<p>The <b>Fisher metric</b> is a Riemannian metric on the interior of the (n-1)-simplex.  That is, given a point <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in the interior of <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1}" class="latex" /> and two tangent vectors <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> at this point the Fisher metric gives a number</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28v%2Cw%29+%3D+%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7Bv_i+w_i%7D%7Bp_i%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(v,w) = &#92;displaystyle{ &#92;sum_{i=1}^n &#92;frac{v_i w_i}{p_i}  } " class="latex" /></p>
<p>Here we are describing the tangent vectors <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> as vectors in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> with the property that the sum of their components is zero: that&#8217;s what makes them tangent to the (n-1)-simplex.  And we&#8217;re demanding that <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> be in the interior of the simplex to avoid dividing by zero, since on the boundary of the simplex we have <img src="https://s0.wp.com/latex.php?latex=p_i+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = 0" class="latex" /> for at least one choice of $i.$</p>
<p>If we have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moving around in the interior of the (n-1)-simplex as a function of time, its <b>Fisher speed</b> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csqrt%7Bg%28%5Cdot%7Bp%7D%28t%29%2C+%5Cdot%7Bp%7D%28t%29%29%7D+%3D+%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%5Cdot%7Bp%7D_i%28t%29%5E2%7D%7Bp_i%28t%29%7D%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sqrt{g(&#92;dot{p}(t), &#92;dot{p}(t))} = &#92;sqrt{&#92;sum_{i=1}^n &#92;frac{&#92;dot{p}_i(t)^2}{p_i(t)}} } " class="latex" /></p>
<p>if the derivative <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}(t)" class="latex" /> exists.   This is the usual formula for the speed of a curve moving in a Riemannian manifold, specialized to the case at hand.</p>
<p>Now we&#8217;ve got all the formulas we&#8217;ll need to prove the result we want.  But for those who don&#8217;t already know and love it, it&#8217;s worthwhile saying a bit more about the Fisher metric.</p>
<p>The factor of <img src="https://s0.wp.com/latex.php?latex=1%2Fx_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/x_i" class="latex" /> in the Fisher metric changes the geometry of the simplex so that it becomes <i>round</i>, like a portion of a sphere:</p>
<div align="center">
<a href="https://johncarlosbaez.files.wordpress.com/2020/10/fisher_metric.jpg"><img src="https://johncarlosbaez.files.wordpress.com/2020/10/fisher_metric.jpg?w=250" alt="" width="250" /></a></div>
<p>But the reason the Fisher metric is important, I think, is its connection to relative information.  Given two probability distributions <img src="https://s0.wp.com/latex.php?latex=p%2C+q+%5Cin+%5CDelta%5E%7Bn-1%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q &#92;in &#92;Delta^{n-1}," class="latex" /> the <b>information of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /></b> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28q%2Cp%29+%3D+%5Csum_%7Bi+%3D+1%7D%5En+q_i+%5Cln%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i%7D%5Cright%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(q,p) = &#92;sum_{i = 1}^n q_i &#92;ln&#92;left(&#92;frac{q_i}{p_i}&#92;right)   } " class="latex" /></p>
<p>You can show this is the expected amount of information gained if <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> was your prior distribution and you receive information that causes you to update your prior to <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   So, sometimes it&#8217;s called the <b>information gain</b>.  It&#8217;s also called <b>relative entropy</b> or&#8212;my least favorite, since it sounds so mysterious&#8212;the <b><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a></b>.</p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> is a smooth curve in the interior of the (n-1)-simplex.  We can ask the rate at which information is gained as time passes.  Perhaps surprisingly, a calculation gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28p%28t%29%2C+p%28t_0%29%29%5CBig%7C_%7Bt+%3D+t_0%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(p(t), p(t_0))&#92;Big|_{t = t_0} = 0 } " class="latex" /></p>
<p>That is, in some sense &#8216;to first order&#8217; no information is being gained at any moment <img src="https://s0.wp.com/latex.php?latex=t_0+%5Cin+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_0 &#92;in &#92;mathbb{R}." class="latex" />   However, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7D+I%28p%28t%29%2C+p%28t_0%29%29%5CBig%7C_%7Bt+%3D+t_0%7D+%3D++g%28%5Cdot%7Bp%7D%28t_0%29%2C+%5Cdot%7Bp%7D%28t_0%29%29%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d^2}{dt^2} I(p(t), p(t_0))&#92;Big|_{t = t_0} =  g(&#92;dot{p}(t_0), &#92;dot{p}(t_0))}  " class="latex" /></p>
<p>So, the square of the Fisher speed has a nice interpretation in terms of relative entropy!</p>
<p>For a derivation of these last two equations, see <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/">Part 7</a> of my posts on information geometry.  For more on the meaning of relative entropy, see <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">Part 6</a>.</p>
<h3> The result</h3>
<p>It&#8217;s now extremely easy to show what we want, but let me state it formally so all the assumptions are crystal clear.</p>
<p><b>Theorem.</b>  Suppose the functions <img src="https://s0.wp.com/latex.php?latex=P_i+%5Ccolon+%5Cmathbb%7BR%7D+%5Cto+%280%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i &#92;colon &#92;mathbb{R} &#92;to (0,&#92;infty)" class="latex" /> obey the Lotka&#8211;Volterra equations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot+P_i+%3D+f_i%28P%29+P_i%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot P_i = f_i(P) P_i}  " class="latex" /></p>
<p>for some differentiable functions <img src="https://s0.wp.com/latex.php?latex=f_i+%5Ccolon+%280%2C%5Cinfty%29%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i &#92;colon (0,&#92;infty)^n &#92;to &#92;mathbb{R}" class="latex" /> called fitness functions.   Define probabilities and the mean fitness as above, and define the <b>variance of the fitness</b> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cmathrm%7BVar%7D%28f%28P%29%29+%3D++%5Csum_j+%28+f_j%28P%29+-+%5Coverline+f%28P%29%29%5E2+%5C%2C+p_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;mathrm{Var}(f(P)) =  &#92;sum_j ( f_j(P) - &#92;overline f(P))^2 &#92;, p_j } " class="latex" /></p>
<p>Then if none of the populations <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> are zero, the square of the Fisher speed of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29+%3D+%28p_1%28t%29%2C+%5Cdots+%2C+p_n%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t) = (p_1(t), &#92;dots , p_n(t))" class="latex" /> is the variance of the fitness:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29++%3D+%5Cmathrm%7BVar%7D%28f%28P%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(&#92;dot{p}, &#92;dot{p})  = &#92;mathrm{Var}(f(P)) " class="latex" /></p>
<p><b>Proof.</b> The proof is near-instantaneous.  We take the square of the Fisher speed:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%5Cdot%7Bp%7D_i%28t%29%5E2%7D%7Bp_i%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ g(&#92;dot{p}, &#92;dot{p}) = &#92;sum_{i=1}^n &#92;frac{&#92;dot{p}_i(t)^2}{p_i(t)} } " class="latex" /></p>
<p>and plug in the replicator equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%28f_i%28P%29+-+%5Coverline+f%28P%29%29+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = (f_i(P) - &#92;overline f(P)) p_i } " class="latex" /></p>
<p>We obtain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29%7D+%26%3D%26++%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%28+f_i%28P%29+-+%5Coverline+f%28P%29+%5Cright%29%5E2+p_i+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cmathrm%7BVar%7D%28f%28P%29%29++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ g(&#92;dot{p}, &#92;dot{p})} &amp;=&amp;  &#92;displaystyle{ &#92;sum_{i=1}^n &#92;left( f_i(P) - &#92;overline f(P) &#92;right)^2 p_i } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;mathrm{Var}(f(P))  &#92;end{array} " class="latex" /></p>
<p>as desired.   &nbsp; █</p>
<p>It&#8217;s hard to imagine anything simpler than this.  We see that given the Lotka–Volterra equation, what causes information to be updated is nothing more and nothing less than variance in fitness!</p>
<hr />
<p>The whole series:</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-1/">Part 1</a>: the obscurity of Fisher&#8217;s original paper.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Part 2</a>: a precise statement of Fisher&#8217;s fundamental theorem of natural selection, and conditions under which it holds.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/">Part 3</a>: a modified version of the fundamental theorem of natural selection, which holds much more generally.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/">Part 4</a>: my paper on the fundamental theorem of natural selection.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comments">10 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/" rel="bookmark" title="Permanent Link to Fisher&#8217;s Fundamental Theorem (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-23248 post type-post status-publish format-standard hentry category-biology category-game-theory category-information-and-entropy category-probability" id="post-23248">
				<h2><a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/" rel="bookmark">Information Geometry (Part&nbsp;16)</a></h2>
				<small>1 February, 2017</small><br />


				<div class="entry">
					<p>This week I&#8217;m giving a talk on biology and information:</p>
<p>• John Baez, <a href="http://math.ucr.edu/home/baez/bio_asu/">Biology as information dynamics</a>, talk for <a href="https://beyond.asu.edu/workshop/biological-complexity-can-it-be-quantified">Biological Complexity: Can it be Quantified?</a>, a workshop at the <a href="https://beyond.asu.edu/">Beyond Center</a>, 2 February 2017.</p>
<p>While preparing this talk, I discovered a cool fact.  I doubt it&#8217;s new, but I haven&#8217;t exactly seen it elsewhere.   I came up with it while trying to give a precise and general statement of &#8216;Fisher&#8217;s fundamental theorem of natural selection&#8217;.   I <i>won&#8217;t</i> start by explaining that theorem, since my version looks rather different than Fisher&#8217;s, and I came up with mine precisely because I had trouble understanding his.  I&#8217;ll say a bit more about this at the end.</p>
<p>Here&#8217;s my version:</p>
<blockquote><p>
The square of the rate at which a population learns information is the variance of its fitness.
</p></blockquote>
<p>This is a nice advertisement for the virtues of diversity: more variance means faster learning.   But it requires some explanation!</p>
<h3> The setup </h3>
<p>Let&#8217;s start by assuming we have <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different kinds of self-replicating entities with populations <img src="https://s0.wp.com/latex.php?latex=P_1%2C+%5Cdots%2C+P_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_1, &#92;dots, P_n." class="latex" />  As usual, these could be all sorts of things:</p>
<p>• molecules of different chemicals<br />
• organisms belonging to different species<br />
• genes of different alleles<br />
• restaurants belonging to different chains<br />
• people with different beliefs<br />
• game-players with different strategies<br />
• etc.</p>
<p>I&#8217;ll call them <b>replicators</b> of different <b>species</b>.</p>
<p>Let&#8217;s suppose each population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> is a function of time that grows at a rate equal to this population times its &#8216;fitness&#8217;.   I explained the resulting equation back in <a href="http://math.ucr.edu/home/baez/information/information_geometry_9.html">Part 9</a>, but it&#8217;s pretty simple:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+P_i%28t%29+%3D+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%2C+P_i%28t%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} P_i(t) = f_i(P_1(t), &#92;dots, P_n(t)) &#92;, P_i(t)   } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is a completely arbitrary smooth function of all the populations!  We call it the <b>fitness</b> of the <i>i</i>th species.</p>
<p>This equation is important, so we want a short way to write it.  I&#8217;ll often write <img src="https://s0.wp.com/latex.php?latex=f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P_1(t), &#92;dots, P_n(t))" class="latex" /> simply as <img src="https://s0.wp.com/latex.php?latex=f_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=P_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t)" class="latex" /> simply as <img src="https://s0.wp.com/latex.php?latex=P_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i." class="latex" />  With these abbreviations, which any red-blooded physicist would take for granted, our equation becomes simply this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdP_i%7D%7Bd+t%7D++%3D+f_i+%5C%2C+P_i+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dP_i}{d t}  = f_i &#92;, P_i   } " class="latex" /></p>
<p>Next, let <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t)" class="latex" /> be the probability that a randomly chosen organism is of the <i>i</i>th species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Csum_j+P_j%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i(t) = &#92;frac{P_i(t)}{&#92;sum_j P_j(t)} } " class="latex" /></p>
<p>Starting from our equation describing how the populations evolve, we can figure out how these probabilities evolve.  The answer is called the <a href="https://en.wikipedia.org/wiki/Replicator_equation"><b>replicator equation</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+p_i%28t%29++%3D+%28+f_i+-+%5Clangle+f+%5Crangle+%29+%5C%2C+p_i%28t%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} p_i(t)  = ( f_i - &#92;langle f &#92;rangle ) &#92;, p_i(t) }" class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Clangle+f+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle f &#92;rangle" class="latex" /> is the average fitness of all the replicators, or <b>mean fitness</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f+%5Crangle+%3D+%5Csum_j+f_j%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%2C+p_j%28t%29++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f &#92;rangle = &#92;sum_j f_j(P_1(t), &#92;dots, P_n(t)) &#92;, p_j(t)  } " class="latex" /></p>
<p>In what follows I&#8217;ll abbreviate the replicator equation as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bdp_i%7D%7Bd+t%7D++%3D+%28+f_i+-+%5Clangle+f+%5Crangle+%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dp_i}{d t}  = ( f_i - &#92;langle f &#92;rangle ) &#92;, p_i }" class="latex" /></p>
<h3> The result </h3>
<p>Okay, now let&#8217;s figure out how fast the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28t%29+%3D+%28p_1%28t%29%2C+%5Cdots%2C+p_n%28t%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t) = (p_1(t), &#92;dots, p_n(t)) " class="latex" /></p>
<p>changes with time.  For this we need to choose a way to measure the length of the vector</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bdp%7D%7Bdt%7D+%3D+%28%5Cfrac%7Bd%7D%7Bdt%7D+p_1%28t%29%2C+%5Cdots%2C+%5Cfrac%7Bd%7D%7Bdt%7D+p_n%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{dp}{dt} = (&#92;frac{d}{dt} p_1(t), &#92;dots, &#92;frac{d}{dt} p_n(t)) } " class="latex" /></p>
<p>And here information geometry comes to the rescue!   We can use the <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">Fisher information metric</a>, which is a Riemannian metric on the space of probability distributions.</p>
<p>I&#8217;ve talked about the Fisher information metric in many ways in this series.  The most important fact is that as a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> changes with time, its speed</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cleft%5C%7C+%5Cfrac%7Bdp%7D%7Bdt%7D+%5Cright%5C%7C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;left&#92;| &#92;frac{dp}{dt} &#92;right&#92;|} " class="latex" /></p>
<p>as measured using the Fisher information metric can be seen as the <i>rate at which information is learned</i>.  I&#8217;ll explain that later.  Right now I just want a simple <i>formula</i> for the Fisher information metric.  Suppose <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w" class="latex" /> are two tangent vectors to the point <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in the space of probability distributions.  Then the <b>Fisher information metric</b> is given as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+v%2C+w+%5Crangle+%3D+%5Csum_i+%5Cfrac%7B1%7D%7Bp_i%7D+%5C%2C+v_i+w_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle v, w &#92;rangle = &#92;sum_i &#92;frac{1}{p_i} &#92;, v_i w_i } " class="latex" /></p>
<p>Using this we can calculate the speed at which <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moves when it obeys the replicator equation.  Actually the square of the speed is simpler:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cleft%5C%7C+%5Cfrac%7Bdp%7D%7Bdt%7D++%5Cright%5C%7C%5E2+%7D+%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i+%5Cfrac%7B1%7D%7Bp_i%7D+%5Cleft%28+%5Cfrac%7Bdp_i%7D%7Bdt%7D+%5Cright%29%5E2+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i+%5Cfrac%7B1%7D%7Bp_i%7D+%5Cleft%28+%28+f_i+-+%5Clangle+f+%5Crangle+%29+%5C%2C+p_i+%5Cright%29%5E2+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i++%28+f_i+-+%5Clangle+f+%5Crangle+%29%5E2+p_i+%7D+++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;left&#92;| &#92;frac{dp}{dt}  &#92;right&#92;|^2 } &amp;=&amp; &#92;displaystyle{ &#92;sum_i &#92;frac{1}{p_i} &#92;left( &#92;frac{dp_i}{dt} &#92;right)^2 } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_i &#92;frac{1}{p_i} &#92;left( ( f_i - &#92;langle f &#92;rangle ) &#92;, p_i &#92;right)^2 } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_i  ( f_i - &#92;langle f &#92;rangle )^2 p_i }   &#92;end{array}  " class="latex" /></p>
<p>The answer has a nice meaning, too!  It&#8217;s just the <a href="https://en.wikipedia.org/wiki/Variance">variance</a> of the fitness: that is, the square of its <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a>.</p>
<p>So, if you&#8217;re willing to buy my claim that the speed <img src="https://s0.wp.com/latex.php?latex=%5C%7Cdp%2Fdt%5C%7C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;|dp/dt&#92;|" class="latex" /> is the rate at which our population learns new information, then we&#8217;ve seen that <i>the square of the rate at which a population learns information is the variance of its fitness!</i></p>
<h3> Fisher&#8217;s fundamental theorem </h3>
<p>Now, how is this related to Fisher&#8217;s fundamental theorem of natural selection?    First of all, what <i>is</i> Fisher&#8217;s fundamental theorem?  Here&#8217;s what <a href="https://en.wikipedia.org/wiki/Fisher's_fundamental_theorem_of_natural_selection" rel="nofollow">Wikipedia says</a> about it:</p>
<blockquote><p>
  It uses some mathematical notation but is not a theorem in the mathematical sense.</p>
<p>  It states:</p>
<blockquote><p>
  &#8220;The rate of increase in fitness of any organism at any time is equal to its genetic variance in fitness at that time.&#8221;
</p></blockquote>
<p>  Or in more modern terminology:</p>
<blockquote><p>
  &#8220;The rate of increase in the mean fitness of any organism at any time ascribable to natural selection acting through changes in gene frequencies is exactly equal to its genetic variance in fitness at that time&#8221;.
</p></blockquote>
<p>  Largely as a result of Fisher&#8217;s feud with the American geneticist Sewall Wright about adaptive landscapes, the theorem was widely misunderstood to mean that the average fitness of a population would always increase, even though models showed this not to be the case. In 1972, George R. Price showed that Fisher&#8217;s theorem was indeed correct (and that Fisher&#8217;s proof was also correct, given a typo or two), but did not find it to be of great significance. The sophistication that Price pointed out, and that had made understanding difficult, is that the theorem gives a formula for part of the change in gene frequency, and not for all of it. This is a part that can be said to be due to natural selection
</p></blockquote>
<p>Price&#8217;s paper is here:</p>
<p>• George R. Price, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.4070&amp;rep=rep1&amp;type=pdf" rel="nofollow">Fisher&#8217;s &#8216;fundamental theorem&#8217; made clear</a>, <em>Annals of Human Genetics</em> <strong>36</strong> (1972), 129–140.</p>
<p>I don&#8217;t find it very clear, perhaps because I didn&#8217;t spend enough time on it.  But I think I get the idea.</p>
<p>My result <i>is</i> a theorem in the mathematical sense, though quite an easy one.  I assume a population distribution evolves according to the replicator equation and derive an equation whose right-hand side matches that of Fisher&#8217;s original equation: the variance of the fitness.</p>
<p>But my left-hand side is different: it&#8217;s the square of the speed of the corresponding probability distribution, where speed is measured using the &#8216;Fisher information metric&#8217;.  This metric was discovered by the same guy, Ronald Fisher, but I don&#8217;t think he used it in <em>his</em> work on the fundamental theorem!</p>
<p>Something a bit similar to my statement appears as Theorem 2 of this paper:</p>
<p>• Marc Harper, <a href="http://arxiv.org/abs/0911.1383" rel="nofollow">Information geometry and evolutionary game theory</a>.</p>
<p>and for that theorem he cites:</p>
<p>• Josef Hofbauer and Karl Sigmund, <em>Evolutionary Games and Population Dynamics</em>, Cambridge University Press, Cambridge, 1998.</p>
<p>However, his Theorem 2 really concerns the rate of increase of fitness, like Fisher&#8217;s fundamental theorem.  Moreover, he assumes that the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> flows along the gradient of a function, and I&#8217;m not assuming that.  Indeed, my version applies to situations where the probability distribution moves round and round in periodic orbits!</p>
<h3> Relative information and the Fisher information metric </h3>
<p>The key to generalizing Fisher&#8217;s fundamental theorem is thus to focus on the speed at which <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moves, rather than the increase in fitness.  Why do I call this speed the &#8216;rate at which the population learns information&#8217;?  It&#8217;s because we&#8217;re measuring this speed using the Fisher information metric, which is closely connected to <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">relative information</a>, also known as relative entropy or the Kullback&ndash;Leibler divergence.</p>
<p>I explained this back in <a href="http://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>, but that explanation seems hopelessly technical to me now, so here&#8217;s a faster one, which I created while preparing my talk.</p>
<p>The information of a probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> <b>relative to</b> a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++++I%28q%2Cp%29+%3D+%5Csum_%7Bi+%3D1%7D%5En+q_i+%5Clog%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i%7D%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{     I(q,p) = &#92;sum_{i =1}^n q_i &#92;log&#92;left(&#92;frac{q_i}{p_i}&#92;right) }" class="latex" /></p>
<p>It says how much information you learn if you start with a hypothesis <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> saying that the probability of the <i>i</i>th situation was <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> and then update this to a new hypothesis <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>Now suppose you have a hypothesis that&#8217;s changing with time in a smooth way, given by a time-dependent probability <img src="https://s0.wp.com/latex.php?latex=p%28t%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)." class="latex" />   Then a calculation shows that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%7D%7Bdt%7D+I%28p%28t%29%2Cp%28t_0%29%29+%5Cright%7C_%7Bt+%3D+t_0%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d}{dt} I(p(t),p(t_0)) &#92;right|_{t = t_0} = 0 } " class="latex" /></p>
<p>for all times <img src="https://s0.wp.com/latex.php?latex=t_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_0" class="latex" />.  This seems paradoxical at first.  I like to jokingly put it this way:</p>
<blockquote><p>
To first order, you&#8217;re never learning anything.
</p></blockquote>
<p>However, as long as the velocity <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bdt%7Dp%28t_0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{dt}p(t_0)" class="latex" /> is nonzero, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7D+I%28p%28t%29%2Cp%28t_0%29%29+%5Cright%7C_%7Bt+%3D+t_0%7D+%3E+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d^2}{dt^2} I(p(t),p(t_0)) &#92;right|_{t = t_0} &gt; 0 } " class="latex" /></p>
<p>so we can say</p>
<blockquote><p>
To second order, you&#8217;re always learning something&#8230; unless your opinions are fixed.
</p></blockquote>
<p>This lets us define a &#8216;rate of learning&#8217;&#8212;that is, a &#8216;speed&#8217; at which the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moves.  <i>And this is precisely the speed given by the Fisher information metric!</i></p>
<p>In other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft%5C%7C%5Cfrac%7Bdp%7D%7Bdt%7D%28t_0%29%5Cright%5C%7C%5E2+%3D++%5Cleft.%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7D+I%28p%28t%29%2Cp%28t_0%29%29+%5Cright%7C_%7Bt+%3D+t_0%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left&#92;|&#92;frac{dp}{dt}(t_0)&#92;right&#92;|^2 =  &#92;left.&#92;frac{d^2}{dt^2} I(p(t),p(t_0)) &#92;right|_{t = t_0} } " class="latex" /></p>
<p>where the length is given by Fisher information metric.   Indeed, this formula can be used to <i>define</i> the Fisher information metric.  From this definition we can easily work out the concrete formula I gave earlier.</p>
<p>In summary: as a probability distribution moves around, the relative information between the new probability distribution and the original one grows approximately as the <i>square</i> of time, not linearly.  So, to talk about a &#8216;rate at which information is learned&#8217;, we need to use the above formula, involving a second time derivative.  This rate is just the speed at which the probability distribution moves, measured using the Fisher information metric.  And when we have a probability distribution describing how many replicators are of different species, and it&#8217;s evolving according to the replicator equation, this speed is also just the variance of the fitness!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comments">29 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/" rel="category tag">game theory</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;16)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2576 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2576">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/" rel="bookmark">Information Geometry (Part&nbsp;7)</a></h2>
				<small>2 March, 2011</small><br />


				<div class="entry">
					<p>Today, I want to describe how the <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Fisher information metric</a> is related to <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  I&#8217;ve explained both these concepts separately (click the links for details); now I want to put them together. </p>
<p>But first, let me explain what this whole series of blog posts is about.  Information geometry, obviously!  But what&#8217;s that?</p>
<p>Information geometry is the geometry of &#8216;statistical manifolds&#8217;.   Let me explain that concept twice: first vaguely, and then precisely.  </p>
<p>Vaguely speaking, a statistical manifold is a <a href="http://en.wikipedia.org/wiki/Manifold">manifold</a> whose points are hypotheses about some situation.   For example, suppose you have a coin.  You could have various hypotheses about what happens when you flip it.  For example: you could hypothesize that the coin will land heads up with probability <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is any number between 0 and 1.  This makes the interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,1]" class="latex" /> into a statistical manifold.  Technically this is a manifold <i>with boundary</i>, but that&#8217;s okay.</p>
<p>Or, you could have various hypotheses about the IQ&#8217;s of American politicians.  For example: you could hypothesize that they&#8217;re distributed according to a Gaussian probability distribution with mean <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and standard deviation <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />.  This makes the space of pairs <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x,y)" class="latex" /> into a statistical manifold.  Of course we require <img src="https://s0.wp.com/latex.php?latex=y+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;ge 0" class="latex" />, which gives us a manifold with boundary. We might also want to assume <img src="https://s0.wp.com/latex.php?latex=x+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;ge 0" class="latex" />, which would give us a manifold <i>with corners</i>, but that&#8217;s okay too.  We&#8217;re going to be pretty relaxed about what counts as a &#8216;manifold&#8217; here.</p>
<p>If we have a manifold whose points are hypotheses about some situation, we say the manifold &#8216;parametrizes&#8217; these hypotheses.  So, the concept of statistical manifold is fundamental to the subject known as <a href="http://en.wikipedia.org/wiki/Parametric_statistics">parametric statistics</a>.  </p>
<p>Parametric statistics is a huge subject!   You could say that information geometry is the application of geometry to this subject.</p>
<p>But now let me go ahead and make the idea of &#8216;statistical manifold&#8217; more precise.  There&#8217;s a classical and a quantum version of this idea.  I&#8217;m working at the <a href="http://www.quantumlah.org/">Centre of Quantum Technologies</a>, so I&#8217;m being paid to be quantum&mdash;but today I&#8217;m in a classical mood, so I&#8217;ll only describe the classical version.  Let&#8217;s say a <b>classical statistical manifold</b> is a smooth function <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> from a manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the space of probability distributions on some measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  </p>
<p>We should think of <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> as a space of <b>events</b>.  In our first example, it&#8217;s just <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%2C+T%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{H, T&#92;}" class="latex" />: we flip a coin and it lands either heads up or tails up.   In our second it&#8217;s <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" />: we measure the IQ of an American politician and get some real number.</p>
<p>We should think of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> as a space of <b>hypotheses</b>.  For each point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />, we have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  This is hypothesis about the events in question: for example &#8220;when I flip the coin, there&#8217;s 55% chance that it will land heads up&#8221;, or &#8220;when I measure the IQ of an American politician, the answer will be distributed according to a Gaussian with mean 0 and standard deviation 100.&#8221;</p>
<p>Now, suppose someone hands you a classical statistical manifold <img src="https://s0.wp.com/latex.php?latex=%28M%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(M,p)" class="latex" />.  Each point in <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> is a hypothesis.  Apparently some hypotheses are more similar than others.  It would be nice to make this precise.  So, you might like to define a metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> that says how &#8216;far apart&#8217; two hypotheses are.  People know lots of ways to do this; the challenge is to find ways that have clear meanings.</p>
<p>Last time I explained the concept of <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  Suppose we have two probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />. Then the <b>entropy of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /></b> is the amount of information you gain when you start with the hypothesis <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> but then discover that you should switch to the new improved hypothesis <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  It equals:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp%7D%7Bq%7D+%5C%3B+%5Cln%28%5Cfrac%7Bp%7D%7Bq%7D%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; &#92;frac{p}{q} &#92;; &#92;ln(&#92;frac{p}{q}) &#92;; q d &#92;omega " class="latex" /></p>
<p>You could try to use this to define a distance between points <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> in our statistical manifold, like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29+%3D++%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp_x%7D%7Bp_y%7D+%5C%3B+%5Cln%28%5Cfrac%7Bp_x%7D%7Bp_y%7D%29+%5C%3B+p_y+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y) =  &#92;int_&#92;Omega &#92;; &#92;frac{p_x}{p_y} &#92;; &#92;ln(&#92;frac{p_x}{p_y}) &#92;; p_y d &#92;omega " class="latex" /></p>
<p>This is definitely an important function.  Unfortunately, as I explained <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">last time</a>, it doesn&#8217;t obey the axioms that a distance function should!   Worst of all, it doesn&#8217;t obey the triangle inequality.</p>
<p>Can we &#8216;fix&#8217; it?  Yes, we can!   And when we do, we get the Fisher information metric, which is actually a <i>Riemannian</i> metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  Suppose we put local coordinates on some patch of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> containing the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  Then the <b>Fisher information metric</b> is given by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D%28x%29+%3D+%5Cint_%5COmega++%5Cpartial_i+%28%5Cln+p_x%29+%5C%3B+%5Cpartial_j+%28%5Cln+p_x%29+%5C%3B+p_x+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}(x) = &#92;int_&#92;Omega  &#92;partial_i (&#92;ln p_x) &#92;; &#92;partial_j (&#92;ln p_x) &#92;; p_x d &#92;omega" class="latex" /></p>
<p>You can think of my whole series of articles so far as an attempt to understand this funny-looking formula.   I&#8217;ve shown how to get it from a few different starting-points, most recently back in <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Part 3</a>.  But now let&#8217;s get it starting from relative entropy!</p>
<p>Fix any point in our statistical manifold and choose local coordinates for which this point is the origin, <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" />.  The amount of information we gain if move to some other point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is the relative entropy <img src="https://s0.wp.com/latex.php?latex=S%28x%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,0)" class="latex" />.   But what&#8217;s this like when <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is really close to <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" />?  We can imagine doing a Taylor series expansion of <img src="https://s0.wp.com/latex.php?latex=S%28x%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,0)" class="latex" /> to answer this question.</p>
<p>Surprisingly, to first order the answer is always zero!  Mathematically:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>In plain English: if you change your mind slightly, you  learn a negligible amount &mdash; <i>not</i> an amount proportional to how much you changed your mind.</p>
<p>This must have some profound significance.  I wish I knew what.  Could it mean that people are reluctant to change their minds except in big jumps?</p>
<p>Anyway, if you think about it, this fact makes it obvious that <img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y)" class="latex" /> can&#8217;t obey the triangle inequality.  <img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y)" class="latex" /> could be pretty big, but if we draw a curve from <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />, and mark <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> closely spaced points <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> on this curve, then <img src="https://s0.wp.com/latex.php?latex=S%28x_%7Bi%2B1%7D%2C+x_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x_{i+1}, x_i)" class="latex" /> is zero to first order, so it must be of order <img src="https://s0.wp.com/latex.php?latex=1%2Fn%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/n^2" class="latex" />, so if the triangle inequality were true we&#8217;d have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29+%5Cle+%5Csum_i+S%28x_%7Bi%2Bi%7D%2Cx_i%29+%5Cle+%5Cmathrm%7Bconst%7D+%5C%2C+n+%5Ccdot+%5Cfrac%7B1%7D%7Bn%5E2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y) &#92;le &#92;sum_i S(x_{i+i},x_i) &#92;le &#92;mathrm{const} &#92;, n &#92;cdot &#92;frac{1}{n^2}" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />, which is a contradiction.</p>
<p>In plain English: if you change your mind in one big jump, the amount of information you gain is more than the sum of the amounts you&#8217;d gain if you change your mind in lots of little steps!  This seems pretty darn strange, but the paper I mentioned in <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">part 1</a> helps:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>.</p>
<p>You&#8217;ll see he takes a curve and chops it into lots of little pieces as I just did, and explains what&#8217;s going on.</p>
<p>Okay, so what about second order?  What&#8217;s</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} ?" class="latex" /></p>
<p>Well, this is the punchline of this blog post: <i>it&#8217;s the Fisher information metric:</i></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>And since the Fisher information metric is a <a href="http://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian metric</a>, we can then apply <a href="http://en.wikipedia.org/wiki/Riemannian_manifold#Riemannian_manifolds_as_metric_spaces">the usual recipe</a> and define distances in a way that obeys the triangle inequality.  Crooks calls this distance <b>thermodynamic length</b> in the special case that he considers, and he explains its physical meaning.</p>
<p>Now let me prove that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>This can be somewhat tedious if you do it by straighforwardly grinding it out&mdash;I know, I did it.  So let me show you a better way, which requires more conceptual acrobatics but less brute force.</p>
<p>The trick is to work with the <b>universal</b> statistical manifold for the measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  Namely, we take <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to be the space of <i>all</i> probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />!  This is typically an <i>infinite-dimensional</i> manifold, but that&#8217;s okay: we&#8217;re being relaxed about what counts as a manifold here.  In this case, we don&#8217;t need to write <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> for the probability distribution corresponding to the point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />.  In this case, a point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> just <i>is</i> a probability distribution on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, so we&#8217;ll just call it <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  </p>
<p>If we can prove the formulas for this universal example, they&#8217;ll automatically follow for every other example, by abstract nonsense.  Why?  Because <i>any</i> statistical manifold with measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> is the same as a manifold with a smooth map to the <i>universal</i> statistical manifold!  So, geometrical structures on the universal one <a href="http://en.wikipedia.org/wiki/Pullback_%28differential_geometry%29">&#8216;pull back&#8217;</a> to give structures on all the rest.  The Fisher information metric and the function <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> can be defined as pullbacks in this way!   So, to study them, we can just study the universal example.  </p>
<p>(If you&#8217;re familiar with &#8216;classifying spaces for bundles&#8217; or other sorts of &#8216;classifying spaces&#8217;, all this should seem awfully familiar.  It&#8217;s a standard math trick.)</p>
<p>So, let&#8217;s prove that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>by proving it in the universal example. Given any probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, and taking a nearby probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bp%7D%7Bq%7D+%3D+1+%2B+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{p}{q} = 1 + f " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is some small function.   We only need to show that <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> is zero to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  And this is pretty easy.  By definition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp%7D%7Bq%7D+%5C%2C+%5Cln%28%5Cfrac%7Bp%7D%7Bq%7D%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; &#92;frac{p}{q} &#92;, &#92;ln(&#92;frac{p}{q}) &#92;; q d &#92;omega " class="latex" /></p>
<p>or in other words,</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+%281+%2B+f%29+%5C%2C+%5Cln%281+%2B+f%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; (1 + f) &#92;, &#92;ln(1 + f) &#92;; q d &#92;omega " class="latex" /></p>
<p>We can calculate this to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and show we get zero.   But let&#8217;s actually work it out to second order, since we&#8217;ll need that later:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cln+%281+%2B+f%29+%3D+f+-+%5Cfrac%7B1%7D%7B2%7D+f%5E2+%2B+%5Ccdots+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (1 + f) = f - &#92;frac{1}{2} f^2 + &#92;cdots " class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%281+%2B+f%29+%5C%2C+%5Cln+%281%2B+f%29+%3D+f+%2B+%5Cfrac%7B1%7D%7B2%7D+f%5E2+%2B+%5Ccdots+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1 + f) &#92;, &#92;ln (1+ f) = f + &#92;frac{1}{2} f^2 + &#92;cdots " class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+S%28p%2Cq%29+%26%3D%26+%5Cint_%5COmega+%5C%3B+%281+%2B+f%29+%5C%3B+%5Cln%281+%2B+f%29+%5C%3B+q+d+%5Comega+%5C%5C+%26%3D%26+%5Cint_%5COmega+f+%5C%2C+q+d+%5Comega+%2B+%5Cfrac%7B1%7D%7B2%7D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+%2B+%5Ccdots+%5Cend%7Baligned%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{aligned} S(p,q) &amp;=&amp; &#92;int_&#92;Omega &#92;; (1 + f) &#92;; &#92;ln(1 + f) &#92;; q d &#92;omega &#92;&#92; &amp;=&amp; &#92;int_&#92;Omega f &#92;, q d &#92;omega + &#92;frac{1}{2} &#92;int_&#92;Omega f^2&#92;, q d &#92;omega + &#92;cdots &#92;end{aligned} " class="latex" /></p>
<p>Why does this vanish to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />?  It&#8217;s because <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> are both probability distributions and  <img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" />, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%281+%2B+f%29+%5C%2C+q+d%5Comega+%3D+%5Cint_%5COmega+p+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega (1 + f) &#92;, q d&#92;omega = &#92;int_&#92;Omega p d&#92;omega = 1" class="latex" /></p>
<p>but also</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+q+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega q d&#92;omega = 1" class="latex" /></p>
<p>so subtracting we see</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+f+%5C%2C+q+d%5Comega+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega f &#92;, q d&#92;omega = 0" class="latex" /></p>
<p>So, <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> vanishes to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  <i>Voil&agrave;!</i></p>
<p>Next let&#8217;s prove the more interesting formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>which relates relative entropy to the Fisher information metric. Since both sides are symmetric matrices, it suffices to show their diagonal entries agree in any coordinate system:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bii%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = g_{ii}" class="latex" /></p>
<p>Devoted followers of this series of posts will note that I keep using this trick, which takes advantage of the <a href="http://en.wikipedia.org/wiki/Polarization_identity">polarization identity</a>.  </p>
<p>To prove </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bii%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = g_{ii}" class="latex" /></p>
<p>it&#8217;s enough to consider the universal example.  We take the origin to be some probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and take <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> to be a nearby probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> which is pushed a tiny bit in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate direction.  As before we write <img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" />.  We look at the second-order term in our formula for <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2} &#92;int_&#92;Omega f^2&#92;, q d &#92;omega " class="latex" /></p>
<p>Using the usual second-order Taylor&#8217;s formula, which has a <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> built into it, we can say</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = &#92;int_&#92;Omega f^2&#92;, q d &#92;omega " class="latex" /> </p>
<p>On the other hand, our formula for the Fisher information metric gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D+%5Cleft.+%5Cint_%5COmega++%5Cpartial_i+%5Cln+p+%5C%3B+%5Cpartial_i+%5Cln+p+%5C%3B+q+d+%5Comega+%5Cright%7C_%7Bp%3Dq%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} = &#92;left. &#92;int_&#92;Omega  &#92;partial_i &#92;ln p &#92;; &#92;partial_i &#92;ln p &#92;; q d &#92;omega &#92;right|_{p=q} " class="latex" /></p>
<p>The right hand sides of the last two formulas look awfully similar!  And indeed they agree, because we can show that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cpartial_i+%5Cln+p+%5Cright%7C_%7Bp+%3D+q%7D+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;partial_i &#92;ln p &#92;right|_{p = q} = f" class="latex" /></p>
<p>How?  Well, we assumed that <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is what we get by taking <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and pushing it a little bit in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate direction; we have also written that little change as</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" /></p>
<p>for some small function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  So, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%28p%2Fq%29+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i (p/q) = f" class="latex" /></p>
<p>and thus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+p+%3D+f+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i p = f q" class="latex" /></p>
<p>and thus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cln+p+%3D+%5Cfrac%7B%5Cpartial_i+p%7D%7Bp%7D+%3D+%5Cfrac%7Bfq%7D%7Bp%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;ln p = &#92;frac{&#92;partial_i p}{p} = &#92;frac{fq}{p}" class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cpartial_i+%5Cln+p+%5Cright%7C_%7Bp%3Dq%7D+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;partial_i &#92;ln p &#92;right|_{p=q} = f" class="latex" /></p>
<p>as desired.</p>
<p>This argument may seem a little hand-wavy and nonrigorous, with words like &#8216;a little bit&#8217;.  If you&#8217;re used to taking arguments involving infinitesimal changes and translating them into calculus (or differential geometry), it should make sense.  If it doesn&#8217;t, I apologize.  It&#8217;s easy to make it more rigorous, but only at the cost of more annoying notation, which doesn&#8217;t seem good in a blog post.</p>
<h4>Boring technicalities</h4>
<p>If you&#8217;re actually the kind of person who reads a section called &#8216;boring technicalities&#8217;, I&#8217;ll admit to you that my calculations don&#8217;t make sense if the integrals diverge, or we&#8217;re dividing by zero in the ratio <img src="https://s0.wp.com/latex.php?latex=p%2Fq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q" class="latex" />.  To avoid these problems, here&#8217;s what we should do.  Fix a <a href="http://en.wikipedia.org/wiki/%CE%A3-finite_measure"><img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma" class="latex" />-finite</a> measure space <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega, d&#92;omega)" class="latex" />.  Then, define the <b>universal statistical manifold</b> to be the space <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> consisting of all probability measures that are <a href="http://en.wikipedia.org/wiki/Equivalence_%28measure_theory%29">equivalent</a> to <img src="https://s0.wp.com/latex.php?latex=d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;omega" class="latex" />, in the usual sense of measure theory.  By <a href="http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem">Radon-Nikodym</a>, we can write any such measure as <img src="https://s0.wp.com/latex.php?latex=q+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q d &#92;omega" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in L^1(&#92;Omega, d&#92;omega)" class="latex" />.  Moreover, given two of these guys, say <img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q d&#92;omega" class="latex" />, they are <a href="http://en.wikipedia.org/wiki/Absolute_continuity#Absolute_continuity_of_measures">absolutely continuous</a> with respect to each other, so we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega+%3D+%5Cfrac%7Bp%7D%7Bq%7D+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega = &#92;frac{p}{q} &#92;; q d &#92;omega " class="latex" /></p>
<p>where the ratio <img src="https://s0.wp.com/latex.php?latex=p%2Fq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q" class="latex" /> is well-defined almost everywhere and lies in <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+q+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, q d&#92;omega)" class="latex" />.  This is enough to guarantee that we&#8217;re never dividing by zero, and I think it&#8217;s enough to make sure all my integrals converge.</p>
<p>We do still need to make <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> into some sort of infinite-dimensional manifold, to justify all the derivatives.   There are various ways to approach this issue, all of which start from the fact that <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, d&#92;omega)" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Banach_space">Banach space</a>, which is about the nicest sort of infinite-dimensional manifold one could imagine.  Sitting in <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, d&#92;omega)" class="latex" /> is the hyperplane consisting of functions <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> with </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+q+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega q d&#92;omega = 1" class="latex" /></p>
<p>and this is a <a href="http://en.wikipedia.org/wiki/Banach_manifold">Banach manifold</a>.  To get <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> we need to take a subspace of that hyperplane.  If this subspace were open then <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> would be a Banach manifold in its own right.  I haven&#8217;t checked this yet, for various reasons.  </p>
<p>For one thing, there&#8217;s a nice theory of <a href="http://en.wikipedia.org/wiki/Diffeology">&#8216;diffeological spaces&#8217;</a>, which generalize manifolds.  Every Banach manifold is a diffeological space, and every subset of a diffeological space is again a diffeological space.  For many purposes we don&#8217;t need our &#8216;statistical manifolds&#8217; to be manifolds: diffeological spaces will do just fine.  This is one reason why I&#8217;m being pretty relaxed here about what counts as a &#8216;manifold&#8217;.</p>
<p>For another, I know that people have worked out a lot of this stuff, so I can just look things up when I need to.  And so can you!  This book is a good place to start:</p>
<p>&bull; Paolo Gibilisco, Eva Riccomagno, Maria Piera Rogantin and Henry P. Wynn, <i>Algebraic and Geometric Methods in Statistics</i>, Cambridge U. Press, Cambridge, 2009.</p>
<p>I find the chapters by Raymond Streater especially congenial.  For the technical issue I&#8217;m talking about now it&#8217;s worth reading section 14.2, &#8220;Manifolds modelled by Orlicz spaces&#8221;, which tackles the problem of constructing a universal statistical manifold in a more sophisticated way than I&#8217;ve just done.  And in chapter 15, &#8220;The Banach manifold of quantum states&#8221;, he tackles the quantum version!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comments">25 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;7)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2298 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2298">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="bookmark">Information Geometry (Part&nbsp;6)</a></h2>
				<small>21 January, 2011</small><br />


				<div class="entry">
					<p>So far, my thread on information geometry hasn&#8217;t said much about <i>information</i>.  It&#8217;s time to remedy that.</p>
<p>I&#8217;ve been telling you about the Fisher information metric.  In statistics this is nice a way to define a &#8216;distance&#8217; between two probability distributions.  But it also has a quantum version.  </p>
<p>So far I&#8217;ve showed you how to define the Fisher information metric in three equivalent ways.  I also showed that in the quantum case, the Fisher information metric is the real part of a complex-valued thing.  The imaginary part is related to the uncertainty principle.</p>
<p>You can see it all here:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">Part 1</a> &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/">Part 2</a>  &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Part 3</a>  &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">Part 4</a> &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/">Part 5</a></p>
<p>But there&#8217;s yet another way to define the Fisher information metric, which really involves <i>information</i>.   </p>
<p>To explain this, I need to start with the idea of &#8216;information gain&#8217;, or &#8216;relative entropy&#8217;.   And it looks like I should do a whole post on this.</p>
<p>So: </p>
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Measure_%28mathematics%29#Definition">measure space</a> &mdash; that is, a space you can do integrals over.  By a <b>probability distribution</b> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, I&#8217;ll mean a nonnegative function </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3A+%5COmega+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : &#92;Omega &#92;to &#92;mathbb{R}" class="latex" /> </p>
<p>whose integral is 1.  Here <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" /> is my name for the measure on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  Physicists might call <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> the &#8216;phase space&#8217; of some classical system, but probability theorists might call it a space of &#8216;events&#8217;.  Today I&#8217;ll use the probability theorist&#8217;s language.  The idea here is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_A+%5C%3B+p%28%5Comega%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_A &#92;; p(&#92;omega) &#92;; d &#92;omega " class="latex" /></p>
<p>gives the probability that when an event happens, it&#8217;ll be one in the subset <img src="https://s0.wp.com/latex.php?latex=A+%5Csubseteq+%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;subseteq &#92;Omega" class="latex" />.  That&#8217;s why we want </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ge 0" class="latex" /></p>
<p>Probabilities are supposed to be nonnegative.  And that&#8217;s also why we want</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5C%3B+d+%5Comega+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;; d &#92;omega = 1 " class="latex" /></p>
<p>This says that the probability of <i>some</i> event happening is 1.</p>
<p>Now, suppose we have two probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  The <b><a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">information gain</a></b> as we go from <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>We also call this the entropy of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> <b>relative to</b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  It says how much information you learn if you discover that the probability distribution of an event is <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, if before you had thought it was <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.</p>
<p>I like relative entropy because it&#8217;s related to the <a href="http://en.wikipedia.org/wiki/Bayesian_probability">Bayesian interpretation of probability</a>.  The idea here is that you can&#8217;t really &#8216;observe&#8217; probabilities as frequencies of events, except in some unattainable limit where you repeat an experiment over and over infinitely many times.  Instead, you start with some hypothesis about how likely things are: a probability distribution called the <a href="http://en.wikipedia.org/wiki/Prior_probability"><b>prior</b></a>.  Then you update this using <a href="http://yudkowsky.net/rational/bayes">Bayes&#8217; rule</a> when you gain new information.  The updated probability distribution &mdash; your new improved hypothesis &mdash; is called the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution"><b>posterior</b></a>.  </p>
<p>And if you don&#8217;t do the updating right, you need a swift kick in the posterior!</p>
<p>So, we can think of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as the prior probability distribution, and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as the posterior.  Then <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> measures the <i>amount of information</i> that caused you to change your views.</p>
<p>For example, suppose you&#8217;re flipping a coin, so your set of events is just </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5COmega+%3D+%5C%7B+%5Cmathrm%7Bheads%7D%2C+%5Cmathrm%7Btails%7D+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega = &#92;{ &#92;mathrm{heads}, &#92;mathrm{tails} &#92;}" class="latex" /></p>
<p>In this case all the integrals are just sums with two terms.  Suppose your prior assumption is that the coin is fair.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=q%28%5Cmathrm%7Bheads%7D%29+%3D+1%2F2%2C+%5C%3B+q%28%5Cmathrm%7Btails%7D%29+%3D+1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;mathrm{heads}) = 1/2, &#92;; q(&#92;mathrm{tails}) = 1/2" class="latex" /></p>
<p>But then suppose someone you trust comes up and says &#8220;Sorry, that&#8217;s a trick coin: it always comes up heads!&#8221;  So you update our probability distribution and get this posterior:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathrm%7Bheads%7D%29+%3D+1%2C+%5C%3B+p%28%5Cmathrm%7Btails%7D%29+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;mathrm{heads}) = 1, &#92;; p(&#92;mathrm{tails}) = 0 " class="latex" /></p>
<p>How much information have you gained?  Or in other words, what&#8217;s the relative entropy?  It&#8217;s this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+%3D+1+%5Ccdot+%5Clog%28%5Cfrac%7B1%7D%7B1%2F2%7D%29+%2B+0+%5Ccdot+%5Clog%28%5Cfrac%7B0%7D%7B1%2F2%7D%29+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega = 1 &#92;cdot &#92;log(&#92;frac{1}{1/2}) + 0 &#92;cdot &#92;log(&#92;frac{0}{1/2}) = 1 " class="latex" /></p>
<p>Here I&#8217;m doing the logarithm in base 2, and you&#8217;re supposed to know that in this game <img src="https://s0.wp.com/latex.php?latex=0+%5Clog+0+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;log 0 = 0" class="latex" />.  </p>
<p>So: you&#8217;ve learned <i>one bit of information!</i></p>
<p>That&#8217;s supposed to make perfect sense.  On the other hand, the reverse scenario takes a bit more thought. </p>
<p>You start out feeling sure that the coin always lands heads up.  Then someone you trust says &#8220;No, that&#8217;s a perfectly fair coin.&#8221;   If you work out the amount of information you learned this time, you&#8217;ll see it&#8217;s <i>infinite</i>.  </p>
<p>Why is that?</p>
<p>The reason is that something that you thought was impossible &mdash; the coin landing tails up &mdash; turned out to be possible.  In this game, it counts as infinitely shocking to learn something like that, so the information gain is infinite.   If you hadn&#8217;t been so darn sure of yourself &mdash; if you had just believed that the coin <i>almost always</i> landed heads up &mdash; your information gain would be large but finite.</p>
<p>The Bayesian philosophy is built into the concept of information gain, because information gain depends on two things: the prior and the posterior.  And that&#8217;s just as it should be: <i>you can only say how much you learned if you know what you believed beforehand!</i></p>
<p>You might say that information gain depends on <i>three</i> things: <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" />.  And you&#8217;d be right!    Unfortunately, the notation <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> is a bit misleading.   Information gain really <i>does</i> depend on just two things, but these things are not <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />: they&#8217;re <img src="https://s0.wp.com/latex.php?latex=p%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;omega) d&#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;omega) d&#92;omega" class="latex" />.   These are called <a href="http://en.wikipedia.org/wiki/Probability_measure">probability measures</a>, and they&#8217;re ultimately more important than the probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />. </p>
<p>To see this, take our information gain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>and juggle it ever so slightly to get this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B++%5Clog%28%5Cfrac%7Bp%28%5Comega%29+d%5Comega%7D%7Bq%28%5Comega%29d+%5Comega%7D%29+%5C%3B+p%28%5Comega%29+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;;  &#92;log(&#92;frac{p(&#92;omega) d&#92;omega}{q(&#92;omega)d &#92;omega}) &#92;; p(&#92;omega) d &#92;omega " class="latex" /></p>
<p>Clearly this depends only on <img src="https://s0.wp.com/latex.php?latex=p%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;omega) d&#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;omega) d&#92;omega" class="latex" />.  Indeed, it&#8217;s good to work directly with these probability measures and give them short names, like</p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cmu+%3D+p%28%5Comega%29+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu = p(&#92;omega) d &#92;omega " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cnu+%3D+q%28%5Comega%29+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu = q(&#92;omega) d &#92;omega" class="latex" /></p>
<p>Then the formula for information gain looks more slick:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+%5Clog%28%5Cfrac%7Bd%5Cmu%7D%7Bd%5Cnu%7D%29+%5C%3B+d%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; &#92;log(&#92;frac{d&#92;mu}{d&#92;nu}) &#92;; d&#92;mu " class="latex" /></p>
<p>And by the way, in case you&#8217;re wondering, the <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d" class="latex" /> here doesn&#8217;t actually mean much: we&#8217;re just so brainwashed into wanting a <img src="https://s0.wp.com/latex.php?latex=d+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x" class="latex" /> in our integrals that people often use <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu" class="latex" /> for a measure even though the simpler notation <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> might be more logical.  So, the function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Cmu%7D%7Bd%5Cnu%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d&#92;mu}{d&#92;nu} " class="latex" /></p>
<p>is really just a ratio of probability measures, but people call it a <a href="http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem">Radon-Nikodym derivative</a>, because it looks like a derivative (and in some important examples it actually is).   So, if I were talking to myself, I could have shortened this blog entry immensely by working with directly probability measures, leaving out the <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d" class="latex" />&#8216;s, and saying:</p>
<blockquote><p>
Suppose <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" /> are probability measures; then the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" />, or information gain, is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Cmu%2C+%5Cnu%29+%3D++%5Cint_%5COmega+%5C%3B+%5Clog%28%5Cfrac%7B%5Cmu%7D%7B%5Cnu%7D%29+%5C%3B+%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(&#92;mu, &#92;nu) =  &#92;int_&#92;Omega &#92;; &#92;log(&#92;frac{&#92;mu}{&#92;nu}) &#92;; &#92;mu " class="latex" /></p></blockquote>
<p>But I&#8217;m under the impression that people are actually reading this stuff, and that most of you are happier with functions than measures.  So, I decided to start with</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>and then gradually work my way up to the more sophisticated way to think about relative entropy!  But having gotten that off my chest, now I&#8217;ll revert to the original naive way.</p>
<p>As a warmup for next time, let me pose a question.  How much is this quantity</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>like a <i>distance</i> between probability distributions?  A distance function, or <a href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29">metric</a>, is supposed to satisfy some axioms.  Alas, relative entropy satisfies some of these, but not the most interesting one!</p>
<p>&bull; If you&#8217;ve got a metric, the distance between points should always be nonnegative. Indeed, this holds:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) &#92;ge 0" class="latex" /></p>
<p>So, we never learn a negative amount when we update our prior, at least according to this definition.   It&#8217;s a fun exercise to prove this inequality, at least if you know some tricks involving inequalities and convex functions &mdash; otherwise it might be hard.  </p>
<p>&bull;  If you&#8217;ve got a metric, the distance between two points should only be zero if they&#8217;re really the same point.  In fact, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = 0" class="latex" /> </p>
<p>if and only if </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+d%5Comega+%3D+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d&#92;omega = q d &#92;omega " class="latex" /></p>
<p>It&#8217;s possible to have <img src="https://s0.wp.com/latex.php?latex=p+d%5Comega+%3D+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d&#92;omega = q d &#92;omega " class="latex" /> even if <img src="https://s0.wp.com/latex.php?latex=p+%5Cne+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ne q" class="latex" />, because <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" /> can be zero somewhere.  But this is just more evidence that we should really be talking about the probability measure <img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega" class="latex" /> instead of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  If we do that, we&#8217;re okay so far!</p>
<p>&bull; If you&#8217;ve got a metric, the distance from your first point to your second point is the same as the distance from the second to the first.  Alas, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%5Cne+S%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) &#92;ne S(q,p)" class="latex" /></p>
<p>in general.  We already saw this in our example of the flipped coin.  This is a slight bummer, but I could live with it, since <a href="http://www.tac.mta.ca/tac/reprints/articles/1/tr1.pdf">Lawvere has already shown</a> that it&#8217;s wise to generalize the concept of metric by dropping this axiom.</p>
<p>&bull; If you&#8217;ve got a metric, it obeys the <a href="http://en.wikipedia.org/wiki/Triangle_inequality">triangle inequality</a>.  This is the really interesting axiom, and alas, this too fails.  Later we&#8217;ll see why.</p>
<p>So, relative entropy does a fairly miserable job of acting like a distance function.  People call it a <a href="http://en.wikipedia.org/wiki/Statistical_distance">divergence</a>.  In fact, they often call it the <b>Kullback-Leibler divergence</b>.  I don&#8217;t like that, because &#8216;the Kullback-Leibler divergence&#8217;  doesn&#8217;t really explain the idea: it sounds more like the title of a bad spy novel.  &#8216;Relative entropy&#8217;, on the other hand, makes a lot of sense if you understand entropy.  And &#8216;information gain&#8217; makes sense if you understand information.</p>
<p>Anyway: how can we save this miserable attempt to get a distance function on the space of probability distributions?  <i>Simple: take its matrix of second derivatives and use that to define a Riemannian metric</i> <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.  This Riemannian metric in turn defines a metric of the more elementary sort we&#8217;ve been discussing today.</p>
<p>And this Riemannian metric is the Fisher information metric I&#8217;ve been talking about all along!</p>
<p>More details later, I hope.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comments">21 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;6)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1573 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1573">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/" rel="bookmark">Information Geometry (Part&nbsp;5)</a></h2>
				<small>2 November, 2010</small><br />


				<div class="entry">
					<p>I&#8217;m trying to understand the Fisher information metric and how it&#8217;s related to <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#Ottinger">&Ouml;ttinger&#8217;s formalism</a> for &#8216;dissipative mechanics&#8217; &mdash; that is, mechanics including friction. They involve similar physics, and they involve similar math, but it&#8217;s not quite clear how they fit together.</p>
<p>I think it will help to do an example.  The harmonic oscillator is a trusty workhorse throughout physics, so let&#8217;s do that.</p>
<p>So: suppose you have a rock hanging on a spring, and it can bounce up and down.  Suppose it&#8217;s in thermal equilibrium with its environment.  It will wiggle up and down ever so slightly, thanks to thermal fluctuations.   The hotter it is, the more it wiggles.  These vibrations are random, so its position and momentum at any given moment can be treated as random variables.  </p>
<p>If we take quantum mechanics into account, there&#8217;s an extra source of randomness: <i>quantum</i> fluctuations.  Now there will be fluctuations even at zero temperature.   Ultimately this is due to the uncertainty principle.  Indeed, if you know the position for sure, you can&#8217;t know the momentum at all!  </p>
<p>Let&#8217;s see how the position, momentum and energy of our rock will fluctuate given that we know all three of these quantities <i>on average</i>.  The fluctuations will form a little fuzzy blob, roughly ellipsoidal in shape, in the 3-dimensional space whose coordinates are position, momentum and energy:</p>
<div align="center">
<img src="https://i0.wp.com/www.math.dartmouth.edu/archive/m22x06/public_html/ellipsoid.jpg" />
</div>
<p>Yeah, I know you&#8217;re sick of this picture, but this time it&#8217;s for real: I want to calculate what this ellipsoid actually looks like!  I&#8217;m not promising I&#8217;ll do it &mdash; I may get stuck, or bored &mdash; but at least I&#8217;ll <i>try</i>.</p>
<p>Before I start the calculation, let&#8217;s guess the answer. A harmonic oscillator has a position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, and its energy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+%2B+p%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2}(q^2 + p^2)" class="latex" /></p>
<p>Here I&#8217;m working in units where lots of things equal 1, to keep things simple.</p>
<p>You&#8217;ll notice that this energy has rotational symmetry in the position-momentum plane.  This is ultimately what makes the harmonic oscillator such a beloved physical system.   So, we might naively guess that our little ellipsoid will have rotational symmetry as well, like this:</p>
<div align="center">
<img width="120" src="http://upload.wikimedia.org/wikipedia/commons/8/88/ProlateSpheroid.png" />
</div>
<p>or this:</p>
<div align="center">
<img width="200" src="http://upload.wikimedia.org/wikipedia/commons/b/b5/OblateSpheroid.PNG" />
</div>
<p>Here I&#8217;m using the <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> coordinates for position and momentum, while the <img src="https://s0.wp.com/latex.php?latex=z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z" class="latex" /> coordinate stands for energy.  So in these examples the position and momentum fluctuations are the same size, while the energy fluctuations, drawn in the vertical direction, might be bigger or smaller.</p>
<p>Unfortunately, this guess really is naive.  After all, there are <i>lots</i> of these ellipsoids, one centered at each point in position-momentum-energy space.  Remember the rules of the game!  You give me any point in this space.  I take the coordinates of this point as the <i>mean</i> values of position, momentum and energy, and I find the maximum-entropy state with these mean values.  Then I work out the fluctuations in this state, and draw them as an ellipsoid. </p>
<p>If you pick a point where position and momentum have mean value zero, you haven&#8217;t broken the rotational symmetry of the problem.  So, my ellipsoid must be rotationally symmetric.  But if you pick some other mean value for position and momentum, all bets are off!</p>
<p>Fortunately, this naive guess is actually right: <i>all</i> the ellipsoids are rotationally symmetric &mdash; even the ones centered at nonzero values of position and momentum!  We&#8217;ll see why soon.  And if you&#8217;ve been following this series of posts, you&#8217;ll know what this implies: the &#8220;Fisher information metric&#8221; <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on position-momentum-energy space has rotational symmetry about any vertical axis.  (Again, I&#8217;m using the vertical direction for energy.)  So, if we slice this space with any horizontal plane, the metric on this plane must be the plane&#8217;s usual metric times a constant:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq%5E2+%2B+dp%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} &#92;, (dq^2 + dp^2) " class="latex" /></p>
<p>Why?  Because only the usual metric on the plane, or any multiple of it, has ordinary rotations around every point as symmetries.  </p>
<p>So, roughly speaking, we&#8217;re recovering the &#8216;obvious&#8217; geometry of the position-momentum plane from the Fisher information metric.  <i>We&#8217;re recovering &#8216;ordinary&#8217; geometry from information geometry!</i>  </p>
<p>But this should not be terribly surprising, since we used the harmonic oscillator Hamiltonian</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+%2B+p%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2}(q^2 + p^2)" class="latex" /></p>
<p>as an input to our game.  It&#8217;s mainly just a confirmation that things are working as we&#8217;d hope.  </p>
<p>There&#8217;s more, though.  <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">Last time</a> I realized that because observables in quantum mechanics don&#8217;t commute, the Fisher information metric has a curious skew-symmetric partner called <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />.   So, we should also study this in our example.  And when we do, we&#8217;ll see that restricted to any horizontal plane in position-momentum-energy space, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq+%5C%2C+dp+-+dp+%5C%2C+dq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = &#92;mathrm{constant} &#92;, (dq &#92;, dp - dp &#92;, dq)" class="latex" /></p>
<p>This looks like a mutant version of the Fisher information metric</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq%5E2+%2B+dp%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} &#92;, (dq^2 + dp^2) " class="latex" /></p>
<p>and if you know your geometry, you&#8217;ll know it&#8217;s the usual <a href="http://en.wikipedia.org/wiki/Symplectic_manifold">&#8216;symplectic structure&#8217;</a> on the position-energy plane &mdash; at least, times some constant.   </p>
<p>All this is very reminiscent of &Ouml;ttinger&#8217;s work on dissipative mechanics.  But we&#8217;ll also see something else: while the constant in <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> depends on the energy &mdash; that is, on which horizontal plane we take &mdash; the constant in <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> does not!</p>
<p>Why?  It&#8217;s perfectly sensible.  The metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on our horizontal plane keeps track of fluctuations in position and momentum.  Thermal fluctuations get bigger when it&#8217;s hotter &mdash; and to boost the average energy of our oscillator, we must heat it up.   So, as we increase the energy, moving our horizontal plane further up in position-momentum-energy space, the metric on the plane gets bigger!   In other words, our ellipsoids get a fat cross-section at high energies.</p>
<p>On the other hand, the symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> arises from the fact that position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> don&#8217;t commute in quantum mechanics.  They obey Heisenberg&#8217;s <a href="http://en.wikipedia.org/wiki/Canonical_commutation_relation">&#8216;canonical commutation relation&#8217;</a>:  </p>
<p><img src="https://s0.wp.com/latex.php?latex=q+p+-+p+q+%3D+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q p - p q = i " class="latex" /></p>
<p>This relation doesn&#8217;t involve energy, so <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> will be the same on every horizontal plane.   And it turns out this relation implies</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega++%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq+%5C%2C+dp+-+dp+%5C%2C+dq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega  = &#92;mathrm{constant} &#92;, (dq &#92;, dp - dp &#92;, dq)" class="latex" /></p>
<p>for some constant we&#8217;ll compute later.  </p>
<p>Okay, that&#8217;s the basic idea.  Now let&#8217;s actually do some computations.  For starters, let&#8217;s see why all our ellipsoids have rotational symmetry!</p>
<p>To do this, we need to understand a bit about the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that maximizes entropy given certain mean values of position, momentum and energy.   So, let&#8217;s choose the numbers we want for these mean values  (also known as &#8216;expected values&#8217; or &#8216;expectation values&#8217;):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = E " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+q+%5Crangle+%3D+q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle q &#92;rangle = q_0" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+p+%5Crangle+%3D+p_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle p &#92;rangle = p_0" class="latex" /></p>
<p>I hope this isn&#8217;t too confusing: <img src="https://s0.wp.com/latex.php?latex=H%2C+p%2C+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H, p, q" class="latex" /> are our observables which are operators, while <img src="https://s0.wp.com/latex.php?latex=E%2C+p_0%2C+q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, p_0, q_0" class="latex" /> are the mean values we have chosen for them.  The state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> depends on <img src="https://s0.wp.com/latex.php?latex=E%2C+p_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, p_0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0" class="latex" />.</p>
<p>We&#8217;re doing quantum mechanics, so position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> are both self-adjoint operators on the Hilbert space <img src="https://s0.wp.com/latex.php?latex=L%5E2%28%5Cmathbb%7BR%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(&#92;mathbb{R})" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28q%5Cpsi%29%28x%29+%3D+x+%5Cpsi%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q&#92;psi)(x) = x &#92;psi(x) " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p%5Cpsi%29%28x%29+%3D+-+i+%5Cfrac%7Bd+%5Cpsi%7D%7Bdx%7D%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p&#92;psi)(x) = - i &#92;frac{d &#92;psi}{dx}(x)" class="latex" /></p>
<p>Indeed all our observables, including the Hamiltonian </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D+%28p%5E2+%2B+q%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2} (p^2 + q^2) " class="latex" /></p>
<p>are self-adjoint operators on this Hilbert space, and the state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Density_matrix">density matrix</a> on this space, meaning a positive self-adjoint operator with trace 1.</p>
<p>Now: how do we compute <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />?   It&#8217;s a <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">Lagrange multiplier</a> problem: maximizing some function given some constraints.  And it&#8217;s well-known that when you solve this problem, you get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1%2C+%5Clambda%5E2%2C+%5Clambda%5E3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1, &#92;lambda^2, &#92;lambda^3" class="latex" /> are three numbers we yet have to find, and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalizing factor called the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} )" class="latex" /></p>
<p>Now let&#8217;s look at a special case.  If we choose <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1+%3D+%5Clambda%5E2+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1 = &#92;lambda^2 = 0" class="latex" />, we&#8217;re back a simpler and more famous problem, namely maximizing entropy subject to a constraint only on energy!  The solution is then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta+H%7D+%2C+%5Cqquad+Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-+%5Cbeta+H%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;beta H} , &#92;qquad Z = &#92;mathrm{tr} (e^{- &#92;beta H} )" class="latex" /></p>
<p>Here I&#8217;m using the letter <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^3" class="latex" /> because this is traditional.  This quantity has an important physical meaning!  It&#8217;s the <i>reciprocal of temperature</i> in units where Boltzmann&#8217;s constant is 1.   </p>
<p>Anyway, back to our special case!  In this special case it&#8217;s easy to explicitly calculate <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  Indeed, people have known how ever since <a href="http://en.wikipedia.org/wiki/Planck%27s_law#Derivation">Planck</a> put the &#8216;quantum&#8217; in quantum mechanics!   He figured out how black-body radiation works.  A box of hot radiation is just a big bunch of harmonic oscillators in thermal equilibrium.  You can work out its partition function by multiplying the partition function of each one.  </p>
<p>So, it would be great to reduce our general problem to this special case.  To do this, let&#8217;s rewrite</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} )" class="latex" /></p>
<p>in terms of some new variables, like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;beta(H - f q - g p)} " class="latex" /></p>
<p>where now </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;beta(H - f q - g p)} )" class="latex" /></p>
<p>Think about it!  Now our problem is just like an oscillator with a modified Hamiltonian</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%27+%3D+H+-+f+q+-+g+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039; = H - f q - g p" class="latex" /></p>
<p>What does this mean, physically?  Well, if you push on something with a force <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />, its potential energy will pick up a term <img src="https://s0.wp.com/latex.php?latex=-+f+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- f q" class="latex" />.  So, the first two terms are just the Hamiltonian for a harmonic oscillator <i>with an extra force pushing on it!</i>  </p>
<p>I don&#8217;t know a nice interpretation for the <img src="https://s0.wp.com/latex.php?latex=-+g+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- g p" class="latex" /> term.  We could say that besides the extra force equal to <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />, we also have an extra &#8216;gorce&#8217; equal to <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />.  I don&#8217;t know what that means.  Luckily, I don&#8217;t need to!   Mathematically, our whole problem is invariant under rotations in the position-momentum plane, so whatever works for <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> must also work for <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.   </p>
<p>Now here&#8217;s the cool part.  We can complete the square:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H%27+%26+%3D+%5Cfrac%7B1%7D%7B2%7D+%28q%5E2+%2B+p%5E2%29+-++f+q+-+g+p+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+-+2+q+f+%2B+f%5E2%29+%2B+%5Cfrac%7B1%7D%7B2%7D%28p%5E2+-+2+q+g+%2B+g%5E2%29+-+%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29++%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%28%28q+-+f%29%5E2+%2B+%28p+-+g%29%5E2%29++-+%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29++%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{aligned} H&#039; &amp; = &#92;frac{1}{2} (q^2 + p^2) -  f q - g p &#92;&#92; &amp;= &#92;frac{1}{2}(q^2 - 2 q f + f^2) + &#92;frac{1}{2}(p^2 - 2 q g + g^2) - &#92;frac{1}{2}(g^2 + f^2)  &#92;&#92; &amp;= &#92;frac{1}{2}((q - f)^2 + (p - g)^2)  - &#92;frac{1}{2}(g^2 + f^2)  &#92;end{aligned}" class="latex" /></p>
<p>so if we define &#8216;translated&#8217; position and momentum operators:</p>
<p><img src="https://s0.wp.com/latex.php?latex=q%27+%3D+q+-+f%2C+%5Cqquad+p%27+%3D+p+-+g+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q&#039; = q - f, &#92;qquad p&#039; = p - g " class="latex" /></p>
<p>we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%27+%3D+%5Cfrac%7B1%7D%7B2%7D%28%7Bq%27%7D%5E2+%2B+%7Bp%27%7D%5E2%29+-++%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039; = &#92;frac{1}{2}({q&#039;}^2 + {p&#039;}^2) -  &#92;frac{1}{2}(g^2 + f^2) " class="latex" /></p>
<p>So: apart from a constant, <img src="https://s0.wp.com/latex.php?latex=H%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039;" class="latex" /> is just the harmonic oscillator Hamiltonian in terms of &#8216;translated&#8217; position and momentum operators!</p>
<p>In other words: we&#8217;re studying a strange variant of the harmonic oscillator, where we are pushing on it with an extra force and also an extra  &#8216;gorce&#8217;.  But this strange variant is <i>exactly the same as the usual harmonic oscillator</i>, except that we&#8217;re working in translated coordinates on position-momentum space, and subtracting a constant from the Hamiltonian.</p>
<p>These are pretty minor differences.  So, we&#8217;ve succeeded in reducing our problem to the problem of a harmonic oscillator in thermal equilibrium at some temperature!</p>
<p>This makes it easy to calculate</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+%29+%3D+%5Cmathrm%7Btr%7D%28e%5E%7B-%5Cbeta+H%27%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;beta(H - f q - g p)} ) = &#92;mathrm{tr}(e^{-&#92;beta H&#039;})" class="latex" /></p>
<p>By our formula for <img src="https://s0.wp.com/latex.php?latex=H%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039;" class="latex" />, this is just</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cfrac%7B1%7D%7B2%7D%28%7Bq%27%7D%5E2+%2B+%7Bp%27%7D%5E2%29%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;mathrm{tr} (e^{-&#92;frac{1}{2}({q&#039;}^2 + {p&#039;}^2)})" class="latex" /></p>
<p>And the second factor here equals the partition function for the good old harmonic oscillator:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta+H%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;mathrm{tr} (e^{-&#92;beta H})" class="latex" /></p>
<p>So now we&#8217;re back to a textbook problem.  The eigenvalues of the <a href="http://en.wikipedia.org/wiki/Harmonic_oscillator_%28quantum%29#Hamiltonian_and_energy_eigenstates">harmonic oscillator Hamiltonian</a> are </p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%2B+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n + &#92;frac{1}{2}" class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%3D+0%2C1%2C2%2C3%2C+%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 0,1,2,3, &#92;dots" class="latex" /></p>
<p>So, the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-%5Cbeta+H%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-&#92;beta H}" class="latex" /> are are just</p>
<p><img src="https://s0.wp.com/latex.php?latex=e%5E%7B-%5Cbeta%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-&#92;beta(n + &#92;frac{1}{2})} " class="latex" /></p>
<p>and to take the trace of this operator, we sum up these eigenvalues:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28e%5E%7B-%5Cbeta+H%7D%29+%3D+%5Csum_%7Bn+%3D+0%7D%5E%5Cinfty+e%5E%7B-%5Cbeta+%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7D+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta%2F2%7D%7D%7B1+-+e%5E%7B-%5Cbeta%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(e^{-&#92;beta H}) = &#92;sum_{n = 0}^&#92;infty e^{-&#92;beta (n + &#92;frac{1}{2})} = &#92;frac{e^{-&#92;beta/2}}{1 - e^{-&#92;beta}} " class="latex" /></p>
<p>So:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cfrac%7Be%5E%7B-%5Cbeta%2F2%7D%7D%7B1+-+e%5E%7B-%5Cbeta%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;frac{e^{-&#92;beta/2}}{1 - e^{-&#92;beta}} " class="latex" /></p>
<p>We can now compute the Fisher information metric using this formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda%5Ei+%5Cpartial+%5Clambda%5Ej%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda^i &#92;partial &#92;lambda^j} &#92;ln Z" class="latex" /></p>
<p>if we remember how our new variables are related to the <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1+%3D+%5Cbeta+f+%2C+%5Cqquad+%5Clambda%5E2+%3D+%5Cbeta+g%2C+%5Cqquad+%5Clambda%5E3+%3D+%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1 = &#92;beta f , &#92;qquad &#92;lambda^2 = &#92;beta g, &#92;qquad &#92;lambda^3 = &#92;beta" class="latex" /></p>
<p>It&#8217;s just calculus!  But I&#8217;m feeling a bit tired, so I&#8217;ll leave this pleasure to you.  </p>
<p>For now, I&#8217;d rather go back to our basic intuition about how the Fisher information metric describes fluctuations of observables.  Mathematically, this means it&#8217;s the real part of the covariance matrix</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>where for us</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_1+%3D+q%2C+%5Cqquad+X_2+%3D+p%2C+%5Cqquad+X_3+%3D+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1 = q, &#92;qquad X_2 = p, &#92;qquad X_3 = E " class="latex" /></p>
<p>Here we are taking expected values using the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  We&#8217;ve seen this mixed state is just like the maximum-entropy state of a harmonic oscillator at fixed temperature &mdash; except for two caveats: we&#8217;re working in translated coordinates on position-momentum space, and subtracting a constant from the Hamiltonian.  But neither of these two caveats affects the fluctuations <img src="https://s0.wp.com/latex.php?latex=%28X_i+-+%5Clangle+X_i+%5Crangle%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X_i - &#92;langle X_i &#92;rangle)" class="latex" /> or the covariance matrix.  </p>
<p>So, as indeed we&#8217;ve already seen, <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> has rotational symmetry in the 1-2 plane.  Thus, we&#8217;ll completely know it once we know <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D+%3D+g_%7B22%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11} = g_{22}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g_%7B33%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{33}" class="latex" />; the other components are zero for symmetry reasons.  <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11}" class="latex" /> will equal the variance of position for a harmonic oscillator at a given temperature, while <img src="https://s0.wp.com/latex.php?latex=g_%7B33%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{33}" class="latex" /> will equal the variance of its energy.   We can work these out or look them up.  </p>
<p>I won&#8217;t do that now: I&#8217;m after insight, not formulas.  For physical reasons, it&#8217;s obvious that <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11}" class="latex" /> must diminish with diminishing energy &mdash; but not go to zero.   Why? Well, as the temperature approaches zero, a harmonic oscillator in thermal equilibrium approaches its state of least energy: the so-called <a href="http://en.wikipedia.org/wiki/Ground_state">&#8216;ground state&#8217;</a>.  In its ground state, the standard deviations of position and momentum are as small as allowed by the Heisenberg uncertainty principle:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+p+%5CDelta+q++%5Cge+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta p &#92;Delta q  &#92;ge &#92;frac{1}{2}" class="latex" /></p>
<p>and they&#8217;re equal, so </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D+%3D+%28%5CDelta+q%29%5E2+%3D+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11} = (&#92;Delta q)^2 = &#92;frac{1}{2}" class="latex" />.</p>
<p>That&#8217;s enough about the metric.  Now, what about the metric&#8217;s skew-symmetric partner?  This is: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cmathrm%7BIm%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;mathrm{Im} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>Last time we saw that <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> is all about expected values of commutators:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cfrac%7B1%7D%7B2i%7D+%5Clangle+%5BX_i%2C+X_j%5D+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;frac{1}{2i} &#92;langle [X_i, X_j] &#92;rangle" class="latex" /></p>
<p>and this makes it easy to compute.  For example, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5BX_1%2C+X_2%5D+%3D+q+p+-+p+q+%3D+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[X_1, X_2] = q p - p q = i" class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B12%7D+%3D+%5Cfrac%7B1%7D%7B2%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{12} = &#92;frac{1}{2} " class="latex" /></p>
<p>Of course </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B11%7D+%3D+%5Comega_%7B22%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{11} = &#92;omega_{22} = 0" class="latex" /></p>
<p>by skew-symmetry, so we know the restriction of <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> to any horizontal plane.  We can also work out other components, like <img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B13%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{13}" class="latex" />, but I don&#8217;t want to.  I&#8217;d rather just state this:</p>
<blockquote><p>
<b>Summary:</b> Restricted to any horizontal plane in the position-momentum-energy space, the Fisher information metric for the harmonic oscillator is</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%28dq_0%5E2+%2B+dp_0%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} (dq_0^2 + dp_0^2) " class="latex" /></p>
<p>with a constant depending on the temperature, equalling <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> in the zero-temperature limit, and increasing as the temperature rises.  Restricted to the same plane, the Fisher information metric&#8217;s skew-symmetric partner is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+%5Cfrac%7B1%7D%7B2%7D+dq_0+%5Cwedge+dp_0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = &#92;frac{1}{2} dq_0 &#92;wedge dp_0 " class="latex" />
</p></blockquote>
<p>(Remember, the mean values <img src="https://s0.wp.com/latex.php?latex=q_0%2C+p_0%2C+E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0, p_0, E_0" class="latex" /> are the coordinates on position-momentum-energy space.  We could also use coordinates <img src="https://s0.wp.com/latex.php?latex=f%2C+g%2C+%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f, g, &#92;beta" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=f%2C+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f, g" class="latex" /> and temperature.  In the chatty intro to this article you saw formulas like those above but without the subscripts; that&#8217;s before I got serious about using <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to mean <i>operators</i>.)</p>
<p>And now for the moral.  Actually I have two: a physics moral and a math moral.    </p>
<p>First, what is the physical meaning of <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> when restricted to a plane of constant <img src="https://s0.wp.com/latex.php?latex=E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_0" class="latex" />, or if you prefer, a plane of constant temperature?</p>
<blockquote><p>
<b>Physics Moral:</b> Restricted to a constant-temperature plane, <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> is the covariance matrix for our observables.  It is temperature-dependent.  In the zero-temperature limit, the thermal fluctuations go away and <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> depends only on quantum fluctuations in the ground state.  On the other hand, <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> restricted to a constant-temperature plane describes Heisenberg uncertainty relations for noncommuting observables.  In our example, it is temperature-independent.
</p></blockquote>
<p>Second, what does this have to do with <a href="http://en.wikipedia.org/wiki/K%C3%A4hler_manifold">K&auml;hler geometry?</a>  Remember, the complex plane has a <i>complex-valued</i> metric on it, called a K&auml;hler structure.  Its real part is a <a href="http://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian metric</a>, and its imaginary part is a <a href="http://en.wikipedia.org/wiki/Symplectic_manifold">symplectic structure</a>.  We can think of the the complex plane as the position-momentum plane for a point particle.  Then the symplectic structure is the basic ingredient needed for <a href="http://en.wikipedia.org/wiki/Hamiltonian_mechanics">Hamiltonian mechanics</a>, while the Riemannian structure is the basic ingredient needed for the harmonic oscillator Hamiltonian.  </p>
<blockquote><p>
<b>Math Moral:</b> In the example we considered, <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> restricted to a constant-temperature plane is equal to <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> the usual symplectic structure on the complex plane.  On the other hand, <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> restricted to a constant-temperature plane is a multiple of the usual Riemannian metric on the complex plane &mdash; but this multiple is <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> <i>only when the temperature is zero!</i>  So, only at temperature zero are <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> the real and imaginary parts of a K&auml;hler structure.
</p></blockquote>
<p>It will be interesting to see how much of this stuff is true more generally.  The harmonic oscillator is much nicer than your average physical system, so it can be misleading, but I think <i>some</i> of the morals we&#8217;ve seen here can be generalized.</p>
<p>Some other time I may so more about how all this is<br />
related to <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#Ottinger">&Ouml;ttinger&#8217;s formalism</a>, but the quick point is that he too has mixed states, and a symmetric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />, and a skew-symmetric <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />.  So it&#8217;s nice to see if they match up in an example.</p>
<p>Finally, two footnotes on terminology:</p>
<p><b>&beta;:</b>   In fact, this quantity <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FkT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/kT" class="latex" /> is so important it deserves a better name than &#8216;reciprocal of temperature&#8217;.   How about &#8216;coolness&#8217;?  An important lesson from statistical mechanics is that coolness is more fundamental than temperature.  This makes some facts more plausible.  For example, if you say &#8220;you can never reach absolute zero,&#8221; it sounds very odd, since you can get as close as you like, and it&#8217;s even possible to get <a href="http://en.wikipedia.org/wiki/Negative_temperature"><i>negative</i> temperatures</a> &mdash; but temperature zero remains tantalizingly out of reach.   But &#8220;you can never attain infinite coolness&#8221; &mdash; now that makes sense.</p>
<p><b>Gorce:</b>  I apologize to Richard Feynman for stealing the word <a href="http://student.fizika.org/~jsisko/Knjige/Opca%20Fizika/Feynman%20Lectures%20on%20Physics/Vol%201%20Ch%2012%20-%20Characteristics%20of%20Force.pdf">&#8216;gorce&#8217;</a> and using it a different way.  Does anyone have a good intuition for what&#8217;s going on when you apply my sort of &#8216;gorce&#8217; to a point particle?  You need to think about velocity-dependent potentials, of that I&#8217;m sure.  In the presence of a velocity-dependent potential, momentum is <i>not</i> just mass times velocity.  Which is good: if it were, we could never have a system where the mean value of both <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> stayed constant over time!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/#comments">53 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1504 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1504">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/" rel="bookmark">Information Geometry (Part&nbsp;4)</a></h2>
				<small>29 October, 2010</small><br />


				<div class="entry">
					<p>Before moving on, I&#8217;d like to clear up a mistake I&#8217;d been making in all my previous posts on this subject.</p>
<p>(By now I&#8217;ve tried to fix those posts, because people often get information from the web in a hasty way, and I don&#8217;t want my mistake to spread.  But you&#8217;ll still see traces of my mistake infecting the <i>comments</i> on those posts.)</p>
<p>So what&#8217;s the mistake?  It&#8217;s embarrassingly simple, but also simple to fix.  A <a href="http://en.wikipedia.org/wiki/Metric_tensor">Riemannian metric</a> must be symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+g_%7Bji%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = g_{ji} " class="latex" />  </p>
<p>Now, I had defined the Fisher information metric to be the so-called <a href="http://en.wikipedia.org/wiki/Covariance_matrix">&#8216;covariance matrix&#8217;</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%3B%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;langle (X_i - &#92;langle X_i &#92;rangle) &#92;;(X_j- &#92;langle X_j &#92;rangle)&#92;rangle" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> are some observable-valued functions on a manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, and the angle brackets mean &#8220;expectation value&#8221;, computed using a mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that also depends on the point in <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.</p>
<p>The covariance matrix is symmetric in classical mechanics, since then observables commute, so:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB+%5Crangle+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB &#92;rangle = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>But it&#8217;s not symmetric is quantum mechanics!  After all, suppose <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is the position operator for a particle, and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is the momentum operator.  Then according to Heisenberg</p>
<p><img src="https://s0.wp.com/latex.php?latex=qp+%3D+pq+%2B+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="qp = pq + i " class="latex" /></p>
<p>in units where Planck&#8217;s constant is 1.  Taking expectation values, we get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+qp+%5Crangle+%3D+%5Clangle+pq+%5Crangle+%2B+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle qp &#92;rangle = &#92;langle pq &#92;rangle + i" class="latex" /></p>
<p>and in particular:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+qp+%5Crangle+%5Cne+%5Clangle+pq+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle qp &#92;rangle &#92;ne &#92;langle pq &#92;rangle " class="latex" /></p>
<p>We can use this to get examples where <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> is not symmetric.    </p>
<p>However, it turns out that the <i>real part</i> of the covariance matrix is symmetric, even in quantum mechanics &mdash; and that&#8217;s what we should use as our Fisher information metric.</p>
<p>Why is the real part of the covariance matrix symmetric, even in quantum mechanics?  Well, suppose <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is any density matrix, and <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> are any observables.  Then by definition</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB+%5Crangle+%3D+%5Cmathrm%7Btr%7D+%28%5Crho+AB%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB &#92;rangle = &#92;mathrm{tr} (&#92;rho AB)" class="latex" /></p>
<p>so taking the complex conjugate of both sides</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A++%3D+%5Cmathrm%7Btr%7D%28%5Crho+AB%29%5E%2A+%3D+%5Cmathrm%7Btr%7D%28%28%5Crho+A+B%29%5E%2A%29+%3D+%5Cmathrm%7Btr%7D%28B%5E%2A+A%5E%2A+%5Crho%5E%2A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^*  = &#92;mathrm{tr}(&#92;rho AB)^* = &#92;mathrm{tr}((&#92;rho A B)^*) = &#92;mathrm{tr}(B^* A^* &#92;rho^*)" class="latex" /></p>
<p>where I&#8217;m using an asterisk both for the complex conjugate of a number and the adjoint of an operator.  But our observables are self-adjoint, and so is our density matrix, so we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28B%5E%2A+A%5E%2A+%5Crho%5E%2A%29+%3D+%5Cmathrm%7Btr%7D%28B+A+%5Crho%29+%3D+%5Cmathrm%7Btr%7D%28%5Crho+B+A%29+%3D+%5Clangle+B+A+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(B^* A^* &#92;rho^*) = &#92;mathrm{tr}(B A &#92;rho) = &#92;mathrm{tr}(&#92;rho B A) = &#92;langle B A &#92;rangle " class="latex" /></p>
<p>where in the second step we used the cyclic property of the trace.  In short:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^* = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>If we take real parts, we get something symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB%5Crangle+%3D++%5Cmathrm%7BRe%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB&#92;rangle =  &#92;mathrm{Re} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>So, if we redefine the Fisher information metric to be the <i>real part</i> of the covariance matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%3B+%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle) &#92;; (X_j- &#92;langle X_j &#92;rangle)&#92;rangle " class="latex" /></p>
<p>then it&#8217;s symmetric, as it should be. </p>
<p>Last time I mentioned a general setup using von Neumann algebras, that handles the classical and quantum situations simultaneously.  That applies here!   Taking the real part has no effect in classical mechanics, so we don&#8217;t need it there &mdash; but it doesn&#8217;t hurt, either.</p>
<p>Taking the real part never has any effect when <img src="https://s0.wp.com/latex.php?latex=i+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = j" class="latex" />, either, since the expected value of the <i>square</i> of an observable is a nonnegative number:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle &#92;ge 0" class="latex" /></p>
<p>This has two nice consequences.  </p>
<p>First, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle++%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} = &#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle  &#92;ge 0 " class="latex" /></p>
<p>and since this is true in <i>any</i> coordinate system, our would-be metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> is indeed nonnegative.  It&#8217;ll be an honest Riemannian metric whenever it&#8217;s positive definite.   </p>
<p>Second, suppose we&#8217;re working in the special case discussed in <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/">Part 2</a>, where our manifold is an open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7B%5Crho%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{&#92;rho}" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;mathbb{R}^n" class="latex" /> is the Gibbs state with <img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i" class="latex" />.  Then all the usual rules of statistical mechanics apply.  So, we can compute the variance of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> using the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i%5E2%7D+%5Cln+Z++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i^2} &#92;ln Z  " class="latex" /></p>
<p>In other words, </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i%5E2%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i^2} &#92;ln Z " class="latex" /></p>
<p>But since this is true in <i>any</i> coordinate system, we must have</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;ln Z " class="latex" /></p>
<p>(Here I&#8217;m using a little math trick: two symmetric bilinear forms whose diagonal entries agree in <i>any</i> basis must be equal.  We&#8217;ve already seen that the left side is symmetric, and the right side is symmetric by a famous fact about mixed partial derivatives.)</p>
<p>However, I&#8217;m pretty sure this cute formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;ln Z " class="latex" /></p>
<p>only holds in the special case I&#8217;m talking about now, where points in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> are parametrizing Gibbs states in the obvious way.   In general we must use</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)(X_j- &#92;langle X_j &#92;rangle)&#92;rangle " class="latex" /></p>
<p>or equivalently, </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>Okay.  So much for cleaning up Last Week&#8217;s Mess.   Here&#8217;s something new.  We&#8217;ve seen that whenever <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> are observables (that is, self-adjoint),</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^* = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>We got something symmetric by taking the real part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB%5Crangle+%3D++%5Cmathrm%7BRe%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB&#92;rangle =  &#92;mathrm{Re} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>Indeed,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB+%5Crangle+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Clangle+AB+%2B+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB &#92;rangle = &#92;frac{1}{2} &#92;langle AB + BA &#92;rangle " class="latex" /></p>
<p>But by the same reasoning, we get something <i>antisymmetric</i> by taking the <i>imaginary</i> part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BIm%7D+%5Clangle+AB%5Crangle+%3D++-+%5Cmathrm%7BIm%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Im} &#92;langle AB&#92;rangle =  - &#92;mathrm{Im} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>and indeed,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BIm%7D+%5Clangle+AB+%5Crangle+%3D+%5Cfrac%7B1%7D%7B2i%7D+%5Clangle+AB+-+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Im} &#92;langle AB &#92;rangle = &#92;frac{1}{2i} &#92;langle AB - BA &#92;rangle " class="latex" /></p>
<p><a href="http://en.wikipedia.org/wiki/Commutator">Commutators</a> like <img src="https://s0.wp.com/latex.php?latex=AB-BA&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="AB-BA" class="latex" /> are important in quantum mechanics, so maybe we shouldn&#8217;t just throw out the imaginary part of the covariance matrix in our desperate search for a Riemannian metric!  Besides the symmetric tensor on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>we can also define a skew-symmetric tensor:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cmathrm%7BIm%7D+%5C%2C++%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;mathrm{Im} &#92;,  &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>This will vanish in the classical case, but not in the quantum case!</p>
<p>If you&#8217;ve studied enough geometry, you should now be reminded of things like &#8216;K&auml;hler manifolds&#8217; and &#8216;almost  K&auml;hler manifolds&#8217;.  A <a href="http://en.wikipedia.org/wiki/K%C3%A4hler_manifold">K&auml;hler manifold</a> is a manifold that&#8217;s equipped with a symmetric tensor <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and a skew-symmetric tensor <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> which fit together in the best possible way.  An <a href="http://en.wikipedia.org/wiki/Almost_K%C3%A4hler_manifold#K.C3.A4hler_manifolds">almost K&auml;hler manifold</a> is something similar, but not quite as nice.   We should probably see examples of these arising in information geometry!  And that could be pretty interesting.</p>
<p>But in general, if we start with any old manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> together with a function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> taking values in mixed states, we seem to be making <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> into something even less nice.  It gets a symmetric bilinear form <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on each tangent space, and a skew-symmetric bilinear form <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />, and they vary smoothly from point to point&#8230; but they might be degenerate, and I don&#8217;t see any reason for them to &#8216;fit together&#8217; in the nice way we need for a K&auml;hler or almost K&auml;hler manifold.</p>
<p>However, I still think something interesting might be going on here.  For one thing, there are <i>other</i> situations in physics where a space of states is equipped with a symmetric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and a skew-symmetric <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />. They show up in &#8216;dissipative mechanics&#8217; &mdash; the study of systems whose entropy increases.</p>
<p><a name="Ottinger">To conclude,</a> let me remind you of some things I said in <a href="http://math.ucr.edu/home/baez/week295.html">week295</a> of This Week&#8217;s Finds.  This is a huge digression from information geometry, but I&#8217;d like to lay out the the puzzle pieces in public view, in case it helps anyone get some good ideas.</p>
<p>I wrote:</p>
<blockquote><p>
&bull;  Hans Christian &Ouml;ttinger, <i>Beyond Equilibrium Thermodynamics</i>, Wiley, 2005.</p>
<p>I thank Arnold Neumaier for pointing out this book!  It considers a fascinating generalization of Hamiltonian mechanics that applies to  systems with dissipation: for example, electrical circuits with resistors, or mechanical systems with friction. </p>
<p>In ordinary Hamiltonian mechanics the space of states is a manifold and time evolution is a flow on this manifold determined by a smooth function called the Hamiltonian, which describes the <i>energy</i> of any state.  In this generalization the space of states is still a manifold, but now time evolution is determined by two smooth functions: the energy and the <i>entropy!</i> In ordinary Hamiltonian mechanics, energy is automatically conserved.  In this generalization that&#8217;s also true, but energy can go into the form of heat&#8230; and entropy automatically <i>increases!</i></p>
<p>Mathematically, the idea goes like this.  We start with a Poisson manifold, but in addition to the skew-symmetric Poisson bracket {F,G} of smooth functions on some manifold, we also have a symmetric bilinear bracket [F,G] obeying the Leibniz law</p>
<p>[F,GH] = [F,G]H + G[F,H]</p>
<p>and this positivity condition:</p>
<p>[F,F] &ge; 0</p>
<p>The time evolution of any function is given by a generalization of Hamilton&#8217;s equations:</p>
<p>dF/dt = {H,F} + [S,F]</p>
<p>where H is a function called the &quot;energy&quot; or &quot;Hamiltonian&quot;, and S is a function called the &quot;entropy&quot;.   The first term on the right is the usual one. The new second term describes dissipation: as we shall see, it pushes the state towards increasing entropy.</p>
<p>If we require that</p>
<p>[H,F] = {S,F} = 0</p>
<p>for every function F, then we get conservation of energy, as usual in Hamiltonian mechanics:</p>
<p>dH/dt = {H,H} + [S,H] = 0</p>
<p>But we also get the second law of thermodynamics:</p>
<p>dS/dt = {H,S} + [S,S] &ge; 0</p>
<p>Entropy always increases!</p>
<p>&Ouml;ttinger calls this framework &#8220;GENERIC&#8221; &#8211; an annoying acronym for &#8220;General Equation for the NonEquilibrium Reversible-Irreversible Coupling&#8221;.  There are lots of papers about it.  But I&#8217;m wondering if any geometers have looked into it!  </p>
<p>If we didn&#8217;t need the equations [H,F] = {S,F} = 0, we could easily get the necessary brackets starting with a K&auml;hler manifold.  The  imaginary part of the K&auml;hler structure is a symplectic structure, say &omega;, so we can define</p>
<p>{F,G} = &omega;(dF,dG)</p>
<p>as usual to get Poisson brackets.  The real part of the K&auml;hler structure is a Riemannian structure, say g, so we can define</p>
<p>[F,G] = g(dF,dG)</p>
<p>This satisfies</p>
<p>[F,GH] = [F,G]H + G[F,H]</p>
<p>and </p>
<p>[F,F] &ge; 0</p>
<p>Don&#8217;t be fooled: this stuff is not rocket science.  In particular, the inequality above has a simple meaning: when we move in the direction of the gradient of F, the function F increases.  So adding the second term to Hamilton&#8217;s equations has the effect of pushing the system towards increasing entropy.</p>
<p>Note that I&#8217;m being a tad unorthodox by letting &omega; and g eat cotangent vectors instead of tangent vectors &#8211; but that&#8217;s no big deal. The big deal is this: if we start with a K&auml;hler manifold and define brackets this way, we don&#8217;t get [H,F] = 0 or {S,F} = 0 for all functions F unless H and S are constant!  That&#8217;s no good for applications to physics.  To get around this problem, we would need to consider some sort of <i>degenerate</i> K&auml;hler structure &#8211; one where &omega; and g are degenerate bilinear forms on the cotangent space.</p>
<p>Has anyone thought about such things?  They remind me a little of &quot;Dirac structures&quot; and &quot;generalized complex geometry&quot; &#8211; but I don&#8217;t know enough about those subjects to know if they&#8217;re relevant here.</p>
<p>This GENERIC framework suggests that energy and entropy should be viewed as two parts of a single entity &#8211; maybe even its real and imaginary parts!  And that in turn reminds me of other strange  things, like the idea of using complex-valued Hamiltonians to describe dissipative systems, or the idea of &#8220;inverse temperature  as imaginary time&#8221;.  I can&#8217;t tell yet if there&#8217;s a big idea lurking here, or just a mess&#8230;.</p>
</blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#comments">36 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1408 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1408">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/" rel="bookmark">Information Geometry (Part&nbsp;3)</a></h2>
				<small>25 October, 2010</small><br />


				<div class="entry">
					<p>So far in this series of posts I&#8217;ve been explaining a paper by Gavin Crooks. Now I want to go ahead and explain a little research of my own. </p>
<p>I&#8217;m not claiming my results are new &mdash; indeed I have no idea whether they are, and I&#8217;d like to hear from any experts who might know.  I&#8217;m just claiming that this is some work I did last weekend.</p>
<p>People sometimes worry that if they explain their ideas before publishing them, someone will &#8216;steal&#8217; them.  But I think this overestimates the value of ideas, at least in esoteric fields like mathematical physics.  The problem is not people stealing your ideas: the hard part is <i>giving them away</i>.  And let&#8217;s face it, people in love with math and physics will do research unless you actively stop them.  I&#8217;m reminded of this scene from the <a href="http://www.marx-brothers.org/whyaduck/info/movies/scenes/ravelli.htm">Marx Brothers movie</a> where Harpo and Chico, playing wandering musicians, walk into a hotel and offer to play:</p>
<blockquote><p>
Groucho: What do you fellows get an hour?</p>
<p>Chico: Oh, for playing we getta ten dollars an hour.</p>
<p>Groucho: I see&#8230;What do you get for not playing?</p>
<p>Chico: Twelve dollars an hour.</p>
<p>Groucho: Well, clip me off a piece of that.</p>
<p>Chico: Now, for rehearsing we make special rate. Thatsa fifteen dollars an hour.</p>
<p>Groucho: That&#8217;s for rehearsing?</p>
<p>Chico: Thatsa for rehearsing.</p>
<p>Groucho: And what do you get for not rehearsing?</p>
<p>Chico: You couldn&#8217;t afford it.
</p></blockquote>
<p>So, I&#8217;m just rehearsing in public here &mdash; but I of course I hope to write a paper about this stuff someday, once I get enough material.</p>
<p>Remember where we were.  We had considered a manifold &mdash; let&#8217;s finally give it a name, say <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> &mdash; that parametrizes Gibbs states of some physical system.  By <b><a href="http://en.wikipedia.org/wiki/Canonical_ensemble">Gibbs state</a></b>, I mean a state that maximizes entropy subject to constraints on the expected values of some observables.  And we had seen that in favorable cases, we get a Riemannian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />!  It looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> are our observables, and the angle bracket means &#8216;expected value&#8217;. </p>
<p>All this applies to both classical or quantum mechanics.  Crooks wrote down a beautiful formula for this metric in the classical case.  But since I&#8217;m at the Centre for <i>Quantum</i> Technologies, not the Centre for Classical Technologies, I redid his calculation in the quantum case.  The big difference is that in quantum mechanics, observables don&#8217;t commute!  But in the calculations I did, that didn&#8217;t seem to matter much &mdash; mainly because I took a lot of traces, which imposes a kind of commutativity:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28AB%29+%3D+%5Cmathrm%7Btr%7D%28BA%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(AB) = &#92;mathrm{tr}(BA) " class="latex" /></p>
<p>In fact, if I&#8217;d wanted to show off, I could have done the classical and quantum cases simultaneously by replacing all operators by elements of any <a href="http://en.wikipedia.org/wiki/Von_Neumann_algebra">von Neumann algebra</a> equipped with a <a href="http://en.wikipedia.org/wiki/Von_Neumann_algebra#Weights.2C_states.2C_and_traces">trace</a>.  Don&#8217;t worry about this much: it&#8217;s just a general formalism for treating classical and quantum mechanics on an equal footing.  One example is the algebra of bounded operators on a Hilbert space, with the usual concept of trace.  Then we&#8217;re doing quantum mechanics as usual.  But another example is the algebra of suitably nice functions on a suitably nice space, where taking the trace of a function means <i>integrating</i> it.  And then we&#8217;re doing classical mechanics!   </p>
<p>For example, I showed you how to derive a beautiful formula for the metric I wrote down a minute ago: </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} ) " class="latex" /></p>
<p>But if we want to do the classical version, we can say <i>Hey, presto!</i> and write it down like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cint_%5COmega+p%28%5Comega%29+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;int_&#92;Omega p(&#92;omega) &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^j} &#92;; d &#92;omega " class="latex" /></p>
<p>What did I do just now?  I changed the trace to an integral over some space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  I rewrote <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to make you think &#8216;probability distribution&#8217;.  And I don&#8217;t need to take the real part anymore, since is everything already real when we&#8217;re doing classical mechanics.    Now this metric is the <b><a href="http://en.wikipedia.org/wiki/Fisher_information_metric">Fisher information metric</a></b> that statisticians know and love!</p>
<p>In what follows, I&#8217;ll keep talking about the quantum case, but in the back of my mind I&#8217;ll be using von Neumann algebras, so everything will apply to the classical case too.</p>
<p>So what am I going to do?  I&#8217;m going to fix a big problem with the story I&#8217;ve told so far.</p>
<p>Here&#8217;s the problem: so far we&#8217;ve only studied a special case of the Fisher information metric.  We&#8217;ve been assuming our states are Gibbs states, parametrized by the expectation values of some observables <img src="https://s0.wp.com/latex.php?latex=X_1%2C+%5Cdots%2C+X_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1, &#92;dots, X_n" class="latex" />.   Our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> was really just some open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />: a point in here was a list of expectation values.</p>
<p>But people like to work a lot more generally.  We could look at <i>any</i> smooth function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> from a smooth manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the set of density matrices for some quantum system.   We can still write down the metric</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} )  " class="latex" /></p>
<p>in this more general situation.  Nobody can stop us!  But it would be better if we could <i>derive</i> this formula, as before, starting from a formula like the one we had before:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>The challenge is that now we don&#8217;t have observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> to start with.  All we have is a smooth function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> from some manifold to some set of states.   How can we pull observables out of thin air?</p>
<p>Well, you may remember that last time we had</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Clambda%5Ei+X_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;lambda^i X_i}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> were some functions on our manifold and </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D%28e%5E%7B-%5Clambda%5Ei+X_i%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr}(e^{-&#92;lambda^i X_i})" class="latex" /></p>
<p>was the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a>.  Let&#8217;s copy this idea.  </p>
<p>So, we&#8217;ll start with our density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, but then write it as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-A}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is some self-adjoint operator and </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-A%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-A})" class="latex" /></p>
<p>(Note that <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, like <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, is really an operator-valued function on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  So, I should write something like <img src="https://s0.wp.com/latex.php?latex=A%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A(x)" class="latex" /> to denote its value at a particular point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />, but I won&#8217;t usually do that.  As usual, I expect some intelligence on your part!)</p>
<p>Now we can repeat some calculations I did last time.  As before, let&#8217;s take the logarithm of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D+%5C%2C+%5Crho+%3D+-A+-+%5Cmathrm%7Bln%7D%5C%2C++Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln} &#92;, &#92;rho = -A - &#92;mathrm{ln}&#92;,  Z" class="latex" /></p>
<p>and then differentiate it.  Suppose <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> are local coordinates near some point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D%5C%2C+%5Crho+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A+-+%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln}&#92;, &#92;rho = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A - &#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z" class="latex" /></p>
<p>Last time we had nice formulas for both terms on the right-hand side above.  To get similar formulas now, let&#8217;s define operators</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i = &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A" class="latex" /></p>
<p>This gives a nice name to the first term on the right-hand side above.  What about the second term?  We can calculate it out:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z+%3D+%5Cfrac%7B1%7D%7BZ%7D++%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Btr%7D%28e%5E%7B-A%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%7D++%5Cmathrm%7Btr%7D%28%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+e%5E%7B-A%7D%29+%3D+-+%5Cfrac%7B1%7D%7BZ%7D++%5Cmathrm%7Btr%7D%28e%5E%7B-A%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z = &#92;frac{1}{Z}  &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} &#92;mathrm{tr}(e^{-A}) = &#92;frac{1}{Z}  &#92;mathrm{tr}(&#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} e^{-A}) = - &#92;frac{1}{Z}  &#92;mathrm{tr}(e^{-A} &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A)" class="latex" /></p>
<p>where in the last step we use the chain rule.   Next, use the definition of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, and get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z+%3D+-+%5Cmathrm%7Btr%7D%28%5Crho+X_i%29+%3D+-+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z = - &#92;mathrm{tr}(&#92;rho X_i) = - &#92;langle X_i &#92;rangle" class="latex" /></p>
<p>This is just what we got last time!  Ain&#8217;t it fun to calculate when it all works out so nicely?</p>
<p>So, putting both terms together, we see </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho+%3D+-+X_i+%2B+%5Clangle+X_i+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho = - X_i + &#92;langle X_i &#92;rangle " class="latex" /></p>
<p>or better:</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho" class="latex" /></p>
<p>This is a nice formula for the &#8216;fluctuation&#8217; of the observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, meaning how much they differ from their expected values.  And it looks exactly like the formula we had last time!  The difference is that last time we <i>started out</i> assuming we had a bunch of observables, <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, and defined <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> to be the state maximizing the entropy subject to constraints on the expectation values of all these observables.<br />
Now we&#8217;re starting with <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and working backwards.</p>
<p>From here on out, it&#8217;s easy.   As before, we can define <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> to be the real part of the covariance matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>Using the formula </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho" class="latex" /></p>
<p>we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^j} &#92;rangle " class="latex" /></p>
<p>or </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D%5C%2C%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ej%7D%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re}&#92;,&#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^j}) " class="latex" /></p>
<p><i>Voil&agrave;!</i>  </p>
<p>When this matrix is positive definite at every point, we get a Riemanian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.   Last time I said this is what people call the <a href="http://en.wikipedia.org/wiki/Bures_metric">&#8216;Bures metric&#8217;</a> &mdash; though frankly, now that I examine the formulas, I&#8217;m not so sure.  But in the classical case, it&#8217;s called the Fisher information metric.   </p>
<p>Differential geometers like to use <img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i" class="latex" /> as a shorthand for <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial_i}" class="latex" />, so they&#8217;d write down our metric in a prettier way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cpartial_i+%28%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+%5C%3B+%5Cpartial_j+%28%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;partial_i (&#92;mathrm{ln} &#92;, &#92;rho) &#92;; &#92;partial_j (&#92;mathrm{ln} &#92;, &#92;rho) )" class="latex" /></p>
<p>Differential geometers like coordinate-free formulas, so let&#8217;s also give a coordinate-free formula for our metric.  Suppose <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" /> is a point in our manifold, and suppose <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> are tangent vectors to this point.  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28v%2Cw%29+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Clangle+v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29+%5C%3B+w%28%5Cmathrm%7Bln%7D+%5C%2C%5Crho%29+%5Crangle+%5C%3B+%3D+%5C%3B+%5Cmathrm%7BRe%7D+%5C%2C%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29+%5C%3B+w%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(v,w) = &#92;mathrm{Re} &#92;, &#92;langle v(&#92;mathrm{ln}&#92;, &#92;rho) &#92;; w(&#92;mathrm{ln} &#92;,&#92;rho) &#92;rangle &#92;; = &#92;; &#92;mathrm{Re} &#92;,&#92;mathrm{tr}(&#92;rho &#92;; v(&#92;mathrm{ln}&#92;, &#92;rho) &#92;; w(&#92;mathrm{ln}&#92;, &#92;rho))  " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, &#92;rho" class="latex" /> is a smooth operator-valued function on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v(&#92;mathrm{ln}&#92;, &#92;rho)" class="latex" /> means the derivative of this function in the <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> direction at the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.</p>
<p>So, this is all very nice.  To conclude, two more points: a technical one, and a more important philosophical one.</p>
<p>First, the technical point.  When I said <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> could be <i>any</i> smooth function from a smooth manifold to some set of states, I was actually lying.  That&#8217;s an important pedagogical technique: the brazen lie.</p>
<p>We can&#8217;t really take the logarithm of <i>every</i> density matrix.  Remember, we take the log of a density matrix by taking the log of all its eigenvalues.  These eigenvalues are &ge; 0, but if one of them is zero, we&#8217;re in trouble!  The logarithm of zero is undefined.</p>
<p>On the other hand, there&#8217;s no problem taking the logarithm of our density-matrix-valued function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> when it&#8217;s <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite</a> at each point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  You see, a density matrix is positive definite iff its eigenvalues are all &gt; 0.   In this case it has a unique self-adjoint logarithm.</p>
<p>So, we must assume <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is positive definite.   But what&#8217;s the physical significance of this &#8216;positive definiteness&#8217; condition?  Well, any density matrix can be diagonalized using some orthonormal basis.  It can then be seen as probabilistic mixture &mdash; not a quantum superposition! &mdash; of pure states taken from this basis. Its eigenvalues are the probabilities of finding the mixed state to be in one of these pure states.  So, saying that all its eigenvalues are all &gt; 0 amounts to saying that all the pure states in this orthonormal basis show up with <i>nonzero</i> probability! Intuitively, this means our mixed state is &#8216;really mixed&#8217;.  For example, it can&#8217;t be a pure state.  In math jargon, it means our mixed state is in the <i>interior</i> of the convex set of mixed states.</p>
<p>Second, the philosophical point.  Instead of starting with the density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, I took <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> as fundamental.   But different choices of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> give the same <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  After all,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-A}" class="latex" /></p>
<p>where we cleverly divide by the normalization factor</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-A%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-A})" class="latex" /></p>
<p>to get <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D+%5C%2C+%5Crho+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr} &#92;, &#92;rho = 1" class="latex" />.  So, if we multiply <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-A}" class="latex" /> by any positive constant, or indeed any positive <i>function</i> on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> will remain unchanged!  </p>
<p>So we have added a little extra information when switching from <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />.   You can think of this as &#8216;gauge freedom&#8217;, because I&#8217;m saying we can do any transformation like</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%5Cmapsto+A+%2B+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;mapsto A + f " class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=f%3A+M+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: M &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is a smooth function.   This doesn&#8217;t change <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, so arguably it doesn&#8217;t change the &#8216;physics&#8217; of what I&#8217;m doing.  It <i>does</i> change <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  It also changes the observables </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i = &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A" class="latex" /></p>
<p>But it doesn&#8217;t change their &#8216;fluctuations&#8217; </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle" class="latex" /></p>
<p>so it doesn&#8217;t change the metric <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.</p>
<p>This gauge freedom is interesting, and I want to understand it better. It&#8217;s related to something very simple yet mysterious.  In statistical mechanics the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> begins life as &#8216;just a normalizing factor&#8217;.   If you change the physics so that <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> gets multiplied by some number, the Gibbs state doesn&#8217;t change.  But then the partition function takes on an incredibly significant role as something whose logarithm you differentiate to get lots of physically interesting information!   So in some sense the partition function doesn&#8217;t matter much&#8230; but <i>changes</i> in the partition function matter a lot.</p>
<p>This is just like the split personality of phases in quantum mechanics.  On the one hand they &#8216;don&#8217;t matter&#8217;: you can multiply a unit vector by any phase and the pure state it defines doesn&#8217;t change.  But on the other hand, <i>changes</i> in phase matter a lot.  </p>
<p>Indeed the analogy here is quite deep: it&#8217;s the analogy between probabilities in statistical mechanics and amplitudes in quantum mechanics, the analogy between <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bexp%7D%28-%5Cbeta+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{exp}(-&#92;beta H)" class="latex" /> in statistical mechanics and <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bexp%7D%28-i+t+H+%2F+%5Chbar%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{exp}(-i t H / &#92;hbar)" class="latex" /> in quantum mechanics, and so on.  This is part of a bigger story about &#8216;rigs&#8217; which I told back in the <a href="http://math.ucr.edu/home/baez/qg-winter2007/qg-winter2007.html#quantization">Winter 2007 quantum gravity seminar</a>, especially in <a href="http://math.ucr.edu/home/baez/qg-winter2007/w07week05a.pdf">week13</a>.  So, it&#8217;s fun to see it showing up yet again&#8230; even though I don&#8217;t completely understand it here.</p>
<p>[Note: in the original version of this post, I omitted the real part in my definition <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" />, giving a &#8216;Riemannian metric&#8217; that was <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">neither real nor symmetric</a> in the quantum case.  Most of the comments below are based on that original version, not the new fixed one.]</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/#comments">58 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/2/?s=fisher+information+metric" >&laquo; Previous Entries</a></div>
			<div class="alignright"></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;fisher information metric&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/07/29/structured-vs-decorated-cospans-part-2/">Structured vs Decorated Cospans (Part&nbsp;2)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="amarashiki" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/#comment-172555">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Robert A. Wilson" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="https://robwilson1.wordpress.com" rel="nofollow"><img alt='' src='https://2.gravatar.com/avatar/24dc7e2371491f36fd4538b3474920b5?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="https://robwilson1.wordpress.com" rel="nofollow">Robert A. Wilson</a> on <a href="https://johncarlosbaez.wordpress.com/2021/04/04/the-koide-formula/#comment-172553">The Koide Formula</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172545">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Wolfgang" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://1.gravatar.com/avatar/d3c6d7ec8069e25c08a0a11263581925?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">Wolfgang on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172543">Classical Mechanics versus The&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (478)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (204)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,227 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/?s=fisher+information+metric"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="0272f16312" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="fisher information metric" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,176,945 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	<div class="grofile-hash-map-24dc7e2371491f36fd4538b3474920b5">
	</div>
	<div class="grofile-hash-map-d3c6d7ec8069e25c08a0a11263581925">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jksOwjAMRC9EarWLSl0gzpImVuXQfLAd2t6eVAIJsWA1nrGePbAV43JSTApBwOOTHJa9C3KBr1Wspqx1oSRgfaRkZssQrShym4yydXf5hdq98KjIx1u6rbgcTeG8H4axZaIfhpJbq0c5oWYxzui79uhPkY38giogdRbHVJRyOjvc4rUfh2nqh2Hswwtq4FN0'></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTYzWnQ/RzQ0TGd+N2VbbVRkJWZubSV4Y3VlNUVXandhdnNPZEVoUCxXenBxdn5GM3ksXUgtPWtrK0xdRmx5QiZMNmJPaWt4a2FDNURXJV0mOCtWRGFoTmxkemtdYlc4VHRfTCt0SnQ0V2I1Jjd3T01jNi5UeW13emNoLHhIb1pLbkROWU5vNFpJbk5YMGh4Lzk4Qk80Y0lfOTRiVD8/WV1IY0h0Ml90ejJRL1t8UWJtY0t3W2pjOFA1cFh1b2hFUG83a1dB'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>