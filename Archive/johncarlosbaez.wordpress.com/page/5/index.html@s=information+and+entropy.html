<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth | Page 5</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F01%2F21%2Finformation-geometry-part-6%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/page\/5\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fpage%2F5%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F01%2F21%2Finformation-geometry-part-6%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Page 5 &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results paged paged-5 search-paged-5 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-2298 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2298">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="bookmark">Information Geometry (Part&nbsp;6)</a></h2>
				<small>21 January, 2011</small><br />


				<div class="entry">
					<p>So far, my thread on information geometry hasn&#8217;t said much about <i>information</i>.  It&#8217;s time to remedy that.</p>
<p>I&#8217;ve been telling you about the Fisher information metric.  In statistics this is nice a way to define a &#8216;distance&#8217; between two probability distributions.  But it also has a quantum version.  </p>
<p>So far I&#8217;ve showed you how to define the Fisher information metric in three equivalent ways.  I also showed that in the quantum case, the Fisher information metric is the real part of a complex-valued thing.  The imaginary part is related to the uncertainty principle.</p>
<p>You can see it all here:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">Part 1</a> &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/">Part 2</a>  &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Part 3</a>  &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">Part 4</a> &nbsp; &nbsp; • <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/">Part 5</a></p>
<p>But there&#8217;s yet another way to define the Fisher information metric, which really involves <i>information</i>.   </p>
<p>To explain this, I need to start with the idea of &#8216;information gain&#8217;, or &#8216;relative entropy&#8217;.   And it looks like I should do a whole post on this.</p>
<p>So: </p>
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Measure_%28mathematics%29#Definition">measure space</a> &mdash; that is, a space you can do integrals over.  By a <b>probability distribution</b> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, I&#8217;ll mean a nonnegative function </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3A+%5COmega+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : &#92;Omega &#92;to &#92;mathbb{R}" class="latex" /> </p>
<p>whose integral is 1.  Here <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" /> is my name for the measure on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  Physicists might call <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> the &#8216;phase space&#8217; of some classical system, but probability theorists might call it a space of &#8216;events&#8217;.  Today I&#8217;ll use the probability theorist&#8217;s language.  The idea here is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_A+%5C%3B+p%28%5Comega%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_A &#92;; p(&#92;omega) &#92;; d &#92;omega " class="latex" /></p>
<p>gives the probability that when an event happens, it&#8217;ll be one in the subset <img src="https://s0.wp.com/latex.php?latex=A+%5Csubseteq+%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;subseteq &#92;Omega" class="latex" />.  That&#8217;s why we want </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ge 0" class="latex" /></p>
<p>Probabilities are supposed to be nonnegative.  And that&#8217;s also why we want</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5C%3B+d+%5Comega+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;; d &#92;omega = 1 " class="latex" /></p>
<p>This says that the probability of <i>some</i> event happening is 1.</p>
<p>Now, suppose we have two probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  The <b><a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">information gain</a></b> as we go from <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>We also call this the entropy of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> <b>relative to</b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  It says how much information you learn if you discover that the probability distribution of an event is <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, if before you had thought it was <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.</p>
<p>I like relative entropy because it&#8217;s related to the <a href="http://en.wikipedia.org/wiki/Bayesian_probability">Bayesian interpretation of probability</a>.  The idea here is that you can&#8217;t really &#8216;observe&#8217; probabilities as frequencies of events, except in some unattainable limit where you repeat an experiment over and over infinitely many times.  Instead, you start with some hypothesis about how likely things are: a probability distribution called the <a href="http://en.wikipedia.org/wiki/Prior_probability"><b>prior</b></a>.  Then you update this using <a href="http://yudkowsky.net/rational/bayes">Bayes&#8217; rule</a> when you gain new information.  The updated probability distribution &mdash; your new improved hypothesis &mdash; is called the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution"><b>posterior</b></a>.  </p>
<p>And if you don&#8217;t do the updating right, you need a swift kick in the posterior!</p>
<p>So, we can think of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as the prior probability distribution, and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as the posterior.  Then <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> measures the <i>amount of information</i> that caused you to change your views.</p>
<p>For example, suppose you&#8217;re flipping a coin, so your set of events is just </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5COmega+%3D+%5C%7B+%5Cmathrm%7Bheads%7D%2C+%5Cmathrm%7Btails%7D+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega = &#92;{ &#92;mathrm{heads}, &#92;mathrm{tails} &#92;}" class="latex" /></p>
<p>In this case all the integrals are just sums with two terms.  Suppose your prior assumption is that the coin is fair.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=q%28%5Cmathrm%7Bheads%7D%29+%3D+1%2F2%2C+%5C%3B+q%28%5Cmathrm%7Btails%7D%29+%3D+1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;mathrm{heads}) = 1/2, &#92;; q(&#92;mathrm{tails}) = 1/2" class="latex" /></p>
<p>But then suppose someone you trust comes up and says &#8220;Sorry, that&#8217;s a trick coin: it always comes up heads!&#8221;  So you update our probability distribution and get this posterior:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathrm%7Bheads%7D%29+%3D+1%2C+%5C%3B+p%28%5Cmathrm%7Btails%7D%29+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;mathrm{heads}) = 1, &#92;; p(&#92;mathrm{tails}) = 0 " class="latex" /></p>
<p>How much information have you gained?  Or in other words, what&#8217;s the relative entropy?  It&#8217;s this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+%3D+1+%5Ccdot+%5Clog%28%5Cfrac%7B1%7D%7B1%2F2%7D%29+%2B+0+%5Ccdot+%5Clog%28%5Cfrac%7B0%7D%7B1%2F2%7D%29+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega = 1 &#92;cdot &#92;log(&#92;frac{1}{1/2}) + 0 &#92;cdot &#92;log(&#92;frac{0}{1/2}) = 1 " class="latex" /></p>
<p>Here I&#8217;m doing the logarithm in base 2, and you&#8217;re supposed to know that in this game <img src="https://s0.wp.com/latex.php?latex=0+%5Clog+0+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;log 0 = 0" class="latex" />.  </p>
<p>So: you&#8217;ve learned <i>one bit of information!</i></p>
<p>That&#8217;s supposed to make perfect sense.  On the other hand, the reverse scenario takes a bit more thought. </p>
<p>You start out feeling sure that the coin always lands heads up.  Then someone you trust says &#8220;No, that&#8217;s a perfectly fair coin.&#8221;   If you work out the amount of information you learned this time, you&#8217;ll see it&#8217;s <i>infinite</i>.  </p>
<p>Why is that?</p>
<p>The reason is that something that you thought was impossible &mdash; the coin landing tails up &mdash; turned out to be possible.  In this game, it counts as infinitely shocking to learn something like that, so the information gain is infinite.   If you hadn&#8217;t been so darn sure of yourself &mdash; if you had just believed that the coin <i>almost always</i> landed heads up &mdash; your information gain would be large but finite.</p>
<p>The Bayesian philosophy is built into the concept of information gain, because information gain depends on two things: the prior and the posterior.  And that&#8217;s just as it should be: <i>you can only say how much you learned if you know what you believed beforehand!</i></p>
<p>You might say that information gain depends on <i>three</i> things: <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" />.  And you&#8217;d be right!    Unfortunately, the notation <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> is a bit misleading.   Information gain really <i>does</i> depend on just two things, but these things are not <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />: they&#8217;re <img src="https://s0.wp.com/latex.php?latex=p%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;omega) d&#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;omega) d&#92;omega" class="latex" />.   These are called <a href="http://en.wikipedia.org/wiki/Probability_measure">probability measures</a>, and they&#8217;re ultimately more important than the probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />. </p>
<p>To see this, take our information gain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>and juggle it ever so slightly to get this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B++%5Clog%28%5Cfrac%7Bp%28%5Comega%29+d%5Comega%7D%7Bq%28%5Comega%29d+%5Comega%7D%29+%5C%3B+p%28%5Comega%29+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;;  &#92;log(&#92;frac{p(&#92;omega) d&#92;omega}{q(&#92;omega)d &#92;omega}) &#92;; p(&#92;omega) d &#92;omega " class="latex" /></p>
<p>Clearly this depends only on <img src="https://s0.wp.com/latex.php?latex=p%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;omega) d&#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;omega) d&#92;omega" class="latex" />.  Indeed, it&#8217;s good to work directly with these probability measures and give them short names, like</p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cmu+%3D+p%28%5Comega%29+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu = p(&#92;omega) d &#92;omega " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cnu+%3D+q%28%5Comega%29+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu = q(&#92;omega) d &#92;omega" class="latex" /></p>
<p>Then the formula for information gain looks more slick:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+%5Clog%28%5Cfrac%7Bd%5Cmu%7D%7Bd%5Cnu%7D%29+%5C%3B+d%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; &#92;log(&#92;frac{d&#92;mu}{d&#92;nu}) &#92;; d&#92;mu " class="latex" /></p>
<p>And by the way, in case you&#8217;re wondering, the <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d" class="latex" /> here doesn&#8217;t actually mean much: we&#8217;re just so brainwashed into wanting a <img src="https://s0.wp.com/latex.php?latex=d+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x" class="latex" /> in our integrals that people often use <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu" class="latex" /> for a measure even though the simpler notation <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> might be more logical.  So, the function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Cmu%7D%7Bd%5Cnu%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d&#92;mu}{d&#92;nu} " class="latex" /></p>
<p>is really just a ratio of probability measures, but people call it a <a href="http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem">Radon-Nikodym derivative</a>, because it looks like a derivative (and in some important examples it actually is).   So, if I were talking to myself, I could have shortened this blog entry immensely by working with directly probability measures, leaving out the <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d" class="latex" />&#8216;s, and saying:</p>
<blockquote><p>
Suppose <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" /> are probability measures; then the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" />, or information gain, is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Cmu%2C+%5Cnu%29+%3D++%5Cint_%5COmega+%5C%3B+%5Clog%28%5Cfrac%7B%5Cmu%7D%7B%5Cnu%7D%29+%5C%3B+%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(&#92;mu, &#92;nu) =  &#92;int_&#92;Omega &#92;; &#92;log(&#92;frac{&#92;mu}{&#92;nu}) &#92;; &#92;mu " class="latex" /></p></blockquote>
<p>But I&#8217;m under the impression that people are actually reading this stuff, and that most of you are happier with functions than measures.  So, I decided to start with</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>and then gradually work my way up to the more sophisticated way to think about relative entropy!  But having gotten that off my chest, now I&#8217;ll revert to the original naive way.</p>
<p>As a warmup for next time, let me pose a question.  How much is this quantity</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>like a <i>distance</i> between probability distributions?  A distance function, or <a href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29">metric</a>, is supposed to satisfy some axioms.  Alas, relative entropy satisfies some of these, but not the most interesting one!</p>
<p>&bull; If you&#8217;ve got a metric, the distance between points should always be nonnegative. Indeed, this holds:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) &#92;ge 0" class="latex" /></p>
<p>So, we never learn a negative amount when we update our prior, at least according to this definition.   It&#8217;s a fun exercise to prove this inequality, at least if you know some tricks involving inequalities and convex functions &mdash; otherwise it might be hard.  </p>
<p>&bull;  If you&#8217;ve got a metric, the distance between two points should only be zero if they&#8217;re really the same point.  In fact, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = 0" class="latex" /> </p>
<p>if and only if </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+d%5Comega+%3D+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d&#92;omega = q d &#92;omega " class="latex" /></p>
<p>It&#8217;s possible to have <img src="https://s0.wp.com/latex.php?latex=p+d%5Comega+%3D+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d&#92;omega = q d &#92;omega " class="latex" /> even if <img src="https://s0.wp.com/latex.php?latex=p+%5Cne+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ne q" class="latex" />, because <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" /> can be zero somewhere.  But this is just more evidence that we should really be talking about the probability measure <img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega" class="latex" /> instead of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  If we do that, we&#8217;re okay so far!</p>
<p>&bull; If you&#8217;ve got a metric, the distance from your first point to your second point is the same as the distance from the second to the first.  Alas, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%5Cne+S%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) &#92;ne S(q,p)" class="latex" /></p>
<p>in general.  We already saw this in our example of the flipped coin.  This is a slight bummer, but I could live with it, since <a href="http://www.tac.mta.ca/tac/reprints/articles/1/tr1.pdf">Lawvere has already shown</a> that it&#8217;s wise to generalize the concept of metric by dropping this axiom.</p>
<p>&bull; If you&#8217;ve got a metric, it obeys the <a href="http://en.wikipedia.org/wiki/Triangle_inequality">triangle inequality</a>.  This is the really interesting axiom, and alas, this too fails.  Later we&#8217;ll see why.</p>
<p>So, relative entropy does a fairly miserable job of acting like a distance function.  People call it a <a href="http://en.wikipedia.org/wiki/Statistical_distance">divergence</a>.  In fact, they often call it the <b>Kullback-Leibler divergence</b>.  I don&#8217;t like that, because &#8216;the Kullback-Leibler divergence&#8217;  doesn&#8217;t really explain the idea: it sounds more like the title of a bad spy novel.  &#8216;Relative entropy&#8217;, on the other hand, makes a lot of sense if you understand entropy.  And &#8216;information gain&#8217; makes sense if you understand information.</p>
<p>Anyway: how can we save this miserable attempt to get a distance function on the space of probability distributions?  <i>Simple: take its matrix of second derivatives and use that to define a Riemannian metric</i> <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.  This Riemannian metric in turn defines a metric of the more elementary sort we&#8217;ve been discussing today.</p>
<p>And this Riemannian metric is the Fisher information metric I&#8217;ve been talking about all along!</p>
<p>More details later, I hope.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comments">21 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;6)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2197 post type-post status-publish format-standard hentry category-information-and-entropy category-quantum-technologies" id="post-2197">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/01/11/quantum-information-processing-2011/" rel="bookmark">Quantum Information Processing 2011 (Day&nbsp;1)</a></h2>
				<small>11 January, 2011</small><br />


				<div class="entry">
					<p>This year&#8217;s session of the big conference on quantum computation, quantum cryptography, and so on is being held in Singapore this year: </p>
<p>&bull; <a href="http://qip2011.quantumlah.org/">QIP 2011</a>, the 14th Workshop on Quantum Information Processing, 8-14 January 2011, Singapore.</p>
<p>Because the battery on my laptop is old and not very energetic, and I can&#8217;t find any sockets in the huge ballroom where the talks are being delivered, I can&#8217;t live-blog the talks.  So, my reports here will be quite incomplete.</p>
<p>Here are microscopic summaries of <i>just three</i> talks from Monday&#8217;s session.  You can see arXiv references, slides, and videos of the talks <a href="http://qip2011.quantumlah.org/scientificprogramme/">here</a>.   I&#8217;ll just give links to the slides.  </p>
<p><a href="http://qip2011.quantumlah.org/scientificprogramme/abstract/1011.0105p.pdf">Christian Kurtsiefer</a> gave a nice talk on how to exploit the physics of photodetectors to attack quantum key distribution systems!  By cutting the optical fiber and shining a lot of light down both directions, evil Eve can &#8216;blind&#8217; Alice and Bob&#8217;s photodetectors.  Then, by shining a quick even brighter pulse of light, she can fool one of their photodetectors into thinking it&#8217;s seen a single photon.   She can even fool them into thinking they&#8217;ve seen a violation of Bell&#8217;s inequality, by purely classical means, thanks to the fact that only a small percentage of photons make it down the cable in the first place.  Christian and his collaborators have actually done this trick in an experiment here at the CQT!</p>
<p><a href="http://qip2011.quantumlah.org/scientificprogramme/abstract/1009.2840p.pdf">Tzu-Chieh Wei and Akimasa Miyake</a> gave a two-part joint talk on how the AKLT ground state is universal for measurement-based quantum computation.  The AKLT ground state works like this: you&#8217;ve got a hexagonal lattice with three spin-1/2 particles at each vertex.  Think of each particle as attached to one of the three edges coming out of that vertex.  In the ground state, you start by putting the pair of particles at the ends of each edge in the spin-0 (also known as &#8220;singlet&#8221;, or antisymmetrized) state, and then you project the three particles at each vertex down to the spin-3/2 (completely symmetrized) state.  This is indeed the ground state of a cleverly chosen antiferromagnetic Hamiltonian.  But has anyone ever prepared this sort of system in the lab?</p>
<p><a href="http://qip2011.quantumlah.org/scientificprogramme/abstract/158p.pdf">David Poulin</a> gave a talk on how to efficiently compute time evolution given a time-dependent quantum Hamiltonian.  The trickiness arises from Hamiltonians that change very rapidly with time.  In a naive evenly spaced discretization of the time-ordered exponential, this would require you to use lots of tiny time steps to get a good approximation.  But using a clever <i>randomly chosen</i> discretization you can avoid this problem, at least for uniformly bounded Hamiltonians, those obeying:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7C+H%28t%29+%5C%7C+%5Cle+K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;| H(t) &#92;| &#92;le K" class="latex" /></p>
<p>for all times <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.  The key is that the high-frequency part of a time-dependent Hamiltonian only couples faraway energy levels, but a uniformly bounded Hamiltonian <i>doesn&#8217;t have</i> faraway energy levels.</p>
<p>A couple more things &mdash; really just notes to myself: </p>
<p><a href="http://www.perimeterinstitute.ca/personal/sflammia/">Steve Flammia</a> told me about this paper relating the Cramer-Rao bound (which involves Fisher information) to the time-energy uncertainty principle:</p>
<p>&bull; Sergio Boixo, Steven T. Flammia, Carlton M. Caves, and J.M. Geremia, <a href="http://arxiv.org/abs/quant-ph/0609179">Generalized limits for single-parameter quantum estimation</a>.</p>
<p><a href="http://www.perimeterinstitute.ca/index.php?option=com_content&amp;task=view&amp;id=30&amp;Itemid=72&amp;pi=Markus_Mueller">Markus M&uuml;ller</a> told me about a paper mentioning relations between Maxwell&#8217;s demon and algorithmic entropy.  I need to get some references on this work &mdash; it might help me make progress on <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/">algorithmic thermodynamics</a>.  It&#8217;s probably one of these:</p>
<p>&bull; Markus M&uuml;ller, <a href="http://arxiv.org/abs/0712.4377">Quantum Kolmogorov complexity and the quantum Turing machine</a> (PhD thesis).</p>
<p>&bull; Markus M&uuml;ller, <a href="http://www.arxiv.org/abs/0707.2924">On the quantum Kolmogorov complexity of classical strings</a>, <i>Int. J. Quant. Inf.</i> <b>7</b> (2009), 701-711.</p>
<p>Hmm &mdash; the first one says:</p>
<blockquote><p>
A concrete proposal for an application of quantum Kolmogorov complexity is to analyze a quantum version of the thought experiment of Maxwell’s demon. In one of the versions of this thought experiment, some microscopic device tries to decrease the entropy of some gas in a box, without the expense of energy, by intelligently opening or closing some little door that separates both halves of the box. It is clear that a device like this cannot work as described, since its existence would violate the second law of thermodynamics. But then, the question is what prevents such a little device (or “demon”) from operating.</p>
<p>Roughly, the answer is that the demon has to make observations to decide whether to close or open the door, and these observations accumulate information. From time to time, the demon must erase this additional information, which is only possible at the expense of energy, due to Landauer’s principle. In Li and Vitanyi&#8217;s book <i>An Introduction to Kolmogorov Complexity and Its Applications</i>, this cost of energy is analyzed under very weak assumptions with the help of Kolmogorov complexity. Basically, the energy that the demon can extract from the gas is limited by the difference of the entropy of the gas, plus the difference of the Kolmogorov complexity of the demon’s memory before and after the demon’s actions. The power of this analysis is that it even encloses the case that the demon has a computer to do clever calculations, e.g. to compress the accumulated information before erasing it.</p>
</blockquote>
<p>So, I guess I need to reread Li and Vitanyi&#8217;s book!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/11/quantum-information-processing-2011/#comments">21 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/" rel="category tag">quantum technologies</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/11/quantum-information-processing-2011/" rel="bookmark" title="Permanent Link to Quantum Information Processing 2011 (Day&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1573 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1573">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/" rel="bookmark">Information Geometry (Part&nbsp;5)</a></h2>
				<small>2 November, 2010</small><br />


				<div class="entry">
					<p>I&#8217;m trying to understand the Fisher information metric and how it&#8217;s related to <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#Ottinger">&Ouml;ttinger&#8217;s formalism</a> for &#8216;dissipative mechanics&#8217; &mdash; that is, mechanics including friction. They involve similar physics, and they involve similar math, but it&#8217;s not quite clear how they fit together.</p>
<p>I think it will help to do an example.  The harmonic oscillator is a trusty workhorse throughout physics, so let&#8217;s do that.</p>
<p>So: suppose you have a rock hanging on a spring, and it can bounce up and down.  Suppose it&#8217;s in thermal equilibrium with its environment.  It will wiggle up and down ever so slightly, thanks to thermal fluctuations.   The hotter it is, the more it wiggles.  These vibrations are random, so its position and momentum at any given moment can be treated as random variables.  </p>
<p>If we take quantum mechanics into account, there&#8217;s an extra source of randomness: <i>quantum</i> fluctuations.  Now there will be fluctuations even at zero temperature.   Ultimately this is due to the uncertainty principle.  Indeed, if you know the position for sure, you can&#8217;t know the momentum at all!  </p>
<p>Let&#8217;s see how the position, momentum and energy of our rock will fluctuate given that we know all three of these quantities <i>on average</i>.  The fluctuations will form a little fuzzy blob, roughly ellipsoidal in shape, in the 3-dimensional space whose coordinates are position, momentum and energy:</p>
<div align="center">
<img src="https://i0.wp.com/www.math.dartmouth.edu/archive/m22x06/public_html/ellipsoid.jpg" />
</div>
<p>Yeah, I know you&#8217;re sick of this picture, but this time it&#8217;s for real: I want to calculate what this ellipsoid actually looks like!  I&#8217;m not promising I&#8217;ll do it &mdash; I may get stuck, or bored &mdash; but at least I&#8217;ll <i>try</i>.</p>
<p>Before I start the calculation, let&#8217;s guess the answer. A harmonic oscillator has a position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, and its energy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+%2B+p%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2}(q^2 + p^2)" class="latex" /></p>
<p>Here I&#8217;m working in units where lots of things equal 1, to keep things simple.</p>
<p>You&#8217;ll notice that this energy has rotational symmetry in the position-momentum plane.  This is ultimately what makes the harmonic oscillator such a beloved physical system.   So, we might naively guess that our little ellipsoid will have rotational symmetry as well, like this:</p>
<div align="center">
<img width="120" src="http://upload.wikimedia.org/wikipedia/commons/8/88/ProlateSpheroid.png" />
</div>
<p>or this:</p>
<div align="center">
<img width="200" src="http://upload.wikimedia.org/wikipedia/commons/b/b5/OblateSpheroid.PNG" />
</div>
<p>Here I&#8217;m using the <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> coordinates for position and momentum, while the <img src="https://s0.wp.com/latex.php?latex=z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z" class="latex" /> coordinate stands for energy.  So in these examples the position and momentum fluctuations are the same size, while the energy fluctuations, drawn in the vertical direction, might be bigger or smaller.</p>
<p>Unfortunately, this guess really is naive.  After all, there are <i>lots</i> of these ellipsoids, one centered at each point in position-momentum-energy space.  Remember the rules of the game!  You give me any point in this space.  I take the coordinates of this point as the <i>mean</i> values of position, momentum and energy, and I find the maximum-entropy state with these mean values.  Then I work out the fluctuations in this state, and draw them as an ellipsoid. </p>
<p>If you pick a point where position and momentum have mean value zero, you haven&#8217;t broken the rotational symmetry of the problem.  So, my ellipsoid must be rotationally symmetric.  But if you pick some other mean value for position and momentum, all bets are off!</p>
<p>Fortunately, this naive guess is actually right: <i>all</i> the ellipsoids are rotationally symmetric &mdash; even the ones centered at nonzero values of position and momentum!  We&#8217;ll see why soon.  And if you&#8217;ve been following this series of posts, you&#8217;ll know what this implies: the &#8220;Fisher information metric&#8221; <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on position-momentum-energy space has rotational symmetry about any vertical axis.  (Again, I&#8217;m using the vertical direction for energy.)  So, if we slice this space with any horizontal plane, the metric on this plane must be the plane&#8217;s usual metric times a constant:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq%5E2+%2B+dp%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} &#92;, (dq^2 + dp^2) " class="latex" /></p>
<p>Why?  Because only the usual metric on the plane, or any multiple of it, has ordinary rotations around every point as symmetries.  </p>
<p>So, roughly speaking, we&#8217;re recovering the &#8216;obvious&#8217; geometry of the position-momentum plane from the Fisher information metric.  <i>We&#8217;re recovering &#8216;ordinary&#8217; geometry from information geometry!</i>  </p>
<p>But this should not be terribly surprising, since we used the harmonic oscillator Hamiltonian</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+%2B+p%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2}(q^2 + p^2)" class="latex" /></p>
<p>as an input to our game.  It&#8217;s mainly just a confirmation that things are working as we&#8217;d hope.  </p>
<p>There&#8217;s more, though.  <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">Last time</a> I realized that because observables in quantum mechanics don&#8217;t commute, the Fisher information metric has a curious skew-symmetric partner called <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />.   So, we should also study this in our example.  And when we do, we&#8217;ll see that restricted to any horizontal plane in position-momentum-energy space, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq+%5C%2C+dp+-+dp+%5C%2C+dq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = &#92;mathrm{constant} &#92;, (dq &#92;, dp - dp &#92;, dq)" class="latex" /></p>
<p>This looks like a mutant version of the Fisher information metric</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq%5E2+%2B+dp%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} &#92;, (dq^2 + dp^2) " class="latex" /></p>
<p>and if you know your geometry, you&#8217;ll know it&#8217;s the usual <a href="http://en.wikipedia.org/wiki/Symplectic_manifold">&#8216;symplectic structure&#8217;</a> on the position-energy plane &mdash; at least, times some constant.   </p>
<p>All this is very reminiscent of &Ouml;ttinger&#8217;s work on dissipative mechanics.  But we&#8217;ll also see something else: while the constant in <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> depends on the energy &mdash; that is, on which horizontal plane we take &mdash; the constant in <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> does not!</p>
<p>Why?  It&#8217;s perfectly sensible.  The metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on our horizontal plane keeps track of fluctuations in position and momentum.  Thermal fluctuations get bigger when it&#8217;s hotter &mdash; and to boost the average energy of our oscillator, we must heat it up.   So, as we increase the energy, moving our horizontal plane further up in position-momentum-energy space, the metric on the plane gets bigger!   In other words, our ellipsoids get a fat cross-section at high energies.</p>
<p>On the other hand, the symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> arises from the fact that position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> don&#8217;t commute in quantum mechanics.  They obey Heisenberg&#8217;s <a href="http://en.wikipedia.org/wiki/Canonical_commutation_relation">&#8216;canonical commutation relation&#8217;</a>:  </p>
<p><img src="https://s0.wp.com/latex.php?latex=q+p+-+p+q+%3D+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q p - p q = i " class="latex" /></p>
<p>This relation doesn&#8217;t involve energy, so <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> will be the same on every horizontal plane.   And it turns out this relation implies</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega++%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq+%5C%2C+dp+-+dp+%5C%2C+dq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega  = &#92;mathrm{constant} &#92;, (dq &#92;, dp - dp &#92;, dq)" class="latex" /></p>
<p>for some constant we&#8217;ll compute later.  </p>
<p>Okay, that&#8217;s the basic idea.  Now let&#8217;s actually do some computations.  For starters, let&#8217;s see why all our ellipsoids have rotational symmetry!</p>
<p>To do this, we need to understand a bit about the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that maximizes entropy given certain mean values of position, momentum and energy.   So, let&#8217;s choose the numbers we want for these mean values  (also known as &#8216;expected values&#8217; or &#8216;expectation values&#8217;):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = E " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+q+%5Crangle+%3D+q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle q &#92;rangle = q_0" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+p+%5Crangle+%3D+p_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle p &#92;rangle = p_0" class="latex" /></p>
<p>I hope this isn&#8217;t too confusing: <img src="https://s0.wp.com/latex.php?latex=H%2C+p%2C+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H, p, q" class="latex" /> are our observables which are operators, while <img src="https://s0.wp.com/latex.php?latex=E%2C+p_0%2C+q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, p_0, q_0" class="latex" /> are the mean values we have chosen for them.  The state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> depends on <img src="https://s0.wp.com/latex.php?latex=E%2C+p_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, p_0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0" class="latex" />.</p>
<p>We&#8217;re doing quantum mechanics, so position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> are both self-adjoint operators on the Hilbert space <img src="https://s0.wp.com/latex.php?latex=L%5E2%28%5Cmathbb%7BR%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(&#92;mathbb{R})" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28q%5Cpsi%29%28x%29+%3D+x+%5Cpsi%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q&#92;psi)(x) = x &#92;psi(x) " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p%5Cpsi%29%28x%29+%3D+-+i+%5Cfrac%7Bd+%5Cpsi%7D%7Bdx%7D%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p&#92;psi)(x) = - i &#92;frac{d &#92;psi}{dx}(x)" class="latex" /></p>
<p>Indeed all our observables, including the Hamiltonian </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D+%28p%5E2+%2B+q%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2} (p^2 + q^2) " class="latex" /></p>
<p>are self-adjoint operators on this Hilbert space, and the state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Density_matrix">density matrix</a> on this space, meaning a positive self-adjoint operator with trace 1.</p>
<p>Now: how do we compute <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />?   It&#8217;s a <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">Lagrange multiplier</a> problem: maximizing some function given some constraints.  And it&#8217;s well-known that when you solve this problem, you get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1%2C+%5Clambda%5E2%2C+%5Clambda%5E3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1, &#92;lambda^2, &#92;lambda^3" class="latex" /> are three numbers we yet have to find, and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalizing factor called the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} )" class="latex" /></p>
<p>Now let&#8217;s look at a special case.  If we choose <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1+%3D+%5Clambda%5E2+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1 = &#92;lambda^2 = 0" class="latex" />, we&#8217;re back a simpler and more famous problem, namely maximizing entropy subject to a constraint only on energy!  The solution is then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta+H%7D+%2C+%5Cqquad+Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-+%5Cbeta+H%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;beta H} , &#92;qquad Z = &#92;mathrm{tr} (e^{- &#92;beta H} )" class="latex" /></p>
<p>Here I&#8217;m using the letter <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^3" class="latex" /> because this is traditional.  This quantity has an important physical meaning!  It&#8217;s the <i>reciprocal of temperature</i> in units where Boltzmann&#8217;s constant is 1.   </p>
<p>Anyway, back to our special case!  In this special case it&#8217;s easy to explicitly calculate <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  Indeed, people have known how ever since <a href="http://en.wikipedia.org/wiki/Planck%27s_law#Derivation">Planck</a> put the &#8216;quantum&#8217; in quantum mechanics!   He figured out how black-body radiation works.  A box of hot radiation is just a big bunch of harmonic oscillators in thermal equilibrium.  You can work out its partition function by multiplying the partition function of each one.  </p>
<p>So, it would be great to reduce our general problem to this special case.  To do this, let&#8217;s rewrite</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} )" class="latex" /></p>
<p>in terms of some new variables, like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;beta(H - f q - g p)} " class="latex" /></p>
<p>where now </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;beta(H - f q - g p)} )" class="latex" /></p>
<p>Think about it!  Now our problem is just like an oscillator with a modified Hamiltonian</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%27+%3D+H+-+f+q+-+g+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039; = H - f q - g p" class="latex" /></p>
<p>What does this mean, physically?  Well, if you push on something with a force <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />, its potential energy will pick up a term <img src="https://s0.wp.com/latex.php?latex=-+f+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- f q" class="latex" />.  So, the first two terms are just the Hamiltonian for a harmonic oscillator <i>with an extra force pushing on it!</i>  </p>
<p>I don&#8217;t know a nice interpretation for the <img src="https://s0.wp.com/latex.php?latex=-+g+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- g p" class="latex" /> term.  We could say that besides the extra force equal to <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />, we also have an extra &#8216;gorce&#8217; equal to <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />.  I don&#8217;t know what that means.  Luckily, I don&#8217;t need to!   Mathematically, our whole problem is invariant under rotations in the position-momentum plane, so whatever works for <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> must also work for <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.   </p>
<p>Now here&#8217;s the cool part.  We can complete the square:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H%27+%26+%3D+%5Cfrac%7B1%7D%7B2%7D+%28q%5E2+%2B+p%5E2%29+-++f+q+-+g+p+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+-+2+q+f+%2B+f%5E2%29+%2B+%5Cfrac%7B1%7D%7B2%7D%28p%5E2+-+2+q+g+%2B+g%5E2%29+-+%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29++%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%28%28q+-+f%29%5E2+%2B+%28p+-+g%29%5E2%29++-+%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29++%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{aligned} H&#039; &amp; = &#92;frac{1}{2} (q^2 + p^2) -  f q - g p &#92;&#92; &amp;= &#92;frac{1}{2}(q^2 - 2 q f + f^2) + &#92;frac{1}{2}(p^2 - 2 q g + g^2) - &#92;frac{1}{2}(g^2 + f^2)  &#92;&#92; &amp;= &#92;frac{1}{2}((q - f)^2 + (p - g)^2)  - &#92;frac{1}{2}(g^2 + f^2)  &#92;end{aligned}" class="latex" /></p>
<p>so if we define &#8216;translated&#8217; position and momentum operators:</p>
<p><img src="https://s0.wp.com/latex.php?latex=q%27+%3D+q+-+f%2C+%5Cqquad+p%27+%3D+p+-+g+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q&#039; = q - f, &#92;qquad p&#039; = p - g " class="latex" /></p>
<p>we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%27+%3D+%5Cfrac%7B1%7D%7B2%7D%28%7Bq%27%7D%5E2+%2B+%7Bp%27%7D%5E2%29+-++%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039; = &#92;frac{1}{2}({q&#039;}^2 + {p&#039;}^2) -  &#92;frac{1}{2}(g^2 + f^2) " class="latex" /></p>
<p>So: apart from a constant, <img src="https://s0.wp.com/latex.php?latex=H%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039;" class="latex" /> is just the harmonic oscillator Hamiltonian in terms of &#8216;translated&#8217; position and momentum operators!</p>
<p>In other words: we&#8217;re studying a strange variant of the harmonic oscillator, where we are pushing on it with an extra force and also an extra  &#8216;gorce&#8217;.  But this strange variant is <i>exactly the same as the usual harmonic oscillator</i>, except that we&#8217;re working in translated coordinates on position-momentum space, and subtracting a constant from the Hamiltonian.</p>
<p>These are pretty minor differences.  So, we&#8217;ve succeeded in reducing our problem to the problem of a harmonic oscillator in thermal equilibrium at some temperature!</p>
<p>This makes it easy to calculate</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+%29+%3D+%5Cmathrm%7Btr%7D%28e%5E%7B-%5Cbeta+H%27%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;beta(H - f q - g p)} ) = &#92;mathrm{tr}(e^{-&#92;beta H&#039;})" class="latex" /></p>
<p>By our formula for <img src="https://s0.wp.com/latex.php?latex=H%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039;" class="latex" />, this is just</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cfrac%7B1%7D%7B2%7D%28%7Bq%27%7D%5E2+%2B+%7Bp%27%7D%5E2%29%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;mathrm{tr} (e^{-&#92;frac{1}{2}({q&#039;}^2 + {p&#039;}^2)})" class="latex" /></p>
<p>And the second factor here equals the partition function for the good old harmonic oscillator:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta+H%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;mathrm{tr} (e^{-&#92;beta H})" class="latex" /></p>
<p>So now we&#8217;re back to a textbook problem.  The eigenvalues of the <a href="http://en.wikipedia.org/wiki/Harmonic_oscillator_%28quantum%29#Hamiltonian_and_energy_eigenstates">harmonic oscillator Hamiltonian</a> are </p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%2B+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n + &#92;frac{1}{2}" class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%3D+0%2C1%2C2%2C3%2C+%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 0,1,2,3, &#92;dots" class="latex" /></p>
<p>So, the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-%5Cbeta+H%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-&#92;beta H}" class="latex" /> are are just</p>
<p><img src="https://s0.wp.com/latex.php?latex=e%5E%7B-%5Cbeta%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-&#92;beta(n + &#92;frac{1}{2})} " class="latex" /></p>
<p>and to take the trace of this operator, we sum up these eigenvalues:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28e%5E%7B-%5Cbeta+H%7D%29+%3D+%5Csum_%7Bn+%3D+0%7D%5E%5Cinfty+e%5E%7B-%5Cbeta+%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7D+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta%2F2%7D%7D%7B1+-+e%5E%7B-%5Cbeta%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(e^{-&#92;beta H}) = &#92;sum_{n = 0}^&#92;infty e^{-&#92;beta (n + &#92;frac{1}{2})} = &#92;frac{e^{-&#92;beta/2}}{1 - e^{-&#92;beta}} " class="latex" /></p>
<p>So:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cfrac%7Be%5E%7B-%5Cbeta%2F2%7D%7D%7B1+-+e%5E%7B-%5Cbeta%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;frac{e^{-&#92;beta/2}}{1 - e^{-&#92;beta}} " class="latex" /></p>
<p>We can now compute the Fisher information metric using this formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda%5Ei+%5Cpartial+%5Clambda%5Ej%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda^i &#92;partial &#92;lambda^j} &#92;ln Z" class="latex" /></p>
<p>if we remember how our new variables are related to the <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1+%3D+%5Cbeta+f+%2C+%5Cqquad+%5Clambda%5E2+%3D+%5Cbeta+g%2C+%5Cqquad+%5Clambda%5E3+%3D+%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1 = &#92;beta f , &#92;qquad &#92;lambda^2 = &#92;beta g, &#92;qquad &#92;lambda^3 = &#92;beta" class="latex" /></p>
<p>It&#8217;s just calculus!  But I&#8217;m feeling a bit tired, so I&#8217;ll leave this pleasure to you.  </p>
<p>For now, I&#8217;d rather go back to our basic intuition about how the Fisher information metric describes fluctuations of observables.  Mathematically, this means it&#8217;s the real part of the covariance matrix</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>where for us</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_1+%3D+q%2C+%5Cqquad+X_2+%3D+p%2C+%5Cqquad+X_3+%3D+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1 = q, &#92;qquad X_2 = p, &#92;qquad X_3 = E " class="latex" /></p>
<p>Here we are taking expected values using the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  We&#8217;ve seen this mixed state is just like the maximum-entropy state of a harmonic oscillator at fixed temperature &mdash; except for two caveats: we&#8217;re working in translated coordinates on position-momentum space, and subtracting a constant from the Hamiltonian.  But neither of these two caveats affects the fluctuations <img src="https://s0.wp.com/latex.php?latex=%28X_i+-+%5Clangle+X_i+%5Crangle%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X_i - &#92;langle X_i &#92;rangle)" class="latex" /> or the covariance matrix.  </p>
<p>So, as indeed we&#8217;ve already seen, <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> has rotational symmetry in the 1-2 plane.  Thus, we&#8217;ll completely know it once we know <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D+%3D+g_%7B22%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11} = g_{22}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g_%7B33%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{33}" class="latex" />; the other components are zero for symmetry reasons.  <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11}" class="latex" /> will equal the variance of position for a harmonic oscillator at a given temperature, while <img src="https://s0.wp.com/latex.php?latex=g_%7B33%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{33}" class="latex" /> will equal the variance of its energy.   We can work these out or look them up.  </p>
<p>I won&#8217;t do that now: I&#8217;m after insight, not formulas.  For physical reasons, it&#8217;s obvious that <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11}" class="latex" /> must diminish with diminishing energy &mdash; but not go to zero.   Why? Well, as the temperature approaches zero, a harmonic oscillator in thermal equilibrium approaches its state of least energy: the so-called <a href="http://en.wikipedia.org/wiki/Ground_state">&#8216;ground state&#8217;</a>.  In its ground state, the standard deviations of position and momentum are as small as allowed by the Heisenberg uncertainty principle:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+p+%5CDelta+q++%5Cge+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta p &#92;Delta q  &#92;ge &#92;frac{1}{2}" class="latex" /></p>
<p>and they&#8217;re equal, so </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D+%3D+%28%5CDelta+q%29%5E2+%3D+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11} = (&#92;Delta q)^2 = &#92;frac{1}{2}" class="latex" />.</p>
<p>That&#8217;s enough about the metric.  Now, what about the metric&#8217;s skew-symmetric partner?  This is: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cmathrm%7BIm%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;mathrm{Im} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>Last time we saw that <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> is all about expected values of commutators:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cfrac%7B1%7D%7B2i%7D+%5Clangle+%5BX_i%2C+X_j%5D+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;frac{1}{2i} &#92;langle [X_i, X_j] &#92;rangle" class="latex" /></p>
<p>and this makes it easy to compute.  For example, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5BX_1%2C+X_2%5D+%3D+q+p+-+p+q+%3D+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[X_1, X_2] = q p - p q = i" class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B12%7D+%3D+%5Cfrac%7B1%7D%7B2%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{12} = &#92;frac{1}{2} " class="latex" /></p>
<p>Of course </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B11%7D+%3D+%5Comega_%7B22%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{11} = &#92;omega_{22} = 0" class="latex" /></p>
<p>by skew-symmetry, so we know the restriction of <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> to any horizontal plane.  We can also work out other components, like <img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B13%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{13}" class="latex" />, but I don&#8217;t want to.  I&#8217;d rather just state this:</p>
<blockquote><p>
<b>Summary:</b> Restricted to any horizontal plane in the position-momentum-energy space, the Fisher information metric for the harmonic oscillator is</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%28dq_0%5E2+%2B+dp_0%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} (dq_0^2 + dp_0^2) " class="latex" /></p>
<p>with a constant depending on the temperature, equalling <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> in the zero-temperature limit, and increasing as the temperature rises.  Restricted to the same plane, the Fisher information metric&#8217;s skew-symmetric partner is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+%5Cfrac%7B1%7D%7B2%7D+dq_0+%5Cwedge+dp_0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = &#92;frac{1}{2} dq_0 &#92;wedge dp_0 " class="latex" />
</p></blockquote>
<p>(Remember, the mean values <img src="https://s0.wp.com/latex.php?latex=q_0%2C+p_0%2C+E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0, p_0, E_0" class="latex" /> are the coordinates on position-momentum-energy space.  We could also use coordinates <img src="https://s0.wp.com/latex.php?latex=f%2C+g%2C+%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f, g, &#92;beta" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=f%2C+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f, g" class="latex" /> and temperature.  In the chatty intro to this article you saw formulas like those above but without the subscripts; that&#8217;s before I got serious about using <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to mean <i>operators</i>.)</p>
<p>And now for the moral.  Actually I have two: a physics moral and a math moral.    </p>
<p>First, what is the physical meaning of <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> when restricted to a plane of constant <img src="https://s0.wp.com/latex.php?latex=E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_0" class="latex" />, or if you prefer, a plane of constant temperature?</p>
<blockquote><p>
<b>Physics Moral:</b> Restricted to a constant-temperature plane, <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> is the covariance matrix for our observables.  It is temperature-dependent.  In the zero-temperature limit, the thermal fluctuations go away and <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> depends only on quantum fluctuations in the ground state.  On the other hand, <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> restricted to a constant-temperature plane describes Heisenberg uncertainty relations for noncommuting observables.  In our example, it is temperature-independent.
</p></blockquote>
<p>Second, what does this have to do with <a href="http://en.wikipedia.org/wiki/K%C3%A4hler_manifold">K&auml;hler geometry?</a>  Remember, the complex plane has a <i>complex-valued</i> metric on it, called a K&auml;hler structure.  Its real part is a <a href="http://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian metric</a>, and its imaginary part is a <a href="http://en.wikipedia.org/wiki/Symplectic_manifold">symplectic structure</a>.  We can think of the the complex plane as the position-momentum plane for a point particle.  Then the symplectic structure is the basic ingredient needed for <a href="http://en.wikipedia.org/wiki/Hamiltonian_mechanics">Hamiltonian mechanics</a>, while the Riemannian structure is the basic ingredient needed for the harmonic oscillator Hamiltonian.  </p>
<blockquote><p>
<b>Math Moral:</b> In the example we considered, <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> restricted to a constant-temperature plane is equal to <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> the usual symplectic structure on the complex plane.  On the other hand, <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> restricted to a constant-temperature plane is a multiple of the usual Riemannian metric on the complex plane &mdash; but this multiple is <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> <i>only when the temperature is zero!</i>  So, only at temperature zero are <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> the real and imaginary parts of a K&auml;hler structure.
</p></blockquote>
<p>It will be interesting to see how much of this stuff is true more generally.  The harmonic oscillator is much nicer than your average physical system, so it can be misleading, but I think <i>some</i> of the morals we&#8217;ve seen here can be generalized.</p>
<p>Some other time I may so more about how all this is<br />
related to <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#Ottinger">&Ouml;ttinger&#8217;s formalism</a>, but the quick point is that he too has mixed states, and a symmetric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />, and a skew-symmetric <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />.  So it&#8217;s nice to see if they match up in an example.</p>
<p>Finally, two footnotes on terminology:</p>
<p><b>&beta;:</b>   In fact, this quantity <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FkT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/kT" class="latex" /> is so important it deserves a better name than &#8216;reciprocal of temperature&#8217;.   How about &#8216;coolness&#8217;?  An important lesson from statistical mechanics is that coolness is more fundamental than temperature.  This makes some facts more plausible.  For example, if you say &#8220;you can never reach absolute zero,&#8221; it sounds very odd, since you can get as close as you like, and it&#8217;s even possible to get <a href="http://en.wikipedia.org/wiki/Negative_temperature"><i>negative</i> temperatures</a> &mdash; but temperature zero remains tantalizingly out of reach.   But &#8220;you can never attain infinite coolness&#8221; &mdash; now that makes sense.</p>
<p><b>Gorce:</b>  I apologize to Richard Feynman for stealing the word <a href="http://student.fizika.org/~jsisko/Knjige/Opca%20Fizika/Feynman%20Lectures%20on%20Physics/Vol%201%20Ch%2012%20-%20Characteristics%20of%20Force.pdf">&#8216;gorce&#8217;</a> and using it a different way.  Does anyone have a good intuition for what&#8217;s going on when you apply my sort of &#8216;gorce&#8217; to a point particle?  You need to think about velocity-dependent potentials, of that I&#8217;m sure.  In the presence of a velocity-dependent potential, momentum is <i>not</i> just mass times velocity.  Which is good: if it were, we could never have a system where the mean value of both <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> stayed constant over time!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/#comments">53 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1504 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1504">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/" rel="bookmark">Information Geometry (Part&nbsp;4)</a></h2>
				<small>29 October, 2010</small><br />


				<div class="entry">
					<p>Before moving on, I&#8217;d like to clear up a mistake I&#8217;d been making in all my previous posts on this subject.</p>
<p>(By now I&#8217;ve tried to fix those posts, because people often get information from the web in a hasty way, and I don&#8217;t want my mistake to spread.  But you&#8217;ll still see traces of my mistake infecting the <i>comments</i> on those posts.)</p>
<p>So what&#8217;s the mistake?  It&#8217;s embarrassingly simple, but also simple to fix.  A <a href="http://en.wikipedia.org/wiki/Metric_tensor">Riemannian metric</a> must be symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+g_%7Bji%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = g_{ji} " class="latex" />  </p>
<p>Now, I had defined the Fisher information metric to be the so-called <a href="http://en.wikipedia.org/wiki/Covariance_matrix">&#8216;covariance matrix&#8217;</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%3B%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;langle (X_i - &#92;langle X_i &#92;rangle) &#92;;(X_j- &#92;langle X_j &#92;rangle)&#92;rangle" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> are some observable-valued functions on a manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, and the angle brackets mean &#8220;expectation value&#8221;, computed using a mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that also depends on the point in <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.</p>
<p>The covariance matrix is symmetric in classical mechanics, since then observables commute, so:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB+%5Crangle+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB &#92;rangle = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>But it&#8217;s not symmetric is quantum mechanics!  After all, suppose <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is the position operator for a particle, and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is the momentum operator.  Then according to Heisenberg</p>
<p><img src="https://s0.wp.com/latex.php?latex=qp+%3D+pq+%2B+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="qp = pq + i " class="latex" /></p>
<p>in units where Planck&#8217;s constant is 1.  Taking expectation values, we get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+qp+%5Crangle+%3D+%5Clangle+pq+%5Crangle+%2B+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle qp &#92;rangle = &#92;langle pq &#92;rangle + i" class="latex" /></p>
<p>and in particular:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+qp+%5Crangle+%5Cne+%5Clangle+pq+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle qp &#92;rangle &#92;ne &#92;langle pq &#92;rangle " class="latex" /></p>
<p>We can use this to get examples where <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> is not symmetric.    </p>
<p>However, it turns out that the <i>real part</i> of the covariance matrix is symmetric, even in quantum mechanics &mdash; and that&#8217;s what we should use as our Fisher information metric.</p>
<p>Why is the real part of the covariance matrix symmetric, even in quantum mechanics?  Well, suppose <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is any density matrix, and <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> are any observables.  Then by definition</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB+%5Crangle+%3D+%5Cmathrm%7Btr%7D+%28%5Crho+AB%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB &#92;rangle = &#92;mathrm{tr} (&#92;rho AB)" class="latex" /></p>
<p>so taking the complex conjugate of both sides</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A++%3D+%5Cmathrm%7Btr%7D%28%5Crho+AB%29%5E%2A+%3D+%5Cmathrm%7Btr%7D%28%28%5Crho+A+B%29%5E%2A%29+%3D+%5Cmathrm%7Btr%7D%28B%5E%2A+A%5E%2A+%5Crho%5E%2A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^*  = &#92;mathrm{tr}(&#92;rho AB)^* = &#92;mathrm{tr}((&#92;rho A B)^*) = &#92;mathrm{tr}(B^* A^* &#92;rho^*)" class="latex" /></p>
<p>where I&#8217;m using an asterisk both for the complex conjugate of a number and the adjoint of an operator.  But our observables are self-adjoint, and so is our density matrix, so we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28B%5E%2A+A%5E%2A+%5Crho%5E%2A%29+%3D+%5Cmathrm%7Btr%7D%28B+A+%5Crho%29+%3D+%5Cmathrm%7Btr%7D%28%5Crho+B+A%29+%3D+%5Clangle+B+A+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(B^* A^* &#92;rho^*) = &#92;mathrm{tr}(B A &#92;rho) = &#92;mathrm{tr}(&#92;rho B A) = &#92;langle B A &#92;rangle " class="latex" /></p>
<p>where in the second step we used the cyclic property of the trace.  In short:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^* = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>If we take real parts, we get something symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB%5Crangle+%3D++%5Cmathrm%7BRe%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB&#92;rangle =  &#92;mathrm{Re} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>So, if we redefine the Fisher information metric to be the <i>real part</i> of the covariance matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%3B+%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle) &#92;; (X_j- &#92;langle X_j &#92;rangle)&#92;rangle " class="latex" /></p>
<p>then it&#8217;s symmetric, as it should be. </p>
<p>Last time I mentioned a general setup using von Neumann algebras, that handles the classical and quantum situations simultaneously.  That applies here!   Taking the real part has no effect in classical mechanics, so we don&#8217;t need it there &mdash; but it doesn&#8217;t hurt, either.</p>
<p>Taking the real part never has any effect when <img src="https://s0.wp.com/latex.php?latex=i+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = j" class="latex" />, either, since the expected value of the <i>square</i> of an observable is a nonnegative number:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle &#92;ge 0" class="latex" /></p>
<p>This has two nice consequences.  </p>
<p>First, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle++%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} = &#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle  &#92;ge 0 " class="latex" /></p>
<p>and since this is true in <i>any</i> coordinate system, our would-be metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> is indeed nonnegative.  It&#8217;ll be an honest Riemannian metric whenever it&#8217;s positive definite.   </p>
<p>Second, suppose we&#8217;re working in the special case discussed in <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/">Part 2</a>, where our manifold is an open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7B%5Crho%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{&#92;rho}" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;mathbb{R}^n" class="latex" /> is the Gibbs state with <img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i" class="latex" />.  Then all the usual rules of statistical mechanics apply.  So, we can compute the variance of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> using the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i%5E2%7D+%5Cln+Z++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i^2} &#92;ln Z  " class="latex" /></p>
<p>In other words, </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i%5E2%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i^2} &#92;ln Z " class="latex" /></p>
<p>But since this is true in <i>any</i> coordinate system, we must have</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;ln Z " class="latex" /></p>
<p>(Here I&#8217;m using a little math trick: two symmetric bilinear forms whose diagonal entries agree in <i>any</i> basis must be equal.  We&#8217;ve already seen that the left side is symmetric, and the right side is symmetric by a famous fact about mixed partial derivatives.)</p>
<p>However, I&#8217;m pretty sure this cute formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;ln Z " class="latex" /></p>
<p>only holds in the special case I&#8217;m talking about now, where points in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> are parametrizing Gibbs states in the obvious way.   In general we must use</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)(X_j- &#92;langle X_j &#92;rangle)&#92;rangle " class="latex" /></p>
<p>or equivalently, </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>Okay.  So much for cleaning up Last Week&#8217;s Mess.   Here&#8217;s something new.  We&#8217;ve seen that whenever <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> are observables (that is, self-adjoint),</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^* = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>We got something symmetric by taking the real part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB%5Crangle+%3D++%5Cmathrm%7BRe%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB&#92;rangle =  &#92;mathrm{Re} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>Indeed,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB+%5Crangle+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Clangle+AB+%2B+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB &#92;rangle = &#92;frac{1}{2} &#92;langle AB + BA &#92;rangle " class="latex" /></p>
<p>But by the same reasoning, we get something <i>antisymmetric</i> by taking the <i>imaginary</i> part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BIm%7D+%5Clangle+AB%5Crangle+%3D++-+%5Cmathrm%7BIm%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Im} &#92;langle AB&#92;rangle =  - &#92;mathrm{Im} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>and indeed,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BIm%7D+%5Clangle+AB+%5Crangle+%3D+%5Cfrac%7B1%7D%7B2i%7D+%5Clangle+AB+-+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Im} &#92;langle AB &#92;rangle = &#92;frac{1}{2i} &#92;langle AB - BA &#92;rangle " class="latex" /></p>
<p><a href="http://en.wikipedia.org/wiki/Commutator">Commutators</a> like <img src="https://s0.wp.com/latex.php?latex=AB-BA&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="AB-BA" class="latex" /> are important in quantum mechanics, so maybe we shouldn&#8217;t just throw out the imaginary part of the covariance matrix in our desperate search for a Riemannian metric!  Besides the symmetric tensor on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>we can also define a skew-symmetric tensor:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cmathrm%7BIm%7D+%5C%2C++%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;mathrm{Im} &#92;,  &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>This will vanish in the classical case, but not in the quantum case!</p>
<p>If you&#8217;ve studied enough geometry, you should now be reminded of things like &#8216;K&auml;hler manifolds&#8217; and &#8216;almost  K&auml;hler manifolds&#8217;.  A <a href="http://en.wikipedia.org/wiki/K%C3%A4hler_manifold">K&auml;hler manifold</a> is a manifold that&#8217;s equipped with a symmetric tensor <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and a skew-symmetric tensor <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> which fit together in the best possible way.  An <a href="http://en.wikipedia.org/wiki/Almost_K%C3%A4hler_manifold#K.C3.A4hler_manifolds">almost K&auml;hler manifold</a> is something similar, but not quite as nice.   We should probably see examples of these arising in information geometry!  And that could be pretty interesting.</p>
<p>But in general, if we start with any old manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> together with a function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> taking values in mixed states, we seem to be making <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> into something even less nice.  It gets a symmetric bilinear form <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on each tangent space, and a skew-symmetric bilinear form <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />, and they vary smoothly from point to point&#8230; but they might be degenerate, and I don&#8217;t see any reason for them to &#8216;fit together&#8217; in the nice way we need for a K&auml;hler or almost K&auml;hler manifold.</p>
<p>However, I still think something interesting might be going on here.  For one thing, there are <i>other</i> situations in physics where a space of states is equipped with a symmetric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and a skew-symmetric <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />. They show up in &#8216;dissipative mechanics&#8217; &mdash; the study of systems whose entropy increases.</p>
<p><a name="Ottinger">To conclude,</a> let me remind you of some things I said in <a href="http://math.ucr.edu/home/baez/week295.html">week295</a> of This Week&#8217;s Finds.  This is a huge digression from information geometry, but I&#8217;d like to lay out the the puzzle pieces in public view, in case it helps anyone get some good ideas.</p>
<p>I wrote:</p>
<blockquote><p>
&bull;  Hans Christian &Ouml;ttinger, <i>Beyond Equilibrium Thermodynamics</i>, Wiley, 2005.</p>
<p>I thank Arnold Neumaier for pointing out this book!  It considers a fascinating generalization of Hamiltonian mechanics that applies to  systems with dissipation: for example, electrical circuits with resistors, or mechanical systems with friction. </p>
<p>In ordinary Hamiltonian mechanics the space of states is a manifold and time evolution is a flow on this manifold determined by a smooth function called the Hamiltonian, which describes the <i>energy</i> of any state.  In this generalization the space of states is still a manifold, but now time evolution is determined by two smooth functions: the energy and the <i>entropy!</i> In ordinary Hamiltonian mechanics, energy is automatically conserved.  In this generalization that&#8217;s also true, but energy can go into the form of heat&#8230; and entropy automatically <i>increases!</i></p>
<p>Mathematically, the idea goes like this.  We start with a Poisson manifold, but in addition to the skew-symmetric Poisson bracket {F,G} of smooth functions on some manifold, we also have a symmetric bilinear bracket [F,G] obeying the Leibniz law</p>
<p>[F,GH] = [F,G]H + G[F,H]</p>
<p>and this positivity condition:</p>
<p>[F,F] &ge; 0</p>
<p>The time evolution of any function is given by a generalization of Hamilton&#8217;s equations:</p>
<p>dF/dt = {H,F} + [S,F]</p>
<p>where H is a function called the &quot;energy&quot; or &quot;Hamiltonian&quot;, and S is a function called the &quot;entropy&quot;.   The first term on the right is the usual one. The new second term describes dissipation: as we shall see, it pushes the state towards increasing entropy.</p>
<p>If we require that</p>
<p>[H,F] = {S,F} = 0</p>
<p>for every function F, then we get conservation of energy, as usual in Hamiltonian mechanics:</p>
<p>dH/dt = {H,H} + [S,H] = 0</p>
<p>But we also get the second law of thermodynamics:</p>
<p>dS/dt = {H,S} + [S,S] &ge; 0</p>
<p>Entropy always increases!</p>
<p>&Ouml;ttinger calls this framework &#8220;GENERIC&#8221; &#8211; an annoying acronym for &#8220;General Equation for the NonEquilibrium Reversible-Irreversible Coupling&#8221;.  There are lots of papers about it.  But I&#8217;m wondering if any geometers have looked into it!  </p>
<p>If we didn&#8217;t need the equations [H,F] = {S,F} = 0, we could easily get the necessary brackets starting with a K&auml;hler manifold.  The  imaginary part of the K&auml;hler structure is a symplectic structure, say &omega;, so we can define</p>
<p>{F,G} = &omega;(dF,dG)</p>
<p>as usual to get Poisson brackets.  The real part of the K&auml;hler structure is a Riemannian structure, say g, so we can define</p>
<p>[F,G] = g(dF,dG)</p>
<p>This satisfies</p>
<p>[F,GH] = [F,G]H + G[F,H]</p>
<p>and </p>
<p>[F,F] &ge; 0</p>
<p>Don&#8217;t be fooled: this stuff is not rocket science.  In particular, the inequality above has a simple meaning: when we move in the direction of the gradient of F, the function F increases.  So adding the second term to Hamilton&#8217;s equations has the effect of pushing the system towards increasing entropy.</p>
<p>Note that I&#8217;m being a tad unorthodox by letting &omega; and g eat cotangent vectors instead of tangent vectors &#8211; but that&#8217;s no big deal. The big deal is this: if we start with a K&auml;hler manifold and define brackets this way, we don&#8217;t get [H,F] = 0 or {S,F} = 0 for all functions F unless H and S are constant!  That&#8217;s no good for applications to physics.  To get around this problem, we would need to consider some sort of <i>degenerate</i> K&auml;hler structure &#8211; one where &omega; and g are degenerate bilinear forms on the cotangent space.</p>
<p>Has anyone thought about such things?  They remind me a little of &quot;Dirac structures&quot; and &quot;generalized complex geometry&quot; &#8211; but I don&#8217;t know enough about those subjects to know if they&#8217;re relevant here.</p>
<p>This GENERIC framework suggests that energy and entropy should be viewed as two parts of a single entity &#8211; maybe even its real and imaginary parts!  And that in turn reminds me of other strange  things, like the idea of using complex-valued Hamiltonians to describe dissipative systems, or the idea of &#8220;inverse temperature  as imaginary time&#8221;.  I can&#8217;t tell yet if there&#8217;s a big idea lurking here, or just a mess&#8230;.</p>
</blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#comments">36 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1408 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1408">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/" rel="bookmark">Information Geometry (Part&nbsp;3)</a></h2>
				<small>25 October, 2010</small><br />


				<div class="entry">
					<p>So far in this series of posts I&#8217;ve been explaining a paper by Gavin Crooks. Now I want to go ahead and explain a little research of my own. </p>
<p>I&#8217;m not claiming my results are new &mdash; indeed I have no idea whether they are, and I&#8217;d like to hear from any experts who might know.  I&#8217;m just claiming that this is some work I did last weekend.</p>
<p>People sometimes worry that if they explain their ideas before publishing them, someone will &#8216;steal&#8217; them.  But I think this overestimates the value of ideas, at least in esoteric fields like mathematical physics.  The problem is not people stealing your ideas: the hard part is <i>giving them away</i>.  And let&#8217;s face it, people in love with math and physics will do research unless you actively stop them.  I&#8217;m reminded of this scene from the <a href="http://www.marx-brothers.org/whyaduck/info/movies/scenes/ravelli.htm">Marx Brothers movie</a> where Harpo and Chico, playing wandering musicians, walk into a hotel and offer to play:</p>
<blockquote><p>
Groucho: What do you fellows get an hour?</p>
<p>Chico: Oh, for playing we getta ten dollars an hour.</p>
<p>Groucho: I see&#8230;What do you get for not playing?</p>
<p>Chico: Twelve dollars an hour.</p>
<p>Groucho: Well, clip me off a piece of that.</p>
<p>Chico: Now, for rehearsing we make special rate. Thatsa fifteen dollars an hour.</p>
<p>Groucho: That&#8217;s for rehearsing?</p>
<p>Chico: Thatsa for rehearsing.</p>
<p>Groucho: And what do you get for not rehearsing?</p>
<p>Chico: You couldn&#8217;t afford it.
</p></blockquote>
<p>So, I&#8217;m just rehearsing in public here &mdash; but I of course I hope to write a paper about this stuff someday, once I get enough material.</p>
<p>Remember where we were.  We had considered a manifold &mdash; let&#8217;s finally give it a name, say <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> &mdash; that parametrizes Gibbs states of some physical system.  By <b><a href="http://en.wikipedia.org/wiki/Canonical_ensemble">Gibbs state</a></b>, I mean a state that maximizes entropy subject to constraints on the expected values of some observables.  And we had seen that in favorable cases, we get a Riemannian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />!  It looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> are our observables, and the angle bracket means &#8216;expected value&#8217;. </p>
<p>All this applies to both classical or quantum mechanics.  Crooks wrote down a beautiful formula for this metric in the classical case.  But since I&#8217;m at the Centre for <i>Quantum</i> Technologies, not the Centre for Classical Technologies, I redid his calculation in the quantum case.  The big difference is that in quantum mechanics, observables don&#8217;t commute!  But in the calculations I did, that didn&#8217;t seem to matter much &mdash; mainly because I took a lot of traces, which imposes a kind of commutativity:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28AB%29+%3D+%5Cmathrm%7Btr%7D%28BA%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(AB) = &#92;mathrm{tr}(BA) " class="latex" /></p>
<p>In fact, if I&#8217;d wanted to show off, I could have done the classical and quantum cases simultaneously by replacing all operators by elements of any <a href="http://en.wikipedia.org/wiki/Von_Neumann_algebra">von Neumann algebra</a> equipped with a <a href="http://en.wikipedia.org/wiki/Von_Neumann_algebra#Weights.2C_states.2C_and_traces">trace</a>.  Don&#8217;t worry about this much: it&#8217;s just a general formalism for treating classical and quantum mechanics on an equal footing.  One example is the algebra of bounded operators on a Hilbert space, with the usual concept of trace.  Then we&#8217;re doing quantum mechanics as usual.  But another example is the algebra of suitably nice functions on a suitably nice space, where taking the trace of a function means <i>integrating</i> it.  And then we&#8217;re doing classical mechanics!   </p>
<p>For example, I showed you how to derive a beautiful formula for the metric I wrote down a minute ago: </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} ) " class="latex" /></p>
<p>But if we want to do the classical version, we can say <i>Hey, presto!</i> and write it down like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cint_%5COmega+p%28%5Comega%29+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;int_&#92;Omega p(&#92;omega) &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^j} &#92;; d &#92;omega " class="latex" /></p>
<p>What did I do just now?  I changed the trace to an integral over some space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  I rewrote <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to make you think &#8216;probability distribution&#8217;.  And I don&#8217;t need to take the real part anymore, since is everything already real when we&#8217;re doing classical mechanics.    Now this metric is the <b><a href="http://en.wikipedia.org/wiki/Fisher_information_metric">Fisher information metric</a></b> that statisticians know and love!</p>
<p>In what follows, I&#8217;ll keep talking about the quantum case, but in the back of my mind I&#8217;ll be using von Neumann algebras, so everything will apply to the classical case too.</p>
<p>So what am I going to do?  I&#8217;m going to fix a big problem with the story I&#8217;ve told so far.</p>
<p>Here&#8217;s the problem: so far we&#8217;ve only studied a special case of the Fisher information metric.  We&#8217;ve been assuming our states are Gibbs states, parametrized by the expectation values of some observables <img src="https://s0.wp.com/latex.php?latex=X_1%2C+%5Cdots%2C+X_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1, &#92;dots, X_n" class="latex" />.   Our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> was really just some open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />: a point in here was a list of expectation values.</p>
<p>But people like to work a lot more generally.  We could look at <i>any</i> smooth function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> from a smooth manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the set of density matrices for some quantum system.   We can still write down the metric</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} )  " class="latex" /></p>
<p>in this more general situation.  Nobody can stop us!  But it would be better if we could <i>derive</i> this formula, as before, starting from a formula like the one we had before:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>The challenge is that now we don&#8217;t have observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> to start with.  All we have is a smooth function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> from some manifold to some set of states.   How can we pull observables out of thin air?</p>
<p>Well, you may remember that last time we had</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Clambda%5Ei+X_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;lambda^i X_i}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> were some functions on our manifold and </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D%28e%5E%7B-%5Clambda%5Ei+X_i%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr}(e^{-&#92;lambda^i X_i})" class="latex" /></p>
<p>was the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a>.  Let&#8217;s copy this idea.  </p>
<p>So, we&#8217;ll start with our density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, but then write it as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-A}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is some self-adjoint operator and </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-A%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-A})" class="latex" /></p>
<p>(Note that <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, like <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, is really an operator-valued function on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  So, I should write something like <img src="https://s0.wp.com/latex.php?latex=A%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A(x)" class="latex" /> to denote its value at a particular point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />, but I won&#8217;t usually do that.  As usual, I expect some intelligence on your part!)</p>
<p>Now we can repeat some calculations I did last time.  As before, let&#8217;s take the logarithm of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D+%5C%2C+%5Crho+%3D+-A+-+%5Cmathrm%7Bln%7D%5C%2C++Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln} &#92;, &#92;rho = -A - &#92;mathrm{ln}&#92;,  Z" class="latex" /></p>
<p>and then differentiate it.  Suppose <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> are local coordinates near some point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D%5C%2C+%5Crho+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A+-+%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln}&#92;, &#92;rho = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A - &#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z" class="latex" /></p>
<p>Last time we had nice formulas for both terms on the right-hand side above.  To get similar formulas now, let&#8217;s define operators</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i = &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A" class="latex" /></p>
<p>This gives a nice name to the first term on the right-hand side above.  What about the second term?  We can calculate it out:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z+%3D+%5Cfrac%7B1%7D%7BZ%7D++%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Btr%7D%28e%5E%7B-A%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%7D++%5Cmathrm%7Btr%7D%28%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+e%5E%7B-A%7D%29+%3D+-+%5Cfrac%7B1%7D%7BZ%7D++%5Cmathrm%7Btr%7D%28e%5E%7B-A%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z = &#92;frac{1}{Z}  &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} &#92;mathrm{tr}(e^{-A}) = &#92;frac{1}{Z}  &#92;mathrm{tr}(&#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} e^{-A}) = - &#92;frac{1}{Z}  &#92;mathrm{tr}(e^{-A} &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A)" class="latex" /></p>
<p>where in the last step we use the chain rule.   Next, use the definition of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, and get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z+%3D+-+%5Cmathrm%7Btr%7D%28%5Crho+X_i%29+%3D+-+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z = - &#92;mathrm{tr}(&#92;rho X_i) = - &#92;langle X_i &#92;rangle" class="latex" /></p>
<p>This is just what we got last time!  Ain&#8217;t it fun to calculate when it all works out so nicely?</p>
<p>So, putting both terms together, we see </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho+%3D+-+X_i+%2B+%5Clangle+X_i+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho = - X_i + &#92;langle X_i &#92;rangle " class="latex" /></p>
<p>or better:</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho" class="latex" /></p>
<p>This is a nice formula for the &#8216;fluctuation&#8217; of the observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, meaning how much they differ from their expected values.  And it looks exactly like the formula we had last time!  The difference is that last time we <i>started out</i> assuming we had a bunch of observables, <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, and defined <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> to be the state maximizing the entropy subject to constraints on the expectation values of all these observables.<br />
Now we&#8217;re starting with <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and working backwards.</p>
<p>From here on out, it&#8217;s easy.   As before, we can define <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> to be the real part of the covariance matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>Using the formula </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho" class="latex" /></p>
<p>we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^j} &#92;rangle " class="latex" /></p>
<p>or </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D%5C%2C%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ej%7D%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re}&#92;,&#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^j}) " class="latex" /></p>
<p><i>Voil&agrave;!</i>  </p>
<p>When this matrix is positive definite at every point, we get a Riemanian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.   Last time I said this is what people call the <a href="http://en.wikipedia.org/wiki/Bures_metric">&#8216;Bures metric&#8217;</a> &mdash; though frankly, now that I examine the formulas, I&#8217;m not so sure.  But in the classical case, it&#8217;s called the Fisher information metric.   </p>
<p>Differential geometers like to use <img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i" class="latex" /> as a shorthand for <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial_i}" class="latex" />, so they&#8217;d write down our metric in a prettier way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cpartial_i+%28%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+%5C%3B+%5Cpartial_j+%28%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;partial_i (&#92;mathrm{ln} &#92;, &#92;rho) &#92;; &#92;partial_j (&#92;mathrm{ln} &#92;, &#92;rho) )" class="latex" /></p>
<p>Differential geometers like coordinate-free formulas, so let&#8217;s also give a coordinate-free formula for our metric.  Suppose <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" /> is a point in our manifold, and suppose <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> are tangent vectors to this point.  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28v%2Cw%29+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Clangle+v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29+%5C%3B+w%28%5Cmathrm%7Bln%7D+%5C%2C%5Crho%29+%5Crangle+%5C%3B+%3D+%5C%3B+%5Cmathrm%7BRe%7D+%5C%2C%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29+%5C%3B+w%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(v,w) = &#92;mathrm{Re} &#92;, &#92;langle v(&#92;mathrm{ln}&#92;, &#92;rho) &#92;; w(&#92;mathrm{ln} &#92;,&#92;rho) &#92;rangle &#92;; = &#92;; &#92;mathrm{Re} &#92;,&#92;mathrm{tr}(&#92;rho &#92;; v(&#92;mathrm{ln}&#92;, &#92;rho) &#92;; w(&#92;mathrm{ln}&#92;, &#92;rho))  " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, &#92;rho" class="latex" /> is a smooth operator-valued function on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v(&#92;mathrm{ln}&#92;, &#92;rho)" class="latex" /> means the derivative of this function in the <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> direction at the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.</p>
<p>So, this is all very nice.  To conclude, two more points: a technical one, and a more important philosophical one.</p>
<p>First, the technical point.  When I said <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> could be <i>any</i> smooth function from a smooth manifold to some set of states, I was actually lying.  That&#8217;s an important pedagogical technique: the brazen lie.</p>
<p>We can&#8217;t really take the logarithm of <i>every</i> density matrix.  Remember, we take the log of a density matrix by taking the log of all its eigenvalues.  These eigenvalues are &ge; 0, but if one of them is zero, we&#8217;re in trouble!  The logarithm of zero is undefined.</p>
<p>On the other hand, there&#8217;s no problem taking the logarithm of our density-matrix-valued function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> when it&#8217;s <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite</a> at each point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  You see, a density matrix is positive definite iff its eigenvalues are all &gt; 0.   In this case it has a unique self-adjoint logarithm.</p>
<p>So, we must assume <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is positive definite.   But what&#8217;s the physical significance of this &#8216;positive definiteness&#8217; condition?  Well, any density matrix can be diagonalized using some orthonormal basis.  It can then be seen as probabilistic mixture &mdash; not a quantum superposition! &mdash; of pure states taken from this basis. Its eigenvalues are the probabilities of finding the mixed state to be in one of these pure states.  So, saying that all its eigenvalues are all &gt; 0 amounts to saying that all the pure states in this orthonormal basis show up with <i>nonzero</i> probability! Intuitively, this means our mixed state is &#8216;really mixed&#8217;.  For example, it can&#8217;t be a pure state.  In math jargon, it means our mixed state is in the <i>interior</i> of the convex set of mixed states.</p>
<p>Second, the philosophical point.  Instead of starting with the density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, I took <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> as fundamental.   But different choices of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> give the same <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  After all,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-A}" class="latex" /></p>
<p>where we cleverly divide by the normalization factor</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-A%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-A})" class="latex" /></p>
<p>to get <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D+%5C%2C+%5Crho+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr} &#92;, &#92;rho = 1" class="latex" />.  So, if we multiply <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-A}" class="latex" /> by any positive constant, or indeed any positive <i>function</i> on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> will remain unchanged!  </p>
<p>So we have added a little extra information when switching from <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />.   You can think of this as &#8216;gauge freedom&#8217;, because I&#8217;m saying we can do any transformation like</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%5Cmapsto+A+%2B+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;mapsto A + f " class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=f%3A+M+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: M &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is a smooth function.   This doesn&#8217;t change <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, so arguably it doesn&#8217;t change the &#8216;physics&#8217; of what I&#8217;m doing.  It <i>does</i> change <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  It also changes the observables </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i = &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A" class="latex" /></p>
<p>But it doesn&#8217;t change their &#8216;fluctuations&#8217; </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle" class="latex" /></p>
<p>so it doesn&#8217;t change the metric <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.</p>
<p>This gauge freedom is interesting, and I want to understand it better. It&#8217;s related to something very simple yet mysterious.  In statistical mechanics the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> begins life as &#8216;just a normalizing factor&#8217;.   If you change the physics so that <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> gets multiplied by some number, the Gibbs state doesn&#8217;t change.  But then the partition function takes on an incredibly significant role as something whose logarithm you differentiate to get lots of physically interesting information!   So in some sense the partition function doesn&#8217;t matter much&#8230; but <i>changes</i> in the partition function matter a lot.</p>
<p>This is just like the split personality of phases in quantum mechanics.  On the one hand they &#8216;don&#8217;t matter&#8217;: you can multiply a unit vector by any phase and the pure state it defines doesn&#8217;t change.  But on the other hand, <i>changes</i> in phase matter a lot.  </p>
<p>Indeed the analogy here is quite deep: it&#8217;s the analogy between probabilities in statistical mechanics and amplitudes in quantum mechanics, the analogy between <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bexp%7D%28-%5Cbeta+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{exp}(-&#92;beta H)" class="latex" /> in statistical mechanics and <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bexp%7D%28-i+t+H+%2F+%5Chbar%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{exp}(-i t H / &#92;hbar)" class="latex" /> in quantum mechanics, and so on.  This is part of a bigger story about &#8216;rigs&#8217; which I told back in the <a href="http://math.ucr.edu/home/baez/qg-winter2007/qg-winter2007.html#quantization">Winter 2007 quantum gravity seminar</a>, especially in <a href="http://math.ucr.edu/home/baez/qg-winter2007/w07week05a.pdf">week13</a>.  So, it&#8217;s fun to see it showing up yet again&#8230; even though I don&#8217;t completely understand it here.</p>
<p>[Note: in the original version of this post, I omitted the real part in my definition <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" />, giving a &#8216;Riemannian metric&#8217; that was <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">neither real nor symmetric</a> in the quantum case.  Most of the comments below are based on that original version, not the new fixed one.]</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/#comments">58 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1372 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1372">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/" rel="bookmark">Information Geometry (Part&nbsp;2)</a></h2>
				<small>23 October, 2010</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">Last time</a> I provided some background to this paper:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>.</p>
<p>Now I&#8217;ll tell you a bit about what it actually says!</p>
<p>Remember the story so far: we&#8217;ve got a physical system that&#8217;s in a state of maximum entropy.  I didn&#8217;t emphasize this yet, but that happens whenever our system is in <a href="http://en.wikipedia.org/wiki/Thermodynamic_equilibrium">thermodynamic equilibrium</a>.  An example would be a box of gas inside a piston.  Suppose you choose any number for the energy of the gas and any number for its volume.   Then there&#8217;s a unique state of the gas that maximizes its entropy, given the constraint that <i>on average</i>, its energy and volume have the values you&#8217;ve chosen.  And this describes what the gas will be like in equilibrium!</p>
<p>Remember, by &#8216;state&#8217; I mean <i>mixed</i> state: it&#8217;s a probabilistic description.  And I say the energy and volume have chosen values <i>on average</i> because there will be random fluctuations.  Indeed, if you look carefully at the head of the piston, you&#8217;ll see it quivering: the volume of the gas only equals the volume you&#8217;ve specified <i>on average</i>.  Same for the energy.</p>
<p>More generally: imagine picking any list of numbers, and finding the maximum entropy state where some chosen observables have these numbers as their average values.  Then there will be fluctuations in the values of these observables &mdash; thermal fluctuations, but also possibly quantum fluctuations.  So, you&#8217;ll get a probability distribution on the space of possible values of your chosen observables.  You should visualize this probability distribution as a little fuzzy cloud centered at the average value!  </p>
<div align="center">
<img width="400" src="https://i0.wp.com/www.math.dartmouth.edu/archive/m22x06/public_html/ellipsoid.jpg" alt="" />
</div>
<p>To a first approximation, this cloud will be shaped like a little ellipsoid.  And if you can pick the average value of your observables to be whatever you&#8217;ll like, you&#8217;ll get lots of little ellipsoids this way, one centered at each point.  And the cool idea is to imagine the space of possible values of your observables as having a weirdly warped geometry, such that <i>relative to this geometry, these ellipsoids are actually spheres</i>.  </p>
<p>This weirdly warped geometry is an example of an <a href="http://en.wikipedia.org/wiki/Information_geometry">&#8216;information geometry&#8217;</a>: a geometry that&#8217;s defined using the concept of information.  This shouldn&#8217;t be surprising: after all, we&#8217;re talking about maximum entropy, and <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">entropy is related to information</a>.  But I want to gradually make this idea more precise.</p>
<p>Bring on the math!</p>
<p>We&#8217;ve got a bunch of observables <img src="https://s0.wp.com/latex.php?latex=X_1%2C+%5Cdots+%2C+X_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1, &#92;dots , X_n" class="latex" />, and we&#8217;re assuming that for any list of numbers <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cdots+%2C+x_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1, &#92;dots , x_n" class="latex" />, the system has a unique maximal-entropy state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> for which the expected value of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>This state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is called the <b>Gibbs state</b> and I told you how to find it when it exists.   In fact it may not exist for <i>every</i> list of numbers <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cdots+%2C+x_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1, &#92;dots , x_n" class="latex" />, but we&#8217;ll be perfectly happy if it does for all choices of </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%28x_1%2C+%5Cdots%2C+x_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = (x_1, &#92;dots, x_n) " class="latex" /></p>
<p>lying in some open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />  </p>
<p>By the way, I should really call this Gibbs state <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho(x)" class="latex" /> or something to indicate how it depends on <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, but I won&#8217;t usually do that.  I expect some intelligence on your part!</p>
<p>Now at each point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> we can define a <a href="http://en.wikipedia.org/wiki/Covariance">covariance matrix</a></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>If we take its real part, we get a symmetric matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>It&#8217;s also nonnegative &mdash; that&#8217;s easy to see, since the variance of a probability distribution can&#8217;t be negative.  When we&#8217;re lucky this matrix will be <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite</a>.  When we&#8217;re even luckier, it will depend smoothly on <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  In this case, <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> will define a <a href="http://en.wikipedia.org/wiki/Riemannian_metric#Riemannian_metrics">Riemannian metric</a> on our open set.   </p>
<p>So far this is all review of <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">last time</a>.  Sorry: I seem to have reached the age where I can&#8217;t say anything interesting without warming up for about 15 minutes first.  It&#8217;s like when my mom tells me about an exciting event that happened to her: she starts by saying &#8220;Well, I woke up, and it was cloudy out&#8230;&#8221;  </p>
<p>But now I want to give you an explicit formula for the metric <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />, and then <i>rewrite it</i> in a way that&#8217;ll work even when the state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is <i>not</i> a maximal-entropy state.  And this formula will then be the general definition of the <a href="http://en.wikipedia.org/wiki/Fisher_information_metric">&#8216;Fisher information metric&#8217;</a> (if we&#8217;re doing classical mechanics), or a quantum version thereof (if we&#8217;re doing quantum mechanics).</p>
<p>Crooks does the classical case &mdash; so let&#8217;s do the quantum case, okay?  Last time I claimed that in the quantum case, our maximum-entropy state is the <b>Gibbs state</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Clambda%5Ei+X_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;lambda^i X_i}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> are the &#8216;conjugate variables&#8217; of the observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, we&#8217;re using the <a href="http://en.wikipedia.org/wiki/Einstein_notation">Einstein summation convention</a> to sum over repeated indices that show up once upstairs and once downstairs, and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is the <b>partition function</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Clambda%5Ei+X_i%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;lambda^i X_i})" class="latex" /></p>
<p>(To be honest: last time I wrote the indices on the conjugate variables <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> as subscripts rather than superscripts, because I didn&#8217;t want some poor schlep out there to think that <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1%2C+%5Cdots+%2C+%5Clambda%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1, &#92;dots , &#92;lambda^n" class="latex" /> were the powers of some number <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" />.  But now I&#8217;m assuming you&#8217;re all grown up and ready to juggle indices!  We&#8217;re doing Riemannian geometry, after all.)</p>
<p>Also last time I claimed that it&#8217;s tremendously fun and enlightening to take the derivative of the logarithm of <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  The reason is that it gives you the mean values of your observables:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;ln Z " class="latex" /></p>
<p>But now let&#8217;s take the derivative of the logarithm of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  Remember, <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is an operator &mdash; in fact a <a href="http://en.wikipedia.org/wiki/Density_matrix">density matrix</a>.  But we can take its logarithm as explained last time, and the usual rules apply, so starting from</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Clambda%5Ei+X_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;lambda^i X_i}" class="latex" /></p>
<p>we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+%5Crho+%3D+-+%5Clambda%5Ei+X_i+-+%5Cmathrm%7Bln%7D+%5C%2CZ+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, &#92;rho = - &#92;lambda^i X_i - &#92;mathrm{ln} &#92;,Z " class="latex" /></p>
<p>Next, let&#8217;s differentiate both sides with respect to <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" />.  Why?  Well, from what I just said, you should be itching to differentiate <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, Z" class="latex" />.   So let&#8217;s give in to that temptation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial++%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D++%5Crho+%3D+-X_i+%2B+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial  }{&#92;partial &#92;lambda^i} &#92;mathrm{ln}  &#92;rho = -X_i + &#92;langle X_i &#92;rangle" class="latex" /> </p>
<p>Hey!  Now we&#8217;ve got a formula for the &#8216;fluctuation&#8217; of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> &mdash; that is, how much it differs from its mean value:</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = - &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i}" class="latex" /></p>
<p>This is incredibly cool!  I should have learned this formula decades ago, but somehow I just bumped into it now.  I knew of course that <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D+%5C%2C+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln} &#92;, &#92;rho" class="latex" /> shows up in the formula for the <b>entropy</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Crho%29+%3D+%5Cmathrm%7Btr%7D+%28+%5Crho+%5C%3B+%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(&#92;rho) = &#92;mathrm{tr} ( &#92;rho &#92;; &#92;mathrm{ln} &#92;, &#92;rho) " class="latex" /></p>
<p>But I never had the brains to think about <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, &#92;rho" class="latex" /> all by itself.  So I&#8217;m really excited to discover that it&#8217;s an interesting entity in its own right &mdash; and fun to differentiate, just like <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, Z" class="latex" />.  </p>
<p>Now we get our cool formula for <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.  Remember, it&#8217;s defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>But now that we know </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i}" class="latex" /></p>
<p>we get the formula we were looking for:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j}  &#92;rangle " class="latex" /></p>
<p>Beautiful, eh?  And of course the expected value of any observable <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Cmathrm%7Btr%7D%28%5Crho+A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;mathrm{tr}(&#92;rho A)" class="latex" /></p>
<p>so we can also write the covariance matrix like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re}&#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} ) " class="latex" /></p>
<p>Lo and behold!  This formula makes sense whenever <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is <i>any</i> density matrix depending smoothly on some parameters <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" />.  We don&#8217;t need it to be a Gibbs state!   So, we can work more generally.</p>
<p>Indeed, whenever we have <i>any</i> smooth function from a manifold to the space of density matrices for some Hilbert space, we can define <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> by the above formula!  And when it&#8217;s positive definite, we get a Riemannian metric on our manifold: the <b>Bures information metric</b>.</p>
<p>The classical analogue is the somewhat more well-known &#8216;Fisher information metric&#8217;.  When we go from quantum to classical, operators become functions and traces become integrals.  There&#8217;s nothing complex anymore, so taking the real part becomes unnecessary.  So the Fisher information metric looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cint_%5COmega+p%28%5Comega%29+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;int_&#92;Omega p(&#92;omega) &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^j} &#92;; d &#92;omega " class="latex" /></p>
<p>Here I&#8217;m assuming we&#8217;ve got a smooth function <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> from some manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the space of probability distributions on some measure space <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega, d&#92;omega)" class="latex" />.    Working in local coordinates <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, the above formula defines a Riemannian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, at least when <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> is positive definite.  And that&#8217;s the <b>Fisher information metric</b>!</p>
<p>Crooks says more: he describes an experiment that would let you measure the length of a path with respect to the Fisher information metric &mdash; at least in the case where the state <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho(x)" class="latex" /> is the Gibbs state with <img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i" class="latex" />.   And that explains why he calls it &#8216;thermodynamic length&#8217;.</p>
<p>There&#8217;s a lot more to say about this, and also about another question: <i>What use is the Fisher information metric in the general case where the states <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho(x)" class="latex" /> aren&#8217;t Gibbs states?</i></p>
<p>But it&#8217;s dinnertime, so I&#8217;ll stop here.</p>
<p>[Note: in the original version of this post, I omitted the real part in my definition <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" />, giving a &#8216;Riemannian metric&#8217; that was <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">neither real nor symmetric</a> in the quantum case.  Most of the comments below are based on that original version, not the new fixed one.]</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comments">22 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1343 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1343">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/" rel="bookmark">Information Geometry (Part&nbsp;1)</a></h2>
				<small>22 October, 2010</small><br />


				<div class="entry">
					<p>I&#8217;d like to provide a bit of background to this interesting paper:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>.</p>
<p>which was pointed out by <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092">John F</a> in our discussion of <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/">entropy and uncertainty</a>.  </p>
<p>The idea here should work for either classical or quantum statistical mechanics.  The paper describes the classical version, so just for a change of pace let me describe the quantum version.</p>
<p>First a lightning review of <a href="http://en.wikipedia.org/wiki/Quantum_statistical_mechanics">quantum statistical mechanics</a>.  Suppose you have a quantum system with some Hilbert space.  When you know as much as possible about your system, then you describe it by a unit vector in this Hilbert space, and you say your system is in a <a href="http://en.wikipedia.org/wiki/Quantum_state#Pure_states_as_rays_in_a_Hilbert_space"><b>pure state</b></a>.  Sometimes people just call a pure state a &#8216;state&#8217;.  But that can be confusing, because in statistical mechanics you also need more general &#8216;mixed states&#8217; where you <i>don&#8217;t</i> know as much as possible.   A mixed state is described by a <a href="http://en.wikipedia.org/wiki/Density_matrix"><b>density matrix</b></a>, meaning a <a href="http://en.wikipedia.org/wiki/Positive_element">positive operator</a> <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> with <a href="http://en.wikipedia.org/wiki/Trace_%28linear_algebra%29">trace</a> equal to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28%5Crho%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(&#92;rho) = 1" class="latex" /></p>
<p>The idea is that any observable is described by a self-adjoint operator <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, and the expected value of this observable in the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Cmathrm%7Btr%7D%28%5Crho+A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;mathrm{tr}(&#92;rho A)" class="latex" /></p>
<p>The <b>entropy</b> of a mixed state is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Crho%29+%3D+-%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(&#92;rho) = -&#92;mathrm{tr}(&#92;rho &#92;; &#92;mathrm{ln} &#92;, &#92;rho)" class="latex" /></p>
<p>where we take the logarithm of the density matrix just by taking the log of each of its eigenvalues, while keeping the same eigenvectors.   This formula for entropy should remind you of the one that Gibbs and Shannon used &mdash; the one I explained <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">a while back</a>.  </p>
<p>Back then I told you about the &#8216;Gibbs ensemble&#8217;: the mixed state that maximizes entropy subject to the constraint that some observable have a given value.  We can do the same thing in quantum mechanics, and we can even do it for a bunch of observables at once.  Suppose we have some observables <img src="https://s0.wp.com/latex.php?latex=X_1%2C+%5Cdots%2C+X_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1, &#92;dots, X_n" class="latex" /> and we want to find the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that maximizes entropy subject to these constraints:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>for some numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />.  Then a little exercise in <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">Lagrange multipliers</a> shows that the answer is the <b>Gibbs state</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Cmathrm%7Bexp%7D%28-%5Clambda_1+X_1+%2B+%5Ccdots+%2B+%5Clambda_n+X_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} &#92;mathrm{exp}(-&#92;lambda_1 X_1 + &#92;cdots + &#92;lambda_n X_n) " class="latex" /></p>
<p>Huh?  <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/sm_confused.gif" alt="" /></p>
<p>This answer needs some explanation.  First of all, the numbers <img src="https://s0.wp.com/latex.php?latex=%5Clambda_1%2C+%5Cdots+%5Clambda_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda_1, &#92;dots &#92;lambda_n" class="latex" /> are called Lagrange multipliers.  You have to choose them right to get </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>So, in favorable cases, they will be functions of the numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />.  And when you&#8217;re really lucky, you can solve for the numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> in terms of the numbers <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda_i" class="latex" />.  We call <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda_i" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Conjugate_variables_%28thermodynamics%29"><b>conjugate variable</b></a> of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />.  For example, the conjugate variable of energy is inverse temperature! </p>
<p>Second of all, we take the exponential of a self-adjoint operator just as we took the logarithm of a density matrix: just take the exponential of each eigenvalue.  </p>
<p>(At least this works when our self-adjoint operator has only eigenvalues in its spectrum, not any <a>continuous spectrum</a>.  Otherwise we need to get serious and use the <a href="http://en.wikipedia.org/wiki/Functional_calculus">functional calculus</a>.  Luckily, if your system&#8217;s Hilbert space is finite-dimensional, you can ignore this parenthetical remark!)</p>
<p>But third: what&#8217;s that number <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />?  It begins life as a humble normalizing factor.  Its job is to make sure <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> has trace equal to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D%28%5Cmathrm%7Bexp%7D%28-%5Clambda_1+X_1+%2B+%5Ccdots+%2B+%5Clambda_n+X_n%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr}(&#92;mathrm{exp}(-&#92;lambda_1 X_1 + &#92;cdots + &#92;lambda_n X_n)) " class="latex" /></p>
<p>However, once you get going, it becomes incredibly important!  It&#8217;s called the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29"><b>partition function</b></a> of your system.</p>
<p>As an example of what it&#8217;s good for, it turns out you can compute the numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=x_i+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cmathrm%7Bln%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda_i} &#92;mathrm{ln} Z" class="latex" /></p>
<p>In other words, you can compute the expected values of the observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> by differentiating the log of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cmathrm%7Bln%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda_i} &#92;mathrm{ln} Z" class="latex" /></p>
<p>Or in still other words,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B1%7D%7BZ%7D+%5C%3B+%5Cfrac%7B%5Cpartial+Z%7D%7B%5Cpartial+%5Clambda_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = - &#92;frac{1}{Z} &#92;; &#92;frac{&#92;partial Z}{&#92;partial &#92;lambda_i}" class="latex" /> </p>
<p>To believe this you just have to take the equations I&#8217;ve given you so far and mess around &mdash; there&#8217;s really no substitute for doing it yourself.  I&#8217;ve done it fifty times, and every time I feel smarter.</p>
<p>But we can go further: after the <a href="http://en.wikipedia.org/wiki/Expected_value">&#8216;expected value&#8217;</a> or &#8216;mean&#8217; of an observable comes its <a href="http://en.wikipedia.org/wiki/Variance">variance</a>, which is the square of its standard deviation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5CDelta+A%29%5E2+%3D+%5Clangle+A%5E2+%5Crangle+-+%5Clangle+A+%5Crangle%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Delta A)^2 = &#92;langle A^2 &#92;rangle - &#92;langle A &#92;rangle^2 " class="latex" /></p>
<p>This measures the size of fluctuations around the mean.  And in the Gibbs state, we can compute the variance of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> as the <i>second</i> derivative of the log of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i%5E2+%5Crangle+-+%5Clangle+X_i+%5Crangle%5E2+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial%5E2+%5Clambda_i%7D+%5Cmathrm%7Bln%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i^2 &#92;rangle - &#92;langle X_i &#92;rangle^2 =  &#92;frac{&#92;partial^2}{&#92;partial^2 &#92;lambda_i} &#92;mathrm{ln} Z" class="latex" /></p>
<p>Again: calculate and see.</p>
<p>But when we&#8217;ve got lots of observables, there&#8217;s something better than the variance of each one.  There&#8217;s the <a href="http://en.wikipedia.org/wiki/Covariance">covariance matrix</a> of the whole lot of them!  Each observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> fluctuates around its mean value <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />&#8230; but these fluctuations are not independent!  They&#8217;re <i>correlated</i>, and the covariance matrix says how.  </p>
<p>All this is very visual, at least for me.  If you imagine the fluctuations as forming a blurry patch near the point <img src="https://s0.wp.com/latex.php?latex=%28x_1%2C+%5Cdots%2C+x_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x_1, &#92;dots, x_n)" class="latex" />, this patch will be ellipsoidal in shape, at least when all our random fluctuations are Gaussian.  And then the <i>shape</i> of this ellipsoid is precisely captured by the covariance matrix!  In particular, the eigenvectors of the covariance matrix will point along the principal axes of this ellipsoid, and the eigenvalues will say how stretched out the ellipsoid is in each direction!</p>
<div align="center">
<img width="400" src="https://i0.wp.com/www.math.dartmouth.edu/archive/m22x06/public_html/ellipsoid.jpg" alt="" />
</div>
<p>To understand the covariance matrix, it may help to start by rewriting the variance of a single observable <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5CDelta+A%29%5E2+%3D+%5Clangle+%28A+-+%5Clangle+A+%5Crangle%29%5E2+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Delta A)^2 = &#92;langle (A - &#92;langle A &#92;rangle)^2 &#92;rangle " class="latex" /></p>
<p>That&#8217;s a lot of angle brackets, but the meaning should be clear. First we look at the difference between our observable and its mean value, namely </p>
<p><img src="https://s0.wp.com/latex.php?latex=A+-+%5Clangle+A+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A - &#92;langle A &#92;rangle" class="latex" /></p>
<p>Then we square this, to get something that&#8217;s big and positive whenever our observable is far from its mean.  Then we take the mean value of the <i>that</i>, to get an idea of how far our observable is from the mean <i>on average</i>.</p>
<p>We can use the same trick to define the covariance of a bunch of observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />.   We get an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix called the <b>covariance matrix</b>, whose entry in the <i>i</i>th row and <i>j</i>th column is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>If you think about it, you can see that this will measure correlations in the fluctuations of your observables.  </p>
<p>An interesting difference between classical and quantum mechanics shows up here.  In classical mechanics the covariance matrix is always symmetric &mdash; but not in quantum mechanics!  You see, in classical mechanics, whenever we have two observables <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+B+%5Crangle+%3D+%5Clangle+B+A+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A B &#92;rangle = &#92;langle B A &#92;rangle" class="latex" /></p>
<p>since observables commute.  But in quantum mechanics this is not true!  For example, consider the position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> of a particle.  We have</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+p+%3D+p+q+%2B+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q p = p q + i " class="latex" /></p>
<p>so taking expectation values we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+q+p+%5Crangle+%3D+%5Clangle+p+q+%5Crangle+%2B+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle q p &#92;rangle = &#92;langle p q &#92;rangle + i " class="latex" /></p>
<p>So, it&#8217;s easy to get a non-symmetric covariance matrix when our observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> don&#8217;t commute.  However, the <i>real part</i> of the covariance matrix is symmetric, even in quantum mechanics.   So let&#8217;s define </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cmathrm%7BRe%7D++%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;mathrm{Re}  &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>You can check that the matrix entries here are the second derivatives of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cmathrm%7Bln%7D+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;mathrm{ln} Z " class="latex" /></p>
<p>And now for the cool part: this is where information geometry comes in!  Suppose that for any choice of values <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> we have a Gibbs state with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>Then for each point </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%28x_1%2C+%5Cdots+%2C+x_n%29+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = (x_1, &#92;dots , x_n) &#92;in &#92;mathbb{R}^n" class="latex" /> </p>
<p>we have a matrix</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cmathrm%7BRe%7D++%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cmathrm%7Bln%7D+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;mathrm{Re}  &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;mathrm{ln} Z " class="latex" /></p>
<p>And this matrix is not only symmetric, it&#8217;s also <a href="http://en.wikipedia.org/wiki/Positive_operator">positive</a>.  And when it&#8217;s <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite</a> we can think of it as an inner product on the <a href="http://en.wikipedia.org/wiki/Tangent_space">tangent space</a> of the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  In other words, we get a <a href="http://en.wikipedia.org/wiki/Riemannian_metric#Riemannian_metrics">Riemannian metric</a> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />.  This is called the <a href="http://en.wikipedia.org/wiki/Fisher_information_metric"><b>Fisher information metric</b></a>.</p>
<p>I hope you can see through the jargon to the simple idea.  We&#8217;ve got a space. Each point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> in this space describes the maximum-entropy state of a quantum system for which our observables have specified mean values.  But in each of these states, the observables are random variables.  They don&#8217;t just sit at their mean value, they fluctuate!  You can picture these fluctuations as forming a little smeared-out blob in our space.  To a first approximation, this blob is an ellipsoid.  And if we think of this ellipsoid as a &#8216;unit ball&#8217;, it gives us a standard for measuring the <i>length</i> of any little vector sticking out of our point.  In other words, we&#8217;ve got a Riemannian metric: <i>the Fisher information metric!</i></p>
<p>Now if you look at the Wikipedia article you&#8217;ll see a more general but to me somewhat <a href="http://en.wikipedia.org/wiki/Fisher_information_metric">scarier definition</a> of the Fisher information metric.  This applies whenever we&#8217;ve got a manifold whose points label <i>arbitrary</i> mixed states of a system.  But Crooks shows this definition reduces to his &mdash; the one I just described &mdash; when our manifold is <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> and it&#8217;s parametrizing Gibbs states in the way we&#8217;ve just seen.  </p>
<p>More precisely: both Crooks and the Wikipedia article describe the classical story, but it parallels the quantum story I&#8217;ve been telling&#8230; and I think the quantum version is well-known.  I believe the quantum version of the Fisher information metric is sometimes called the <a href="http://en.wikipedia.org/wiki/Bures_metric">Bures metric</a>, though I&#8217;m a bit confused about what the Bures metric actually is.</p>
<p>[Note: in the original version of this post, I omitted the real part in my definition <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" />, giving a &#8216;Riemannian metric&#8217; that was <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">neither real nor symmetric</a> in the quantum case.  Most of the comments below are based on that original version, not the new fixed one.]</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/#comments">27 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1308 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-quantum-technologies" id="post-1308">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/" rel="bookmark">Entropy and Uncertainty</a></h2>
				<small>19 October, 2010</small><br />


				<div class="entry">
					<p>I was going to write about a talk at the CQT, but I found a preprint lying on a table in the lecture hall, and it was so cool I&#8217;ll write about that instead:</p>
<p>&bull; Mario Berta, Matthias Christandl, Roger Colbeck, Joseph M. Renes, Renato Renner, <a href="http://arxiv.org/abs/0909.0950">The uncertainty principle in the presence of quantum memory</a>, <i>Nature Physics</i>, July 25, 2010.</p>
<p>Actually I won&#8217;t talk about the paper per se, since it&#8217;s better if I tell you a more basic result that I first learned from reading this paper: <i>the entropic uncertainty principle!</i></p>
<p>Everyone loves the concept of entropy, and everyone loves the uncertainty principle.  Even folks who don&#8217;t understand &#8217;em still love &#8217;em. They just sound so mysterious and spooky and <i>dark</i>.  I love &#8217;em too. So, it&#8217;s nice to see a mathematical relation between them.  </p>
<p>I explained entropy <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">back here</a>, so let me say a word about the uncertainty principle.  It&#8217;s a limitation on how accurately you can measure two things at once in quantum mechanics.  Sometimes you can only know a lot about one thing if you don&#8217;t know much about the other.  This happens when those two things &#8220;fail to commute&#8221;.  </p>
<p>Mathematically, the usual uncertainty principle says this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+A+%5Ccdot+%5CDelta+B+%5Cge+%5Cfrac%7B1%7D%7B2%7D+%7C%5Clangle+%5BA%2CB%5D+%5Crangle+%7C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta A &#92;cdot &#92;Delta B &#92;ge &#92;frac{1}{2} |&#92;langle [A,B] &#92;rangle | " class="latex" /></p>
<p>In plain English: the uncertainty in <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> times the uncertainty in <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> is bigger than the absolute value of the expected value of their <b>commutator</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5BA%2CB%5D+%3D+A+B+-+B+A+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[A,B] = A B - B A " class="latex" /></p>
<p>Whoops!   That started off as plain English, but it degenerated into plain gibberish near the end&#8230; which is probably why most people don&#8217;t understand the uncertainty principle.  I don&#8217;t think I&#8217;m gonna cure that today, but let me just nail down the math a bit.</p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> are observables &mdash; and to keep things really simple, by <b>observable</b> I&#8217;ll just mean a self-adjoint <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix.  Suppose <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is a <b>state</b>: that is, a unit vector in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{C}^n" class="latex" />.   Then the <b>expected value</b> of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is the average answer you get when you measure that observable in that state.  Mathematically it&#8217;s equal to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Clangle+%5Cpsi%2C+A+%5Cpsi+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;langle &#92;psi, A &#92;psi &#92;rangle " class="latex" /></p>
<p>Sorry, there are a lot of angle brackets running around here: the ones at right stand for the inner product in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{C}^n" class="latex" />, which I&#8217;m assuming you understand, while the ones at left are being defined by this equation. They&#8217;re just a shorthand.</p>
<p>Once we can compute averages, we can compute standard deviations, so we define the <b>standard deviation</b> of an observable <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=%5CDelta+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta A" class="latex" /> where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5CDelta+A%29%5E2+%3D+%5Clangle+A%5E2+%5Crangle+-+%5Clangle+A+%5Crangle%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Delta A)^2 = &#92;langle A^2 &#92;rangle - &#92;langle A &#92;rangle^2 " class="latex" /></p>
<p>Got it?  Just like in probability theory.  So now I hope you know what every symbol here means:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+A+%5Ccdot+%5CDelta+B+%5Cge+%5Cfrac%7B1%7D%7B2%7D+%7C%5Clangle+%5BA%2CB%5D+%5Crangle+%7C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta A &#92;cdot &#92;Delta B &#92;ge &#92;frac{1}{2} |&#92;langle [A,B] &#92;rangle | " class="latex" /></p>
<p>and if you&#8217;re a certain sort of person you can have fun going home and proving this.  Hint: it takes an inequality to prove an inequality.  Other hint: what&#8217;s the <a href="http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">most important inequality in the universe?</a></p>
<p>But now for the fun part: entropy!</p>
<p>Whenever you have an observable <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and a state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" />, you get a probability distribution: the distribution of outcomes when you measure that observable in that state.  And this probability distribution has an entropy!  Let&#8217;s call the entropy <img src="https://s0.wp.com/latex.php?latex=S%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(A)" class="latex" />.  I&#8217;ll define it a bit more carefully later.</p>
<p>But the point is: this entropy is really a very nice way to think about our uncertainty, or ignorance, of the observable <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />.  It&#8217;s better, in many ways, than the standard deviation.  For example, it doesn&#8217;t change if we multiply <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> by 2. The standard deviation  doubles, but we&#8217;re not twice as ignorant!  </p>
<p>Entropy is invariant under lots of transformations of our observables.   So we should want an uncertainty principle that only involves entropy.  And here it is, the <b>entropic uncertainty principle</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28A%29+%2B+S%28B%29+%5Cge+%5Cmathrm%7Blog%7D+%5C%2C+%5Cfrac%7B1%7D%7Bc%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(A) + S(B) &#92;ge &#92;mathrm{log} &#92;, &#92;frac{1}{c} " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> is defined as follows.  To keep things simple, suppose that <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is <b>nondegenerate</b>, meaning that all its eigenvalues are distinct.  If it&#8217;s not, we can tweak it a tiny bit and it will be. Let its eigenvectors be called <img src="https://s0.wp.com/latex.php?latex=%5Cphi_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi_i" class="latex" />.  Similarly, suppose <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> is nondegenerate and call its eigenvectors <img src="https://s0.wp.com/latex.php?latex=%5Cchi_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;chi_j" class="latex" />.  Then we let</p>
<p><img src="https://s0.wp.com/latex.php?latex=c+%3D+%5Cmathrm%7Bmax%7D_%7Bi%2Cj%7D+%7C%5Clangle+%5Cphi_i%2C+%5Cchi_j+%5Crangle%7C%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c = &#92;mathrm{max}_{i,j} |&#92;langle &#92;phi_i, &#92;chi_j &#92;rangle|^2 " class="latex" /></p>
<p>Note this becomes 1 when there&#8217;s an eigenvector of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> that&#8217;s also an eigenvector of <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />. In this case its possible to find a state where we know both observables precisely, and in this case also </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Blog%7D%5C%2C+%5Cfrac%7B1%7D%7Bc%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{log}&#92;, &#92;frac{1}{c} = 0" class="latex" />  </p>
<p>And that makes sense: in this case <img src="https://s0.wp.com/latex.php?latex=S%28A%29+%2B+S%28B%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(A) + S(B)" class="latex" />, which measures our ignorance of <i>both</i> observables, is indeed zero.</p>
<p>But if there&#8217;s no eigenvector of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> that&#8217;s also an eigenvector of <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> is smaller than 1, so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Blog%7D+%5C%2C+%5Cfrac%7B1%7D%7Bc%7D+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{log} &#92;, &#92;frac{1}{c} &gt; 0" class="latex" />  </p>
<p>so the entropic uncertainty principle says we really must have some ignorance about either <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> (or both).</p>
<p>So the entropic uncertainty principle makes intuitive sense.  But let me define the entropy <img src="https://s0.wp.com/latex.php?latex=S%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(A)" class="latex" />, to make the principle precise.  If <img src="https://s0.wp.com/latex.php?latex=%5Cphi_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi_i" class="latex" /> are the eigenvectors of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, the probabilities of getting various outcomes when we measure <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> are</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%7C%5Clangle+%5Cphi_i%2C+%5Cpsi+%5Crangle%7C%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = |&#92;langle &#92;phi_i, &#92;psi &#92;rangle|^2 " class="latex" /></p>
<p>So, we define the entropy by</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28A%29+%3D+-+%5Csum_i+p_i+%5C%3B+%5Cmathrm%7Blog%7D%5C%2C+p_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(A) = - &#92;sum_i p_i &#92;; &#92;mathrm{log}&#92;, p_i " class="latex" /></p>
<p>Here you can use any base for your logarithm, as long as you&#8217;re consistent.  Mathematicians and physicists use <i>e</i>, while computer scientists, who prefer integers, settle for the best known integer approximation: 2.   </p>
<p>Just kidding!  Darn &mdash; now I&#8217;ve insulted all the computer scientists.  I hope none of them reads this.  <img src="https://i2.wp.com/math.ucr.edu/home/baez/emoticons/uhh.gif" alt="" /></p>
<p>Who came up with this entropic uncertainty principle?  I&#8217;m not an expert on this, so I&#8217;ll probably get this wrong, but I gather it came from an idea of Deutsch:</p>
<p>&bull; David Deutsch, Uncertainty in quantum measurements, <i>Phys. Rev. Lett.</i> <b>50</b> (1983), 631-633.</p>
<p>Then it got improved and formulated as a conjecture by Kraus:</p>
<p>&bull; K. Kraus, Complementary observables and uncertainty relations, <i>Phys. Rev. D</i> <b>35</b> (1987), 3070-3075.</p>
<p>and then that conjecture was proved here:</p>
<p>&bull; H. Maassen and J. B. Uffink, Generalized entropic uncertainty relations, <i>Phys. Rev. Lett.</i> <b>60</b> (1988), 1103-1106.</p>
<p>The paper I found in the lecture hall proves a more refined version where the system being measured &mdash; let&#8217;s call it <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> &mdash; is entangled to the observer&#8217;s memory apparatus &mdash; let&#8217;s call it <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" />.  In this situation they show</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28A%7CO%29+%2B+S%28B%7CO%29+%5Cge+S%28X%7CO%29+%2B+%5Cmathrm%7Blog%7D+%5C%2C+%5Cfrac%7B1%7D%7Bc%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(A|O) + S(B|O) &#92;ge S(X|O) + &#92;mathrm{log} &#92;, &#92;frac{1}{c} " class="latex" /></p>
<p>where I&#8217;m using a concept of &#8220;conditional entropy&#8221;: the entropy of something <i>given</i> something else.  Here&#8217;s their abstract:</p>
<blockquote><p>
The uncertainty principle, originally formulated by Heisenberg, clearly illustrates the difference between classical and quantum mechanics. The principle bounds the uncertainties about the outcomes of two incompatible measurements, such as position and momentum, on a particle. It implies that one cannot predict the outcomes for both possible choices of measurement to arbitrary precision, even if information about the preparation of the particle is available in a classical memory. However, if the particle is prepared entangled with a quantum memory, a device that might be available in the not-too-distant future, it is possible to predict the outcomes for both measurement choices precisely. Here, we extend the uncertainty principle to incorporate this case, providing a lower bound on the uncertainties, which depends on the amount of entanglement between the particle and the quantum memory. We detail the application of our result to witnessing entanglement and to quantum key distribution.
</p></blockquote>
<p>By the way, on a really trivial note&#8230;</p>
<p>My wisecrack about 2 being the best known integer approximation to <i>e</i> made me wonder: since 3 is actually closer to <i>e</i>, are there some applications where ternary digits would theoretically be better than binary ones?  I&#8217;ve heard of &quot;<a href="http://en.wikipedia.org/wiki/Ternary_numeral_system">trits</a>&quot; but I don&#8217;t actually know any applications where they&#8217;re optimal.</p>
<p>Oh &mdash; <a href="http://en.wikipedia.org/wiki/Radix_economy">here&#8217;s one</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comments">50 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/" rel="category tag">quantum technologies</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/" rel="bookmark" title="Permanent Link to Entropy and Uncertainty">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-23234 post type-post status-publish format-standard hentry category-biology category-conferences category-information-and-entropy" id="post-23234">
				<h2><a href="https://johncarlosbaez.wordpress.com/2017/01/23/quantifying-biological-complexity/" rel="bookmark">Quantifying Biological Complexity</a></h2>
				<small>23 January, 2017</small><br />


				<div class="entry">
					<div align="center">
<a href="https://beyond.asu.edu/workshop/biological-complexity-can-it-be-quantified"><img src="https://beyond.asu.edu/sites/default/files/styles/large/public/images/workshop/beyond_center_3.png?itok=wt2nB54h" /></a></div>
<p>Next week I&#8217;m going to this workshop:</p>
<p>• <a href="https://beyond.asu.edu/workshop/biological-complexity-can-it-be-quantified">Biological Complexity: Can It Be Quantified?</a>, 1-3 February 2017, <a href="https://beyond.asu.edu/">Beyond Center for Fundamental Concepts in Science</a>, Arizona State University, Tempe Arizona.  Organized by <a href="http://cosmos.asu.edu/">Paul Davies</a>.</p>
<p>I haven&#8217;t heard that any of it will be made publicly available, but I&#8217;ll see if there&#8217;s something I can show you.  Here&#8217;s the schedule:</p>
<h3> Wednesday February 1st  </h3>
<p>9:00 – 9:30 am  Paul Davies</p>
<p>Brief welcome address, outline of the subject and aims of the meeting</p>
<h4>Session 1.  Life: do we know it when we see it?</h4>
<p>9:30 – 10:15 am: Chris McKay, “Mission to Enceladus”</p>
<p>10:15 &#8211; 10:45 am: Discussion</p>
<p>10:45– 11:15 am: Tea/coffee break</p>
<p>11:15 – 12:00 pm: Kate Adamala, “Alive but not life”</p>
<p>12:00 – 12:30 pm: Discussion</p>
<p>12:30 – 2:00 pm: Lunch</p>
<h4>Session 2.  Quantifying life</h4>
<p>2:00 – 2:45 pm: Lee Cronin, “The living and the dead: molecular signatures of life”</p>
<p>2:45 – 3:30 pm: Sara Walker, “Can we build a life meter?”</p>
<p>3:30 – 4:00 pm: Discussion</p>
<p>4:00 – 4:30 pm: Tea/coffee break</p>
<p>4:30 – 5:15 pm: Manfred Laubichler, “Complexity is smaller than you think”</p>
<p>5:15 – 5:30 pm: Discussion</p>
<h4>The Beyond Annual Lecture</h4>
<p>7:00 – 8:30 pm: Sean Carroll, “Our place in the universe”</p>
<h3>Thursday February 2nd</h3>
<h4> Session 3: Life, information and the second law of thermodynamics</h4>
<p>9:00 – 9:45 am: James Crutchfield, “Vital bits: the fuel of life”</p>
<p>9:45 – 10:00 am: Discussion</p>
<p>10:00 – 10:45 pm: John Baez, “Information and entropy in biology”</p>
<p>10:45 – 11:00 am: Discussion</p>
<p>11:00 – 11:30 pm: Tea/coffee break</p>
<p>11:30 – 12:15 pm: Chris Adami, “What is biological information?”</p>
<p>12:15 – 12:30 pm: Discussion</p>
<p>12:30 – 2:00 pm: Lunch</p>
<h4>Session 4: The emergence of agency</h4>
<p>2:00 – 2:45 pm: Olaf Khang Witkowski, “When do autonomous agents act collectively?”</p>
<p>2:45 – 3:00 pm: Discussion</p>
<p>3:00 – 3:45 pm: William Marshall, “When macro beats micro”</p>
<p>3:45 – 4:00 pm: Discussion</p>
<p>4:00 – 4:30 am: Tea/coffee break</p>
<p>4:30 – 5:15pm: Alexander Boyd, “Biology’s demons”</p>
<p>5:15 – 5:30 pm: Discussion</p>
<h3>Friday February 3rd</h3>
<h4>Session 5: New physics?</h4>
<p>9:00 – 9:45 am: Sean Carroll, “Laws of complexity, laws of life?”</p>
<p>9:45 – 10:00 am: Discussion</p>
<p>10:00 – 10:45 am: Andreas Wagner, “The arrival of the fittest”</p>
<p>10:45 – 11:00 am: Discussion</p>
<p>11:00 – 11:30 am: Tea/coffee break</p>
<p>11:30 – 12:30 pm: George Ellis, “Top-down causation demands new laws”</p>
<p>12:30 – 2:00 pm: Lunch</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/01/23/quantifying-biological-complexity/#comments">9 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/01/23/quantifying-biological-complexity/" rel="bookmark" title="Permanent Link to Quantifying Biological Complexity">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-17608 post type-post status-publish format-standard hentry category-computer-science category-conferences category-mathematics category-networks" id="post-17608">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/02/22/network-theory-overview/" rel="bookmark">Network Theory Overview</a></h2>
				<small>22 February, 2014</small><br />


				<div class="entry">
					<p>&nbsp;</p>
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="450" height="300" src="https://www.youtube.com/embed/p9VmyR-OMpM?version=3&#038;rel=0&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
<p>Here&#8217;s a video of a talk I gave yesterday, made by Brendan Fong.  You can see the slides <a href="http://math.ucr.edu/home/baez/networks_oxford/networks_overview.pdf">here</a>&#8212;and then click the items in blue, and the pictures, for more information!</p>
<p>The idea: nature and the world of human technology are full of networks!   People like to draw diagrams of networks.  Mathematical physicists know that in principle these diagrams can be understood using category theory. But why should physicists have all the fun?  This is the century of <i>understanding living systems</i> and <i>adapting to life on a finite planet</i>.   Math isn&#8217;t the main thing we need for this, but it&#8217;s got to be part of the solution&#8230; so <i>one</i> thing we should do is develop a unified and powerful theory of networks.</p>
<p>We are still far from doing this.  In this overview, I briefly described three parts of the jigsaw puzzle, and invited everyone to help fit them together:</p>
<p>• electrical circuits and signal-flow graphs.</p>
<p>• stochastic Petri nets, chemical reaction networks and Feynman diagrams.</p>
<p>• Bayesian networks, information and entropy.</p>
<p>In my talks coming up, I&#8217;ll go into more detail on each of these.﻿  With luck, you&#8217;ll be able to see videos here.</p>
<p>But if you&#8217;re near Oxford, you might as well actually attend!  You can see dates, times, locations, my slides, and the talks themselves as they show up by going <a href="https://johncarlosbaez.wordpress.com/2014/02/07/network-theory-talks-at-oxford/">here</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/22/network-theory-overview/#comments">17 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/computer-science/" rel="category tag">computer science</a>, <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/22/network-theory-overview/" rel="bookmark" title="Permanent Link to Network Theory Overview">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/6/?s=information+and+entropy" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/page/4/?s=information+and+entropy" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/page/5/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jksOwjAMRC9EarWLSl0gzpImVuXQfLAd2t6eVAIJsWA1nrGePbAV43JSTApBwOOTHJa9C3KBr1Wspqx1oSRgfaRkZssQrShym4yydXf5hdq98KjIx1u6rbgcTeG8H4axZaIfhpJbq0c5oWYxzui79uhPkY38giogdRbHVJRyOjvc4rUfh2nqh2Hswwtq4FN0'></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZWcldfeWhBZmU2RE9oRG9ifjJ1djZrLnA1V0ZqWTBPNzlwcW42NSVnYldiTXJKWz9mK0VNMzQlL0FUY19VN1VbME41OX5oNnV6Y29nQVpLLDdzVGVyJlZNfE5ab3wrWEFQdU1TN3xHS0J2Ml9McWw/LUxzaEpZSlphcDR+TlZRRG4rfHJ6Ulh6R3A/ekVhK0VJdUpSamR5R0YsUEowW0ltbT84UHJRakdsYkFGc2E3YWYzPSVoUXgzV2R2YnomY2JaZlk='}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script defer id="bilmur"  data-provider="wordpress.com" data-service="simple" src="/wp-content/js/bilmur.min.js?i=2&m=202138"></script><script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>