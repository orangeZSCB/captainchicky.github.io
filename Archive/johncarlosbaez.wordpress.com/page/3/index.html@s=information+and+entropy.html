<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth | Page 3</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2015%2F11%2F27%2Frelative-entropy-in-biological-systems%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/page\/3\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fpage%2F3%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2015%2F11%2F27%2Frelative-entropy-in-biological-systems%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Page 3 &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results paged paged-3 search-paged-3 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-20189 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-probability" id="post-20189">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/" rel="bookmark">Relative Entropy in Biological&nbsp;Systems</a></h2>
				<small>27 November, 2015</small><br />


				<div class="entry">
					<p>Here&#8217;s a paper for the proceedings of a workshop on <a href="http://math.ucr.edu/home/baez/nimbios/">Information and Entropy in Biological System</a> this spring:</p>
<p>• John Baez and Blake Pollard, <a href="http://arxiv.org/abs/1512.02742">Relative entropy in biological systems</a>, with Blake S. Pollard, <i><a href="http://www.mdpi.com/1099-4300/18/2/46">Entropy</a></i> <b>18</b> (2016), 46.</p>
<p>We&#8217;d love any comments or questions you might have. I&#8217;m not happy with the title. In the paper we advocate using the term &#8216;relative information&#8217; instead of &#8216;relative entropy&#8217;&#8212;yet the latter is much more widely used, so I feel we need it in the title to let people know what the paper is about!</p>
<p>Here&#8217;s the basic idea.</p>
<p>Life relies on nonequilibrium thermodynamics, since in thermal equilibrium there are no flows of free energy. Biological systems are also open systems, in the sense that both matter and energy flow in and out of them. Nonetheless, it is important in biology that systems can sometimes be treated as approximately closed, and sometimes approach equilibrium before being disrupted in one way or another. This can occur on a wide range of scales, from large ecosystems to within a single cell or organelle. Examples include:</p>
<p>• A population approaching an evolutionarily stable state.</p>
<p>• Random processes such as mutation, genetic drift, the diffusion of organisms in an environment or the diffusion of molecules in a liquid.</p>
<p>• A chemical reaction approaching equilibrium.</p>
<p>An interesting common feature of these processes is that as they occur, quantities mathematically akin to entropy tend to increase. Closely related quantities such as free energy tend to decrease. In this review, we explain some mathematical results that make this idea precise.</p>
<p>Most of these results involve a quantity that is variously known as &#8216;relative information&#8217;, &#8216;relative entropy&#8217;, &#8216;information gain&#8217; or the &#8216;Kullback&#8211;Leibler divergence&#8217;. We&#8217;ll use the first term. Given two probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />, their <b>relative information</b>, or more precisely the <b>information of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /></b>, is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28p%5C%7Cq%29+%3D+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5Cln%5Cleft%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(p&#92;|q) = &#92;sum_{i &#92;in X} p_i &#92;ln&#92;left(&#92;frac{p_i}{q_i}&#92;right) } " class="latex" /></p>
<p>We use the word &#8216;information&#8217; instead of &#8216;entropy&#8217; because one expects entropy to increase with time, and the theorems we present will say that <img src="https://s0.wp.com/latex.php?latex=I%28p%5C%7Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p&#92;|q)" class="latex" /> decreases with time under various conditions. The reason is that the Shannon entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28p%29+%3D+-%5Csum_%7Bi+%5Cin+X%7D+p_i+%5Cln+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(p) = -&#92;sum_{i &#92;in X} p_i &#92;ln p_i } " class="latex" /></p>
<p>contains a minus sign that is missing from the definition of relative information.</p>
<p>Intuitively, <img src="https://s0.wp.com/latex.php?latex=I%28p%5C%7Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p&#92;|q)" class="latex" /> is the amount of information gained when we start with a hypothesis given by some probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and then learn the &#8216;true&#8217; probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />. For example, if we start with the hypothesis that a coin is fair and then are told that it landed heads up, the relative information is <img src="https://s0.wp.com/latex.php?latex=%5Cln+2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln 2" class="latex" />, so we have gained 1 bit of information. If however we started with the hypothesis that the coin always lands heads up, we would have gained no information.</p>
<p>We put the word &#8216;true&#8217; in quotes here, because the notion of a &#8216;true&#8217; probability distribution, which subjective Bayesians reject, is not required to use relative information. A more cautious description of relative information is that it is a <b>divergence</b>: a way of measuring the difference between probability distributions that obeys</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28p+%5C%7C+q%29+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p &#92;| q) &#92;ge 0 " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28p+%5C%7C+q%29+%3D+0+%5Ciff+p+%3D+q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p &#92;| q) = 0 &#92;iff p = q " class="latex" /></p>
<p>but not necessarily the other axioms for a distance function, symmetry and the triangle inequality, which indeed fail for relative information.</p>
<p>There are many other divergences besides relative information, some of which we discuss in Section 6. However, relative information can be singled out by a number of characterizations, including <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/">one based on ideas from Bayesian inference</a>. The relative information is also close to the expected number of extra bits required to code messages distributed according to the probability measure <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> using a code optimized for messages distributed according to <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.</p>
<p>In this review, we describe various ways in which a population or probability distribution evolves continuously according to some differential equation. For all these differential equations, I describe conditions under which relative information decreases. Briefly, the results are as follows. We hasten to reassure the reader that our paper explains all the jargon involved, and the proofs of the claims are given in full:</p>
<p>• In Section 2, we consider a very general form of the Lotka&#8211;Volterra equations, which are a commonly used model of population dynamics. Starting from the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of each type of replicating entity, we can define a probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cdisplaystyle%7B%5Cfrac%7BP_i%7D%7B%5Csum_%7Bi+%5Cin+X%7D+P_i+%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;displaystyle{&#92;frac{P_i}{&#92;sum_{i &#92;in X} P_i }} " class="latex" /></p>
<p>which evolves according to a nonlinear equation called the replicator equation. We describe a necessary and sufficient condition under which <img src="https://s0.wp.com/latex.php?latex=I%28q%5C%7Cp%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q&#92;|p(t))" class="latex" /> is nonincreasing when <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> evolves according to the replicator equation while <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is held fixed.</p>
<p>• In Section 3, we consider a special case of the replicator equation that is widely studied in evolutionary game theory. In this case we can think of probability distributions as mixed strategies in a two-player game. When <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a dominant strategy, <img src="https://s0.wp.com/latex.php?latex=I%28q%5C%7Cp%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q&#92;|p(t))" class="latex" /> can never increase when <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> evolves according to the replicator equation. We can think of <img src="https://s0.wp.com/latex.php?latex=I%28q%5C%7Cp%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q&#92;|p(t))" class="latex" /> as the information that the population has left to learn. Thus, evolution is analogous to a learning process&#8212;an analogy that in the field of artificial intelligence is exploited by evolutionary algorithms!</p>
<p>• In Section 4 we consider continuous-time, finite-state Markov processes. Here we have probability distributions on a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> evolving according to a linear equation called the master equation. In this case <img src="https://s0.wp.com/latex.php?latex=I%28p%28t%29%5C%7Cq%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p(t)&#92;|q(t))" class="latex" /> can never increase. Thus, if <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a steady state solution of the master equation, both <img src="https://s0.wp.com/latex.php?latex=I%28p%28t%29%5C%7Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p(t)&#92;|q)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=I%28q%5C%7Cp%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q&#92;|p(t))" class="latex" /> are nonincreasing. We can always write <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as the Boltzmann distribution for some energy function <img src="https://s0.wp.com/latex.php?latex=E+%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E : X &#92;to &#92;mathbb{R}" class="latex" />, meaning that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_i+%3D+%5Cfrac%7B%5Cexp%28-E_i+%2F+k+T%29%7D%7B%5Cdisplaystyle%7B%5Csum_%7Bj+%5Cin+X%7D+%5Cexp%28-E_j+%2F+k+T%29%7D%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_i = &#92;frac{&#92;exp(-E_i / k T)}{&#92;displaystyle{&#92;sum_{j &#92;in X} &#92;exp(-E_j / k T)}} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is temperature and <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> is Boltzmann&#8217;s constant. In this case, <img src="https://s0.wp.com/latex.php?latex=I%28p%28t%29%5C%7Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p(t)&#92;|q)" class="latex" /> is proportional to a difference of free energies:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28p%28t%29%5C%7Cq%29+%3D+%5Cfrac%7BF%28p%29+-+F%28q%29%7D%7BT%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(p(t)&#92;|q) = &#92;frac{F(p) - F(q)}{T} } " class="latex" /></p>
<p>Thus, the nonincreasing nature of <img src="https://s0.wp.com/latex.php?latex=I%28p%28t%29%5C%7Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p(t)&#92;|q)" class="latex" /> is a version of the Second Law of Thermodynamics.</p>
<p>• In Section 5, we consider chemical reactions and other processes described by reaction networks. In this context we have populations <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of entities of various kinds <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" />, and these populations evolve according to a nonlinear equation called the rate equation. We can generalize relative information from probability distributions to populations by setting</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28P%5C%7CQ%29+%3D+%5Csum_%7Bi+%5Cin+X%7D+P_i+%5Cln%5Cleft%28%5Cfrac%7BP_i%7D%7BQ_i%7D%5Cright%29+-+%5Cleft%28P_i+-+Q_i%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(P&#92;|Q) = &#92;sum_{i &#92;in X} P_i &#92;ln&#92;left(&#92;frac{P_i}{Q_i}&#92;right) - &#92;left(P_i - Q_i&#92;right) } " class="latex" /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a special sort of steady state solution of the rate equation, called a complex balanced equilibrium, <img src="https://s0.wp.com/latex.php?latex=I%28P%28t%29%5C%7CQ%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(P(t)&#92;|Q)" class="latex" /> can never increase when <img src="https://s0.wp.com/latex.php?latex=P%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(t)" class="latex" /> evolves according to the rate equation.</p>
<p>• Finally, in Section 6, we consider a class of functions called <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />-divergences which include relative information as a special case. For any convex function <img src="https://s0.wp.com/latex.php?latex=f+%3A+%5B0%2C%5Cinfty%29+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : [0,&#92;infty) &#92;to [0,&#92;infty)" class="latex" />, the <b><i>f</i>-divergence</b> of two probability distributions <img src="https://s0.wp.com/latex.php?latex=p%2C+q+%3A+X+%5Cto+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q : X &#92;to [0,1]" class="latex" /> is given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I_f%28p%5C%7Cq%29+%3D+%5Csum_%7Bi+%5Cin+X%7D+q_i+f%5Cleft%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%5Cright%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I_f(p&#92;|q) = &#92;sum_{i &#92;in X} q_i f&#92;left(&#92;frac{p_i}{q_i}&#92;right)} " class="latex" /></p>
<p>Whenever <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(t)" class="latex" /> are probability distributions evolving according to the master equation of some Markov process, <img src="https://s0.wp.com/latex.php?latex=I_f%28p%28t%29%5C%7Cq%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I_f(p(t)&#92;|q(t))" class="latex" /> is nonincreasing. The <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />-divergence is also well-defined for populations, and nonincreasing for two populations that both evolve according to the master equation.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/#comments">12 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/" rel="bookmark" title="Permanent Link to Relative Entropy in Biological&nbsp;Systems">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-19395 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-19395">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/" rel="bookmark">Thermodynamics with Continuous Information&nbsp;Flow</a></h2>
				<small>21 March, 2015</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://www.azimuthproject.org/azimuth/show/Blake+Pollard">Blake S. Pollard</a></b></i></p>
<p>Over a century ago James Clerk Maxwell created a thought experiment that has helped shape our understanding of the Second Law of Thermodynamics: the law that says entropy can never decrease.</p>
<p>Maxwell&#8217;s proposed experiment was simple. Suppose you had a box filled with an ideal gas at equilibrium at some temperature. You stick in an insulating partition, splitting the box into two halves. These two halves are isolated from one another except for one important caveat: somewhere along the partition resides a being capable of opening and closing a door, allowing gas particles to flow between the two halves. This being is also capable of observing the velocities of individual gas particles. Every time a particularly fast molecule is headed towards the door the being opens it, letting fly into the other half of the box. When a slow particle heads towards the door the being keeps it closed. After some time, fast molecules would build up on one side of the box, meaning half the box would heat up! To an observer it would seem like the box, originally at a uniform temperature, would for some reason start splitting up into a hot half and a cold half. This seems to violate the Second Law (as well as all our experience with boxes of gas).</p>
<p>Of course, this apparent violation probably has something to do with positing the existence of intelligent microscopic doormen. This being, and the thought experiment itself, are typically referred to as Maxwell&#8217;s demon.</p>
<div align="center">
<a href="http://math.ucr.edu/home/baez/mathematical/pollard_marchmeeting/demon.jpg"><img loading="lazy" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/pollard_marchmeeting/demon.jpg" alt="Demon2" width="225" height="300" class="aligncenter size-medium wp-image-567" /></a><br />
Photo credit: Peter MacDonald, Edmonds, UK
</div>
<p>When people cook up situations that seem to violate the Second Law there is typically a simple resolution: you have to consider the whole system! In the case of Maxwell&#8217;s demon, while the entropy of the box decreases, the entropy of the system as a whole, demon include, goes up. Precisely quantifying how Maxwell&#8217;s demon doesn&#8217;t violate the Second Law has led people to a better understanding of the role of information in thermodynamics.</p>
<p>At the American Physical Society March Meeting in San Antonio, Texas, I had the pleasure of hearing some great talks on entropy, information, and the Second Law.  <a href="http://seneca.fis.ucm.es/index.php/miembros-menu-item/jordan-horowitz">Jordan Horowitz</a>, a postdoc at Boston University, gave a talk on his work with <a href="https://sites.google.com/site/massimilianoespositogennaro/home">Massimiliano Esposito</a>, a researcher at the University of Luxembourg, on how one can understand situations like Maxwell&#8217;s demon (and a whole lot more) by analyzing the flow of information between subsystems.</p>
<p>Consider a system made up of two parts, <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" />. Each subsystem has a discrete set of states. Each systems makes transitions among these discrete states. These dynamics can be modeled as Markov processes. They are interested in modeling the thermodynamics of information flow between subsystems. To this end they consider a <b>bipartite</b> system, meaning that either <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> transitions or <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> transitions, never both at the same time. The probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x,y)" class="latex" /> of the whole system evolves according to the <b>master equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bdp%28x%2Cy%29%7D%7Bdt%7D+%3D+%5Csum_%7Bx%27%2C+y%27%7D+H_%7Bx%2Cx%27%7D%5E%7By%2Cy%27%7Dp%28x%27%2Cy%27%29+-+H_%7Bx%27%2Cx%7D%5E%7By%27%2Cy%7Dp%28x%2Cy%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dp(x,y)}{dt} = &#92;sum_{x&#039;, y&#039;} H_{x,x&#039;}^{y,y&#039;}p(x&#039;,y&#039;) - H_{x&#039;,x}^{y&#039;,y}p(x,y) } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=H_%7Bx%2Cx%27%7D%5E%7By%2Cy%27%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{x,x&#039;}^{y,y&#039;} " class="latex" /> is the rate at which the system transitions from <img src="https://s0.wp.com/latex.php?latex=%28x%27%2Cy%27%29+%5Cto+%28x%2Cy%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x&#039;,y&#039;) &#92;to (x,y)." class="latex" />  The &#8216;bipartite&#8217; condition means that <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> has the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7Bx%2Cx%27%7D%5E%7By%2Cy%27%7D+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bcc%7D+H_%7Bx%2Cx%27%7D%5Ey+%26+x+%5Cneq+x%27%3B+y%3Dy%27+%5C%5C+++H_x%5E%7By%2Cy%27%7D+%26+x%3Dx%27%3B+y+%5Cneq+y%27+%5C%5C++0+%26+%5Ctext%7Botherwise.%7D+%5Cend%7Barray%7D+%5Cright.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{x,x&#039;}^{y,y&#039;} = &#92;left&#92;{ &#92;begin{array}{cc} H_{x,x&#039;}^y &amp; x &#92;neq x&#039;; y=y&#039; &#92;&#92;   H_x^{y,y&#039;} &amp; x=x&#039;; y &#92;neq y&#039; &#92;&#92;  0 &amp; &#92;text{otherwise.} &#92;end{array} &#92;right. " class="latex" /></p>
<p>The joint system is an open system that satisfies the second law of thermodynamics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_i%7D%7Bdt%7D+%3D+%5Cfrac%7BdS_%7BXY%7D%7D%7Bdt%7D+%2B+%5Cfrac%7BdS_e%7D%7Bdt%7D+%5Cgeq+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_i}{dt} = &#92;frac{dS_{XY}}{dt} + &#92;frac{dS_e}{dt} &#92;geq 0 }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S_%7BXY%7D+%3D+-+%5Csum_%7Bx%2Cy%7D+p%28x%2Cy%29+%5Cln+%28+p%28x%2Cy%29+%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S_{XY} = - &#92;sum_{x,y} p(x,y) &#92;ln ( p(x,y) ) }" class="latex" /></p>
<p>is the Shannon entropy of the system, satisfying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_%7BXY%7D+%7D%7Bdt%7D+%3D+%5Csum_%7Bx%2Cy%7D+%5Cleft%5B+H_%7Bx%2Cx%27%7D%5E%7By%2Cy%27%7Dp%28x%27%2Cy%27%29+-+H_%7Bx%27%2Cx%7D%5E%7By%27%2Cy%7D+++p%28x%2Cy%29+%5Cright%5D+%5Cln+%5Cleft%28+%5Cfrac%7Bp%28x%27%2Cy%27%29%7D%7Bp%28x%2Cy%29%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_{XY} }{dt} = &#92;sum_{x,y} &#92;left[ H_{x,x&#039;}^{y,y&#039;}p(x&#039;,y&#039;) - H_{x&#039;,x}^{y&#039;,y}   p(x,y) &#92;right] &#92;ln &#92;left( &#92;frac{p(x&#039;,y&#039;)}{p(x,y)} &#92;right) } " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_e%7D%7Bdt%7D++%3D+%5Csum_%7Bx%2Cy%7D+%5Cleft%5B+H_%7Bx%2Cx%27%7D%5E%7By%2Cy%27%7Dp%28x%27%2Cy%27%29+-+H_%7Bx%27%2Cx%7D%5E%7By%27%2Cy%7D+p%28x%2Cy%29+%5Cright%5D+%5Cln+%5Cleft%28+%5Cfrac%7B+H_%7Bx%2Cx%27%7D%5E%7By%2Cy%27%7D+%7D+%7BH_%7Bx%27%2Cx%7D%5E%7By%27%2Cy%7D+%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_e}{dt}  = &#92;sum_{x,y} &#92;left[ H_{x,x&#039;}^{y,y&#039;}p(x&#039;,y&#039;) - H_{x&#039;,x}^{y&#039;,y} p(x,y) &#92;right] &#92;ln &#92;left( &#92;frac{ H_{x,x&#039;}^{y,y&#039;} } {H_{x&#039;,x}^{y&#039;,y} } &#92;right) }" class="latex" /></p>
<p>is the entropy change of the environment.</p>
<p>We want to investigate how the entropy production of the whole system relates to entropy production in the bipartite pieces <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" />. To this end they define a new flow, the information flow, as the time rate of change of the <b>mutual information</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I+%3D+%5Csum_%7Bx%2Cy%7D+p%28x%2Cy%29+%5Cln+%5Cleft%28+%5Cfrac%7Bp%28x%2Cy%29%7D%7Bp%28x%29p%28y%29%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I = &#92;sum_{x,y} p(x,y) &#92;ln &#92;left( &#92;frac{p(x,y)}{p(x)p(y)} &#92;right) }" class="latex" /></p>
<p>Its time derivative can be split up as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdI%7D%7Bdt%7D+%3D+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%2B+%5Cfrac%7BdI%5EY%7D%7Bdt%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dI}{dt} = &#92;frac{dI^X}{dt} + &#92;frac{dI^Y}{dt}}" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%3D+%5Csum_%7Bx%2Cy%7D+%5Cleft%5B+H_%7Bx%2Cx%27%7D%5E%7By%7D+p%28x%27%2Cy%29+-+H_%7Bx%27%2Cx%7D%5E%7By%7Dp%28x%2Cy%29+%5Cright%5D+%5Cln+%5Cleft%28+%5Cfrac%7B+p%28y%7Cx%29+%7D%7Bp%28y%7Cx%27%29%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dI^X}{dt} = &#92;sum_{x,y} &#92;left[ H_{x,x&#039;}^{y} p(x&#039;,y) - H_{x&#039;,x}^{y}p(x,y) &#92;right] &#92;ln &#92;left( &#92;frac{ p(y|x) }{p(y|x&#039;)} &#92;right) }" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdI%5EY%7D%7Bdt%7D+%3D+%5Csum_%7Bx%2Cy%7D+%5Cleft%5B+H_%7Bx%7D%5E%7By%2Cy%27%7Dp%28x%2Cy%27%29+-+H_%7Bx%7D%5E%7By%27%2Cy%7Dp%28x%2Cy%29+%5Cright%5D+%5Cln+%5Cleft%28+%5Cfrac%7Bp%28x%7Cy%29%7D%7Bp%28x%7Cy%27%29%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dI^Y}{dt} = &#92;sum_{x,y} &#92;left[ H_{x}^{y,y&#039;}p(x,y&#039;) - H_{x}^{y&#039;,y}p(x,y) &#92;right] &#92;ln &#92;left( &#92;frac{p(x|y)}{p(x|y&#039;)} &#92;right) }" class="latex" /></p>
<p>are the information flows associated with the subsystems <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> respectively.</p>
<p>When</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%3E+0%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dI^X}{dt} &gt; 0}" class="latex" /></p>
<p>a transition in <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> increases the mutual information <img src="https://s0.wp.com/latex.php?latex=I%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I," class="latex" /> meaning that <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> &#8216;knows&#8217; more about <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> and vice versa.</p>
<p>We can rewrite the entropy production entering into the second law in terms of these information flows as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_i%7D%7Bdt%7D+%3D+%5Cfrac%7BdS_i%5EX%7D%7Bdt%7D+%2B+%5Cfrac%7BdS_i%5EY%7D%7Bdt%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_i}{dt} = &#92;frac{dS_i^X}{dt} + &#92;frac{dS_i^Y}{dt} }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_i%5EX%7D%7Bdt%7D+%3D+%5Csum_%7Bx%2Cy%7D+%5Cleft%5B+H_%7Bx%2Cx%27%7D%5Ey+p%28x%27%2Cy%29+-+H_%7Bx%27%2Cx%7D%5Ey+p%28x%2Cy%29+%5Cright%5D+%5Cln+%5Cleft%28+%5Cfrac%7BH_%7Bx%2Cx%27%7D%5Ey+p%28x%27%2Cy%29+%7D+%7BH_%7Bx%27%2Cx%7D%5Ey+p%28x%2Cy%29+%7D+%5Cright%29+%5Cgeq+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_i^X}{dt} = &#92;sum_{x,y} &#92;left[ H_{x,x&#039;}^y p(x&#039;,y) - H_{x&#039;,x}^y p(x,y) &#92;right] &#92;ln &#92;left( &#92;frac{H_{x,x&#039;}^y p(x&#039;,y) } {H_{x&#039;,x}^y p(x,y) } &#92;right) &#92;geq 0 }" class="latex" /></p>
<p>and similarly for <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdS_Y%7D%7Bdt%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{dS_Y}{dt} " class="latex" />.  This gives the following decomposition of entropy production in each subsystem:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_i%5EX%7D%7Bdt%7D+%3D+%5Cfrac%7BdS%5EX%7D%7Bdt%7D+%2B+%5Cfrac%7BdS%5EX_e%7D%7Bdt%7D+-+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%5Cgeq+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_i^X}{dt} = &#92;frac{dS^X}{dt} + &#92;frac{dS^X_e}{dt} - &#92;frac{dI^X}{dt} &#92;geq 0 } " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS_i%5EY%7D%7Bdt%7D+%3D+%5Cfrac%7BdS%5EY%7D%7Bdt%7D+%2B+%5Cfrac%7BdS%5EX_e%7D%7Bdt%7D+-+%5Cfrac%7BdI%5EY%7D%7Bdt%7D+%5Cgeq+0%7D%2C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS_i^Y}{dt} = &#92;frac{dS^Y}{dt} + &#92;frac{dS^X_e}{dt} - &#92;frac{dI^Y}{dt} &#92;geq 0}, " class="latex" /></p>
<p>where the inequalities hold for each subsystem. To see this, if you write out the left hand side of each inequality you will find that they are both of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bx%2Cy%7D+%5Cleft%5B+x-y+%5Cright%5D+%5Cln+%5Cleft%28+%5Cfrac%7Bx%7D%7By%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{x,y} &#92;left[ x-y &#92;right] &#92;ln &#92;left( &#92;frac{x}{y} &#92;right) }" class="latex" /></p>
<p>which is non-negative for <img src="https://s0.wp.com/latex.php?latex=x%2Cy+%5Cgeq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x,y &#92;geq 0" class="latex" />.</p>
<p>The interaction between the subsystems is contained entirely in the information flow terms. Neglecting these terms gives rise to situations like Maxwell&#8217;s demon where a subsystem seems to violate the second law.</p>
<p>Lots of Markov processes have boring equilibria <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bdp%7D%7Bdt%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{dp}{dt} = 0" class="latex" /> where there is no net flow among the states. Markov processes also admit non-equilibrium steady states, where there may be some constant flow of information. In this steady state all explicit time derivatives are zero, including the net information flow:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdI%7D%7Bdt%7D+%3D+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dI}{dt} = 0 }" class="latex" /></p>
<p>which implies that <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%3D+-+%5Cfrac%7BdI%5EY%7D%7Bdt%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{dI^X}{dt} = - &#92;frac{dI^Y}{dt}." class="latex" /> In this situation the above inequalities become</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS%5EX_i%7D%7Bdt%7D+%3D+%5Cfrac%7BdS_e%5EX%7D%7Bdt%7D+-+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS^X_i}{dt} = &#92;frac{dS_e^X}{dt} - &#92;frac{dI^X}{dt} }" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdS%5EY_i%7D%7Bdt%7D+%3D+%5Cfrac%7BdS_e%5EX%7D%7Bdt%7D+%2B+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dS^Y_i}{dt} = &#92;frac{dS_e^X}{dt} + &#92;frac{dI^X}{dt} }." class="latex" /></p>
<p>If</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+%3E+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dI^X}{dt} &gt; 0 }" class="latex" /></p>
<p>then <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is learning something about <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> or acting as a sensor. The first inequality</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdS_e%5EX%7D%7Bdt%7D+%5Cgeq+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{dS_e^X}{dt} &#92;geq &#92;frac{dI^X}{dt} " class="latex" /> quantifies the minimum amount of energy <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> must supply to do this sensing. Similarly <img src="https://s0.wp.com/latex.php?latex=-%5Cfrac%7BdS_e%5EY%7D%7Bdt%7D+%5Cleq+%5Cfrac%7BdI%5EX%7D%7Bdt%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;frac{dS_e^Y}{dt} &#92;leq &#92;frac{dI^X}{dt} " class="latex" /> bounds the amount of useful energy is available to <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> as a result of this information transfer.</p>
<p>In their paper Horowitz and Esposito explore a few other examples and show the utility of this simple breakup of a system into two interacting subsystems in explaining various interesting situations in which the flow of information has thermodynamic significance.</p>
<p>For the whole story, read their paper!</p>
<p>&bull; Jordan Horowitz and Massimiliano Esposito, <a href="http://arxiv.org/abs/1402.3276">Thermodynamics with continuous information flow</a>, <i>Phys. Rev. X</i> <b>4</b> (2014), 031015.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/#comments">25 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/03/21/19395/" rel="bookmark" title="Permanent Link to Thermodynamics with Continuous Information&nbsp;Flow">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-18938 post type-post status-publish format-standard hentry category-biology category-conferences category-information-and-entropy" id="post-18938">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/10/30/sensing-and-acting-under-information-constraints/" rel="bookmark">Sensing and Acting Under Information&nbsp;Constraints</a></h2>
				<small>30 October, 2014</small><br />


				<div class="entry">
					<p>I&#8217;m having a great time at a workshop on <a href="https://johncarlosbaez.wordpress.com/2014/01/31/bio-inspired-information-theory/">Biological and Bio-Inspired Information Theory</a> in Banff, Canada.  You can see videos of the talks online.  There have been lots of good talks so far, but this one really blew my mind:</p>
<p>&bull; Naftali Tishby, <a href="http://www.birs.ca/events/2014/5-day-workshops/14w5170/videos/watch/201410281032-Tishby.mp4">Sensing and acting under information constraints&#8212;a principled approach to biology and intelligence</a>, 28 October 2014.</p>
<div align="center">
<img width="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/naftali_tishby.jpg" />
</div>
<p>Tishby&#8217;s talk wasn&#8217;t easy for me to follow&#8212;he assumed you already knew rate-distortion theory and the Bellman equation, and I didn&#8217;t&#8212;but it was <i>great!</i></p>
<p>It was about the &#8216;action-perception loop&#8217;:</p>
<div align="center">
<a href="http://www.uni-bielefeld.de/biologie/cns/"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/action-perception_loop.jpg" /></a>
</div>
<p>This is the feedback loop in which living organisms&#8212;like us&#8212;<i>take actions</i> depending on our goals and what we perceive, and <i>perceive things</i> depending on the actions we take and the state of the world.</p>
<p>How do we do this so well?  Among other things, we need to balance the <i>cost</i> of storing information about the <i>past</i> against the <i>payoff</i> of achieving our desired goals in the <i>future</i>.</p>
<p>Tishby presented a detailed yet highly general mathematical model of this!  And he ended by testing the model on experiments with cats listening to music and rats swimming to land.</p>
<p>It&#8217;s beautiful stuff.  I want to learn it.  I hope to blog about it as I understand more.  But for now, let me just dive in and say some basic stuff.  I&#8217;ll start with the two buzzwords I dropped on you.  I hate it when people use terminology without ever explaining it.</p>
<h3> Rate-distortion theory </h3>
<p><b><a href="https://en.wikipedia.org/wiki/Rate-distortion_theory">Rate-distortion theory</a></b> is a branch of information theory which seeks to find the minimum rate at which bits must be communicated over a noisy channel so that the signal can be approximately reconstructed at the other end without exceeding a given distortion.  Shannon&#8217;s first big result in this theory, the &#8216;rate-distortion theorem&#8217;, gives a formula for this minimum rate.  Needless to say, it still requires a lot of extra work to determine and achieve this minimum rate in practice.</p>
<p>For the basic definitions and a statement of the theorem, try this:</p>
<p>&bull; Natasha Devroye, <a href="https://www.cs.uic.edu/pub/ECE534/WebHome/ch10.pdf">Rate-distortion theory</a>, course notes, University of Chicago, Illinois, Fall 2009.</p>
<p>One of the people organizing this conference is a big expert on rate-distortion theory, and he wrote a book about it.</p>
<p>&bull; Toby Berger, <i>Rate Distortion Theory: A Mathematical Basis for Data Compression</i>, Prentice&#8211;Hall, 1971.</p>
<p>Unfortunately it&#8217;s out of print and selling for $259 used on Amazon!  An easier option might be this:</p>
<p>&bull; Thomas M. Cover and Joy A. Thomas, <i>Elements of Information Theory</i>, Chapter 10: Rate Distortion Theory, Wiley, New York, 2006.</p>
<h3> The Bellman equation </h3>
<p>The <b><a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a></b> reduces the task of finding an optimal course of action to choosing what to do at each step.  So, you&#8217;re trying to maximize the &#8216;total reward&#8217;&#8212;the sum of rewards at each time step&#8212;and Bellman&#8217;s equation says what to do at each time step.</p>
<p>If you&#8217;ve studied physics, this should remind you of how starting from the principle of least action, we can get a differential equation describing the motion of a particle: the <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation"><b>Euler&#8211;Lagrange equation</b></a>.</p>
<p>And in fact they&#8217;re deeply related.  The relation is obscured by two little things.  First, Bellman&#8217;s equation is usually formulated in a context where time passes in discrete steps, while the Euler&#8211;Lagrange equation is usually formulated in continuous time.  Second, Bellman&#8217;s equation is really the discrete-time version not of the Euler&#8211;Lagrange equation but a more or less equivalent thing: the &#8216;Hamilton&#8211;Jacobi equation&#8217;.</p>
<p>Ah, another buzzword to demystify!   I was scared of the Hamilton&#8211;Jacobi equation for years, until I taught <a href="http://math.ucr.edu/home/baez/classical/#lagrangian">a course on classical mechanics</a> that covered it.  Now I think it&#8217;s the greatest thing in the world!</p>
<p>Briefly: the Hamilton&#8211;Jacobi equation concerns the least possible action to get from a fixed starting point to a point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in space at time <img src="https://s0.wp.com/latex.php?latex=t.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t." class="latex" />  If we call this least possible action <img src="https://s0.wp.com/latex.php?latex=W%28t%2Cq%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W(t,q)," class="latex" /> the <a href="https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi_equation"><b>Hamilton&#8211;Jacobi equation</b></a> says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+W%28t%2Cq%29%7D%7B%5Cpartial+q_i%7D+%3D+p_i++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;partial W(t,q)}{&#92;partial q_i} = p_i  }" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+W%28t%2Cq%29%7D%7B%5Cpartial+t%7D+%3D+-E++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;partial W(t,q)}{&#92;partial t} = -E  }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is the particle&#8217;s momentum at the endpoint of its path, and <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> is its energy there.</p>
<p>If we replace derivatives by differences, and talk about <i>maximizing total reward</i> instead of <i>minimizing action</i>, we get Bellman&#8217;s equation:</p>
<p>&bull; <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>, Wikipedia.</p>
<h3> Markov decision processes </h3>
<p>Bellman&#8217;s equation can be useful whenever you&#8217;re trying to figure out an optimal course of action.  An important example is a &#8216;Markov decision process&#8217;.  To prepare you for Tishby&#8217;s talk, I should say what this is.</p>
<p>In a Markov process, something randomly hops around from state to state with fixed probabilities.  In the simplest case there&#8217;s a finite set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> of <b>states</b>, and time proceeds in discrete steps.  At each time step, the probability to hop from state <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> to state <img src="https://s0.wp.com/latex.php?latex=s%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s&#039;" class="latex" /> is some fixed number <img src="https://s0.wp.com/latex.php?latex=P%28s%2Cs%27%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(s,s&#039;)." class="latex" /></p>
<p>This sort of thing is called a <a href="https://en.wikipedia.org/wiki/Markov_chain"><b>Markov chain</b></a>, or if you feel the need to be more insistent, a <b>discrete-time Markov chain</b>.</p>
<p>A <b>Markov decision process</b> is a generalization where an outside agent gets to change these probabilities!  The agent gets to choose <b>actions</b> from some set <img src="https://s0.wp.com/latex.php?latex=A.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A." class="latex" />   If at a given time he chooses the action <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cin+A%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;in A," class="latex" /> the probability of the system hopping from state <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> to state <img src="https://s0.wp.com/latex.php?latex=s%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s&#039;" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=P_%5Calpha%28s%2Cs%27%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_&#92;alpha(s,s&#039;)." class="latex" />  Needless to say, these probabilities have to sum to one for any fixed <img src="https://s0.wp.com/latex.php?latex=s.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s." class="latex" /></p>
<p>That would already be interesting, but the real fun is that there&#8217;s also a <b>reward</b> <img src="https://s0.wp.com/latex.php?latex=R_%5Calpha%28s%2Cs%27%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R_&#92;alpha(s,s&#039;)." class="latex" />   This is a real number saying how much joy or misery the agent experiences if he does action <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> and the system hops from <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=s%27.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s&#039;." class="latex" /></p>
<p>The problem is to choose a <b>policy</b>&#8212;a function from states to actions&#8212;that maximizes the total expected reward over some period of time.  This is precisely the kind of thing Bellman&#8217;s equation is good for!</p>
<p>If you&#8217;re an economist you might also want to &#8216;discount&#8217; future rewards, saying that a reward <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> time steps in the future gets multiplied by <img src="https://s0.wp.com/latex.php?latex=%5Cgamma%5En%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma^n," class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Cgamma+%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &lt; &#92;gamma &#92;le 1" class="latex" /> is some <b>discount factor</b>.  This extra tweak is easily handled, and you can see it all here:</p>
<p>&bull; <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a>, Wikipedia.</p>
<h3> Partially observable Markov decision processes </h3>
<p>There&#8217;s a further generalization where the agent can&#8217;t see all the details of the system!  Instead, when he takes an action <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cin+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;in A" class="latex" /> and the system hops from state <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> to state <img src="https://s0.wp.com/latex.php?latex=s%27%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s&#039;," class="latex" /> he sees something: a point in some set <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> of <b>observations</b>.  He makes the observation <img src="https://s0.wp.com/latex.php?latex=o+%5Cin+O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="o &#92;in O" class="latex" /> with probability <img src="https://s0.wp.com/latex.php?latex=%5COmega_%5Calpha%28o%2Cs%27%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega_&#92;alpha(o,s&#039;)." class="latex" /></p>
<p>(I don&#8217;t know why this probability depends on <img src="https://s0.wp.com/latex.php?latex=s%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s&#039;" class="latex" /> but not <img src="https://s0.wp.com/latex.php?latex=s.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s." class="latex" />  Maybe it ultimately doesn&#8217;t matter much.)</p>
<p>Again, the goal is to choose a <b>policy</b> that maximizes the expected total reward.  But a policy is a bit different now.  The action at any time can only depend on all the observations made thus far.</p>
<p>Partially observable Markov decision processes are also called <b>POMPD</b>s.   If you want to learn about them, try these:</p>
<p>&bull; <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">Partially observable Markov decision process</a>, Wikipedia.</p>
<p>&bull; Tony Cassandra, <a href="http://www.pomdp.org/index.shtml">Partially observable Markov decision processes</a>.</p>
<p>The latter includes an introduction <i>without any formulas</i> to POMDPs and how to choose optimal policies.  I&#8217;m not sure who would study this subject and not want to see formulas, but it&#8217;s certainly a good exercise to explain things using just words&#8212;and it makes certain things easier to understand (though not others, in a way that depends on who is trying to learn the stuff).</p>
<h3> The action-perception loop </h3>
<p>I already explained the action-perception loop, with the help of this picture from the University of Bielefeld&#8217;s Department of Cognitive Neuroscience:</p>
<div align="center">
<a href="http://www.uni-bielefeld.de/biologie/cns/"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/action-perception_loop.jpg" /></a>
</div>
<p>Nafthali Tishby has a nice picture of it that&#8217;s more abstract:</p>
<div align="center">
<img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/action-perception_loop_tishby.jpg" />
</div>
<p>We&#8217;re assuming time comes in discrete steps, just to keep things simple.</p>
<p>At each time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /></p>
<p>&bull; the <b>world</b> has some state <img src="https://s0.wp.com/latex.php?latex=W_t%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W_t," class="latex" /> and<br />
&bull; the <b>agent</b> has some state <img src="https://s0.wp.com/latex.php?latex=M_t.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M_t." class="latex" /></p>
<p>Why the letter <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />?  This stands for <b>memory</b>: it can be the state of the agent&#8217;s memory, but I prefer to think of it as the state of the agent.</p>
<p>At each time</p>
<p>&bull; the agent takes an <b>action</b> <img src="https://s0.wp.com/latex.php?latex=A_t%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_t," class="latex" /> which affects the world&#8217;s next state, and</p>
<p>&bull; the world provides a <b>sensation</b> <img src="https://s0.wp.com/latex.php?latex=S_t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_t" class="latex" /> to the agent, which affect&#8217;s the agent&#8217;s next state.</p>
<p>This is simplified but very nice.  Note that there&#8217;s a symmetry interchanging the world and the agent!</p>
<p>We could make it fancier by having lots of agents who all interact, but there are a lot of questions already.  The big question Tishby focuses on is optimizing how much the agent should remember about the past if they</p>
<p>&bull; get a reward depending on the action taken and the resulting state of the world</p>
<p>but</p>
<p>&bull; pay a price for the information stored from sensations.</p>
<p>Tishby formulates this optimization question as something like a partially observed Markov decision process, uses rate-distortion theory to analyze how much information needs to be stored to achieve a given reward, and uses Bellman&#8217;s equation to solve the optimization problem!  <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/surprised.gif" /></p>
<p>So, everything I sketched fits together somehow!</p>
<p>I hope what I&#8217;m saying now is roughly right: it will take me more time to get the details straight.    If you&#8217;re having trouble absorbing all the information I just threw at you, don&#8217;t feel bad: so am I.  But the math feels really natural and good to me.  It involves a lot of my favorite ideas (like generalizations of the principle of least action, and relative entropy), and it seems ripe to be combined with network theory ideas.</p>
<p>For details, I highly recommend this paper:</p>
<p>&bull; Naftali Tishby and Daniel Polani, <a href="http://www.cs.huji.ac.il/labs/learning/Papers/IT-PAC.pdf">Information theory of decisions and actions</a>, in <i>Perception-Reason-Action Cycle: Models, Algorithms and System</i>. Vassilis, Hussain and Taylor, Springer, Berlin, 2010.</p>
<p>I&#8217;m going to print this out, put it by my bed, and read it every night until I&#8217;ve absorbed it.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/10/30/sensing-and-acting-under-information-constraints/#comments">13 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/10/30/sensing-and-acting-under-information-constraints/" rel="bookmark" title="Permanent Link to Sensing and Acting Under Information&nbsp;Constraints">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-18924 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy" id="post-18924">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/10/27/biodiversity-entropy-and-thermodynamics/" rel="bookmark">Biodiversity, Entropy and&nbsp;Thermodynamics</a></h2>
				<small>27 October, 2014</small><br />


				<div class="entry">
					<p>&nbsp;</p>
<p><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/bio_info/408px-Forest_fruits_from_Barro_Colorado.jpg" /></p>
<p>I&#8217;m giving a short 30-minute talk at a workshop on <a href="https://johncarlosbaez.wordpress.com/2014/01/31/bio-inspired-information-theory/"> Biological and Bio-Inspired Information Theory</a> at the Banff International Research Institute.</p>
<p>I&#8217;ll say more about the workshop later, but here&#8217;s my talk, in PDF and video form:</p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/bio_info/">Biodiversity, entropy and thermodynamics</a>.</p>
<p>Most of the people at this workshop study neurobiology and cell signalling, not evolutionary game theory or biodiversity.  So, the talk is just a quick intro to some things we&#8217;ve seen before here.  Starting from scratch, I derive the Lotka&#8211;Volterra equation describing how the distribution of organisms of different species changes with time.  Then I use it to prove a version of the Second Law of Thermodynamics.</p>
<p>This law says that if there is a &#8216;dominant distribution&#8217;&#8212;a distribution of species whose mean fitness is at least as great as that of any population it finds itself amidst&#8212;then as time passes, the information <i>any</i> population has &#8216;left to learn&#8217; always decreases!</p>
<p>Of course reality is more complicated, but this result is a good start.</p>
<p>This was proved by Siavash Shahshahani in 1979.  For more, see:</p>
<p>&bull; Lou Jost, <a href="http://www.loujost.com/Statistics%20and%20Physics/Diversity%20and%20Similarity/JostEntropy%20AndDiversity.pdf">Entropy and diversity</a>.</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>and <a href="http://arxiv.org/find/math/1/au:+Harper\_M/0/1/0/all/0/1">more recent papers by Harper</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/10/27/biodiversity-entropy-and-thermodynamics/#comments">3 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/10/27/biodiversity-entropy-and-thermodynamics/" rel="bookmark" title="Permanent Link to Biodiversity, Entropy and&nbsp;Thermodynamics">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-17569 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics" id="post-17569">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/" rel="bookmark">Relative Entropy (Part&nbsp;4)</a></h2>
				<small>16 February, 2014</small><br />


				<div class="entry">
					<p><img width="450" src="https://i2.wp.com/sciphilos.info/docs_pages/images/Entropy%20cartoon%20lg.jpg" /></p>
<p>In recent posts by <a href="https://johncarlosbaez.wordpress.com/2014/01/07/lyapunov-functions-for-complex-balanced-systems/">Manoj Gopalkrishnan</a> and <a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/">Marc Harper</a> we&#8217;ve seen how not just <i>entropy</i> but <i><a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">relative entropy</a></i>&#8212;the entropy of a probability distribution relative to the equilibrium distribution&#8212;is a driving force in chemistry and evolution.  Now Tobias Fritz and I have finally finished our paper on this subject:</p>
<p>&bull; John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>, <i><a href="http://www.tac.mta.ca/tac/volumes/29/16/29-16abs.html">Theory and Applications of Categories</a></i> <b>29</b> (2014), 421&ndash;456.</p>
<p>Very roughly, we proved that any reasonable measure of the information you gain when you to update your assumptions about the world based on discovering what a system is really doing must be some constant <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> times the relative entropy.</p>
<p>I&#8217;ve blogged about this here before:</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy (Part 1)</a>: how various structures important in probability theory arise naturally when you do linear algebra using only the nonnegative real numbers.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Relative Entropy (Part 2)</a>: a category related to statistical inference, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and how relative entropy defines a functor on this category.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Relative Entropy (Part 3)</a>: statement of our main theorem, which characterizes relative entropy up to a constant multiple as the only functor <img src="https://s0.wp.com/latex.php?latex=F+%3A+%5Cmathrm%7BFinStat%7D+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F : &#92;mathrm{FinStat} &#92;to [0,&#92;infty)" class="latex" /> with a few nice properties.</p>
<p>Now that the paper is done, we&#8217;re having a nice <a href="http://golem.ph.utexas.edu/category/2014/02/relative_entropy.html">conversation about it on the <i>n</i>-Category Caf&eacute;</a>.  Since I don&#8217;t want to splinter the conversation, I won&#8217;t enable comments here&#8212;please go there and join the fun!</p>
<p>But having blogged about this thrice before, what&#8217;s new?</p>
<p>One thing is that our conversation is getting more deeply into the category-theoretic aspects.  Read the long parenthetical remarks in my post on the <i>n</i>-Caf&eacute; to get up to speed on that aspect.</p>
<p>Another is that by looking at our paper, you can actually see the <i>proof</i> of our result.  As I mention on the <i>n</i>-Caf&eacute;.</p>
<blockquote>
<p>The proof is surprisingly hard. Or maybe we’re just surprisingly bad at proving things. But the interesting thing is this: the proof is swift and effective in the ‘generic’ case&#8212;the case where the support of the probability measures involved is the whole set they’re living on, and the constant <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> is finite.</p>
<p>It takes some more work to handle the case where the probability measures have smaller support.</p>
<p>But the really hard work starts when we handle the case that, in the end, has <img src="https://s0.wp.com/latex.php?latex=c+%3D+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c = &#92;infty." class="latex" /> Then the proof becomes more like analysis than what you normally expect in category theory. We slowly corner the result, blocking off all avenues of escape. Then we close in, grab its neck, and strangle it, crushing its larynx ever tighter, as it loses the will to fight back and finally expires&#8230; still twitching.
</p></blockquote>
<p>We haven&#8217;t gotten into discussing this much yet, perhaps because the mathematicians on the <i>n</i>-Caf&eacute; are too dainty and civilized.   But someone into analysis might be able to find a more efficient proof.</p>
<p>That would make me a bit sad&#8212;since why didn&#8217;t <i>we</i> find it?&#8212;but mainly happy&#8212;since this subject deserves to be clean and elegant.  We really need a category-theoretic formulation of the second law of thermodynamics that&#8217;s suitable for studying complex networks: that&#8217;s the long-term goal here.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/#comments">1 Comment</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/" rel="bookmark" title="Permanent Link to Relative Entropy (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-17311 post type-post status-publish format-standard hentry category-biology category-game-theory category-information-and-entropy category-mathematics" id="post-17311">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/" rel="bookmark">Relative Entropy in Evolutionary&nbsp;Dynamics</a></h2>
				<small>22 January, 2014</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://www.marcallenharper.com/">Marc Harper</a></b></i></p>
<p>In John&#8217;s <a href="http://math.ucr.edu/home/baez/information/index.html">information geometry</a> series, he mentioned some of my work in evolutionary dynamics. Today I&#8217;m going to tell you about some exciting extensions!</p>
<h3> The replicator equation </h3>
<p>First a little refresher. For a population of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> replicating types, such as individuals with different eye colors or a gene with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> distinct <a href="http://en.wikipedia.org/wiki/Allele">alleles</a>, the &#8216;replicator equation&#8217; expresses the main idea of natural selection: the relative rate of growth of each type should be proportional to the difference between the fitness of the type and the mean fitness in the population.</p>
<p>To see why this equation should be true, let <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> be the population of individuals of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type, which we allow to be any nonnegative real number.  We can list all these numbers and get a vector:</p>
<p><img src="https://s0.wp.com/latex.php?latex=P+%3D+%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P = (P_1, &#92;dots, P_n) " class="latex" /></p>
<p>The <b>Lotka&ndash;Volterra equation</b> is a very general rule for how these numbers can change with time:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P%29+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P) P_i } " class="latex" /></p>
<p>Each population grows at a rate proportional to itself, where the &#8216;constant of proportionality&#8217;, <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)," class="latex" /> is not necessarily constant: it can be any real-valued function of <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" />  This function is called the <b>fitness</b> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type.  Taken all together, these functions <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> are called the <b>fitness landscape</b>.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> be the fraction of individuals who are of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_%7Bi+%3D1%7D%5En+P_i+%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_{i =1}^n P_i } }" class="latex" /></p>
<p>These numbers <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are between 0 and 1, and they add up to 1.  So, we can also think of them as probabilities: <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the probability that a randomly chosen individual is of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type.  This is how probability theory, and eventually entropy, gets into the game.</p>
<p>Again, we can bundle these numbers into a vector:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28p_1%2C+%5Cdots%2C+p_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (p_1, &#92;dots, p_n) " class="latex" /></p>
<p>which we call the <b>population distribution</b>.  It turns out that the Lotka&ndash;Volterra equation implies the <b>replicator equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left( f_i(P) - &#92;langle f(P) &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_%7Bi+%3D1%7D%5En++f_i%28P%29++p_i++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_{i =1}^n  f_i(P)  p_i  } " class="latex" /></p>
<p>is the <b>mean fitness</b> of all the individuals.  You can see the proof in <a href="http://math.ucr.edu/home/baez/information/information_geometry_9.html">Part 9</a> of the information geometry series.</p>
<p>By the way: if each fitness <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)" class="latex" /> only depends on the fraction of individuals of each type, not the total numbers, we can write the replicator equation in a simpler way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i%28p%29+-+%5Clangle+f%28p%29+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left( f_i(p) - &#92;langle f(p) &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>From now on, when talking about this equation, that&#8217;s what I&#8217;ll do.</p>
<p>Anyway, the take-home message is this: the replicator equation says the fraction of individuals of any type changes at a rate proportional to fitness of that type minus the mean fitness.</p>
<p>Now, it has been known since the late 1970s or early 1980s, thanks to the work of Akin, Bomze, Hofbauer, Shahshahani, and others, that the replicator equation has some very interesting properties.   For one thing, it often makes &#8216;relative entropy&#8217; decrease.   For another, it&#8217;s often an example of &#8216;gradient flow&#8217;.   Let&#8217;s look at both of these in turn, and then talk about some new generalizations of these facts.</p>
<h3> Relative entropy as a Lyapunov function </h3>
<p>I mentioned that we can think of a population distribution as a probability distribution.  This lets us take ideas from probability theory and even information theory and apply them to evolutionary dynamics!  For example, given two population distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> the <b>information</b> of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> <b>relative to</b> <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Cdisplaystyle%7B+%5Csum_i+q_i+%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i+%7D%5Cright%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;displaystyle{ &#92;sum_i q_i &#92;ln &#92;left(&#92;frac{q_i}{p_i }&#92;right)} " class="latex" /></p>
<p>This measures how much information you gain if you have a hypothesis about some state of affairs given by the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> and then someone tells you &#8220;no, the best hypothesis is <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />!”</p>
<p>It may seem weird to treat a <i>population distribution</i> as a <i>hypothesis</i>, but this turns out to be a good idea.  Evolution can then be seen as a learning process: a process of improving the hypothesis.</p>
<p>We can make this precise by seeing how the relative information changes with the passage of time.  Suppose we have two population distributions <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />   Suppose <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is fixed, while <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> evolves in time according to the replicator equation.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bd+t%7D+I%28q%2Cp%29++%3D++%5Csum_i+f_i%28P%29+%28p_i+-+q_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d}{d t} I(q,p)  =  &#92;sum_i f_i(P) (p_i - q_i) } " class="latex" /></p>
<p>For the proof, see <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/">Part 11</a> of the information geometry series.</p>
<p>So, the information of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> will decrease as <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> evolves according to the replicator equation if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Csum_i+f_i%28P%29+%28p_i+-+q_i%29+%7D+%5Cle+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;sum_i f_i(P) (p_i - q_i) } &#92;le 0 " class="latex" /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> makes this true for all <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> we say <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an <b>evolutionarily stable state</b>.   For some reasons why, see <a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/">Part 13</a>.</p>
<p>What matters now is that when <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an evolutionarily stable state, <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> says how much information the population has &#8216;left to learn&#8217;&mdash;and we&#8217;re seeing that this always <i>decreases</i>.  Moreover, it turns out that we always have</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) &#92;ge 0" class="latex" /></p>
<p>and <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = 0" class="latex" /> precisely when <img src="https://s0.wp.com/latex.php?latex=p+%3D+q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = q." class="latex" /></p>
<p>People summarize all this by saying that relative information is a &#8216;Lyapunov function&#8217;.  Very roughly, a Lyapunov function is something that decreases with the passage of time, and is zero only at the unique stable state.   To be a bit more precise, suppose we have a differential equation like</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bd+t%7D+x%28t%29+%3D+v%28x%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d}{d t} x(t) = v(x(t)) } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=x%28t%29+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) &#92;in &#92;mathbb{R}^n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> is some smooth vector field on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n." class="latex" />  Then a smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=V+%3A+%5Cmathbb%7BR%7D%5En+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V : &#92;mathbb{R}^n &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>is a <b>Lyapunov function</b> if</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V%28x%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x) &#92;ge 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V%28x%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x) = 0" class="latex" /> iff <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is some particular point <img src="https://s0.wp.com/latex.php?latex=x_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_0" class="latex" /></p>
<p>and</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+V%28x%28t%29%29+%5Cle+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} V(x(t)) &#92;le 0 }" class="latex" /> for every solution of our differential equation.</p>
<p>In this situation, the point <img src="https://s0.wp.com/latex.php?latex=x_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_0" class="latex" /> is a stable equilibrium for our differential equation: this is <b><a href="http://control.ee.ethz.ch/~ifalst/docs/lyapunov.pdf">Lyapunov&#8217;s theorem</a></b>.</p>
<h3> The replicator equation as a gradient flow equation </h3>
<p>The basic idea of Lyapunov&#8217;s theorem is that when a ball likes to roll downhill and the landscape has just one bottom point, that point will be the unique stable equilibrium for the ball.</p>
<p>The idea of gradient flow is similar, but different: sometimes things like to roll downhill <i>as efficiently as possible</i>: they move in the exactly the <i>best direction</i> to make some quantity smaller!  Under certain conditions, the replicator equation is an example of this phenomenon.</p>
<p>Let&#8217;s fill in some details.  For starters, suppose we have some function</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3A+%5Cmathbb%7BR%7D%5En+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F : &#92;mathbb{R}^n &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>Think of <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> as &#8216;height&#8217;.  Then the <b>gradient flow equation</b> says how a point <img src="https://s0.wp.com/latex.php?latex=x%28t%29+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) &#92;in &#92;mathbb{R}^n" class="latex" /> will move if it&#8217;s always trying its very best to go downhill:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+x%28t%29+%3D+-+%5Cnabla+V%28x%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} x(t) = - &#92;nabla V(x(t)) } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla" class="latex" /> is the usual gradient in Euclidean space:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cnabla+V+%3D+%5Cleft%28%5Cpartial_1+V%2C+%5Cdots%2C+%5Cpartial_n+V+%5Cright%29++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;nabla V = &#92;left(&#92;partial_1 V, &#92;dots, &#92;partial_n V &#92;right)  } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i" class="latex" /> is short for the partial derivative with respect to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate.</p>
<p>The interesting thing is that under certain conditions, the replicator equation is an example of a gradient flow equation&#8230; but typically <i>not</i> one where <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla" class="latex" /> is the usual gradient in Euclidean space.  Instead, it&#8217;s the gradient on some other space, the space of all population distributions, which has a non-Euclidean geometry!</p>
<p>The space of all population distributions is a <a href="https://en.wikipedia.org/wiki/Simplex">simplex</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7B+p+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+p_i+%5Cge+0%2C+%5C%3B+%5Csum_%7Bi+%3D+1%7D%5En+p_i+%3D+1+%5C%7D+.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{ p &#92;in &#92;mathbb{R}^n : &#92;; p_i &#92;ge 0, &#92;; &#92;sum_{i = 1}^n p_i = 1 &#92;} . " class="latex" /></p>
<p>For example, it&#8217;s an equilateral triangle when <img src="https://s0.wp.com/latex.php?latex=n+%3D+3.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 3." class="latex" />  The equilateral triangle looks flat, but if we measure distances another way it becomes round, exactly like a portion of a sphere, and that&#8217;s the non-Euclidean geometry we need!</p>
<div align="center"><img width="400" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/simplex_eighth-sphere.png" /></div>
<p>In fact this trick works in any dimension.  The idea is to give the simplex a special <a href="https://en.wikipedia.org/wiki/Metric_tensor">Riemannian metric</a>, the &#8216;Fisher information metric&#8217;.  The usual metric on Euclidean space is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7Bi+j%7D+%3D+%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bccl%7D+1+%26+%5Cmathrm%7B+if+%7D+%26+i+%3D+j+%5C%5C+++++++++++++++++++++++++++++++++++++++0+%26%5Cmathrm%7B+if+%7D+%26+i+%5Cne+j+%5Cend%7Barray%7D+%5Cright.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta_{i j} = &#92;left&#92;{&#92;begin{array}{ccl} 1 &amp; &#92;mathrm{ if } &amp; i = j &#92;&#92;                                       0 &amp;&#92;mathrm{ if } &amp; i &#92;ne j &#92;end{array} &#92;right. " class="latex" /></p>
<p>This simply says that two standard basis vectors like <img src="https://s0.wp.com/latex.php?latex=%280%2C1%2C0%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0,1,0,0)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%280%2C0%2C1%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0,0,1,0)" class="latex" /> have dot product zero if the 1&#8217;s are in different places, and one if they&#8217;re in the same place.  The <b>Fisher information metric</b> is a bit more complicated:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+g_%7Bi+j%7D+%3D+%5Cfrac%7B%5Cdelta_%7Bi+j%7D%7D%7Bp_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ g_{i j} = &#92;frac{&#92;delta_{i j}}{p_i} } " class="latex" /></p>
<p>As before, <img src="https://s0.wp.com/latex.php?latex=g_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{i j}" class="latex" /> is a formula for the dot product of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th and <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th standard basis vectors, but now it depends on where you are in the simplex of population distributions.</p>
<p>We saw how this formula arises from information theory back in <a href="http://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>.  I won&#8217;t repeat the calculation, but the idea is this.  Fix a population distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and consider the information of another one, say <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> relative to this.  We get <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)." class="latex" />  If <img src="https://s0.wp.com/latex.php?latex=q+%3D+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = p" class="latex" /> this is zero:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.+I%28q%2Cp%29%5Cright%7C_%7Bq+%3D+p%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left. I(q,p)&#92;right|_{q = p} = 0 } " class="latex" /></p>
<p>and this point is a local minimum for the relative information. So, the first derivative of <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> as we change <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> must be zero:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+I%28q%2Cp%29+%5Cright%7C_%7Bq+%3D+p%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left. &#92;frac{&#92;partial}{&#92;partial q_i} I(q,p) &#92;right|_{q = p} = 0 } " class="latex" /></p>
<p>But the second derivatives are not zero. In fact, since we&#8217;re at a local minimum, it should not be surprising that we get a positive definite <a href="https://en.wikipedia.org/wiki/Hessian_matrix">matrix of second derivatives</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++g_%7Bi+j%7D+%3D+%5Cleft.+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+q_i+%5Cpartial+q_j%7D+I%28q%2Cp%29+%5Cright%7C_%7Bq+%3D+p%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  g_{i j} = &#92;left. &#92;frac{&#92;partial^2}{&#92;partial q_i &#92;partial q_j} I(q,p) &#92;right|_{q = p} = 0 } " class="latex" /></p>
<p>And, this is the Fisher information metric!  So, the Fisher information metric is a way of taking dot products between vectors in the simplex of population distribution that&#8217;s based on the concept of relative information.</p>
<p>This is not the place to explain <a href="https://en.wikipedia.org/wiki/Riemannian_geometry">Riemannian geometry</a>, but any metric gives a way to measure angles and distances, and thus a way to define the gradient of a function.  After all, the gradient of a function should point at right angles to the level sets of that function, and its length should equal the slope of that function:</p>
<div align="center">
<a href="http://www.math.udel.edu/~driscoll/teaching/243/maple/Gradients.html"><img src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/gradient.gif" /></a>
</div>
<p>So, if we change our way of measuring angles and distances, we get a new concept of gradient!  The <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th component of this new gradient vector field turns out to b</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5Cnabla_g+V%29%5Ei+%3D+g%5E%7Bi+j%7D+%5Cpartial_j+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;nabla_g V)^i = g^{i j} &#92;partial_j V" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=g%5E%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g^{i j}" class="latex" /> is the inverse of the matrix <img src="https://s0.wp.com/latex.php?latex=g_%7Bi+j%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{i j}," class="latex" /> and we sum over the repeated index <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />   As a sanity check, make sure you see why this is the usual Euclidean gradient when <img src="https://s0.wp.com/latex.php?latex=g_%7Bi+j%7D+%3D+%5Cdelta_%7Bi+j%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{i j} = &#92;delta_{i j}." class="latex" /></p>
<p>Now suppose the fitness landscape is the good old Euclidean gradient of some function.  Then it turns out that the replicator equation is a special case of gradient flow on the space of population distributions&#8230; but where we use the Fisher information metric to define our concept of gradient!</p>
<p>To get a feel for this, it&#8217;s good to start with the Lotka&ndash;Volterra equation, which describes how the total number of individuals of each type changes.  Suppose the fitness landscape is the Euclidean gradient of some function <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f_i%28P%29+%3D+%5Cfrac%7B%5Cpartial+V%7D%7B%5Cpartial+P_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f_i(P) = &#92;frac{&#92;partial V}{&#92;partial P_i} } " class="latex" /></p>
<p>Then the Lotka&ndash;Volterra equation becomes this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+%5Cfrac%7B%5Cpartial+V%7D%7B%5Cpartial+P_i%7D+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = &#92;frac{&#92;partial V}{&#92;partial P_i} &#92;, P_i } " class="latex" /></p>
<p>This doesn&#8217;t look like the gradient flow equation, thanks to that annoying <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> on the right-hand side!  It certainly ain&#8217;t the gradient flow coming from the function <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> and the usual Euclidean gradient.  However, it <i>is</i> gradient flow coming from <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> and some <i>other</i> metric on the space</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7B+P+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+P_i+%5Cge+0+%5C%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{ P &#92;in &#92;mathbb{R}^n : &#92;; P_i &#92;ge 0 &#92;}  " class="latex" /></p>
<p>For a proof, and the formula for this other metric, see Section 3.7 in this survey:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>Now let&#8217;s turn to the replicator equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i%28p%29++-+%5Clangle+f%28p%29+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left( f_i(p)  - &#92;langle f(p) &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>Again, if the fitness landscape is a Euclidean gradient, we can rewrite the replicator equation as a gradient flow equation&#8230; but again, not with respect to the Euclidean metric.  This time we need to use the Fisher information metric! I sketch a proof in my paper above.</p>
<p>In fact, both these results were first worked out by Shahshahani:</p>
<p>&bull; Siavash Shahshahani, <i>A New Mathematical Framework for the Study of Linkage and Selection</i>, <i>Memoirs of the AMS</i> <b>17</b>, 1979.</p>
<h3> New directions </h3>
<p>All this is just the beginning!  The ideas I just explained are unified in information geometry, where distance-like quantities such as the relative entropy and the Fisher information metric are studied.  From here it&#8217;s a short walk to a very nice version of <a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection">Fisher&#8217;s fundamental theorem of natural selection</a>, which is familiar to researchers both in evolutionary dynamics and in information geometry.</p>
<p>You can see some very nice versions of this story for maximum likelihood estimators and linear programming here:</p>
<p>&bull; Akio Fujiwara and Shun-ichi Amari, Gradient systems in view of information geometry, <a href="http://www.sciencedirect.com/science/article/pii/016727899400175P"><i>Physica D: Nonlinear Phenomena</i></a> <b>80</b> (1995), 317&ndash;327.</p>
<p>Indeed, this seems to be the first paper discussing the similarities between evolutionary game theory and information geometry.</p>
<p>Dash Fryer (at Pomona College) and I have generalized this story in several interesting ways.</p>
<p>First, there are two famous ways to generalize the usual formula for entropy: <a href="http://en.wikipedia.org/wiki/Tsallis_entropy">Tsallis entropy</a> and <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a>, both of which involve a parameter <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  There are Tsallis and R&eacute;nyi versions of relative entropy and the Fisher information metric as well.  Everything I just explained about:</p>
<p>&bull; conditions under which relative entropy is a Lyapunov function for the replicator equation, and</p>
<p>&bull; conditions under which the replicator equation is a special case of gradient flow</p>
<p>generalize to these cases!  However, these generalized entropies give modified versions of the replicator equation.  When we set <img src="https://s0.wp.com/latex.php?latex=q%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q=1" class="latex" /> we get back the usual story.  See</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1764">Escort evolutionary game theory</a>.</p>
<p>My initial interest in these alternate entropies was mostly mathematical&#8212;what is so special about the corresponding geometries?&#8212;but now researchers are starting to find populations that evolve according to these kinds of modified population dynamics!  For example:</p>
<p>&bull; A. Hernando <i>et al</i>, <a href="http://arxiv.org/abs/1201.0905">The workings of the Maximum Entropy Principle in collective human behavior</a>.</p>
<p>There&#8217;s an interesting special case worth some attention. Lots of people fret about the relative entropy not being a distance function obeying the axioms that mathematicians like: for example, it doesn&#8217;t obey the triangle inequality.   Many describe the relative entropy as a <i>distance-like</i> function, and this is often a valid interpretation contextually.  On the other hand, the <img src="https://s0.wp.com/latex.php?latex=q%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q=0" class="latex" /> relative entropy is one-half the Euclidean distance squared!  In this case the modified version of the replicator equation looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+f_i%28p%29+-+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bj+%3D+1%7D%5En+f_j%28p%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = f_i(p) - &#92;frac{1}{n} &#92;sum_{j = 1}^n f_j(p) } " class="latex" /></p>
<p>This equation is called the <b>projection dynamic</b>.</p>
<p>Later, I showed that there is a reasonable definition of relative entropy for a much larger family of geometries that satisfies a similar <i>distance minimization</i> property.</p>
<p>In a different direction, Dash showed that you can change the way that selection acts by using a variety of alternative ‘incentives’, extending the story to some other well-known equations describing evolutionary dynamics. By replacing the terms <img src="https://s0.wp.com/latex.php?latex=x_i+f_i%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i f_i(x)" class="latex" /> in the replicator equation with a variety of other functions, called <b>incentives</b>, we can generate many commonly studied models of evolutionary dynamics. For instance if we exponentiate the fitness landscape (to make it always positive), we get what is commonly known as the <b>logit dynamic</b>.  This amounts to changing the fitness landscape as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f_i+%5Cmapsto+%5Cfrac%7Bx_i+e%5E%7B%5Cbeta+f_i%7D%7D%7B%5Csum_j%7Bx_j+e%5E%7B%5Cbeta+f_j%7D%7D%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f_i &#92;mapsto &#92;frac{x_i e^{&#92;beta f_i}}{&#92;sum_j{x_j e^{&#92;beta f_j}}} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is known as an <b>inverse temperature</b> in statistical thermodynamics and as an <b>intensity of selection</b> in evolutionary dynamics. There are lots of modified versions of the replicator equation, like the best-reply and projection dynamics, more common in economic applications of evolutionary game theory, and they can all be captured in this family. (There are also other ways to simultaneously capture such families, such as Bill Sandholm&#8217;s revision protocols, which were introduced earlier in his exploration of the foundations of game dynamics.)</p>
<p>Dash showed that there is a natural generalization of evolutionarily stable states to &#8216;incentive stable states&#8217;, and that for incentive stable states, the relative entropy is decreasing to zero when the trajectories get near the equilibrium. For the logit and projection dynamics, incentive stable states are simply evolutionarily stable states, and this happens frequently, but not always.</p>
<p>The third generalization is to look at different &#8216;time-scales&#8217;&#8212;that is, different ways of describing time!  We can make up the symbol <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T}" class="latex" /> for a general choice of &#8216;time-scale&#8217;.  So far I&#8217;ve been treating time as a real number, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R} " class="latex" /></p>
<p>But we can also treat time as coming in discrete evenly spaced steps, which amounts to treating time as an integer:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BZ%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{Z} " class="latex" /></p>
<p>More generally, we can make the steps have duration <img src="https://s0.wp.com/latex.php?latex=h%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h," class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=h&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h" class="latex" /> is any positive real number:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z} " class="latex" /></p>
<p>There is a nice way to simultaneously describe the cases <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z}" class="latex" /> using the <a href="http://en.wikipedia.org/wiki/Time-scale_calculus">time-scale calculus</a> and time-scale derivatives. For the time-scale <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R}" class="latex" /> the time-scale derivative is just the ordinary derivative. For the time-scale <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z}," class="latex" /> the time-scale derivative is given by the difference quotient from first year calculus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%5E%7B%5CDelta%7D%28z%29+%3D+%5Cfrac%7Bf%28z%2Bh%29+-+f%28z%29%7D%7Bh%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f^{&#92;Delta}(z) = &#92;frac{f(z+h) - f(z)}{h} } " class="latex" /></p>
<p>and using this as a substitute for the derivative gives difference equations like a discrete-time version of the replicator equation.  There are many other choices of time-scale, such as the <b>quantum time-scale</b> given by <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+q%5E%7B%5Cmathbb%7BZ%7D%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = q^{&#92;mathbb{Z}}," class="latex" /> in which case the time-scale derivative is called the <a href="https://en.wikipedia.org/wiki/Q-derivative"><i>q</i>-derivative</a>, but that&#8217;s a tale for another time.  In any case, the fact that the successive relative entropies are decreasing can be simply state by saying they have negative <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z}" class="latex" /> time-scale derivative.   The continuous case we started with corresponds to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R}." class="latex" /></p>
<p>Remarkably, Dash and I were able to show that you can combine all three of these generalizations into one theorem, and even allow for multiple interacting populations! This produces some really neat population trajectories, such as the following two populations with three types, with fitness functions corresponding to the <a href="https://en.wikipedia.org/wiki/Rock-paper-scissors#Rock-paper-scissors_analogies_in_nature">rock-paper-scissors game</a>. On top we have the replicator equation, which goes along with the Fisher information metric; on the bottom we have the logit dynamic, which goes along with the Euclidean metric on the simplex:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_8_1.png" width="300" /></p>
<p><img src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_8_2.png" width="300" />
</div>
<p>From our theorem, it follows that the relative entropy (ordinary relative entropy on top, the <img src="https://s0.wp.com/latex.php?latex=q+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 0" class="latex" /> entropy on bottom) converges to zero along the population trajectories.</p>
<p>The final form of the theorem is loosely as follows. Pick a Riemannian geometry given by a metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> (obeying some mild conditions) and an incentive for each population, as well as a time scale (<img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=h+%5Cmathbb%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h &#92;mathbb{Z}" class="latex" />) for every population. This gives an evolutionary dynamic with a natural generalization of evolutionarily stable states, and a suitable version of the relative entropy.   Then, if there is an evolutionarily stable state in the interior of the simplex, the time-scale derivative of sum of the relative entropies for each population will decrease as the trajectories converge to the stable state!</p>
<p>When there isn&#8217;t such a stable state, we still get some interesting population dynamics, like the following:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_9_1.png" width="300" /><br />
<img src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_9_2.png" width="300" />
</div>
<p>See this paper for details:</p>
<p>&bull; Marc Harper and Dashiell E. A. Fryer, <a href="http://arxiv.org/abs/1210.5539">Stability of evolutionary dynamics on time scales</a>.</p>
<p>Next time we&#8217;ll see how to make the main idea work in finite populations, without derivatives or deterministic trajectories!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/#comments">30 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/" rel="category tag">game theory</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/" rel="bookmark" title="Permanent Link to Relative Entropy in Evolutionary&nbsp;Dynamics">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-17070 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics" id="post-17070">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/" rel="bookmark">Relative Entropy (Part&nbsp;3)</a></h2>
				<small>25 December, 2013</small><br />


				<div class="entry">
					<p>Holidays are great. There&#8217;s nothing I need to do! Everybody is celebrating! So, I can finally get some real work done.</p>
<p>In the last couple of days I&#8217;ve finished a paper with Jamie Vicary on wormholes and entanglement&#8230; subject to his approval and corrections. More on that later. And now I&#8217;ve returned to working on a paper with Tobias Fritz where we give a Bayesian characterization of the concept of &#8216;relative entropy&#8217;. This summer I wrote two blog articles about this paper.  But then Tobias Fritz noticed a big problem. Our characterization of relative entropy was inspired by this paper:</p>
<p>• D. Petz, <a href="http://www.renyi.hu/~petz/pdf/52.pdf">Characterization of the relative entropy of states of matrix algebras</a>, <i>Acta Math. Hungar. </i> <b>59</b> (1992), 449&#8211;455.</p>
<p>Here Petz sought to characterize relative entropy both in the &#8216;classical&#8217; case we are concerned with and in the more general &#8216;quantum&#8217; setting. Our original goal was merely to express his results in a more category-theoretic framework! Unfortunately Petz&#8217;s proof contained a significant flaw. Tobias noticed this and spent a lot of time fixing it, with no help from me.</p>
<p>Our paper is now self-contained, and considerably longer. My job now is to polish it up and make it pretty. What follows is the introduction, which should explain the basic ideas.</p>
<h3>A Bayesian characterization of relative entropy</h3>
<p>This paper gives a new characterization of the concept of relative entropy, also known as &#8216;relative information&#8217;, &#8216;information gain&#8217; or &#8216;Kullback-Leibler divergence&#8217;. Whenever we have two probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on the same finite set <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> we can define the entropy of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28q%2Cp%29+%3D+%5Csum_%7Bx%5Cin+X%7D+q_x+%5Cln%5Cleft%28+%5Cfrac%7Bq_x%7D%7Bp_x%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(q,p) = &#92;sum_{x&#92;in X} q_x &#92;ln&#92;left( &#92;frac{q_x}{p_x} &#92;right) }" class="latex" /></p>
<p>Here we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_x+%5Cln%5Cleft%28+%5Cfrac%7Bq_x%7D%7Bp_x%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_x &#92;ln&#92;left( &#92;frac{q_x}{p_x} &#92;right) }" class="latex" /></p>
<p>equal to <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;infty" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=p_x+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = 0," class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=q_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x" class="latex" /> is also zero, in which case we set it equal to 0. Relative entropy thus takes values in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /></p>
<p>Intuitively speaking, <img src="https://s0.wp.com/latex.php?latex=S%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q,p)" class="latex" /> is the expected amount of information gained when we discover the probability distribution is really <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> when we had thought it was <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /> We should think of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as a &#8216;prior&#8217; and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as a &#8216;posterior&#8217;. When we take <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to be the uniform distribution on <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> relative entropy reduces to the ordinary Shannon entropy, up to an additive constant. The advantage of relative entropy is that it makes the role of the prior explicit.</p>
<p>Since Bayesian probability theory emphasizes the role of the prior, relative entropy naturally lends itself to a Bayesian interpretation: it measures how much information we gain <i>given a certain prior</i>. Our goal here is to make this precise in a mathematical characterization of relative entropy. We do this using a category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> where:</p>
<p>• an object <img src="https://s0.wp.com/latex.php?latex=%28X%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,q)" class="latex" /> consists of a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and a probability distribution <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+q_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;mapsto q_x" class="latex" /> on that set;</p>
<p>• a morphism <img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /> consists of a measure-preserving function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> together with a probability distribution <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+s_%7Bx+y%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;mapsto s_{x y}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> for each element <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y," class="latex" /> with the property that <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy} = 0" class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y." class="latex" /></p>
<p>We can think of an object of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> as a system with some finite set of <b>states</b> together with a probability distribution on its states. A morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /></p>
<p>then consists of two parts. First, there is a deterministic <b>measurement process</b> <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;to Y" class="latex" /> mapping states of the system being measured, <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> to states of the measurement apparatus, <img src="https://s0.wp.com/latex.php?latex=Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y." class="latex" /> The condition that <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> be measure-preserving says that, after the measurement, the probability that the apparatus be in any state <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y" class="latex" /> is the sum of the probabilities of all states of <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> leading to that outcome:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+r_y+%3D+%5Csum_%7Bx+%5Cin+f%5E%7B-1%7D%28y%29%7D+q_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ r_y = &#92;sum_{x &#92;in f^{-1}(y)} q_x } " class="latex" /></p>
<p>Second, there is a <b>hypothesis</b> <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" />: an assumption about the probability <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy}" class="latex" /> that the system being measured is in the state <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> given any measurement outcome <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y." class="latex" /></p>
<p>Suppose we have any morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /></p>
<p>in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" /> From this we obtain two probability distributions on the states of the system being measured. First, we have the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Csum_%7By+%5Cin+Y%7D+s_%7Bxy%7D+r_y+%7D+%5Cqquad+%5Cqquad+%281%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;sum_{y &#92;in Y} s_{xy} r_y } &#92;qquad &#92;qquad (1) " class="latex" /></p>
<p>This is our <b>prior</b>, given our hypothesis and the probability distribution of measurement results. Second we have the &#8216;true&#8217; probability distribution <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> which would be the <b>posterior</b> if we updated our prior using complete direct knowledge of the system being measured.</p>
<p>It follows that any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> has a relative entropy <img src="https://s0.wp.com/latex.php?latex=S%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q,p)" class="latex" /> associated to it. This is the expected amount of information we gain when we update our prior <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to the posterior <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>In fact, this way of assigning relative entropies to morphisms defines a functor</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_0+%3A+%5Cmathrm%7BFinStat%7D+%5Cto+%5B0%2C%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0 : &#92;mathrm{FinStat} &#92;to [0,&#92;infty] " class="latex" /></p>
<p>where we use <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> to denote the category with one object, the numbers <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+x+%5Cle+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le x &#92;le &#92;infty" class="latex" /> as morphisms, and addition as composition. More precisely, if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /></p>
<p>is any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> we define</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_0%28f%2Cs%29+%3D+S%28q%2Cp%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0(f,s) = S(q,p) " class="latex" /></p>
<p>where the prior <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is defined as in the equation (1).</p>
<p>The fact that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is a functor is nontrivial and rather interesting. It says that given any composable pair of measurement processes:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28X%2Cq%29+%5Cstackrel%7B%28f%2Cs%29%7D%7B%5Clongrightarrow%7D+%28Y%2Cr%29+%5Cstackrel%7B%28g%2Ct%29%7D%7B%5Clongrightarrow%7D+%28Z%2Cu%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,q) &#92;stackrel{(f,s)}{&#92;longrightarrow} (Y,r) &#92;stackrel{(g,t)}{&#92;longrightarrow} (Z,u) " class="latex" /></p>
<p>the relative entropy of their composite is the sum of the relative entropies of the two parts:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_0%28%28g%2Ct%29+%5Ccirc+%28f%2Cs%29%29+%3D+F_0%28g%2Ct%29+%2B+F_0%28f%2Cs%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0((g,t) &#92;circ (f,s)) = F_0(g,t) + F_0(f,s) ." class="latex" /></p>
<p>We prove that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is a functor. However, we go much further: we <i>characterize</i> relative entropy by saying that up to a constant multiple, <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is the <i>unique</i> functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> obeying three reasonable conditions.</p>
<p>The first condition is that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> vanishes on morphisms <img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /> where the hypothesis <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is <b>optimal</b>. By this, we mean that Equation (1) gives a prior <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> equal to the &#8216;true&#8217; probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on the states of the system being measured.</p>
<p>The second condition is that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is lower semicontinuous. The set <img src="https://s0.wp.com/latex.php?latex=P%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(X)" class="latex" /> of probability distibutions on a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> naturally has the topology of an <img src="https://s0.wp.com/latex.php?latex=%28n-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(n-1)" class="latex" />-simplex when <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> elements. The set <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> has an obvious topology where it&#8217;s homeomorphic to a closed interval. However, with these topologies, the relative entropy does not define a continuous function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+S+%3A+P%28X%29+%5Ctimes+P%28X%29+%26%5Cto%26+%5B0%2C%5Cinfty%5D+%5C%5C+%28q%2Cp%29+%26%5Cmapsto+%26+S%28q%2Cp%29+.++%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{rcl} S : P(X) &#92;times P(X) &amp;&#92;to&amp; [0,&#92;infty] &#92;&#92; (q,p) &amp;&#92;mapsto &amp; S(q,p) .  &#92;end{array}" class="latex" /></p>
<p>The problem is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28q%2Cp%29+%3D+%5Csum_%7Bx%5Cin+X%7D+q_x+%5Cln%5Cleft%28+%5Cfrac%7Bq_x%7D%7Bp_x%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(q,p) = &#92;sum_{x&#92;in X} q_x &#92;ln&#92;left( &#92;frac{q_x}{p_x} &#92;right) } " class="latex" /></p>
<p>and we define <img src="https://s0.wp.com/latex.php?latex=q_x+%5Cln%28q_x%2Fp_x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x &#92;ln(q_x/p_x)" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;infty" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=p_x+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = 0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_x+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x &gt; 0" class="latex" /> but <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=p_x+%3D+q_x+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = q_x = 0." class="latex" /> So, it turns out that <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is only <b>lower semicontinuous</b>, meaning that if <img src="https://s0.wp.com/latex.php?latex=p%5Ei+%2C+q%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^i , q^i" class="latex" /> are sequences of probability distributions on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=p%5Ei+%5Cto+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^i &#92;to p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%5Ei+%5Cto+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i &#92;to q" class="latex" /> then</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28q%2Cp%29+%5Cle+%5Climinf_%7Bi+%5Cto+%5Cinfty%7D+S%28q%5Ei%2C+p%5Ei%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q,p) &#92;le &#92;liminf_{i &#92;to &#92;infty} S(q^i, p^i) " class="latex" /></p>
<p>We give the set of morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> its most obvious topology, and show that with this topology, <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> maps morphisms to morphisms in a lower semicontinuous way.</p>
<p>The third condition is that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is convex linear. We describe how to take convex linear combinations of morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and then the functor <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is convex linear in the sense that it maps any convex linear combination of morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to the corresponding convex linear combination of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /> Intuitively, this means that if we take a coin with probability <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> of landing heads up, and flip it to decide whether to perform one measurement process or another, the expected information gained is <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> times the expected information gain of the first process plus <img src="https://s0.wp.com/latex.php?latex=1-P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1-P" class="latex" /> times the expected information gain of the second process.</p>
<p>Here, then, is our main theorem:</p>
<p><b>Theorem.</b> Any lower semicontinuous, convex-linear functor</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3A+%5Cmathrm%7BFinStat%7D+%5Cto+%5B0%2C%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F : &#92;mathrm{FinStat} &#92;to [0,&#92;infty] " class="latex" /></p>
<p>that vanishes on every morphism with an optimal hypothesis must equal some constant times the relative entropy. In other words, there exists some constant <img src="https://s0.wp.com/latex.php?latex=c+%5Cin+%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;in [0,&#92;infty]" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f%2Cs%29+%3D+c+F_0%28f%2Cs%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f,s) = c F_0(f,s) " class="latex" /></p>
<p>for any any morphism <img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,p) &#92;to (Y,q)" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" /></p>
<h3>Addendum</h3>
<p>If you&#8217;re a maniacally thorough reader of this blog, with a photographic memory, you&#8217;ll recall that our theorem now says &#8216;lower semicontinuous&#8217;, where in <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Part 2</a> of this series I&#8217;d originally said &#8216;continuous&#8217;.</p>
<p>I&#8217;ve fixed that blog article now&#8230; but it was Tobias who noticed this mistake. In the process of fixing our proof to address this issue, he eventually noticed that the proof of Petz&#8217;s theorem, which we&#8217;d been planning to use in our work, was also flawed.</p>
<p>Here&#8217;s the paper we eventually wrote:</p>
<p>• John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>, <i><a href="http://www.tac.mta.ca/tac/volumes/29/16/29-16abs.html">Theory and Applications of Categories</a></i> <b>29</b> (2014), 421&ndash;456.</p>
<p>And here&#8217;s my whole series of blog articles about it:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy (Part 1)</a>: how various structures important in probability theory arise naturally when you do linear algebra using only the nonnegative real numbers.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Relative Entropy (Part 2)</a>: a category related to statistical inference, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and how relative entropy defines a functor on this category.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Relative Entropy (Part 3)</a>: how to characterize relative entropy as a functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /></p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/">Relative Entropy (Part 4)</a>: wrap-up, and an invitation to read more about the underlying math at the <a href="https://golem.ph.utexas.edu/category/2014/02/relative_entropy.html"><i>n</i>-Category Caf&eacute;</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comments">15 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/" rel="bookmark" title="Permanent Link to Relative Entropy (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16333 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-probability" id="post-16333">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/" rel="bookmark">Relative Entropy (Part&nbsp;2)</a></h2>
				<small>2 July, 2013</small><br />


				<div class="entry">
					<p>In the <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">first part</a> of this mini-series, I describe how various ideas important in probability theory arise naturally when you start doing linear algebra using only the nonnegative real numbers.</p>
<p>But after writing it, I got an email from a rather famous physicist saying he got &#8220;lost at line two&#8221;.  So, you&#8217;ll be happy to hear that the first part is <i>not a prerequisite</i> for the remaining parts!  I wrote it just to intimidate that guy.</p>
<p>Tobias Fritz and I have proved a theorem characterizing the concept of <a href="http://math.ucr.edu/home/baez/information/information_geometry_6.html">relative entropy</a>, which is also known as &#8216;relative information&#8217;, &#8216;information gain&#8217; or&#8212;most terrifying and least helpful of all&#8212;&#8216;Kullback-Leibler divergence&#8217;.   In this second part I&#8217;ll introduce two key players in this theorem.  The first, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> is a category where:</p>
<p>&bull; an object consists of a system with finitely many states, and a probability distribution on those states</p>
<p>and</p>
<p>&bull; a morphism consists of a deterministic &#8216;measurement process&#8217; mapping states of one system to states of another, together with a &#8216;hypothesis&#8217; that lets the observer guess a probability distribution of states of the system being measured, based on what they observe.</p>
<p>The second, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}," class="latex" /> is a subcategory of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" />  It has all the same objects, but only morphisms where the hypothesis is &#8216;optimal&#8217;.  This means that if the observer measures the system many times, and uses the probability distribution of their observations together with their hypothesis to guess the probability distribution of states of the system, they <i>get the correct answer</i> (in the limit of many measurements).</p>
<p>In this part all I will really do is explain precisely what <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> are.  But to whet your appetite, let me explain how we can use them to give a new characterization of relative entropy!</p>
<p>Suppose we have any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" />  In other words: suppose we have a deterministic measurement process, together with a hypothesis that lets the observer guess a probability distribution of states of the system being measured, based on what they observe.</p>
<p>Then we have <i>two</i> probability distributions on the states of the system being measured!  First, the &#8216;true&#8217; probability distribution.  Second, the probability that the observer will guess based on their observations.</p>
<p>Whenever we have two probability distributions on the same set, we can compute the entropy of the first <i>relative to</i> to the second.  This describes how surprised you&#8217;ll be if you discover the probability distribution is really the first, when you thought it was the second.</p>
<p>So: any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> will have a relative entropy.  It will describe how surprised the observer will be when they discover the true probability distribution, given what they had guessed.</p>
<p>But this amount of surprise will be <i>zero</i> if their hypothesis was &#8216;optimal&#8217; in the sense I described.   So, the relative entropy will vanish on morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}." class="latex" /></p>
<p>Our theorem says this fact almost characterizes the concept of relative entropy!  More precisely, it says that any convex-linear lower semicontinuous functor</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3A+%5Cmathrm%7BFinStat%7D+%5Cto+%5B0%2C%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F : &#92;mathrm{FinStat} &#92;to [0,&#92;infty] " class="latex" /></p>
<p>that vanishes on the subcategory <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> must equal some constant times the relative entropy.</p>
<p>Don&#8217;t be scared!  This should not make sense to you yet, since I haven&#8217;t said how I&#8217;m thinking of <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%2B%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,+&#92;infty] " class="latex" /> as a category, nor what a &#8216;convex-linear lower semicontinuous functor&#8217; is, nor how relative entropy gives one.  I will explain all that later.  I just want you to get a vague idea of where I&#8217;m going.</p>
<p>Now let me explain the categories <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}." class="latex" />  We need to warm up a bit first.</p>
<h3> FinStoch </h3>
<p>A stochastic map <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;leadsto Y" class="latex" /> is different from an ordinary function, because instead of assigning a unique element of <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> to each element of <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> it assigns a <i>probability distribution on</i> <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> to each element of <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  So you should imagine it as being like a function &#8216;with random noise added&#8217;, so that <img src="https://s0.wp.com/latex.php?latex=f%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)" class="latex" /> is not a specific element of <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> but instead has a probability of taking on different values.  This is why I&#8217;m using a weird wiggly arrow to denote a stochastic map.</p>
<p>More formally:</p>
<p><b>Definition.</b>  Given finite sets <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> a <b>stochastic map</b> <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;leadsto Y" class="latex" /> assigns a real number <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx}" class="latex" /> to each pair <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X%2C+y+%5Cin+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X, y &#92;in Y," class="latex" /> such that fixing any element <img src="https://s0.wp.com/latex.php?latex=x%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x," class="latex" /> the numbers <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx}" class="latex" /> form a probability distribution on <img src="https://s0.wp.com/latex.php?latex=Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y." class="latex" />  We call <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx}" class="latex" /> <b>the probability of <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x." class="latex" /></b></p>
<p>In more detail:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx} &#92;ge 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y." class="latex" /></p>
<p>and</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7By+%5Cin+Y%7D+f_%7Byx%7D+%3D+1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{y &#92;in Y} f_{yx} = 1}" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X." class="latex" /></p>
<p>Note that we can think of <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;leadsto Y" class="latex" /> as a <img src="https://s0.wp.com/latex.php?latex=Y+%5Ctimes+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y &#92;times X" class="latex" />-shaped matrix of numbers.  A matrix obeying the two properties above is called <b>stochastic</b>.  This viewpoint is nice because it reduces the problem of composing stochastic maps to matrix multiplication.  It&#8217;s easy to check that multiplying two stochastic matrices gives a stochastic matrix.  So, composing stochastic maps gives a stochastic map.</p>
<p>We thus get a category:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}" class="latex" /> be the category of finite sets and stochastic maps between them.</p>
<p>In case you&#8217;re wondering why I&#8217;m restricting attention to <i>finite</i> sets, it&#8217;s merely because I want to keep things simple.  I don&#8217;t want to worry about whether sums or integrals converge.</p>
<h3> FinProb </h3>
<p>Now take your favorite 1-element set and call it <img src="https://s0.wp.com/latex.php?latex=1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1." class="latex" />  A function <img src="https://s0.wp.com/latex.php?latex=p+%3A+1+%5Cto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : 1 &#92;to X" class="latex" /> is just a point of <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  But a stochastic map <img src="https://s0.wp.com/latex.php?latex=p+%3A+1+%5Cleadsto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : 1 &#92;leadsto X" class="latex" /> is something more interesting: it&#8217;s a probability distribution on <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" /></p>
<p>Why?  Because it gives a probability distribution on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> for each element of <img src="https://s0.wp.com/latex.php?latex=1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1," class="latex" /> but that set has just one element.</p>
<p>Last time I introduced the rather long-winded phrase <b>finite probability measure space</b> to mean a finite set with a probability distribution on it.  But now we&#8217;ve seen a very quick way to describe such a thing within <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}:" class="latex" /></p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>And this gives a quick way to think about a measure-preserving function between finite probability measure spaces!  It&#8217;s just a commutative triangle like this:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>Note that the horizontal arrow <img src="https://s0.wp.com/latex.php?latex=f%3A++X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f:  X &#92;to Y" class="latex" /> is not wiggly. The straight arrow means it&#8217;s an honest function, not a stochastic map.  But a function is a special case of a stochastic map!  So it makes sense to compose a straight arrow with a wiggly arrow&#8212;and the result is, in general, a wiggly arrow.  So, it makes sense to demand that this triangle commutes, and this says that the function <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;to Y" class="latex" /> is measure-preserving.</p>
<p>Let me work through the details, in case they&#8217;re not clear.</p>
<p>First: how is a function a special case of a stochastic map?  Here&#8217;s how.  If we start with a function <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cto+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;to Y," class="latex" /> we get a matrix of numbers</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D+%3D+%5Cdelta_%7By%2Cf%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx} = &#92;delta_{y,f(x)} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta" class="latex" /> is the Kronecker delta.  So, each element <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> gives a probability distribution that&#8217;s zero except at <img src="https://s0.wp.com/latex.php?latex=f%28x%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)." class="latex" /></p>
<p>Given this, we can work out what this commuting triangle really says:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>If use <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> to stand for the probability distribution that <img src="https://s0.wp.com/latex.php?latex=p%3A+1+%5Cleadsto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p: 1 &#92;leadsto X" class="latex" /> puts on <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> and similarly for <img src="https://s0.wp.com/latex.php?latex=q_y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_y," class="latex" /> the commuting triangle says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+X%7D+%5Cdelta_%7By%2Cf%28x%29%7D+p_x%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in X} &#92;delta_{y,f(x)} p_x} " class="latex" /></p>
<p>or in other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+X+%3A+f%28x%29+%3D+y%7D+p_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in X : f(x) = y} p_x } " class="latex" /></p>
<p>or if you like:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+f%5E%7B-1%7D%28y%29%7D+p_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in f^{-1}(y)} p_x } " class="latex" /></p>
<p>In this situation people say <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> <b>pushed forward along <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /></b>, and they say <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a <b>measure-preserving function</b>.</p>
<p>So, we&#8217;ve used <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}" class="latex" /> to describe another important category:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}" class="latex" /> be the category of finite probability measure spaces and measure-preserving functions between them.</p>
<p>I can&#8217;t resist mentioning another variation:</p>
<div align="center">
<img height="200" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/measure-preserving_stochastic_map.jpg" />
</div>
<p>A commuting triangle like this is a <b>measure-preserving stochastic map</b>.  In other words, <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> gives a probability measure on <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gives a probability measure on <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;leadsto Y" class="latex" /> is a stochastic map with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+X%7D+f_%7Byx%7D+p_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in X} f_{yx} p_x } " class="latex" /></p>
<h3> FinStat </h3>
<p>The category we really need for relative entropy is a bit more subtle.  An object is a finite probability measure space:</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>but a morphism looks like this:</p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>The whole diagram doesn&#8217;t commute, but the two equations I wrote down hold.  The first equation says that <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;to Y" class="latex" /> is a measure-preserving function.  In other words, this triangle, which we&#8217;ve seen before, commutes:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>The second equation says that <img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ s" class="latex" /> is the identity, or in math jargon, <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is a <a href="https://en.wikipedia.org/wiki/Section_%28category_theory%29"><b>section</b></a> for <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>But what does that <i>really mean?</i></p>
<p>The idea is that <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is the set of &#8216;states&#8217; of some system, while <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> is a set of possible &#8216;observations&#8217; you might make.  The function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a &#8216;measurement process&#8217;. You &#8216;measure&#8217; the system using <img src="https://s0.wp.com/latex.php?latex=f%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f," class="latex" /> and if the system is in the the state <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> you get the observation <img src="https://s0.wp.com/latex.php?latex=f%28x%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)." class="latex" />  The probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> says the probability that the system is any given state, while <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> says the probability that you get any given observation when you do your measurement.</p>
<p>Note: are assuming for now that that there&#8217;s no random noise in the observation process!  That&#8217;s why <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a function instead of a stochastic map.</p>
<p>But what about <img src="https://s0.wp.com/latex.php?latex=s%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s?" class="latex" />  That&#8217;s the fun part: <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> describes your &#8216;hypothesis&#8217; about the system&#8217;s state given a particular measurement!  If you measure the system and get a result <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y," class="latex" /> you guess it&#8217;s in the state <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> with probability <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy}." class="latex" /></p>
<p>And we don&#8217;t want this hypothesis to be really dumb: that&#8217;s what</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+s+%3D+1_Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ s = 1_Y" class="latex" /></p>
<p>says.  You see, this equation says that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+X%7D+%5Cdelta_%7By%27%2C+f%28x%29%7D+s_%7Bxy%7D+%3D+%5Cdelta_%7By%27+y%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in X} &#92;delta_{y&#039;, f(x)} s_{xy} = &#92;delta_{y&#039; y} " class="latex" /></p>
<p>or in other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+f%5E%7B-1%7D%28y%27%29%7D+s_%7Bxy%7D+%3D+%5Cdelta_%7By%27+y%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in f^{-1}(y&#039;)} s_{xy} = &#92;delta_{y&#039; y} " class="latex" /></p>
<p>If you think about it, this implies <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy} = 0" class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y." class="latex" /></p>
<p>So, if you make an observation <img src="https://s0.wp.com/latex.php?latex=y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y," class="latex" /> you will guess the system is in state <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> with <i>probability zero</i> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y." class="latex" />  In short, you won&#8217;t make a really dumb guess about the system&#8217;s state.</p>
<p>Here&#8217;s how we compose morphisms:</p>
<div align="center">
<img height="250" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_composition.jpg" />
</div>
<p>We get a measure-preserving function <img src="https://s0.wp.com/latex.php?latex=g+%5Ccirc+f+%3A+X+%5Cto+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;circ f : X &#92;to Z" class="latex" /> and a stochastic map going back, <img src="https://s0.wp.com/latex.php?latex=s+%5Ccirc+t+%3A+Z+%5Cto+Z.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s &#92;circ t : Z &#92;to Z." class="latex" /> You can check that these obey the required equations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%5Ccirc+f+%5Ccirc+p+%3D+r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;circ f &#92;circ p = r" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%5Ccirc+f+%5Ccirc+s+%5Ccirc+t+%3D+1_Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;circ f &#92;circ s &#92;circ t = 1_Z" class="latex" /></p>
<p>So, we get a category:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> be the category where an object is a finite probability measure space:</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>a morphism is a diagram obeying these equations:</p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>and composition is defined as above.</p>
<h3> FP  </h3>
<p>As we&#8217;ve just seen, a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> consists of a &#8216;measurement process&#8217; <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and a &#8216;hypothesis&#8217; <img src="https://s0.wp.com/latex.php?latex=s%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s:" class="latex" /></p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>But sometimes we&#8217;re lucky and our hypothesis is optimal, in the sense that</p>
<p><img src="https://s0.wp.com/latex.php?latex=s+%5Ccirc+q+%3D+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s &#92;circ q = p" class="latex" /></p>
<p>Conceptually, this says that if you take the probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on our observations and use it to guess a probability distribution for the system&#8217;s state using our hypothesis <img src="https://s0.wp.com/latex.php?latex=s%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s," class="latex" /> you <i>get the correct answer</i>: <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>Mathematically, it says that this diagram commutes:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/measure-preserving_stochastic_map_2.jpg" />
</div>
<p>In other words, <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is a measure-preserving stochastic map.</p>
<p>There&#8217;s a subcategory of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> with all the same objects, but only these &#8216;optimal&#8217; morphisms.  It&#8217;s important, but the name we have for it is not very exciting:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> be the subcategory of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> where an object is a finite probability measure space</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>and a morphism is a diagram obeying these equations:</p>
<div align="center">
<img height="260" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/FP_morphism.jpg" />
</div>
<p>Why do we call this category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" />?  Because it&#8217;s a close relative of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}," class="latex" /> where a morphism, you&#8217;ll remember, looks like this:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>The point is that for a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}," class="latex" /> the conditions on <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> are so strong that they completely determine it <i>unless there are observations that happen with probability zero</i>&#8212;that is, unless there are <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=q_y+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_y = 0." class="latex" />  To see this, note that</p>
<p><img src="https://s0.wp.com/latex.php?latex=s+%5Ccirc+q+%3D+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s &#92;circ q = p" class="latex" /></p>
<p>actually says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7By+%5Cin+Y%7D+s_%7Bxy%7D+q_y+%3D+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{y &#92;in Y} s_{xy} q_y = p_x " class="latex" /></p>
<p>for any choice of <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X." class="latex" />   But we&#8217;ve already seen <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy} = 0" class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y," class="latex" /> so the sum has just one term, and the equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=s_%7Bx%2Cf%28x%29%7D+q_%7Bf%28x%29%7D+%3D+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{x,f(x)} q_{f(x)} = p_x " class="latex" /></p>
<p>We can solve this for <img src="https://s0.wp.com/latex.php?latex=s_%7Bx%2Cf%28x%29%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{x,f(x)}," class="latex" /> so <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is completely determined&#8230; <i>unless <img src="https://s0.wp.com/latex.php?latex=q_%7Bf%28x%29%7D+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_{f(x)} = 0." class="latex" /></i></p>
<p>This covers the case when <img src="https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y = f(x)." class="latex" />   We also can&#8217;t figure out <img src="https://s0.wp.com/latex.php?latex=s_%7Bx%2Cy%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{x,y}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> isn&#8217;t in the image of <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>So, to be utterly precise, <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is determined by <img src="https://s0.wp.com/latex.php?latex=p%2C+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> unless there&#8217;s an element <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y" class="latex" /> that has <img src="https://s0.wp.com/latex.php?latex=q_y+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_y = 0." class="latex" />  Except for this special case, a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> is just a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}." class="latex" />  But in this special case, a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> has a little extra information: an arbitrary probability distribution on the inverse image of each point <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> with this property.</p>
<p>In short, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> is the same as <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}" class="latex" /> except that our observer&#8217;s &#8216;optimal hypothesis&#8217; must provide a guess about the state of the system given an observation, <i>even in cases of observations that occur with probability zero.</i></p>
<p>I&#8217;m going into these nitpicky details for two reasons.  First, we&#8217;ll need <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> for our characterization of relative entropy.  But second, Tom Leinster <i>already ran into this category</i> in his work on entropy and category theory!  He discussed it here:</p>
<p>&bull; Tom Leinster, <a href="http://golem.ph.utexas.edu/category/2011/05/an_operadic_introduction_to_en.html">An operadic introduction to entropy</a>.</p>
<p>Despite the common theme of entropy, he arrived at it from a very different starting-point.</p>
<h3> Conclusion </h3>
<p>So, I hope that next time I can show you something like this:</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a probability distribution on the states of some system!&#8221;  Intuitively, you should think of the wiggly arrow <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as picking out a &#8216;random element&#8217; of the set <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" /></p>
<p>I hope I can show you this:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a deterministic measurement process, sending a probability distribution on the states of the measured system to a probability distribution on observations!&#8221;</p>
<p>I hope I can show you this:</p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a deterministic measurement process, together with a hypothesis about the system&#8217;s state, given what is observed!&#8221;</p>
<p>And I hope I can show you this:</p>
<div align="center">
<img height="260" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/FP_morphism.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a deterministic measurement process, together with an <i>optimal</i> hypothesis about the system&#8217;s state, given what is observed!&#8221;</p>
<p>I don&#8217;t count on it&#8230; but I can hope.</p>
<h3> Postscript </h3>
<p>And speaking of unrealistic hopes, if I were <i>really</i> optimistic I would hope you noticed that <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}" class="latex" /> and  <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}," class="latex" /> which underlie the more fancy categories I&#8217;ve discussed today, were themselves constructed starting from linear algebra over the nonnegative numbers <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> in <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Part 1</a>.   That &#8216;foundational&#8217; work is not really needed for what we&#8217;re doing now.  However, I like the fact that we&#8217;re ultimately getting the concept of relative entropy starting from very little: just linear algebra, using only nonnegative numbers!</p>
<p>For more details, here&#8217;s the actual paper:</p>
<p>• John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>, <i><a href="http://www.tac.mta.ca/tac/volumes/29/16/29-16abs.html">Theory and Applications of Categories</a></i> <b>29</b> (2014), 421&ndash;456.</p>
<p>And here&#8217;s my whole series of blog articles about it:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy (Part 1)</a>: how various structures important in probability theory arise naturally when you do linear algebra using only the nonnegative real numbers.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Relative Entropy (Part 2)</a>: a category related to statistical inference, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and how relative entropy defines a functor on this category.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Relative Entropy (Part 3)</a>: how to characterize relative entropy as a functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty])." class="latex" /></p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/">Relative Entropy (Part 4)</a>: wrap-up, and an invitation to read more about the underlying math at the <a href="https://golem.ph.utexas.edu/category/2014/02/relative_entropy.html"><i>n</i>-Category Caf&eacute;</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/#comments">11 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/" rel="bookmark" title="Permanent Link to Relative Entropy (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16174 post type-post status-publish format-standard hentry category-information-and-entropy category-probability" id="post-16174">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/" rel="bookmark">Relative Entropy (Part&nbsp;1)</a></h2>
				<small>20 June, 2013</small><br />


				<div class="entry">
					<p>I&#8217;m trying to finish off a paper that Tobias Fritz and I have been working on, which gives a category-theoretic (and Bayesian!) characterization of <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">relative entropy</a>.  It&#8217;s a kind of sequel to <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/">our paper with Tom Leinster</a>, in which we characterized entropy.</p>
<p>That earlier paper was developed in conversations <a href="http://ncatlab.org/johnbaez/show/A+characterization+of+entropy+in+terms+of+information+loss">on the <i>n</i>-Category Caf&eacute;</a>.  It was a lot of fun; I sort of miss that style of working.    Also, to get warmed up, I need to think through some things I&#8217;ve thought about before.   So, I might as well write them down here.</p>
<h3> The idea </h3>
<p>There are many categories related to probability theory, and they&#8217;re related in many ways.  Last summer&#8212;on the 24th of August 2012, according to my notes here&#8212;Jamie Vicary, Brendan Fong and I worked through a bunch of these relationships.  I need to write them down now, even if they&#8217;re not all vitally important to my paper with Tobias.  They&#8217;re sort of buzzing around my brain like flies.</p>
<p>(Tobias knows this stuff too, and this is how we think about probability theory, but we weren&#8217;t planning to stick it in our paper.  Maybe we should.)</p>
<p>Let&#8217;s restrict attention to probability measures on <i>finite sets</i>, and related structures.  We could study these questions more generally, and we should, but not today.  What we&#8217;ll do is give a unified purely algebraic description of:</p>
<p>&bull; finite sets</p>
<p>&bull; measures on finite sets</p>
<p>&bull; probability measures on finite sets</p>
<p>and various kinds of maps between these:</p>
<p>&bull; functions</p>
<p>&bull; bijections</p>
<p>&bull; measure-preserving functions</p>
<p>&bull; stochastic maps</p>
<h3>  Finitely generated free [0,&infin;)-modules </h3>
<p>People often do linear algebra over a <a href="http://en.wikipedia.org/wiki/Field_%28mathematics%29">field</a>, which is&#8212;roughly speaking&#8212;a number system where you can add, subtract, multiply and divide.  But algebraists have long realized that a lot of linear algebra still works with a <a href="http://en.wikipedia.org/wiki/Commutative_ring">commutative ring</a>, where you can&#8217;t necessarily divide.  It gets more complicated, but also a lot more interesting.</p>
<p>But in fact, a lot still works with a commutative <a href="http://ncatlab.org/nlab/show/rig">rig</a>, where we can&#8217;t necessarily subtract either!  Something I keep telling everyone is that linear algebra over rigs is a good idea for studying things like probability theory, thermodynamics, and the principle of least action.</p>
<p>Today we&#8217;ll start with the rig of nonnegative real numbers with their usual addition and multiplication; let&#8217;s call this <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ." class="latex" />   The idea is that measure theory, and probability theory, are closely related to linear algebra over this rig.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> be the category with of <a href="http://en.wikipedia.org/wiki/Finitely-generated_module">finitely generated</a> <a href="http://en.wikipedia.org/wiki/Free_module">free</a> <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" />-modules as objects, and <a href="http://en.wikipedia.org/wiki/Module_homomorphism#Submodules_and_homomorphisms">module homomorphisms</a> as morphisms.   I&#8217;ll call these morphisms <b>maps</b>.</p>
<p><b>Puzzle.</b> Do we need to say &#8216;free&#8217; here?  Are there finitely generated modules over <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" /> that aren&#8217;t free?</p>
<p>Every finitely generated free <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" />-module is isomorphic to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S " class="latex" /> for some finite set <img src="https://s0.wp.com/latex.php?latex=S+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S ." class="latex" /> In other words, it&#8217;s isomorphic to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5En+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^n " class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=n+%3D+0%2C+1%2C+2%2C+%5Cdots+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 0, 1, 2, &#92;dots ." class="latex" />   So, <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is equivalent to the category where objects are natural numbers, a morphism from <img src="https://s0.wp.com/latex.php?latex=m+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m " class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n " class="latex" /> is an <img src="https://s0.wp.com/latex.php?latex=m+%5Ctimes+n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m &#92;times n " class="latex" /> matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ," class="latex" /> and composition is done by matrix multiplication.  I&#8217;ll also call this equivalent category <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /></p>
<p>We can take tensor products of finitely generated free modules, and this makes  <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> into a <a href="http://ncatlab.org/nlab/show/symmetric+monoidal+dagger-category">symmetric monoidal &dagger;-category</a>.  This means we can draw maps using <a href="http://arxiv.org/abs/0903.0340">string diagrams</a> in the usual way.  However, I&#8217;m feeling lazy so I&#8217;ll often write equations when I could be drawing diagrams.</p>
<p>One of the rules of the game is that all these equations will make sense in <i>any</i> symmetric monoidal &dagger;-category.  So we could, if we wanted, generalize ideas from probability theory this way.  If you want to do this, you&#8217;ll need to know that <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> is the unit for the tensor product in <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" />   We&#8217;ll be seeing this guy <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> a lot.  So if you want to generalize, replace <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> by any symmetric monoidal &dagger;-category, and replace <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> by the unit for the tensor product.</p>
<h3> Finite sets </h3>
<p>There&#8217;s a way to see the category of finite sets lurking in <img src="https://s0.wp.com/latex.php?latex=C%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C," class="latex" /> which we can borrow from this paper:</p>
<p>&bull; Bob Coecke, Dusko Pavlovic and Jamie Vicary, <a href="http://arxiv.org/abs/0810.0812">A new description of orthogonal bases</a>.</p>
<p>For any finite set <img src="https://s0.wp.com/latex.php?latex=S+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S ," class="latex" /> we get a free finitely generated <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" />-module, namely <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S ." class="latex" />  This comes with some structure:</p>
<p>&bull; a multiplication <img src="https://s0.wp.com/latex.php?latex=m%3A+%5B0%2C%5Cinfty%29%5ES+%5Cotimes+%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C%5Cinfty%29%5ES+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m: [0,&#92;infty)^S &#92;otimes [0,&#92;infty)^S &#92;to [0,&#92;infty)^S ," class="latex" /> coming from pointwise multiplication of <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" />-valued functions on <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /></p>
<p>&bull; the unit for this multiplication, an element of <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S," class="latex" /> which we can write as a morphism <img src="https://s0.wp.com/latex.php?latex=i%3A+%5B0%2C%5Cinfty%29+%5Cto+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i: [0,&#92;infty) &#92;to [0,&#92;infty)^S " class="latex" /></p>
<p>&bull; a comultiplication, obtained by taking the diagonal map <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%3A+S+%5Cto+S+%5Ctimes+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta : S &#92;to S &#92;times S " class="latex" /> and promoting it to a linear map <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%3A+%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C+%5Cinfty%29%5ES+%5Cotimes+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta : [0,&#92;infty)^S &#92;to [0, &#92;infty)^S &#92;otimes [0,&#92;infty)^S " class="latex" /></p>
<p>&bull; a counit for this comultiplication, obtained by taking the unique map to the terminal set <img src="https://s0.wp.com/latex.php?latex=%21+%3A+S+%5Cto+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="! : S &#92;to 1 " class="latex" /> and promoting it to a linear map <img src="https://s0.wp.com/latex.php?latex=e%3A+%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e: [0,&#92;infty)^S &#92;to [0,&#92;infty)" class="latex" /></p>
<p>These morphisms <img src="https://s0.wp.com/latex.php?latex=m%2C+i%2C+%5CDelta%2C+e+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m, i, &#92;Delta, e " class="latex" /> make</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = [0,&#92;infty)^S " class="latex" /></p>
<p>into a commutative Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C ." class="latex" />  That&#8217;s a thing where the unit, counit, multiplication and comultiplication obey these laws:</p>
<div align="center">
<img src="https://i0.wp.com/math.ucr.edu/home/baez/commutative_frobenius_algebra.jpg" alt="" />
</div>
<p>(I drew these back when I was feeling less lazy.)   This Frobenius algebra is also &#8216;special&#8217;, meaning it obeys this:</p>
<div align="center">
<img width="300" src="https://i2.wp.com/math.ucr.edu/home/baez/separability.jpg" alt="" />
</div>
<p>And it&#8217;s also a &dagger;-Frobenius algebra, meaning that the counit and comultiplication are obtained from the unit and multiplication by &#8216;flipping&#8217; them using the <a href="http://ncatlab.org/nlab/show/dagger-category">&dagger;category</a> structure.  (If we think of a morphism in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> as a matrix, its dagger is its transpose.)</p>
<p>Conversely, suppose we have <i>any</i> special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ." class="latex" />  Then using the ideas in the paper by Coecke, Pavlovich and Vicary we can recover a basis for <img src="https://s0.wp.com/latex.php?latex=x+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ," class="latex" /> consisting of the vectors <img src="https://s0.wp.com/latex.php?latex=e_i+%5Cin+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e_i &#92;in x " class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta%28e_i%29+%3D+e_i+%5Cotimes+e_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta(e_i) = e_i &#92;otimes e_i " class="latex" /></p>
<p>This basis forms a set <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Ccong+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;cong [0,&#92;infty)^S " class="latex" /></p>
<p>for some <i>specified</i> isomorphism in <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" />  Furthermore, this is an isomorphism of special commutative &dagger;-Frobenius algebras!</p>
<p>In case you&#8217;re wondering, these vectors <img src="https://s0.wp.com/latex.php?latex=e_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e_i" class="latex" /> correspond to the functions on <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> that are zero everywhere except at one point <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+S%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in S," class="latex" /> where they equal 1.</p>
<p>In short, a special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is just a fancy way of talking about a finite set.  This may seem silly, but it&#8217;s a way to start describing probability theory using linear algebra very much as we do with quantum theory.  This analogy between quantum theory and probability theory is so interesting that it deserves a <a href="http://math.ucr.edu/home/baez/stoch_stable.pdf">book</a>.</p>
<h3> Functions and bijections </h3>
<p>Now suppose we have two special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y ." class="latex" /></p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /> is a Frobenius algebra homomorphism: that is, a map preserving <i>all</i> the structure&#8212;the unit, counit, multiplication and comultiplication.  Then it comes from an isomorphism of finite sets.   This lets us find <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinSet%7D_0+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinSet}_0 ," class="latex" /> the groupoid of finite sets and bijections, inside <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /></p>
<p>Alternatively, suppose <img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /> is just a coalgebra homomorphism: that is a map preserving just the counit and comultiplication.  Then it comes from an arbitrary function between finite sets.  This lets us find <img src="https://s0.wp.com/latex.php?latex=FinSet+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="FinSet ," class="latex" /> the category of finite sets and functions, inside <img src="https://s0.wp.com/latex.php?latex=C+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C ." class="latex" /></p>
<p>But what if <img src="https://s0.wp.com/latex.php?latex=f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f " class="latex" /> preserves just the counit?  This sounds like a dry, formal question.  But it&#8217;s not: the answer is something useful, a &#8216;stochastic map&#8217;.</p>
<h3>  Stochastic maps </h3>
<p>A <b>stochastic map</b> from a finite set <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> to a finite set <img src="https://s0.wp.com/latex.php?latex=T+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T " class="latex" /> is a map sending each point of <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> to a probability measure on <img src="https://s0.wp.com/latex.php?latex=T+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T ." class="latex" /></p>
<p>We can think of this as a <img src="https://s0.wp.com/latex.php?latex=T+%5Ctimes+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;times S " class="latex" />-shaped matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ," class="latex" /> where a given column gives the probability that a given point in <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> goes to any point in <img src="https://s0.wp.com/latex.php?latex=T+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T ." class="latex" />  The sum of the numbers in each column will be 1.  And conversely, any  <img src="https://s0.wp.com/latex.php?latex=T+%5Ctimes+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;times S " class="latex" />-shaped matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ," class="latex" /> where each column sums to 1, gives a stochastic map from <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=T+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T ." class="latex" /></p>
<p>But now let&#8217;s describe this idea using the category <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" />  We&#8217;ve seen a finite set is the same as a special commutative &dagger;-Frobenius algebra.  So, say we have two of these, <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y ." class="latex" />  Our matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" /> is just a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: x &#92;to y " class="latex" /></p>
<p>So, we just need a way to state the condition that each column in the matrix sums to 1.  And this condition simply says that <img src="https://s0.wp.com/latex.php?latex=f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f " class="latex" /> preserves the counit:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+%5Ccirc+f+%3D+%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y &#92;circ f = &#92;epsilon_x " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_x+%3A+x+%5Cto+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_x : x &#92;to [0,&#92;infty) " class="latex" /> is the counit for <img src="https://s0.wp.com/latex.php?latex=x+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ," class="latex" /> and similarly for <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y ." class="latex" /></p>
<p>To understand this, note that if we use the canonical isomorphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Ccong+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;cong [0,&#92;infty)^S " class="latex" /></p>
<p>the counit <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_x " class="latex" /> can be seen as the map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S &#92;to [0,&#92;infty) " class="latex" /></p>
<p>that takes any <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" />-tuple of numbers and sums them up.   In other words, it&#8217;s integration with respect to counting measure.  So, the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+%5Ccirc+f+%3D+%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y &#92;circ f = &#92;epsilon_x " class="latex" /></p>
<p>says that if we take any <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" />-tuple of numbers, multiply it by the matrix <img src="https://s0.wp.com/latex.php?latex=f+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f ," class="latex" /> and then sum up the entries of the resulting <img src="https://s0.wp.com/latex.php?latex=T+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T " class="latex" />-tuple, it&#8217;s the same as if we summed up the original <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" />-tuple.  But this says precisely that each column of the matrix <img src="https://s0.wp.com/latex.php?latex=f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f " class="latex" /> sums to 1.</p>
<p>So, we can use our formalism to describe <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}," class="latex" /> the category with finite sets as objects and stochastic maps as morphisms.  We&#8217;ve seen this category is equivalent to the category with special commutative &dagger;-Frobenius algebras in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> as objects and counit-preserving maps as morphisms.</p>
<h3> Finite measure spaces </h3>
<p>Now let&#8217;s use our formalism to describe finite measure spaces&#8212;by which, beware, I mean a finite sets equipped with measures!  To do this, we&#8217;ll use a special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> together with any map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>Starting from these, we get a specified isomorphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Ccong+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;cong [0,&#92;infty)^S " class="latex" /></p>
<p>and <img src="https://s0.wp.com/latex.php?latex=%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu " class="latex" /> sends the number 1 to a vector in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S " class="latex" />: that is, a function on <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> taking values in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ." class="latex" />   Multiplying this function by counting measure, we get a <i>measure</i> on <img src="https://s0.wp.com/latex.php?latex=S+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S ." class="latex" /></p>
<p><b>Puzzle.</b>  How can we describe this measure without the annoying use of counting measure?</p>
<p>Conversely, any measure on a finite set gives a special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C " class="latex" /> equipped with a map from <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ." class="latex" /></p>
<p>So, we can say a finite measure space is a special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C " class="latex" /> equipped with a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>And given two of these,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+%2C+%5Cqquad++%5Cnu%3A+%5B0%2C%5Cinfty%29+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x , &#92;qquad  &#92;nu: [0,&#92;infty) &#92;to y" class="latex" /></p>
<p>and a coalgebra morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /></p>
<p>obeying this equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+%5Cmu++%3D+%5Cnu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ &#92;mu  = &#92;nu " class="latex" /></p>
<p>then we get a measure-preserving function between finite measure spaces!    If you&#8217;re a category theorist, you&#8217;ll draw this equation as a commutative triangle:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/relative_entropy_commutative_triangle.jpg" />
</div>
<p>Conversely, any measure-preserving function between finite measure spaces obeys this equation.  So, we get an algebraic way of describing the category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinMeas%7D+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinMeas} ," class="latex" /> with finite measure spaces as objects and measure-preserving maps as morphisms.</p>
<h3> Finite probability measure spaces </h3>
<p>I&#8217;m mainly interested in probability measures.  So suppose <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> is a special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> equipped with a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>We&#8217;ve seen this gives a finite measure space.  But this is a probability measure space if and only if</p>
<p><img src="https://s0.wp.com/latex.php?latex=e+%5Ccirc+%5Cmu+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e &#92;circ &#92;mu = 1 " class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=e+%3A+x+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e : x &#92;to [0,&#92;infty)" class="latex" /></p>
<p>is the counit for <img src="https://s0.wp.com/latex.php?latex=x+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ." class="latex" />  The equation simply says the total integral of our measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu " class="latex" /> is 1.</p>
<p>So, we get a way of describing the category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb} ," class="latex" /> which has finite probability measure spaces as objects and measure-preserving maps as objects.  Given finite probability measure spaces described this way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+%2C+%5Cqquad++%5Cnu%3A+%5B0%2C%5Cinfty%29+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x , &#92;qquad  &#92;nu: [0,&#92;infty) &#92;to y" class="latex" /></p>
<p>a measure-preserving function is a coalgebra morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /></p>
<p>such that the obvious triangle commutes:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+%5Cmu++%3D+%5Cnu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ &#92;mu  = &#92;nu " class="latex" /></p>
<h3> Measure-preserving stochastic maps </h3>
<p>Say we have two finite measure spaces.  Then we can ask whether a stochastic map from one to the other is measure-preserving.  And we can answer this question in the language of <img src="https://s0.wp.com/latex.php?latex=C+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C ." class="latex" /></p>
<p>Remember, a finite measure space is a special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C " class="latex" /> together with a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>Say we have another one:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnu%3A+%5B0%2C%5Cinfty%29+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu: [0,&#92;infty) &#92;to y " class="latex" /></p>
<p>A stochastic map is just a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: x &#92;to y " class="latex" /></p>
<p>that preserves the counit:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+%5Ccirc+f+%3D+%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y &#92;circ f = &#92;epsilon_x " class="latex" /></p>
<p>But it&#8217;s a <b>measure-preserving stochastic map</b> if also</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+%5Cmu++%3D+%5Cnu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ &#92;mu  = &#92;nu " class="latex" /></p>
<h3> Addendum </h3>
<p>In this article I didn&#8217;t get anywhere near to talking about what Tobias and I were actually doing!  But it was good to get this basic stuff written down.</p>
<p>Here&#8217;s the paper we eventually wrote:</p>
<p>• John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>, <i><a href="http://www.tac.mta.ca/tac/volumes/29/16/29-16abs.html">Theory and Applications of Categories</a></i> <b>29</b> (2014), 421&ndash;456.</p>
<p>And here&#8217;s my whole series of blog articles about it:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy (Part 1)</a>: how various structures important in probability theory arise naturally when you do linear algebra using only the nonnegative real numbers.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Relative Entropy (Part 2)</a>: a category related to statistical inference, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and how relative entropy defines a functor on this category.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Relative Entropy (Part 3)</a>: how to characterize relative entropy as a functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /></p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/">Relative Entropy (Part 4)</a>: wrap-up, and an invitation to read more about the underlying math at the <a href="https://golem.ph.utexas.edu/category/2014/02/relative_entropy.html"><i>n</i>-Category Caf&eacute;</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/#comments">19 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/" rel="bookmark" title="Permanent Link to Relative Entropy (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-15757 post type-post status-publish format-standard hentry category-sustainability" id="post-15757">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/05/08/localizing-and-networking-basic-technology/" rel="bookmark">Localizing and Networking Basic&nbsp;Technology</a></h2>
				<small>8 May, 2013</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://culturalspeciation.blogspot.com/">Iuval Clejan</a></b></i></p>
<p>Natural philosophy (aka science) is distinguished from pure philosophy or mathematics by coupling theory to experiment. Engineering is distinguished from science in its focus on solving practical problems rather than merely coming up with more accurate models of the universe. Climate change will not be fixed by pure philosophy or argumentation. We need to use the methods of science and engineering to make progress towards a solution. The problem is complicated and involves not just climate dynamics and ecology, but psychology, economics and technology.  Besides theory and experiment, we now have the tool of simulation. I propose a think-tank (or more properly, a think/do/simulate-tank) analogous to the Manhattan Project, which developed the first atomic bomb.  However, this project would  involve social and physical scientists, computer programmers, engineers, farmers and craftspeople who are trying to collaboratively solve the problem of how to provide food, shelter, water, clothes, medicine and recreation for a self contained village in a sustainable way. Sustainability has psychological dimensions, not just ecological. For example, it implies that people would want to keep living in this village, or similar villages. If we are interested in sustainability beyond the initial village, then sustainability implies replicability—that the village would inspire  many other people to live similarly.</p>
<p>Initial outputs of this project would be well-founded suggestions regarding what kinds of production skills are needed and how to effectively network them, how many people, how much land, how much time spent on production in order to achieve village-scale independence and sustainability. An eventual outcome would be an actual demonstration of a functional village.</p>
<p>Why village? The word village is used here to mean a group of people who are economically networked in isolation from the rest of the global economy. It also implies choosing a particular geographic location, so not all outputs would be transferable  to other locations, though with the initial simulation stage many locations could be tried. </p>
<p>Why economic isolation? Without putting a boundary on the experiment, the problem is too complex, even for simulation. Entropy reduction is the same reason cells have membranes and scientists have labs. The membrane could be permeable to sunlight, wind (and emissions) and water, but at first it might be simpler to keep it impermeable to economic exchange. In addition, it is easy to externalize all unsustainable practices without a membrane. But the size of the membrane is not predetermined. One possible conclusion might be that the village has to be the size of the whole earth.  Another reason for starting with a village is that changes in biological (and probably other complex) systems always proceed from small populations that can spread out by replication. It is more practical to achieve a global change in lifestyle and technology starting with a small group of willing people who can then inspire others by example, rather than try to impose a change on a large population, the way fascist and communist experiments have proceeded. Another reason for keeping things smaller and more local is that a stronger feedback between production and consumption may arise, which would regulate unsustainable consumption, because the environmental, social and psychological costs of production are visible in the village, as opposed to hidden or abstracted from the consumers. There are other reasons for localization (e.g. resilience, freedom, more meaningful employment for more people, better relations among people or between people and nature), less directly related to climate change, and more speculative. </p>
<p>This is probably the place to admit my main bias. I am a Gandhist Luddite (who has a PhD in Physics, worked as a semiconductor engineer and a molecular biologist) , not the angry, machine-smashing kind, and I like not only to tinker with technology, but to think how it affects people and nature. I don&#8217;t think all technology can be equated with progress. I call this project the Luddite Manhattan Project (or or Localizing and Networking Basic Technology project) for that reason and because it parallels the project that produced the nuclear bomb. I think that the craftspeople and farmers would contribute more to this project than the scientists and engineers. I think that in the multidimensional optimization of technology, we have focused too much on efficiency (disregarding other human values) and that the industrial revolution was largely a mistake (though some good things came out of it, like global communication). If we focus on other human values, we can optimize technology better. I think that localism of basic-needs production (when coupled to non-technological things like democracy) is a constraint from which many other good things such as sustainability, full, meaningful employment, freedom, and good social relations would follow, though it too can be taken to extremes.  Given my bias, I suspect that the kind of technology network that would be most sustainable would be pre-industrial, with a few modern innovations. If we really did the book-keeping accurately we would probably find that industrial production is unsustainable. Or rather we would find that pre-industrial production can be sustainable, while current industrial production is not (I leave open the possibility that industrial production might be sustainable in the future, with new innovations, but even then it tramples too many human values). But these conclusions would be outputs of the project, not pre-assumptions or inputs of the project. I welcome some discussion of these ideas,  followed by computation, testing and implementation.</p>
<p>The technical part of the project is basically a networking problem. It would allow initial imports (in a way that would allow replicability&#8212;that is don&#8217;t hog a disproportionate fraction of resources into the village) into a specific location and then network existing technologies so that the system is self-sustaining. What one craftsperson produces, others in the village must use so that the village can continue in perpetuity. A blacksmith needs some fuel, but also customers who need his products and can exchange stuff that he needs. A cooper is mostly useless in the current industrial economy, but would probably find some use in a local village economy, where people need ways to store water and other liquids. </p>
<p>Here are some typical challenges and questions the project would face: How can antibiotics be made on a village scale with no external inputs? What can&#8217;t be made and can we find substitutes? Are there missing technology links and can we invent them, or do we need to start with another scenario? What food needs to be produced to provide basic caloric needs to all inhabitants of the village? How much area is required? How can water be captured and transported without plastic or rubber? How much carbon is emitted in production of everything? Where does garbage go? How can metals be recycled? Can plastic be produced? Can electronics be produced? Is there enough time for art, science, scholarship and other forms of edifying human activity? What kind of economic systems work? Is there an optimal one as far as sustainability, or is it a matter of personal preference? These are all questions that can be tackled, if we face them with curiosity and realism, instead of with fear and the kind of magical thinking that most people have towards technology and other things they don&#8217;t understand. I&#8217;ve heard that Leonardo Da Vinci was the last man to understand the technology of his age, but we have computers to help us.</p>
<p>It might be appropriate at this stage to mention that I do not advocate giving up entirely the industrial mode of production, or the global trade it requires. The Localizing and Networking Basic Technology project would address only food, shelter, water, medicine, all the subsidiary crafts necessary to sustain these, and a few edifying human activities like art, music and scholarship. Computers and internet hardware are almost certainly best left to industrial production, and so are cars, airplanes (but the need for these will drastically decrease if this project is successful), some of the parts for particle accelerators and fancy biotech equipment, etc.</p>
<p>The initial computational stage of the project could model itself on online multiplayer games like Warcraft and planning games like Sim City (I have tried to contact Will Wright, to no avail). I do not play these games (I prefer simple low tech games personally), but I see the usefulness of online collaboration and computation for this project, as a sort of in-silico evolution. Programmers and mathematicians could set up the software to allow both online collaboration and some central planning. I think the simplest solutions should be tried first, i.e. the most primitive technologies, like hunting and gathering. My educated guess is that they will be shown incapable of providing basic needs given the current world population. The same conclusion would probably follow for current industrial production, except the incapacity would be with regards to sustainability. I predict the sweet spot where both sustainability and capacity to “feed the world” (meaning provide a decent life) would be achieved by pre-industrial, agrarian and craft-based production. </p>
<p>I am totally willing to be proven wrong by this experiment about my anti-industrialization bias. With regards to scientific experimentation, there needs to be well posed hypotheses that can be proven wrong, and good controls. The engineering approach is an alternative. Who is willing to work on this project? Let&#8217;s make amends for unleashing the horror of the Bomb on the earth, tackle climate change realistically and have some technical fun. For further information please see:</p>
<p>&bull; Iuval Clejan, <a href="http://culturalspeciation.blogspot.com/2012/04/luddite-manhattan-project-first-stage.html">Luddite Manhattan Project, first stage</a>, 16 April 2012.</p>
<p>&bull; Iuval Clejan, <a href="http://culturalspeciation.blogspot.com/2012/02/proposal-for-funding-blueprint-of.html">A proposal for funding a blueprint of a village-based technology ecosystem</a>, 5 February 2012.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/05/08/localizing-and-networking-basic-technology/#comments">68 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/sustainability/" rel="category tag">sustainability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/05/08/localizing-and-networking-basic-technology/" rel="bookmark" title="Permanent Link to Localizing and Networking Basic&nbsp;Technology">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/4/?s=information+and+entropy" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/page/2/?s=information+and+entropy" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/page/3/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s0.wp.com/_static/??/wp-content/mu-plugins/carousel/swiper-bundle.css,/wp-content/mu-plugins/carousel/jetpack-carousel.css?m=1630955947j&cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2015%2F03%2F21%2F19395%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jt0OgjAMhV/I0sCFCRfGZxlbQzbZj2sn8PZOI4lBw1V7TvP1HJwT6BiEgqBjNPSwmtLSOD7h18kXSFMZbWBUxtsAg8roFQvluoFkpW+8h+o/dy+U189o5qSjh5TjskKm6rFsjA16Kob4BVVJfiDT1KCDIlrlWJgmdCSp5sNmHDCzNSMJI5eBdbZJbAw/vf9kwLv6Tlbu6i/tuev7tuvOrXsCIKR8xw=='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZVVEYlL09GczNGJkhCfGxfSzIsTTFVZWlnc0ZuY2gxOWx5cjdVaF0/TFA5c0pUeHpGYS9pL1BFeTE1al1hXXorOFY0MF10eDVpTzcuRDlbSyx5c2xEZF9OdkpDQ2prTWNVa3NFV1lbLlVhcV04UCs1JXQtZF1bfGd+JWtuVFF5RXZ8dWRCLFNiYT9xd1tmaXYsbXVmUEtVVDBfaVImRVRuRWFGZkdPJjRNLTYmLXl2bklofnBheGFQYUN0ZEZKamJJZkRE'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>