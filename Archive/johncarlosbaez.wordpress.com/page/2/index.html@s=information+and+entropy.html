<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth | Page 2</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F08%2Finformation-geometry-part-19%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/page\/2\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fpage%2F2%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F08%2Finformation-geometry-part-19%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Page 2 &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results paged paged-2 search-paged-2 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-31491 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31491">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/" rel="bookmark">Information Geometry (Part&nbsp;19)</a></h2>
				<small>8 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Last time</a> I figured out the analogue of <i>momentum</i> in probability theory, but I didn&#8217;t say what it&#8217;s called.  Now I will tell you&#8212;thanks to some help from <a href="https://twitter.com/Abelaer/status/1423656405286916098">Abel Jansma</a> and <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/#comment-171310">Toby Bartels</a>.</p>
<p><b>SURPRISE:</b> it&#8217;s called <b>SURPRISAL!</b></p>
<p>This is a well-known concept in information theory.  It&#8217;s also called &#8216;<a href="https://en.wikipedia.org/wiki/Information_content">information content</a>&#8216;.</p>
<p>Let&#8217;s see why.  First, let&#8217;s remember the setup.   We have a manifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+q_i+%3E+0%2C+%5C%3B+%5Csum_%7Bi%3D1%7D%5En+q_i+%3D+1+%5C%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = &#92;{ q &#92;in &#92;mathbb{R}^n : &#92;; q_i &gt; 0, &#92;; &#92;sum_{i=1}^n q_i = 1 &#92;} } " class="latex" /></p>
<p>whose points <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> are nowhere vanishing probability distributions on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}." class="latex" />   We have a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>called the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>, defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Csum_%7Bj+%3D+1%7D%5En+q_j+%5Cln+q_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;sum_{j = 1}^n q_j &#92;ln q_j } " class="latex" /></p>
<p>For each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> we define a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+T%5E%5Cast_q+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in T^&#92;ast_q Q" class="latex" /> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q" class="latex" /></p>
<p>As mentioned <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">last time</a>, this is the analogue of momentum in probability theory.   In the second half of this post I&#8217;ll say more about exactly why.   But first let&#8217;s compute it and see what it actually equals!</p>
<p>Let&#8217;s start with a naive calculation, acting as if the probabilities <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> were a coordinate system on the manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   We get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} }" class="latex" /></p>
<p>so using the definition of the Shannon entropy we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++p_i+%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+%5Csum_%7Bj+%3D+1%7D%5En+q_j+%5Cln+q_j++%7D%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+%5Cleft%28+q_i+%5Cln+q_i+%5Cright%29+%7D+%5C%5C+%5C%5C++%26%3D%26+-%5Cln%28q_i%29+-+1++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  p_i &amp;=&amp; &#92;displaystyle{ -&#92;frac{&#92;partial}{&#92;partial q_i} &#92;sum_{j = 1}^n q_j &#92;ln q_j  }&#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{&#92;partial}{&#92;partial q_i} &#92;left( q_i &#92;ln q_i &#92;right) } &#92;&#92; &#92;&#92;  &amp;=&amp; -&#92;ln(q_i) - 1  &#92;end{array}  " class="latex" /></p>
<p>Now, the quantity <img src="https://s0.wp.com/latex.php?latex=-%5Cln+q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;ln q_i" class="latex" /> is called the <b>surprisal</b> of the probability distribution at <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />   Intuitively, it&#8217;s a measure of how surprised you should be if an event of probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> occurs.    For example, if you flip a fair coin and it lands heads up, your surprisal is ln 2.   If you flip 100 fair coins and they all land heads up, your surprisal is 100 times ln 2.</p>
<p>Of course &#8216;surprise&#8217; is a psychological term, not a term from math or physics, so we shouldn&#8217;t take it too seriously here.  We can derive the concept of surprisal from three axioms:</p>
<ol>
<li>The surprisal of an event of probability <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is some function of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=F%28q%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(q)." class="latex" /></li>
<li>The less probable an event is, the larger its surprisal is: <img src="https://s0.wp.com/latex.php?latex=q_1+%5Cle+q_2+%5Cimplies++F%28q_1%29+%5Cge+F%28q_2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1 &#92;le q_2 &#92;implies  F(q_1) &#92;ge F(q_2)." class="latex" /></li>
<li>The surprisal of two independent events is the sum of their surprisals: <img src="https://s0.wp.com/latex.php?latex=F%28q_1+q_2%29+%3D+F%28q_1%29+%2B+F%28q_2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(q_1 q_2) = F(q_1) + F(q_2)." class="latex" /></li>
</ol>
<p>It follows from work on <a href="https://en.wikipedia.org/wiki/Cauchy%27s_functional_equation">Cauchy&#8217;s functional equation</a> that <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> must be of this form:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28q%29+%3D+-+%5Clog_b+q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(q) = - &#92;log_b q " class="latex" /></p>
<p>for some constant <img src="https://s0.wp.com/latex.php?latex=b+%3E+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b &gt; 1." class="latex" />   We shall choose <img src="https://s0.wp.com/latex.php?latex=b%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b," class="latex" /> the base of our logarithms, to be <img src="https://s0.wp.com/latex.php?latex=e.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e." class="latex" />   We had a similar freedom of choice in defining the Shannon entropy, and we will use base <img src="https://s0.wp.com/latex.php?latex=e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e" class="latex" /> for both to be consistent.  If we chose something else, it would change the surprisal and the Shannon entropy by the same constant factor.</p>
<p>So far, so good.  But what about the irksome &#8220;-1&#8221; in our formula?</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+-%5Cln%28q_i%29+-+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = -&#92;ln(q_i) - 1 " class="latex" /></p>
<p>Luckily it turns out we can just get rid of this!   The reason is that the probabilities <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are not really coordinates on the manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />  They&#8217;re not independent: they must sum to 1.  So, when we change them a little, the sum of their changes must vanish.  Putting it more technically, the tangent space <img src="https://s0.wp.com/latex.php?latex=T_q+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_q Q" class="latex" /> is not all of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n," class="latex" /> but just the subspace consisting of vectors whose components sum to zero:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+T_q+Q+%3D+%5C%7B+v+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+%5Csum_%7Bj+%3D+1%7D%5En+v_j+%3D+0+%5C%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ T_q Q = &#92;{ v &#92;in &#92;mathbb{R}^n : &#92;; &#92;sum_{j = 1}^n v_j = 0 &#92;} }" class="latex" /></p>
<p>The cotangent space is the dual of the tangent space.  The dual of a subspace</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%5Csubseteq+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S &#92;subseteq V" class="latex" /></p>
<p>is the quotient space</p>
<p><img src="https://s0.wp.com/latex.php?latex=V%5E%5Cast%2F%5C%7B+%5Cell+%5Ccolon+V+%5Cto+%5Cmathbb%7BR%7D+%3A+%5C%3B+%5Cforall+v+%5Cin+S+%5C%3B+%5C%2C+%5Cell%28v%29+%3D+0+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V^&#92;ast/&#92;{ &#92;ell &#92;colon V &#92;to &#92;mathbb{R} : &#92;; &#92;forall v &#92;in S &#92;; &#92;, &#92;ell(v) = 0 &#92;} " class="latex" /></p>
<p>The cotangent space <img src="https://s0.wp.com/latex.php?latex=T_q%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_q^&#92;ast Q" class="latex" /> thus consists of linear functionals <img src="https://s0.wp.com/latex.php?latex=%5Cell+%5Ccolon+%5Cmathbb%7BR%7D%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell &#92;colon &#92;mathbb{R}^n &#92;to &#92;mathbb{R}" class="latex" /> modulo those that vanish on vectors <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> obeying the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bj+%3D+1%7D%5En+v_j+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{j = 1}^n v_j = 0 } " class="latex" /></p>
<p>Of course, we can identify the dual of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> in the usual way, using the Euclidean inner product: a vector <img src="https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="u &#92;in &#92;mathbb{R}^n" class="latex" /> corresponds to the linear functional</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cell%28v%29+%3D+%5Csum_%7Bj+%3D+1%7D%5En+u_j+v_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;ell(v) = &#92;sum_{j = 1}^n u_j v_j } " class="latex" /></p>
<p>From this, you can see that a linear functional <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> vanishes on all vectors <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> obeying the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bj+%3D+1%7D%5En+v_j+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{j = 1}^n v_j = 0 } " class="latex" /></p>
<p>if and only if its corresponding vector <img src="https://s0.wp.com/latex.php?latex=u&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="u" class="latex" /> has</p>
<p><img src="https://s0.wp.com/latex.php?latex=u_1+%3D+%5Ccdots+%3D+u_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="u_1 = &#92;cdots = u_n " class="latex" /></p>
<p>So, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast_q+Q+%5Ccong+%5Cmathbb%7BR%7D%5En%2F%5C%7B+u+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+u_1+%3D+%5Ccdots+%3D+u_n+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast_q Q &#92;cong &#92;mathbb{R}^n/&#92;{ u &#92;in &#92;mathbb{R}^n : &#92;; u_1 = &#92;cdots = u_n &#92;}" class="latex" /></p>
<p>In words: we can describe cotangent vectors to <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> as lists of <i>n</i> numbers if we want, <i>but</i> we have to remember that adding the same constant to each number in the list doesn&#8217;t change the cotangent vector!</p>
<p>This suggests that our naive formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cln%28q_i%29+-+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;ln(q_i) - 1 " class="latex" /></p>
<p>is on the right track, but we&#8217;re free to get rid of the constant 1 if we want!   And that&#8217;s true.</p>
<p>To check this rigorously, we need to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p%28v%29+%3D+-%5Csum_%7Bj%3D1%7D%5En+%5Cln%28q_i%29+v_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p(v) = -&#92;sum_{j=1}^n &#92;ln(q_i) v_i} " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+T_q+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v &#92;in T_q Q." class="latex" />   We compute:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++p%28v%29+%26%3D%26+df%28v%29+%5C%5C+%5C%5C++%26%3D%26+v%28f%29+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7Bj%3D1%7D%5En+v_j+%5C%2C+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_j%7D+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7Bj%3D1%7D%5En+v_j+%28-%5Cln%28q_i%29+-+1%29+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Csum_%7Bj%3D1%7D%5En+%5Cln%28q_i%29+v_i+%7D++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  p(v) &amp;=&amp; df(v) &#92;&#92; &#92;&#92;  &amp;=&amp; v(f) &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_{j=1}^n v_j &#92;, &#92;frac{&#92;partial f}{&#92;partial q_j} } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_{j=1}^n v_j (-&#92;ln(q_i) - 1) } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;sum_{j=1}^n &#92;ln(q_i) v_i }  &#92;end{array}  " class="latex" /></p>
<p>where in the second to last step we used our earlier calculation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+%5Csum_%7Bj+%3D+1%7D%5En+q_j+%5Cln+q_j+%3D+-%5Cln%28q_i%29+-+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;partial f}{&#92;partial q_i} = -&#92;frac{&#92;partial}{&#92;partial q_i} &#92;sum_{j = 1}^n q_j &#92;ln q_j = -&#92;ln(q_i) - 1 } " class="latex" /></p>
<p>and in the last step we used</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_j+v_j+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_j v_j = 0 } " class="latex" /></p>
<h3> Back to the big picture </h3>
<p>Now let&#8217;s take stock of where we are.   We can fill in the question marks in the charts from last time, and combine those charts while we&#8217;re at it.</p>
<div align="center">
<table border="1">
<tbody>
<tr>
<td></td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;<i>q</i> &nbsp;</td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">probabilities</td>
</tr>
<tr>
<td>&nbsp;<i>p</i>&nbsp;</td>
<td style="text-align:center;"> momentum</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">surprisals</td>
</tr>
<tr>
<td>&nbsp;<i>S</i> &nbsp;</td>
<td style="text-align:center;"> action</td>
<td style="text-align:center;">entropy</td>
<td style="text-align:center;"> Shannon entropy</td>
</tr>
</tbody>
</table>
</div>
<p>What&#8217;s going on here?   In classical mechanics, action is minimized (or at least the system finds a critical point of the action).  In thermodynamics, entropy is maximized.  In the maximum entropy approach to probability, Shannon entropy is maximized.  This leads to a mathematical analogy that&#8217;s quite precise.   For classical mechanics and thermodynamics, I explained it here:</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2012/01/19/classical-mechanics-versus-thermodynamics-part-1/">Classical mechanics versus thermodynamics (part 1)</a>.</p>
<p>• <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/">Classical mechanics versus thermodynamics (part 2)</a>.</p>
<p>These posts may give a more approachable introduction to what I&#8217;m doing now: now I&#8217;m bringing <i>probability theory</i> into the analogy, with a big emphasis on symplectic and contact geometry.</p>
<p>Let me spell out a bit of the analogy more carefully:</p>
<p><b>Classical Mechanics.</b>   In classical mechanics, we have a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> whose points are positions of a particle.  There&#8217;s an important function on this manifold: Hamilton&#8217;s principal function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>What&#8217;s this?   It&#8217;s basically <b>action</b>: <img src="https://s0.wp.com/latex.php?latex=f%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q)" class="latex" /> is the action of the least-action path from the position <img src="https://s0.wp.com/latex.php?latex=q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0" class="latex" /> at some earlier time <img src="https://s0.wp.com/latex.php?latex=t_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_0" class="latex" /> to the position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> at time 0.   The Hamilton&#8211;Jacobi equations say the particle&#8217;s momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> at time 0 is given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p><b>Thermodynamics.</b>  In thermodynamics, we have a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> whose points are equilibrium states of a system.  The coordinates of a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> are called <b>extensive variables</b>.   There&#8217;s an important function on this manifold: the <b>entropy</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>There is a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>The components of this vector are the <b>intensive variables</b> corresponding to the extensive variables.</p>
<p><b>Probability Theory.</b>  In probability theory, we have a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> whose points are nowhere vanishing probability distributions on a finite set.   The coordinates of a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> are <b>probabilities</b>.   There&#8217;s an important function on this manifold: the <b>Shannon entropy</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>There is a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>The components of this vector are the <b>surprisals</b> corresponding to the probabilities.</p>
<p>In all three cases, <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is a symplectic manifold and imposing the constraint <img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q" class="latex" /> picks out a Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B+%28q%2Cp%29+%5Cin+T%5E%5Cast+Q%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{ (q,p) &#92;in T^&#92;ast Q: &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>There is also a contact manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> where the extra dimension comes with an extra coordinate <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> that means</p>
<p>• action in classical mechanics,<br />
• entropy in thermodynamics, and<br />
• Shannon entropy in probability theory.</p>
<p>We can then decree that <img src="https://s0.wp.com/latex.php?latex=S+%3D+f%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f(q)" class="latex" /> along with <img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q," class="latex" /> and these constraints pick out a Legendrian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+S+%3D+f%28q%29%2C+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{(q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; S = f(q), &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>There&#8217;s a lot more to do with these ideas, and I&#8217;ll continue next time.</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p>• <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comments">29 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;19)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31321 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31321">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/" rel="bookmark">Information Geometry (Part&nbsp;18)</a></h2>
				<small>5 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">Last time</a> I sketched how two related forms of geometry, <a href="https://en.wikipedia.org/wiki/Symplectic_geometry">symplectic</a> and <a href="https://en.wikipedia.org/wiki/Contact_geometry">contact</a> geometry, show up in thermodynamics.  Today I want to explain how they show up in probability theory.</p>
<p>For some reason I haven&#8217;t seen much discussion of this!  But people should have looked into this.  After all, statistical mechanics explains thermodynamics in terms of probability theory, so if some mathematical structure shows up in thermodynamics it should appear in statistical mechanics&#8230; and thus ultimately in probability theory.</p>
<p>I just figured out how this works for symplectic and contact geometry.</p>
<p>Suppose a system has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> possible states.  We&#8217;ll call these <b><a href="https://en.wikipedia.org/wiki/Microstate_(statistical_mechanics)">microstates</a></b>, following the tradition in statistical mechanics.   If you don&#8217;t know what &#8216;microstate&#8217; means, don&#8217;t worry about it!  But the rough idea is that if you have a macroscopic system like a rock, the precise details of what its atoms are doing are described by a microstate, and many different microstates could be indistinguishable unless you look very carefully.</p>
<p>We&#8217;ll call the microstates <img src="https://s0.wp.com/latex.php?latex=1%2C+2%2C+%5Cdots%2C+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1, 2, &#92;dots, n." class="latex" />  So, if you don&#8217;t want to think about physics, when I say <b>microstate</b> I&#8217;ll just mean an integer from 1 to <em>n</em>.</p>
<p>Next, a <b>probability distribution</b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> assigns a real number <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> to each microstate, and these numbers must sum to 1 and be nonnegative.  So, we have <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathbb%7BR%7D%5En%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in &#92;mathbb{R}^n," class="latex" /> though not every vector in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> is a probability distribution.</p>
<p>I&#8217;m sure you&#8217;re wondering why I&#8217;m using <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> rather than <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to stand for an observable instead of a probability distribution.   Am I just trying to confuse you?</p>
<p>No: I&#8217;m trying to set up an analogy to physics!</p>
<p><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">Last time</a> I introduced symplectic geometry using classical mechanics.  The most important example of a symplectic manifold is the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> of a manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   A point of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is a pair <img src="https://s0.wp.com/latex.php?latex=%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q,p)" class="latex" /> consisting of a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> and a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+T%5E%5Cast_q+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in T^&#92;ast_q Q." class="latex" />  In classical mechanics the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> describes the position of some physical system, while <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> describes its momentum.</p>
<p>So, I&#8217;m going to set up an analogy like this:</p>
<div align="center">
<table border="1">
<tr>
<td> </td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;"><b>Probability Theory</b></td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q " class="latex" /> &nbsp; </td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">&nbsp; probability distribution &nbsp; </td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> &nbsp;</td>
<td style="text-align:center;">momentum</td>
<td style="text-align:center;">???</td>
</tr>
</table>
</div>
<p>But <i>what is to momentum as probability is to position?</i></p>
<p>A big clue is the appearance of symplectic geometry in thermodynamics, which I also outlined <a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">last time</a>.    We can use this to get some intuition about the analogue of momentum in probability theory.</p>
<p>In thermodynamics, a system has a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> of states.  (These are not the &#8216;microstates&#8217; I mentioned before: we&#8217;ll see the relation later.)  There is a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>describing the <a href="https://en.wikipedia.org/wiki/Entropy">entropy</a> of the system as a function of its state.   There is a law of thermodynamics saying that</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>This equation picks out a submanifold of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q," class="latex" /> namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{(q,p) &#92;in T^&#92;ast Q : &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>Moreover this submanifold is Lagrangian: the symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> vanishes when restricted to it:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Comega+%7C_%5CLambda+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;omega |_&#92;Lambda = 0 } " class="latex" /></p>
<p>This is very beautiful, but it goes by so fast you might almost miss it!   So let&#8217;s clutter it up a bit with coordinates.  We often use local coordinates on <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> and describe a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> using these coordinates, getting a point</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28q_1%2C+%5Cdots%2C+q_n%29+%5Cin+%5Cmathbb%7BR%7D%5En+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1, &#92;dots, q_n) &#92;in &#92;mathbb{R}^n " class="latex" /></p>
<p>They give rise to local coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n%2C+p_1%2C+%5Cdots%2C+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n, p_1, &#92;dots, p_n" class="latex" /> on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   The <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are called <b>extensive variables</b>, because they are typically things that you can measure only by totalling up something over the whole system, like the energy or volume of a cylinder of gas.   The <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are called <b>intensive variables</b>, because they are typically things that you can measure locally at any point, like temperature or pressure.</p>
<p>In these local coordinates, the symplectic structure on <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is the 2-form given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+dp_1+%5Cwedge+dq_1+%2B+%5Ccdots+%2B+dp_n+%5Cwedge+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = dp_1 &#92;wedge dq_1 + &#92;cdots + dp_n &#92;wedge dq_n " class="latex" /></p>
<p>The equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q" class="latex" /></p>
<p>serves as a law of physics that determines the intensive variables given the extensive ones when our system is in thermodynamic equilibrium.   Written out using coordinates, this law says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>It looks pretty bland here, but in fact it gives formulas for the temperature and pressure of a gas, and many other useful formulas in thermodynamics.</p>
<p>Now we are ready to see how all this plays out in probability theory!  We&#8217;ll get an analogy like this, which goes hand-in-hand with our earlier one:</p>
<div align="center">
<table border="1">
<tr>
<td> </td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q " class="latex" /> &nbsp; </td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">&nbsp; probability distribution &nbsp; </td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> &nbsp;</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">???</td>
</tr>
</table>
</div>
<p>This analogy is clearer than the last, because statistical mechanics reveals that the extensive variables in thermodynamics are really just summaries of probability distributions on microstates.  Furthermore, both thermodynamics and probability theory have a concept of <i>entropy</i>.</p>
<p>So, let&#8217;s take our manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> to consist of probability distributions on the set of microstates I was talking about before: the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}." class="latex" />   Actually, let&#8217;s use <i>nowhere vanishing</i> probability distributions:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+q_i+%3E+0%2C+%5C%3B+%5Csum_%7Bi%3D1%7D%5En+q_i+%3D+1+%5C%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = &#92;{ q &#92;in &#92;mathbb{R}^n : &#92;; q_i &gt; 0, &#92;; &#92;sum_{i=1}^n q_i = 1 &#92;} } " class="latex" /></p>
<p>I&#8217;m requiring <img src="https://s0.wp.com/latex.php?latex=q_i+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i &gt; 0" class="latex" /> to ensure <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a manifold, and also to make sure <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is differentiable: it ceases to be differentiable when one of the probabilities <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> hits zero.</p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a manifold, its cotangent bundle is a symplectic manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   And here&#8217;s the good news: we have a god-given entropy function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>namely the <b><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a></b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Csum_%7Bi+%3D+1%7D%5En+q_i+%5Cln+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;sum_{i = 1}^n q_i &#92;ln q_i } " class="latex" /></p>
<p>So, everything I just described about thermodynamics works in the setting of plain old probability theory!  Starting from our manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> and the entropy function, we get all the rest, leading up to the Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{(q,p) &#92;in T^&#92;ast Q : &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>that describes the relation between extensive and intensive variables.</p>
<p>For computations it helps to pick coordinates on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   Since the probabilities <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> sum to 1, they aren&#8217;t independent coordinates on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   So, we can either pick all but one of them as coordinates, or learn how to deal with non-independent coordinates, which are already completely standard in <a href="https://en.wikipedia.org/wiki/Homogeneous_coordinates">projective geometry</a>.  Let&#8217;s do the former, just to keep things simple.</p>
<p>These coordinates on <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> give rise in the usual way to coordinates <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   These play the role of extensive and intensive variables, respectively, and it should be very interesting to impose the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is the Shannon entropy.   This picks out a Lagrangian submanifold <img src="https://s0.wp.com/latex.php?latex=%5CLambda+%5Csubseteq+T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda &#92;subseteq T^&#92;ast Q." class="latex" /></p>
<p>So, the question becomes: what does this mean?  If this formula gives the analogue of momentum for probability theory, what does this analogue of momentum <i>mean?</i></p>
<p>Here&#8217;s a preliminary answer: <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how fast entropy increases as we increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> that our system is in the <i>i</i>th microstate.  So if we think of nature as &#8216;wanting&#8217; to maximize entropy, the quantity <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how eager it is to increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" /></p>
<p>Indeed, you can think of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> as a bit like <i>pressure</i>&#8212;one of the most famous intensive quantities in thermodynamics.   A gas &#8216;wants&#8217; to expand, and its pressure says precisely how eager it is to expand.   Similarly, a probability distribution &#8216;wants&#8217; to flatten out, to maximize entropy, and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how eager it is to increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> in order to do this.</p>
<p>But what can we <i>do</i> with this concept?  And what does symplectic geometry <i>do</i> for probability theory?</p>
<p>I will start tackling these questions next time.</p>
<p>One thing I&#8217;ll show is that when we reduce thermodynamics to probability theory using the ideas of statistical mechanics, the appearance of symplectic geometry in thermodynamics <i>follows</i> from its appearance in probability theory.</p>
<p>Another thing I want to investigate is how other geometrical structures on the space of probability distributions, like the <a href="https://math.ucr.edu/home/baez/information/information_geometry_1.html">Fisher information metric</a>, interact with the symplectic structure on its cotangent bundle.   This will integrate symplectic geometry and information geometry.</p>
<p>I also want to bring contact geometry into the picture.  It&#8217;s already easy to see from our work last time how this should go.  We treat the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as an independent variable, and replace <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> with a larger manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> having <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as an extra coordinate.  This is a contact manifold with contact form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_1+dq_i+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_1 dq_i + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>This contact manifold has a submanifold <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> where we remember that entropy is a function of the probability distribution <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> and define <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in terms of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as usual:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+S+%3D+f%28q%29%2C+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{(q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; S = f(q), &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>And as we saw last time, <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> is a Legendrian submanifold, meaning</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Calpha%7C_%7B%5CSigma%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;alpha|_{&#92;Sigma} = 0 } " class="latex" /></p>
<p>But again, we want to understand what these ideas from contact geometry really <i>do</i> for probability theory!</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p>• <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/#comments">2 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;18)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31330 post type-post status-publish format-standard hentry category-information-and-entropy category-physics category-probability" id="post-31330">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/" rel="bookmark">Information Geometry (Part&nbsp;17)</a></h2>
				<small>27 July, 2021</small><br />


				<div class="entry">
					<p>I&#8217;m getting back into information geometry, which is the geometry of the space of probability distributions, studied using tools from information theory.  I&#8217;ve written a bunch about it already, which you can see here:</p>
<p>• <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
<p>Now I&#8217;m fascinated by something new: how <a href="https://en.wikipedia.org/wiki/Symplectic_geometry">symplectic geometry</a> and <a href="https://en.wikipedia.org/wiki/Contact_geometry">contact geometry</a> show up in information geometry.   But before I say anything about this, let me say a bit about how they show up in thermodynamics.  This is more widely discussed, and it&#8217;s a good starting point.</p>
<p>Symplectic geometry was born as the geometry of <a href="https://en.wikipedia.org/wiki/Phase_space">phase space</a> in classical mechanics: that is, the space of possible positions and momenta of a classical system.  The simplest example of a symplectic manifold is the vector space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n}," class="latex" /> with <i>n</i> position coordinates <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <i>n</i> momentum coordinates <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /></p>
<p>It turns out that symplectic manifolds are always even-dimensional, because we can always cover them with coordinate charts that look like <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n}." class="latex" />  When we change coordinates, it turns out that the splitting of coordinates into positions and momenta is somewhat arbitrary.  For example, the position of a rock on a spring now may determine its momentum a while later, and vice versa.   What&#8217;s not arbitrary?  It&#8217;s the so-called &#8216;<a href="https://en.wikipedia.org/wiki/Symplectic_manifold#Definition">symplectic structure</a>’:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+dp_1+%5Cwedge+dq_1+%2B+%5Ccdots+%2B+dp_n+%5Cwedge+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = dp_1 &#92;wedge dq_1 + &#92;cdots + dp_n &#92;wedge dq_n " class="latex" /></p>
<p>While far from obvious at first, we know by now that the symplectic structure is exactly what needs to be preserved under valid changes of coordinates in classical mechanics!  In fact, we can develop the whole formalism of classical mechanics starting from a manifold with a symplectic structure.</p>
<p>Symplectic geometry also shows up in thermodynamics.  In thermodynamics we can start with a system in equilibrium whose state is described by some variables <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n." class="latex" />  Its entropy will be a function of these variables, say</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+f%28q_1%2C+%5Cdots%2C+q_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f(q_1, &#92;dots, q_n)" class="latex" /></p>
<p>We can then take the partial derivatives of entropy and call them something:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>These new variables <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are said to be &#8216;<a href="https://en.wikipedia.org/wiki/Conjugate_variables_(thermodynamics)">conjugate</a>’ to the <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> and they turn out to be very interesting.  For example, if <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> is energy then <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is &#8216;coolness&#8217;: the reciprocal of temperature.  The coolness of a system is its change in entropy per change in energy.</p>
<p>Often the variables <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are &#8216;<a href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties#Extensive_properties">extensive</a>’: that is, you can measure them only by looking at your whole system and totaling up some quantity.  Examples are energy and volume.  Then the new variables <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are &#8216;<a href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties#Intensive_properties">intensive</a>’: that is, you can measure them at any one location in your system.   Examples are coolness and pressure.</p>
<p>Now for a twist: sometimes we do not know the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> ahead of time.   Then we cannot define the <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> as above.  We&#8217;re forced into a different approach where we treat them as independent quantities, at least until someone tells us what <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is.</p>
<p>In this approach, we start with a space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n}" class="latex" /> having <i>n</i> coordinates called <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <i>n</i> coordinates called <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />   This is a symplectic manifold, with the symplectic struture <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> described earlier!</p>
<p>But what about the entropy?   We don&#8217;t yet know what it is as a function of the <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> but we may still want to talk about it.  So, we build a space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}" class="latex" /> having one extra coordinate <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> in addition to the <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />  This new coordinate stands for entropy.  And this new space has an important 1-form on it:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_1+dq_i+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_1 dq_i + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>This is called the &#8216;contact 1-form&#8217;.</p>
<p>This makes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}" class="latex" /> into an example of a &#8216;contact manifold&#8217;.  Contact geometry is the odd-dimensional partner of symplectic geometry.  Just as symplectic manifolds are always even-dimensional, contact manifolds are always odd-dimensional.</p>
<p>What is the point of the contact 1-form?  Well, suppose someone tells us the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> relating entropy to the coordinates <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   Now we know that we want</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f" class="latex" /></p>
<p>and also</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} }" class="latex" /></p>
<p>So, we can impose these equations, which pick out a subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}." class="latex" />   You can check that this subset, say <img src="https://s0.wp.com/latex.php?latex=%5CSigma%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma," class="latex" /> is an <i>n</i>-dimensional submanifold.  But even better, the contact 1-form vanishes when restricted to this submanifold:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.%5Calpha%5Cright%7C_%5CSigma+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left.&#92;alpha&#92;right|_&#92;Sigma = 0 " class="latex" /></p>
<p>Let&#8217;s see why!   Suppose <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Sigma" class="latex" /> and suppose <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+T_x+%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v &#92;in T_x &#92;Sigma" class="latex" /> is a vector tangent to <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> at this point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  It suffices to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha%28v%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha(v) = 0" class="latex" /></p>
<p>Using the definition of <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> this equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-dS%28v%29+%2B+%5Csum_i+p_i+dq_i%28v%29+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -dS(v) + &#92;sum_i p_i dq_i(v) = 0 } " class="latex" /></p>
<p>But on the surface <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+f%2C+%5Cqquad++%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f, &#92;qquad  &#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} }" class="latex" /></p>
<p>So, the equation we&#8217;re trying to show can be written as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-df%28v%29+%2B+%5Csum_i+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+dq_i%28v%29+%3D+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -df(v) + &#92;sum_i &#92;frac{&#92;partial f}{&#92;partial q_i} dq_i(v) = 0 }" class="latex" /></p>
<p>But this follows from</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+df+%3D+%5Csum_i+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+dq_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ df = &#92;sum_i &#92;frac{&#92;partial f}{&#92;partial q_i} dq_i } " class="latex" /></p>
<p>which holds because <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a function only of the coordinates <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" /></p>
<p>So, any formula for entropy <img src="https://s0.wp.com/latex.php?latex=S+%3D+f%28q_1%2C+%5Cdots%2C+q_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f(q_1, &#92;dots, q_n)" class="latex" /> picks out a so-called &#8216;Legendrian submanifold&#8217; of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}:" class="latex" /> that is, an <i>n</i>-dimensional submanifold such that the contact 1-form vanishes when restricted to this submanifold.  And the idea is that this submanifold tells you everything you need to know about a thermodynamic system.</p>
<p>Indeed, V. I. Arnol&#8217;d says this was implicitly known to the great founder of statistical mechanics, Josiah Willard Gibbs.  Arnol&#8217;d calls <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E5&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^5" class="latex" /> with coordinates energy, entropy, temperature, pressure and volume the &#8216;Gibbs manifold&#8217;, and he proclaims:</p>
<blockquote><p>
<b>Gibbs&#8217; thesis</b>: substances are Legendrian submanifolds of the Gibbs manifold.</p></blockquote>
<p>This is from here:</p>
<p>• V. I. Arnol&#8217;d, Contact geometry: the geometrical method of Gibbs&#8217; thermodynamics, <i>Proceedings of the Gibbs Symposium (New Haven, CT, 1989)</i>, AMS, Providence, Rhode Island, 1990.</p>
<h3> A bit more detail</h3>
<p>Now I want to say everything again, with a bit of extra detail, assuming more familiarity with manifolds.  Above I was using <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> with coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> to describe the &#8216;extensive&#8217; variables of a thermodynamic system.  But let&#8217;s be a bit more general and use any smooth <i>n</i>-dimensional manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />  Even if <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a vector space, this viewpoint is nice because it&#8217;s manifestly coordinate-independent!</p>
<p>So: starting from <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> we build the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   A point in cotangent describes both extensive variables, namely <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> and &#8216;intensive&#8217; variables, namely a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+T%5E%5Cast_q+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in T^&#92;ast_q Q." class="latex" /></p>
<p>The manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> has a 1-form <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta" class="latex" /> on it called the <a href="https://en.wikipedia.org/wiki/Tautological_one-form">tautological 1-form</a>.  We can describe it as follows.  Given a tangent vector <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+T_%7B%28q%2Cp%29%7D+T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v &#92;in T_{(q,p)} T^&#92;ast Q" class="latex" /> we have to say what <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%28v%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta(v)" class="latex" /> is.   Using the projection</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi+%5Ccolon+T%5E%5Cast+Q+%5Cto+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi &#92;colon T^&#92;ast Q &#92;to Q" class="latex" /></p>
<p>we can project <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> down to a tangent vector <img src="https://s0.wp.com/latex.php?latex=d%5Cpi%28v%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;pi(v)" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  But the 1-form <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> eats tangent vectors at <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and spits out numbers!  So, we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctheta%28v%29+%3D+p%28d%5Cpi%28v%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta(v) = p(d&#92;pi(v))" class="latex" /></p>
<p>This is sort of mind-boggling at first, but it&#8217;s worth pondering until it makes sense.  It helps to work out what <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta" class="latex" /> looks like in local coordinates.  Starting with any local coordinates <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> on an open set of <img src="https://s0.wp.com/latex.php?latex=Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q," class="latex" /> we get local coordinates <img src="https://s0.wp.com/latex.php?latex=q_i%2C+p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i, p_i" class="latex" /> on the cotangent bundle of this open set in the usual way.  On this open set you then get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%3D+p_1+dq_1+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta = p_1 dq_1 + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>This is a standard calculation, which is really worth doing!</p>
<p>It follows that we can define a symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+d+%5Ctheta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = d &#92;theta " class="latex" /></p>
<p>and get this formula in local coordinates:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+dp_1+%5Cwedge+dq_1+%2B+%5Ccdots+%2B+dp_n+%5Cwedge+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = dp_1 &#92;wedge dq_1 + &#92;cdots + dp_n &#92;wedge dq_n " class="latex" /></p>
<p>Now, suppose we choose a smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>which describes the entropy.   We get a 1-form <img src="https://s0.wp.com/latex.php?latex=df&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df" class="latex" />, which we can think of as a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=df+%5Ccolon+Q+%5Cto+T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df &#92;colon Q &#92;to T^&#92;ast Q" class="latex" /></p>
<p>assigning to each choice <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> of extensive variables the pair <img src="https://s0.wp.com/latex.php?latex=%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q,p)" class="latex" /> of extensive and intensive variables where</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+df_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = df_q " class="latex" /></p>
<p>The image of the map <img src="https://s0.wp.com/latex.php?latex=df&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df" class="latex" /> is a &#8216;<a href="https://en.wikipedia.org/wiki/Symplectic_manifold#Lagrangian_and_other_submanifolds">Lagrangian submanifold</a>&#8216; of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q:" class="latex" /> that is, an <i>n</i>-dimensional submanifold <img src="https://s0.wp.com/latex.php?latex=%5CLambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.%5Comega%5Cright%7C_%7B%5CLambda%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left.&#92;omega&#92;right|_{&#92;Lambda} = 0" class="latex" /></p>
<p>Lagrangian submanifolds are to symplectic geometry as Legendrian submanifolds are to contact geometry!  What we&#8217;re seeing here is that if Gibbs had preferred symplectic geometry, he could have described substances as Lagrangian submanifolds rather than Legendrian submanifolds.  But this approach would only keep track of the derivatives of entropy, <img src="https://s0.wp.com/latex.php?latex=df%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df," class="latex" /> not the actual value of the entropy function <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>If we prefer to keep track of the actual value of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> using contact geometry, we can do that.   For this we add an extra dimension to <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> and form the manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}." class="latex" />  The extra dimension represents entropy, so we&#8217;ll use <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as our name for the coordinate on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}." class="latex" /></p>
<p>We can make <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> into a contact manifold with contact 1-form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-d+S+%2B+%5Ctheta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -d S + &#92;theta " class="latex" /></p>
<p>In local coordinates we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_1+dq_i+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_1 dq_i + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>just as we had earlier.   And just as before, if we choose a smooth function <img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /> describing entropy, the subset</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+S+%3D+f%28q%29%2C+%5C%3B+p+%3D+df_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{(q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; S = f(q), &#92;; p = df_q &#92;} " class="latex" /></p>
<p>is a Legendrian submanifold of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}." class="latex" /></p>
<p>Okay, this concludes my lightning review of symplectic and contact geometry in thermodynamics!  Next time I&#8217;ll talk about something a bit less well understood: how they show up in statistical mechanics.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/#comments">10 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;17)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-27387 post type-post status-publish format-standard hentry category-astronomy category-information-and-entropy" id="post-27387">
				<h2><a href="https://johncarlosbaez.wordpress.com/2020/01/25/entropy-in-the-universe/" rel="bookmark">Entropy in the&nbsp;Universe</a></h2>
				<small>25 January, 2020</small><br />


				<div class="entry">
					<p>If you click on this picture, you&#8217;ll see a zoomable image of the Milky Way with <a href="https://www.eso.org/public/news/eso1242/">84 million stars</a>:</p>
<div align="center">
<a href="https://eso.org/public/images/eso1242a/zoomable/"><br />
<img width="450" border="2" src="https://i0.wp.com/math.ucr.edu/home/baez/astronomical/milky_way_gigapixel.jpg" /><br />
</a>
</div>
<p>But stars contribute only a tiny fraction of the total entropy in the observable Universe.  If it&#8217;s random information you want, look elsewhere!</p>
<p>First: what&#8217;s the &#8216;observable Universe&#8217;, exactly?</p>
<p>The further you look out into the Universe, the further you look back in time.  You can&#8217;t see through the hot gas from 380,000 years after the Big Bang.  That &#8216;wall of fire&#8217; marks the limits of the observable Universe.</p>
<div align="center">
<img width="400" src="https://i1.wp.com/math.ucr.edu/home/baez/astronomical/reionization.jpg" />
</div>
<p>But as the Universe expands, the distant ancient stars and gas we see have moved even farther away, so they&#8217;re no longer observable.  Thus, the so-called &#8216;observable Universe&#8217; is really the &#8216;formerly observable Universe&#8217;.  Its edge is 46.5 billion light years away now!</p>
<p>This is true even though the Universe is only 13.8 billion years old.  A standard challenge in understanding general relativity is to figure out how this is possible, given that nothing can move faster than light.</p>
<div align="center">
<img width="440" src="https://i0.wp.com/math.ucr.edu/home/baez/physical/big_bang.jpg" />
</div>
<p>What&#8217;s the total number of stars in the observable Universe? Estimates go up as telescopes improve.  Right now people think there are between 100 and 400 billion stars in the Milky Way.  They think there are between 170 billion and 2 trillion galaxies in the Universe.</p>
<p>In 2009, Chas Egan and Charles Lineweaver estimated the total entropy of all the stars in the observable Universe at 10<sup>81</sup> bits. You should think of these as qubits: it&#8217;s the amount of information to describe the quantum state of <i>everything</i> in all these stars.</p>
<p>But the entropy of interstellar and intergalactic gas and dust is about ten times more the entropy of stars!  It&#8217;s about 10<sup>82</sup> bits.</p>
<p>The entropy in all the photons in the Universe is even more!  The Universe is full of radiation left over from the Big Bang. The photons in the observable Universe left over from the Big Bang have a total entropy of about 10<sup>90</sup> bits.  It&#8217;s called the &#8216;cosmic microwave background radiation&#8217;.</p>
<p>The neutrinos from the Big Bang also carry about 10<sup>90</sup> bits—a bit less than the photons.  The gravitons carry much less, about 10<sup>88</sup> bits.  That&#8217;s because they decoupled from other matter and radiation very early, and have been cooling ever since.  On the other hand, photons in the cosmic microwave background radiation were formed by annihilating<br />
electron-positron pairs until about 10 seconds after the Big Bang. Thus the graviton radiation is expected to be cooler than the microwave background radiation: about 0.6 kelvin as compared to 2.7 kelvin.</p>
<p>Black holes have <i>immensely</i> more entropy than anything listed so far.  Egan and Lineweaver estimate the entropy of stellar-mass black holes in the observable Universe at 10<sup>98</sup> bits.  This is connected to why black holes are so stable: the Second Law says entropy likes to increase.</p>
<p>But the entropy of black holes grows <i>quadratically</i> with mass!  So black holes tend to merge and form bigger black holes — ultimately forming the &#8216;supermassive&#8217; black holes at the centers of most galaxies.  These dominate the entropy of the observable Universe: about 10<sup>104</sup> bits.</p>
<p>Hawking predicted that black holes slowly radiate away their mass when they&#8217;re in a cold enough environment.  But the Universe is much too hot for supermassive black holes to be losing mass now.  Instead, they very slowly <i>grow</i> by eating the cosmic microwave background, even when they&#8217;re not eating stars, gas and dust.</p>
<p>So, only in the far future will the Universe cool down enough for large black holes to start slowly decaying via Hawking radiation. Entropy will continue to increase&#8230; going mainly into photons and gravitons!  This process will take a very long time.  Assuming nothing is falling into it and no unknown effects intervene, a solar-mass black hole takes about 10<sup>67</sup> years to evaporate due to Hawking radiation — while a really big one, comparable to the mass of a galaxy, should take about 10<sup>99</sup> years.</p>
<p>If our current most popular ideas on dark energy are correct, the Universe will continue to expand exponentially.  Thanks to this, there will be a <a href="https://en.wikipedia.org/wiki/Cosmological_horizon#Event_horizon">cosmological event horizon</a> surrounding each observer, which will radiate Hawking radiation at a temperature of roughly 10<sup>-30</sup> kelvin.</p>
<p>In this scenario the Universe in the very far future will mainly consist of massless particles produced as Hawking radiation at this temperature: photons and gravitons.  The entropy within the exponentially expanding ball of space that is <i>today</i> our &#8216;observable Universe&#8217; will continue to increase exponentially&#8230; but more to the point, the entropy density will approach that of a gas of photons and gravitons in thermal equilibrium at 10<sup>-30</sup> kelvin.</p>
<p>Of course, it&#8217;s quite likely that some new physics will turn up, between now and then, that changes the story!  I hope so: this would be a rather dull ending to the Universe.</p>
<p>For more details, go here:</p>
<p>•  Chas A. Egan and Charles H. Lineweaver, <a href="https://arxiv.org/abs/0909.3983">A  larger estimate of the entropy of the universe</a>, <i>The Astrophysical Journal</i> <b>710</b> (2010), 1825.</p>
<p>Also read my page on <a href="http://math.ucr.edu/home/baez/information.html">information</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/01/25/entropy-in-the-universe/#comments">19 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/astronomy/" rel="category tag">astronomy</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/01/25/entropy-in-the-universe/" rel="bookmark" title="Permanent Link to Entropy in the&nbsp;Universe">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-24032 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics" id="post-24032">
				<h2><a href="https://johncarlosbaez.wordpress.com/2017/11/22/the-golden-ratio-and-the-entropy-of-braids/" rel="bookmark">The Golden Ratio and the Entropy of&nbsp;Braids</a></h2>
				<small>22 November, 2017</small><br />


				<div class="entry">
					<p>Here&#8217;s a cute connection between topological entropy, braids, and the golden ratio.  I learned about it in this paper:</p>
<p>&bull; Jean-Luc Thiffeault and Matthew D. Finn, <a href="https://arxiv.org/abs/nlin/0603003">Topology, braids, and mixing in fluids</a>.</p>
<h3>Topological entropy</h3>
<p>I&#8217;ve talked a lot about entropy on this blog, but not much about <a href="https://en.wikipedia.org/wiki/Topological_entropy">topological entropy</a>.  This is a way to define the entropy of a continuous map <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> from a compact topological space <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> to itself.  The idea is that a map that mixes things up a lot should have a lot of entropy.   In particular, any map defining a &#8216;chaotic&#8217; dynamical system should have positive entropy, while non-chaotic maps maps should have zero entropy.</p>
<p>How can we make this precise?   First, cover <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with finitely many open sets <img src="https://s0.wp.com/latex.php?latex=U_1%2C+%5Cdots%2C+U_k.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U_1, &#92;dots, U_k." class="latex" />  Then take any point in <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> apply the map <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> to it over and over, say <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> times, and report which open set the point lands in each time.  You can record this information in a string of symbols.  How much information does this string have?  The easiest way to define this is to simply count the total number of strings that can be produced this way by choosing different points initially.  Then, take the logarithm of this number.</p>
<p>Of course the answer depends on <img src="https://s0.wp.com/latex.php?latex=n%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n," class="latex" /> typically growing bigger as <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> increases.  So, divide it by <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> and try to take the limit as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty." class="latex" />  Or, to be careful, take the lim sup: this could be infinite, but it&#8217;s always well-defined.  This will tell us how much new information we get, on average, each time we apply the map and report which set our point lands in.</p>
<p>And of course the answer also depends on our choice of open cover <img src="https://s0.wp.com/latex.php?latex=U_1%2C+%5Cdots%2C+U_k.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U_1, &#92;dots, U_k." class="latex" />  So, take the supremum over all finite open covers.  This is called the <b>topological entropy</b> of <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>Believe it or not, this is often finite!   Even though the log of the number of symbol strings we get will be larger when we use a cover with lots of small sets, when we divide by <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> and take the limit as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty" class="latex" /> this dependence often washes out.</p>
<h3> Braids </h3>
<p>Any braid gives a bunch of maps from the disc to itself.  So, we define the <b>entropy of a braid</b> to be the minimum&#8212;or more precisely, the infimum&#8212;of the topological entropies of these maps.</p>
<p>How does a braid give a bunch of maps from the disc to itself?  Imagine the disc as made of very flexible rubber.  Grab it at some finite set of points and then move these points around in the pattern traced out by the braid.    When you&#8217;re done you get a map from the disc to itself.  The map you get is not unique, since the rubber is wiggly and you could have moved the points around in slightly different ways.  So, you get a bunch of maps.</p>
<p>I&#8217;m being sort of lazy in giving precise details here, since the idea seems so intuitively obvious.  But that could be because I&#8217;ve spent a lot of time thinking about braids, the <a href="https://en.wikipedia.org/wiki/Braid_group">braid group</a>, and their relation to maps from the disc to itself!</p>
<p>This picture by <a href="//arxiv.org/abs/nlin/0603003">Thiffeault and Finn</a> may help explain the idea:</p>
<div align="center">
<a href="https://arxiv.org/abs/nlin/0603003"><br />
<img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/braids_thiffeault_finn.jpg" /><br />
</a></div>
<p>As we keep move points around each other, we keep building up more complicated braids with 4 strands, and keep getting more complicated maps from the disc to itself.  In fact, these maps are often chaotic!  More precisely: they often have positive entropy.</p>
<p>In this other picture the vertical axis represents time, and we more clearly see the braid traced out as our 4 points move around:</p>
<div align="center">
<a href="https://arxiv.org/abs/nlin/0603003"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/braid_thiffeault_finn.jpg" /><br />
</a></div>
<p>Each horizontal slice depicts a map from the disc (or square: this is topology!) to itself, but we only see their effect on a little rectangle drawn in black.</p>
<h3> The golden ratio </h3>
<p>Okay, now for the punchline!</p>
<p><b>Puzzle 1.</b>  Which braid with 3 strands has the highest entropy per generator?  What is its entropy per generator?</p>
<p>I should explain: any braid with 3 strands can be written as a product of generators <img src="https://s0.wp.com/latex.php?latex=%5Csigma_1%2C+%5Csigma_2%2C+%5Csigma_1%5E%7B-1%7D%2C+%5Csigma_2%5E%7B-1%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_1, &#92;sigma_2, &#92;sigma_1^{-1}, &#92;sigma_2^{-1}." class="latex" />   Here <img src="https://s0.wp.com/latex.php?latex=%5Csigma_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_1" class="latex" /> switches strands 1 and 2 moving the counterclockwise around each other, <img src="https://s0.wp.com/latex.php?latex=%5Csigma_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_2" class="latex" /> does the same for strands 2 and 3, and <img src="https://s0.wp.com/latex.php?latex=%5Csigma_1%5E%7B-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_1^{-1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Csigma_2%5E%7B-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_2^{-1}" class="latex" /> do the same but moving the strands clockwise.</p>
<p>For any braid we can write it as a product of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> generators with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> as small as possible, and then we can evaluate its entropy divided by <img src="https://s0.wp.com/latex.php?latex=n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n." class="latex" />  This is the right way to compare the entropy of braids, because if a braid gives a chaotic map we expect powers of that braid to have entropy growing linearly with <img src="https://s0.wp.com/latex.php?latex=n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n." class="latex" /></p>
<p>Now for the answer to the puzzle!</p>
<p><b>Answer 1.</b>  A 3-strand braid maximizing the entropy per generator is <img src="https://s0.wp.com/latex.php?latex=%5Csigma_1+%5Csigma_2%5E%7B-1%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_1 &#92;sigma_2^{-1}." class="latex" />  And the entropy of this braid, per generator, is the logarithm of the golden ratio:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clog+%5Cleft%28+%5Cfrac%7B%5Csqrt%7B5%7D+%2B+1%7D%7B2%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;log &#92;left( &#92;frac{&#92;sqrt{5} + 1}{2} &#92;right) }" class="latex" /></p>
<p>In other words, the entropy of this braid is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clog+%5Cleft%28+%5Cfrac%7B%5Csqrt%7B5%7D+%2B+1%7D%7B2%7D+%5Cright%29%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;log &#92;left( &#92;frac{&#92;sqrt{5} + 1}{2} &#92;right)^2 } " class="latex" /></p>
<p>All this works regardless of which base we use for our logarithms.  But if we use base e, which seems pretty natural, the maximum possible entropy per generator is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cln+%5Cleft%28+%5Cfrac%7B%5Csqrt%7B5%7D+%2B+1%7D%7B2%7D+%5Cright%29+%5Capprox+0.48121182506%5Cdots+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;ln &#92;left( &#92;frac{&#92;sqrt{5} + 1}{2} &#92;right) &#92;approx 0.48121182506&#92;dots }" class="latex" /></p>
<p>Or if you prefer base 2, then each time you stir around a point in the disc with this braid, you&#8217;re creating</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clog_2+%5Cleft%28+%5Cfrac%7B%5Csqrt%7B5%7D+%2B+1%7D%7B2%7D+%5Cright%29+%5Capprox+0.69424191363%5Cdots+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;log_2 &#92;left( &#92;frac{&#92;sqrt{5} + 1}{2} &#92;right) &#92;approx 0.69424191363&#92;dots }" class="latex" /></p>
<p>bits of unknown information.</p>
<p>This fact was proved here:</p>
<p>&bull; D. D’Alessandro, M. Dahleh and I Mez&iacute;c, Control of mixing in fluid flow: A maximum entropy approach, <em>IEEE Transactions on Automatic Control</em> <strong>44</strong> (1999), 1852&ndash;1863.</p>
<p>So, people call this braid  <img src="https://s0.wp.com/latex.php?latex=%5Csigma_1+%5Csigma_2%5E%7B-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_1 &#92;sigma_2^{-1}" class="latex" /> the <b>golden braid</b>.  But since you can use it to generate entropy forever, perhaps it should be called the <em>eternal</em> golden braid.</p>
<p>What does it all mean? Well, the 3-strand braid group is called <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BB%7D_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{B}_3" class="latex" />, and I wrote a long story about it:</p>
<p>&bull; John Baez, <a href="http://math.ucr.edu/home/baez/week233.html">This Week&#8217;s Finds in Mathematical Physics (Week 233)</a>.</p>
<p>You&#8217;ll see there that <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BB%7D_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{B}_3" class="latex" /> has a representation as 2 &times; 2 matrices:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csigma_1+%5Cmapsto+%5Cleft%28%5Cbegin%7Barray%7D%7Brr%7D+1+%26+1+%5C%5C+0+%26+1+%5Cend%7Barray%7D%5Cright%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sigma_1 &#92;mapsto &#92;left(&#92;begin{array}{rr} 1 &amp; 1 &#92;&#92; 0 &amp; 1 &#92;end{array}&#92;right)} " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csigma_2+%5Cmapsto+%5Cleft%28%5Cbegin%7Barray%7D%7Brr%7D+1+%26+0+%5C%5C+-1+%26+1+%5Cend%7Barray%7D%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sigma_2 &#92;mapsto &#92;left(&#92;begin{array}{rr} 1 &amp; 0 &#92;&#92; -1 &amp; 1 &#92;end{array}&#92;right) }" class="latex" /></p>
<p>These matrices are shears, which is connected to how the braids <img src="https://s0.wp.com/latex.php?latex=%5Csigma_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Csigma_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_2" class="latex" /> give maps from the disc to itself that shear points.  If we take the golden braid and turn it into a matrix using this representation, we get a matrix for which the magnitude of its largest eigenvalue is the square of the golden ratio!  So, the amount of stretching going on is &#8216;the golden ratio per generator&#8217;.</p>
<p>I guess this must be part of the story too:</p>
<p><b>Puzzle 2.</b> Is it true that when we multiply <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> matrices of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft%28%5Cbegin%7Barray%7D%7Brr%7D+1+%26+1+%5C%5C+0+%26+1+%5Cend%7Barray%7D%5Cright%29++%2C+%5Cquad+%5Cleft%28%5Cbegin%7Barray%7D%7Brr%7D+1+%26+0+%5C%5C+-1+%26+1+%5Cend%7Barray%7D%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left(&#92;begin{array}{rr} 1 &amp; 1 &#92;&#92; 0 &amp; 1 &#92;end{array}&#92;right)  , &#92;quad &#92;left(&#92;begin{array}{rr} 1 &amp; 0 &#92;&#92; -1 &amp; 1 &#92;end{array}&#92;right) } " class="latex" /></p>
<p>or their inverses:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft%28%5Cbegin%7Barray%7D%7Brr%7D+1+%26+-1+%5C%5C+0+%26+1+%5Cend%7Barray%7D%5Cright%29++%2C+%5Cquad+%5Cleft%28%5Cbegin%7Barray%7D%7Brr%7D+1+%26+0+%5C%5C+1+%26+1+%5Cend%7Barray%7D%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left(&#92;begin{array}{rr} 1 &amp; -1 &#92;&#92; 0 &amp; 1 &#92;end{array}&#92;right)  , &#92;quad &#92;left(&#92;begin{array}{rr} 1 &amp; 0 &#92;&#92; 1 &amp; 1 &#92;end{array}&#92;right) } " class="latex" /></p>
<p>the magnitude of the largest eigenvalue of the resulting product can never exceed the <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />th power of the golden ratio?</p>
<p>There&#8217;s also a strong connection between braid groups, certain quasiparticles in the plane called <a href="https://arxiv.org/abs/0902.3275">Fibonacci anyons</a>, and the golden ratio.  But I don&#8217;t see the relation between these things and topological entropy!  So, there is a mystery here&#8212;at least for me.</p>
<p>For more, see:</p>
<p>&bull; Matthew D. Finn and Jean-Luc Thiffeault, <a href="https://arxiv.org/abs/1004.0639">Topological optimisation of rod-stirring devices</a>, <em>SIAM Review</em> <strong>53</strong> (2011), 723&mdash;743.</p>
<blockquote><p>
  <strong>Abstract.</strong> There are many industrial situations where rods are used to stir a fluid, or where rods repeatedly stretch a material such as bread dough or taffy. The goal in these applications is to stretch either material lines (in a fluid) or the material itself (for dough or taffy) as rapidly as possible. The growth rate of material lines is conveniently given by the topological entropy of the rod motion. We discuss the problem of optimising such rod devices from a topological viewpoint. We express rod motions in terms of generators of the braid group, and assign a cost based on the minimum number of generators needed to write the braid. We show that for one cost function&#8212;the topological entropy per generator&#8212;the optimal growth rate is the logarithm of the golden ratio. For a more realistic cost function,involving the topological entropy per operation where rods are allowed to move together, the optimal growth rate is the logarithm of the silver ratio, <img src="https://s0.wp.com/latex.php?latex=1%2B+%5Csqrt%7B2%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1+ &#92;sqrt{2}." class="latex" /> We show how to construct devices that realise this optimal growth, which we call <strong>silver mixers</strong>.
</p></blockquote>
<p>Here is the <a href="https://en.wikipedia.org/wiki/Silver_ratio">silver ratio</a>:</p>
<div align="center">
<img width="300" src="https://upload.wikimedia.org/wikipedia/commons/b/b0/Silver_ratio_octagon.png" />
</div>
<p>But now for some reason I feel it&#8217;s time to stop!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/11/22/the-golden-ratio-and-the-entropy-of-braids/#comments">5 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/11/22/the-golden-ratio-and-the-entropy-of-braids/" rel="bookmark" title="Permanent Link to The Golden Ratio and the Entropy of&nbsp;Braids">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-23677 post type-post status-publish format-standard hentry category-conferences category-information-and-entropy" id="post-23677">
				<h2><a href="https://johncarlosbaez.wordpress.com/2017/07/06/entropy-2018/" rel="bookmark">Entropy 2018</a></h2>
				<small>6 July, 2017</small><br />


				<div class="entry">
					<p>The editors of the journal <i><a href="http://www.mdpi.com/journal/entropy">Entropy</a></i> are organizing this conference:</p>
<p>• <a href="http://bit.ly/entropy2018">Entropy 2018 — From Physics to Information Sciences and Geometry</a>, 14&#8211;16 May 2018, Auditorium Enric Casassas, Faculty of Chemistry, University of Barcelona, Barcelona, Spain.</p>
<p>They write:</p>
<blockquote><p>
One of the most frequently used scientific words is the word “entropy”. The reason is that it is related to two main scientific domains: physics and information theory. Its origin goes back to the start of physics (thermodynamics), but since Shannon, it has become related to information theory. This conference is an opportunity to bring researchers of these two communities together and create a synergy. The main topics and sessions of the conference cover: </p>
<p>•    Physics: classical and quantum thermodynamics<br />
•    Statistical physics and Bayesian computation<br />
•    Geometrical science of information, topology and metrics<br />
•    Maximum entropy principle and inference<br />
•    Kullback and Bayes or information theory and Bayesian inference<br />
•    Entropy in action (applications)</p>
<p>The inter-disciplinary nature of contributions from both theoretical and applied perspectives are very welcome, including papers addressing conceptual and methodological developments, as well as new applications of entropy and information theory.</p>
<p>All accepted papers will be published in the proceedings of the conference. A selection of invited and contributed talks presented during the conference will be invited to submit an extended version of their paper for a special issue of the open access journal <a href="http://www.mdpi.com/journal/entropy"><i>Entropy</i></a>.
</p></blockquote>
<div align="center">
<img width="450" src="https://i2.wp.com/sciforum.net/bundles/sciforumversion2/images/conference-page-banners/Entropy2018-1.jpg" />
</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/07/06/entropy-2018/#comments">5 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/07/06/entropy-2018/" rel="bookmark" title="Permanent Link to Entropy 2018">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-23248 post type-post status-publish format-standard hentry category-biology category-game-theory category-information-and-entropy category-probability" id="post-23248">
				<h2><a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/" rel="bookmark">Information Geometry (Part&nbsp;16)</a></h2>
				<small>1 February, 2017</small><br />


				<div class="entry">
					<p>This week I&#8217;m giving a talk on biology and information:</p>
<p>• John Baez, <a href="http://math.ucr.edu/home/baez/bio_asu/">Biology as information dynamics</a>, talk for <a href="https://beyond.asu.edu/workshop/biological-complexity-can-it-be-quantified">Biological Complexity: Can it be Quantified?</a>, a workshop at the <a href="https://beyond.asu.edu/">Beyond Center</a>, 2 February 2017.</p>
<p>While preparing this talk, I discovered a cool fact.  I doubt it&#8217;s new, but I haven&#8217;t exactly seen it elsewhere.   I came up with it while trying to give a precise and general statement of &#8216;Fisher&#8217;s fundamental theorem of natural selection&#8217;.   I <i>won&#8217;t</i> start by explaining that theorem, since my version looks rather different than Fisher&#8217;s, and I came up with mine precisely because I had trouble understanding his.  I&#8217;ll say a bit more about this at the end.</p>
<p>Here&#8217;s my version:</p>
<blockquote><p>
The square of the rate at which a population learns information is the variance of its fitness.
</p></blockquote>
<p>This is a nice advertisement for the virtues of diversity: more variance means faster learning.   But it requires some explanation!</p>
<h3> The setup </h3>
<p>Let&#8217;s start by assuming we have <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different kinds of self-replicating entities with populations <img src="https://s0.wp.com/latex.php?latex=P_1%2C+%5Cdots%2C+P_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_1, &#92;dots, P_n." class="latex" />  As usual, these could be all sorts of things:</p>
<p>• molecules of different chemicals<br />
• organisms belonging to different species<br />
• genes of different alleles<br />
• restaurants belonging to different chains<br />
• people with different beliefs<br />
• game-players with different strategies<br />
• etc.</p>
<p>I&#8217;ll call them <b>replicators</b> of different <b>species</b>.</p>
<p>Let&#8217;s suppose each population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> is a function of time that grows at a rate equal to this population times its &#8216;fitness&#8217;.   I explained the resulting equation back in <a href="http://math.ucr.edu/home/baez/information/information_geometry_9.html">Part 9</a>, but it&#8217;s pretty simple:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+P_i%28t%29+%3D+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%2C+P_i%28t%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} P_i(t) = f_i(P_1(t), &#92;dots, P_n(t)) &#92;, P_i(t)   } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is a completely arbitrary smooth function of all the populations!  We call it the <b>fitness</b> of the <i>i</i>th species.</p>
<p>This equation is important, so we want a short way to write it.  I&#8217;ll often write <img src="https://s0.wp.com/latex.php?latex=f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P_1(t), &#92;dots, P_n(t))" class="latex" /> simply as <img src="https://s0.wp.com/latex.php?latex=f_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=P_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t)" class="latex" /> simply as <img src="https://s0.wp.com/latex.php?latex=P_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i." class="latex" />  With these abbreviations, which any red-blooded physicist would take for granted, our equation becomes simply this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BdP_i%7D%7Bd+t%7D++%3D+f_i+%5C%2C+P_i+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dP_i}{d t}  = f_i &#92;, P_i   } " class="latex" /></p>
<p>Next, let <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t)" class="latex" /> be the probability that a randomly chosen organism is of the <i>i</i>th species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Csum_j+P_j%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i(t) = &#92;frac{P_i(t)}{&#92;sum_j P_j(t)} } " class="latex" /></p>
<p>Starting from our equation describing how the populations evolve, we can figure out how these probabilities evolve.  The answer is called the <a href="https://en.wikipedia.org/wiki/Replicator_equation"><b>replicator equation</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+p_i%28t%29++%3D+%28+f_i+-+%5Clangle+f+%5Crangle+%29+%5C%2C+p_i%28t%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} p_i(t)  = ( f_i - &#92;langle f &#92;rangle ) &#92;, p_i(t) }" class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Clangle+f+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle f &#92;rangle" class="latex" /> is the average fitness of all the replicators, or <b>mean fitness</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f+%5Crangle+%3D+%5Csum_j+f_j%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%2C+p_j%28t%29++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f &#92;rangle = &#92;sum_j f_j(P_1(t), &#92;dots, P_n(t)) &#92;, p_j(t)  } " class="latex" /></p>
<p>In what follows I&#8217;ll abbreviate the replicator equation as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bdp_i%7D%7Bd+t%7D++%3D+%28+f_i+-+%5Clangle+f+%5Crangle+%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dp_i}{d t}  = ( f_i - &#92;langle f &#92;rangle ) &#92;, p_i }" class="latex" /></p>
<h3> The result </h3>
<p>Okay, now let&#8217;s figure out how fast the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28t%29+%3D+%28p_1%28t%29%2C+%5Cdots%2C+p_n%28t%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t) = (p_1(t), &#92;dots, p_n(t)) " class="latex" /></p>
<p>changes with time.  For this we need to choose a way to measure the length of the vector</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bdp%7D%7Bdt%7D+%3D+%28%5Cfrac%7Bd%7D%7Bdt%7D+p_1%28t%29%2C+%5Cdots%2C+%5Cfrac%7Bd%7D%7Bdt%7D+p_n%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{dp}{dt} = (&#92;frac{d}{dt} p_1(t), &#92;dots, &#92;frac{d}{dt} p_n(t)) } " class="latex" /></p>
<p>And here information geometry comes to the rescue!   We can use the <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">Fisher information metric</a>, which is a Riemannian metric on the space of probability distributions.</p>
<p>I&#8217;ve talked about the Fisher information metric in many ways in this series.  The most important fact is that as a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> changes with time, its speed</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cleft%5C%7C+%5Cfrac%7Bdp%7D%7Bdt%7D+%5Cright%5C%7C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;left&#92;| &#92;frac{dp}{dt} &#92;right&#92;|} " class="latex" /></p>
<p>as measured using the Fisher information metric can be seen as the <i>rate at which information is learned</i>.  I&#8217;ll explain that later.  Right now I just want a simple <i>formula</i> for the Fisher information metric.  Suppose <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w" class="latex" /> are two tangent vectors to the point <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in the space of probability distributions.  Then the <b>Fisher information metric</b> is given as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+v%2C+w+%5Crangle+%3D+%5Csum_i+%5Cfrac%7B1%7D%7Bp_i%7D+%5C%2C+v_i+w_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle v, w &#92;rangle = &#92;sum_i &#92;frac{1}{p_i} &#92;, v_i w_i } " class="latex" /></p>
<p>Using this we can calculate the speed at which <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moves when it obeys the replicator equation.  Actually the square of the speed is simpler:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cleft%5C%7C+%5Cfrac%7Bdp%7D%7Bdt%7D++%5Cright%5C%7C%5E2+%7D+%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i+%5Cfrac%7B1%7D%7Bp_i%7D+%5Cleft%28+%5Cfrac%7Bdp_i%7D%7Bdt%7D+%5Cright%29%5E2+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i+%5Cfrac%7B1%7D%7Bp_i%7D+%5Cleft%28+%28+f_i+-+%5Clangle+f+%5Crangle+%29+%5C%2C+p_i+%5Cright%29%5E2+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i++%28+f_i+-+%5Clangle+f+%5Crangle+%29%5E2+p_i+%7D+++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;left&#92;| &#92;frac{dp}{dt}  &#92;right&#92;|^2 } &amp;=&amp; &#92;displaystyle{ &#92;sum_i &#92;frac{1}{p_i} &#92;left( &#92;frac{dp_i}{dt} &#92;right)^2 } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_i &#92;frac{1}{p_i} &#92;left( ( f_i - &#92;langle f &#92;rangle ) &#92;, p_i &#92;right)^2 } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_i  ( f_i - &#92;langle f &#92;rangle )^2 p_i }   &#92;end{array}  " class="latex" /></p>
<p>The answer has a nice meaning, too!  It&#8217;s just the <a href="https://en.wikipedia.org/wiki/Variance">variance</a> of the fitness: that is, the square of its <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a>.</p>
<p>So, if you&#8217;re willing to buy my claim that the speed <img src="https://s0.wp.com/latex.php?latex=%5C%7Cdp%2Fdt%5C%7C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;|dp/dt&#92;|" class="latex" /> is the rate at which our population learns new information, then we&#8217;ve seen that <i>the square of the rate at which a population learns information is the variance of its fitness!</i></p>
<h3> Fisher&#8217;s fundamental theorem </h3>
<p>Now, how is this related to Fisher&#8217;s fundamental theorem of natural selection?    First of all, what <i>is</i> Fisher&#8217;s fundamental theorem?  Here&#8217;s what <a href="https://en.wikipedia.org/wiki/Fisher's_fundamental_theorem_of_natural_selection" rel="nofollow">Wikipedia says</a> about it:</p>
<blockquote><p>
  It uses some mathematical notation but is not a theorem in the mathematical sense.</p>
<p>  It states:</p>
<blockquote><p>
  &#8220;The rate of increase in fitness of any organism at any time is equal to its genetic variance in fitness at that time.&#8221;
</p></blockquote>
<p>  Or in more modern terminology:</p>
<blockquote><p>
  &#8220;The rate of increase in the mean fitness of any organism at any time ascribable to natural selection acting through changes in gene frequencies is exactly equal to its genetic variance in fitness at that time&#8221;.
</p></blockquote>
<p>  Largely as a result of Fisher&#8217;s feud with the American geneticist Sewall Wright about adaptive landscapes, the theorem was widely misunderstood to mean that the average fitness of a population would always increase, even though models showed this not to be the case. In 1972, George R. Price showed that Fisher&#8217;s theorem was indeed correct (and that Fisher&#8217;s proof was also correct, given a typo or two), but did not find it to be of great significance. The sophistication that Price pointed out, and that had made understanding difficult, is that the theorem gives a formula for part of the change in gene frequency, and not for all of it. This is a part that can be said to be due to natural selection
</p></blockquote>
<p>Price&#8217;s paper is here:</p>
<p>• George R. Price, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.4070&amp;rep=rep1&amp;type=pdf" rel="nofollow">Fisher&#8217;s &#8216;fundamental theorem&#8217; made clear</a>, <em>Annals of Human Genetics</em> <strong>36</strong> (1972), 129–140.</p>
<p>I don&#8217;t find it very clear, perhaps because I didn&#8217;t spend enough time on it.  But I think I get the idea.</p>
<p>My result <i>is</i> a theorem in the mathematical sense, though quite an easy one.  I assume a population distribution evolves according to the replicator equation and derive an equation whose right-hand side matches that of Fisher&#8217;s original equation: the variance of the fitness.</p>
<p>But my left-hand side is different: it&#8217;s the square of the speed of the corresponding probability distribution, where speed is measured using the &#8216;Fisher information metric&#8217;.  This metric was discovered by the same guy, Ronald Fisher, but I don&#8217;t think he used it in <em>his</em> work on the fundamental theorem!</p>
<p>Something a bit similar to my statement appears as Theorem 2 of this paper:</p>
<p>• Marc Harper, <a href="http://arxiv.org/abs/0911.1383" rel="nofollow">Information geometry and evolutionary game theory</a>.</p>
<p>and for that theorem he cites:</p>
<p>• Josef Hofbauer and Karl Sigmund, <em>Evolutionary Games and Population Dynamics</em>, Cambridge University Press, Cambridge, 1998.</p>
<p>However, his Theorem 2 really concerns the rate of increase of fitness, like Fisher&#8217;s fundamental theorem.  Moreover, he assumes that the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> flows along the gradient of a function, and I&#8217;m not assuming that.  Indeed, my version applies to situations where the probability distribution moves round and round in periodic orbits!</p>
<h3> Relative information and the Fisher information metric </h3>
<p>The key to generalizing Fisher&#8217;s fundamental theorem is thus to focus on the speed at which <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moves, rather than the increase in fitness.  Why do I call this speed the &#8216;rate at which the population learns information&#8217;?  It&#8217;s because we&#8217;re measuring this speed using the Fisher information metric, which is closely connected to <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">relative information</a>, also known as relative entropy or the Kullback&ndash;Leibler divergence.</p>
<p>I explained this back in <a href="http://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>, but that explanation seems hopelessly technical to me now, so here&#8217;s a faster one, which I created while preparing my talk.</p>
<p>The information of a probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> <b>relative to</b> a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++++I%28q%2Cp%29+%3D+%5Csum_%7Bi+%3D1%7D%5En+q_i+%5Clog%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i%7D%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{     I(q,p) = &#92;sum_{i =1}^n q_i &#92;log&#92;left(&#92;frac{q_i}{p_i}&#92;right) }" class="latex" /></p>
<p>It says how much information you learn if you start with a hypothesis <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> saying that the probability of the <i>i</i>th situation was <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> and then update this to a new hypothesis <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>Now suppose you have a hypothesis that&#8217;s changing with time in a smooth way, given by a time-dependent probability <img src="https://s0.wp.com/latex.php?latex=p%28t%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)." class="latex" />   Then a calculation shows that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%7D%7Bdt%7D+I%28p%28t%29%2Cp%28t_0%29%29+%5Cright%7C_%7Bt+%3D+t_0%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d}{dt} I(p(t),p(t_0)) &#92;right|_{t = t_0} = 0 } " class="latex" /></p>
<p>for all times <img src="https://s0.wp.com/latex.php?latex=t_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_0" class="latex" />.  This seems paradoxical at first.  I like to jokingly put it this way:</p>
<blockquote><p>
To first order, you&#8217;re never learning anything.
</p></blockquote>
<p>However, as long as the velocity <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bdt%7Dp%28t_0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{dt}p(t_0)" class="latex" /> is nonzero, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7D+I%28p%28t%29%2Cp%28t_0%29%29+%5Cright%7C_%7Bt+%3D+t_0%7D+%3E+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d^2}{dt^2} I(p(t),p(t_0)) &#92;right|_{t = t_0} &gt; 0 } " class="latex" /></p>
<p>so we can say</p>
<blockquote><p>
To second order, you&#8217;re always learning something&#8230; unless your opinions are fixed.
</p></blockquote>
<p>This lets us define a &#8216;rate of learning&#8217;&#8212;that is, a &#8216;speed&#8217; at which the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moves.  <i>And this is precisely the speed given by the Fisher information metric!</i></p>
<p>In other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft%5C%7C%5Cfrac%7Bdp%7D%7Bdt%7D%28t_0%29%5Cright%5C%7C%5E2+%3D++%5Cleft.%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7D+I%28p%28t%29%2Cp%28t_0%29%29+%5Cright%7C_%7Bt+%3D+t_0%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left&#92;|&#92;frac{dp}{dt}(t_0)&#92;right&#92;|^2 =  &#92;left.&#92;frac{d^2}{dt^2} I(p(t),p(t_0)) &#92;right|_{t = t_0} } " class="latex" /></p>
<p>where the length is given by Fisher information metric.   Indeed, this formula can be used to <i>define</i> the Fisher information metric.  From this definition we can easily work out the concrete formula I gave earlier.</p>
<p>In summary: as a probability distribution moves around, the relative information between the new probability distribution and the original one grows approximately as the <i>square</i> of time, not linearly.  So, to talk about a &#8216;rate at which information is learned&#8217;, we need to use the above formula, involving a second time derivative.  This rate is just the speed at which the probability distribution moves, measured using the Fisher information metric.  And when we have a probability distribution describing how many replicators are of different species, and it&#8217;s evolving according to the replicator equation, this speed is also just the variance of the fitness!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comments">29 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/" rel="category tag">game theory</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;16)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-23239 post type-post status-publish format-standard hentry category-biology category-game-theory category-information-and-entropy category-probability" id="post-23239">
				<h2><a href="https://johncarlosbaez.wordpress.com/2017/01/31/biology-as-information-dynamics/" rel="bookmark">Biology as Information Dynamics (Part&nbsp;1)</a></h2>
				<small>31 January, 2017</small><br />


				<div class="entry">
					<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/bio_asu/Forest-fruits-from-Barro-Colorado.png" />
</div>
<p>This is my talk for the workshop <a href="https://johncarlosbaez.wordpress.com/2017/01/23/quantifying-biological-complexity/">Biological Complexity: Can It Be Quantified?</a></p>
<p>• John Baez, <a href="http://math.ucr.edu/home/baez/bio_asu/bio_asu_web.pdf">Biology as information dynamics</a>, 2 February 2017.</p>
<blockquote><p>
<b>Abstract.</b> If biology is the study of self-replicating entities, and we want to understand the role of information, it makes sense to see how information theory is connected to the &#8216;replicator equation&#8217;—a simple model of population dynamics for self-replicating entities. The relevant concept of information turns out to be the information of one probability distribution relative to another, also known as the Kullback–Leibler divergence. Using this we can get a new outlook on free energy, see evolution as a learning process, and give a clean general formulation of Fisher&#8217;s fundamental theorem of natural selection.
</p></blockquote>
<p>For more, read:</p>
<p>• Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>• Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>• Barry Sinervo and Curt M. Lively, <a href="http://www.indiana.edu/~curtweb/L567/readings/Sinervo&amp;Lively1996.pdf">The rock-paper-scissors game and the evolution of alternative male strategies</a>, <i>Nature</i> <b>380</b> (1996), 240–243.</p>
<p>• John Baez, <a href="http://math.ucr.edu/home/baez/biodiversity/">Diversity, entropy and thermodynamics</a>.</p>
<p>• John Baez, <a href="http://math.ucr.edu/home/baez/">Information geometry</a>.</p>
<p>The last reference contains proofs of the equations shown in red in my slides.<br />
In particular, <a href="../information/information_geometry_16.html">Part 16</a> contains a proof of my updated version of Fisher&#8217;s fundamental theorem.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/01/31/biology-as-information-dynamics/#comments">6 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/" rel="category tag">game theory</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2017/01/31/biology-as-information-dynamics/" rel="bookmark" title="Permanent Link to Biology as Information Dynamics (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-20306 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-physics category-probability" id="post-20306">
				<h2><a href="https://johncarlosbaez.wordpress.com/2016/01/14/information-geometry-part-15/" rel="bookmark">Information Geometry (Part&nbsp;15)</a></h2>
				<small>14 January, 2016</small><br />


				<div class="entry">
					<p><i>joint with <b><a href="http://www.azimuthproject.org/azimuth/show/Blake+Pollard">Blake Pollard</a></b></i></p>
<p>Lately we&#8217;ve been thinking about open Markov processes.  These are random processes where something can hop randomly from one state to another (that&#8217;s the &#8216;Markov process&#8217; part) but also enter or leave the system (that&#8217;s the &#8216;open&#8217; part).</p>
<p>The ultimate goal is to understand the nonequilibrium thermodynamics of open systems&#8212;systems where energy and maybe matter flows in and out.  If we could understand this well enough, we could understand in detail how <i>life</i> works.  That&#8217;s a difficult job!  But one has to start somewhere, and this is one place to start.</p>
<p>We have a few papers on this subject:</p>
<p>&bull; Blake Pollard, <a href="http://arxiv.org/abs/1410.6531">A Second Law for open Markov processes</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2014/11/15/a-second-law-for-open-markov-processes/">here</a>.)</p>
<p>&bull; John Baez, Brendan Fong and Blake Pollard, <a href="http://arxiv.org/abs/1504.05625">A compositional framework for Markov processes</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2015/09/04/a-compositional-framework-for-markov-processes/">here</a>.)</p>
<p>&bull; Blake Pollard, <a href="http://arxiv.org/abs/1601.00711">Open Markov processes: A compositional perspective on non-equilibrium steady states in biology</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2016/01/11/information-geometry-part-15/">here</a>.)</p>
<p>However, right now we just want to show you three closely connected results about how relative entropy changes in open Markov processes.</p>
<h3> Definitions </h3>
<p>An <b>open Markov process</b> consists of a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> of <b>states</b>, a subset <img src="https://s0.wp.com/latex.php?latex=B+%5Csubseteq+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B &#92;subseteq X" class="latex" /> of <b>boundary states</b>, and an <b>infinitesimal stochastic</b> operator <img src="https://s0.wp.com/latex.php?latex=H%3A+%5Cmathbb%7BR%7D%5EX+%5Cto+%5Cmathbb%7BR%7D%5EX%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H: &#92;mathbb{R}^X &#92;to &#92;mathbb{R}^X," class="latex" /> meaning a linear operator with</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D+%5Cgeq+0+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i+%5Cneq+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij} &#92;geq 0 &#92; &#92;  &#92;text{for all} &#92; &#92; i &#92;neq j " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_i+H_%7Bij%7D+%3D+0+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i H_{ij} = 0 &#92; &#92;  &#92;text{for all} &#92; &#92; j " class="latex" /></p>
<p>For each state <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" /> we introduce a <b>population</b> <img src="https://s0.wp.com/latex.php?latex=p_i++%5Cin+%5B0%2C%5Cinfty%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i  &#92;in [0,&#92;infty)." class="latex" />  We call the resulting function <img src="https://s0.wp.com/latex.php?latex=p+%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : X &#92;to [0,&#92;infty)" class="latex" /> the <b>population distribution</b>.</p>
<p>Populations evolve in time according to the <b>open master equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bdp_i%7D%7Bdt%7D+%3D+%5Csum_j+H_%7Bij%7Dp_j%7D+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C++i+%5Cin+X-B+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dp_i}{dt} = &#92;sum_j H_{ij}p_j} &#92; &#92;  &#92;text{for all} &#92; &#92;  i &#92;in X-B " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+%3D+b_i%28t%29+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C++i+%5Cin+B+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) = b_i(t) &#92; &#92;  &#92;text{for all} &#92; &#92;  i &#92;in B " class="latex" /></p>
<p>So, the populations <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> obey a linear differential equation at states <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> that are not in the boundary, but they are specified &#8216;by the user&#8217; to be chosen functions <img src="https://s0.wp.com/latex.php?latex=b_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b_i" class="latex" /> at the boundary states.   The off-diagonal entry <img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=i+%5Cneq+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;neq j" class="latex" /> describe the rate at which population transitions from the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state.</p>
<p>A <b>closed Markov process</b>, or continuous-time discrete-state Markov chain, is an open Markov process whose boundary is empty. For a closed Markov process, the open master equation becomes the usual <b>master equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bdp%7D%7Bdt%7D+%3D+Hp+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{dp}{dt} = Hp } " class="latex" /></p>
<p>In a closed Markov process the total population is conserved:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+%5Csum_%7Bi+%5Cin+X%7D+p_i+%3D+%5Csum_%7Bi%2Cj%7D+H_%7Bij%7Dp_j+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} &#92;sum_{i &#92;in X} p_i = &#92;sum_{i,j} H_{ij}p_j = 0 } " class="latex" /></p>
<p>This lets us normalize the initial total population to 1 and have it stay equal to 1.  If we do this, we can talk about <i>probabilities</i> instead of populations.  In an open Markov process, population can flow in and out at the boundary states.</p>
<p>For any pair of distinct states <img src="https://s0.wp.com/latex.php?latex=i%2Cj%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i,j," class="latex" />  <img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7Dp_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij}p_j" class="latex" /> is the flow of population from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />  The <b>net flux</b> of population from the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th state to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state is the flow from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> minus the flow from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=J_%7Bij%7D+%3D+H_%7Bij%7Dp_j+-+H_%7Bji%7Dp_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="J_{ij} = H_{ij}p_j - H_{ji}p_i " class="latex" /></p>
<p>A <b>steady state</b> is a solution of the open master equation that does not change with time.  A steady state for a closed Markov process is typically called an <b>equilibrium</b>.  So, an equilibrium obeys the master equation at all states, while for a steady state this may not be true at the boundary states.  The idea is that population can flow in or out at the boundary states.</p>
<p>We say an equilibrium <img src="https://s0.wp.com/latex.php?latex=p+%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : X &#92;to [0,&#92;infty)" class="latex" /> of a Markov process is <b>detailed balanced</b> if all the net fluxes vanish:</p>
<p><img src="https://s0.wp.com/latex.php?latex=J_%7Bij%7D+%3D+0+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i%2Cj+%5Cin+X+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="J_{ij} = 0 &#92; &#92;  &#92;text{for all} &#92; &#92; i,j &#92;in X " class="latex" /></p>
<p>or in other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7Dp_j+%3D+H_%7Bji%7Dp_i+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i%2Cj+%5Cin+X+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij}p_j = H_{ji}p_i &#92; &#92;  &#92;text{for all} &#92; &#92; i,j &#92;in X " class="latex" /></p>
<p>Given two population distributions <img src="https://s0.wp.com/latex.php?latex=p%2C+q+%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q : X &#92;to [0,&#92;infty)" class="latex" /> we can define the <b>relative entropy</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++I%28p%2Cq%29+%3D+%5Csum_i+p_i+%5Cln+%5Cleft%28+%5Cfrac%7Bp_i%7D%7Bq_i%7D+%5Cright%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  I(p,q) = &#92;sum_i p_i &#92;ln &#92;left( &#92;frac{p_i}{q_i} &#92;right)} " class="latex" /></p>
<p>When <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a detailed balanced equilibrium solution of the master equation, the relative entropy can be seen as the &#8216;free energy&#8217; of <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />  For a precise statement, see Section 4 of <a href="http://arxiv.org/abs/1512.02742">Relative entropy in biological systems</a>.</p>
<p>The Second Law of Thermodynamics implies that the free energy of a closed system tends to decrease with time, so for <i>closed</i> Markov processes we expect <img src="https://s0.wp.com/latex.php?latex=I%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p,q)" class="latex" /> to be nonincreasing.  And this is true!  But for <i>open</i> Markov processes, free energy can flow in from outside.  This is just one of several nice results about how relative entropy changes with time.</p>
<h3> Results </h3>
<p><b>Theorem 1.</b>  Consider an open Markov process with <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> as its set of states and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> as the set of boundary states.   Suppose <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(t)" class="latex" /> obey the open master equation, and let the quantities</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BDp_i%7D%7BDt%7D+%3D+%5Cfrac%7Bdp_i%7D%7Bdt%7D+-+%5Csum_%7Bj+%5Cin+X%7D+H_%7Bij%7Dp_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{Dp_i}{Dt} = &#92;frac{dp_i}{dt} - &#92;sum_{j &#92;in X} H_{ij}p_j }" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7BDq_i%7D%7BDt%7D+%3D+%5Cfrac%7Bdq_i%7D%7Bdt%7D+-+%5Csum_%7Bj+%5Cin+X%7D+H_%7Bij%7Dq_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{Dq_i}{Dt} = &#92;frac{dq_i}{dt} - &#92;sum_{j &#92;in X} H_{ij}q_j } " class="latex" /></p>
<p>measure how much the time derivatives of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> fail to obey the master equation.   Then we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+++%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%28t%29%29+%7D+%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29%7D+%5C%5C+%5C%5C++%26%26+%5C%3B+%2B+%5C%3B+%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+B%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7BDp_i%7D%7BDt%7D+%2B++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+%5Cfrac%7BDq_i%7D%7BDt%7D++%7D++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}   &#92;displaystyle{  &#92;frac{d}{dt}  I(p(t),q(t)) } &amp;=&amp; &#92;displaystyle{ &#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right)} &#92;&#92; &#92;&#92;  &amp;&amp; &#92;; + &#92;; &#92;displaystyle{ &#92;sum_{i &#92;in B} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{Dp_i}{Dt} +  &#92;frac{&#92;partial I}{&#92;partial q_i} &#92;frac{Dq_i}{Dt}  }  &#92;end{array} " class="latex" /></p>
<p>This result separates the change in relative entropy change into two parts: an &#8216;internal&#8217; part and a &#8216;boundary&#8217; part.</p>
<p>It turns out the &#8216;internal&#8217; part is always less than or equal to zero.  So, from Theorem 1 we can deduce a version of the Second Law of Thermodynamics for open Markov processes:</p>
<p><b>Theorem 2.</b>  Given the conditions of Theorem 1, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%28t%29%29+%5C%3B+%5Cle+%5C%3B+++%5Csum_%7Bi+%5Cin+B%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7BDp_i%7D%7BDt%7D+%2B++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+%5Cfrac%7BDq_i%7D%7BDt%7D+++%7D+++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d}{dt}  I(p(t),q(t)) &#92;; &#92;le &#92;;   &#92;sum_{i &#92;in B} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{Dp_i}{Dt} +  &#92;frac{&#92;partial I}{&#92;partial q_i} &#92;frac{Dq_i}{Dt}   }   " class="latex" /></p>
<p>Intuitively, this says that free energy can only increase if it comes in from the boundary!</p>
<p>There is another nice result that holds when <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an equilibrium solution of the master equation.  This idea seems to go back to Schnakenberg:</p>
<p><b>Theorem 3.</b>  Given the conditions of Theorem 1, suppose also that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an equilibrium solution of the master equation.  Then we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%29+%3D++-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj+%5Cin+X%7D+J_%7Bij%7D+A_%7Bij%7D+%5C%3B+%2B+%5C%3B+%5Csum_%7Bi+%5Cin+B%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7BDp_i%7D%7BDt%7D++%7D+++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d}{dt}  I(p(t),q) =  -&#92;frac{1}{2} &#92;sum_{i,j &#92;in X} J_{ij} A_{ij} &#92;; + &#92;; &#92;sum_{i &#92;in B} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{Dp_i}{Dt}  }   " class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=J_%7Bij%7D+%3D+H_%7Bij%7Dp_j+-+H_%7Bji%7Dp_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="J_{ij} = H_{ij}p_j - H_{ji}p_i " class="latex" /></p>
<p>is the <b>net flux</b> from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> while</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+A_%7Bij%7D+%3D+%5Cln+%5Cleft%28%5Cfrac%7Bp_j+q_i%7D%7Bp_i+q_j%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ A_{ij} = &#92;ln &#92;left(&#92;frac{p_j q_i}{p_i q_j} &#92;right) } " class="latex" /></p>
<p>is the conjugate <b>thermodynamic force</b>.</p>
<p>The flux <img src="https://s0.wp.com/latex.php?latex=J_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="J_{ij}" class="latex" /> has a nice meaning: it&#8217;s the net flow of population from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />  The thermodynamic force is a bit subtler, but this theorem reveals its meaning: it says how much the population <i>wants</i> to flow from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" /></p>
<p>More precisely, up to that factor of <img src="https://s0.wp.com/latex.php?latex=1%2F2%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/2," class="latex" /> the thermodynamic force <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> says how much free energy loss is caused by net flux from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />  There&#8217;s a nice analogy here to water losing potential energy as it flows downhill due to the force of gravity.</p>
<h3> Proofs </h3>
<p><b>Proof of Theorem 1.</b> We begin by taking the time derivative of the relative information:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%28t%29%29+%7D+%26%3D%26+++%5Cdisplaystyle%7B++%5Csum_%7Bi+%5Cin+X%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7Bdp_i%7D%7Bdt%7D+%2B++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+%5Cfrac%7Bdq_i%7D%7Bdt%7D+%7D++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;frac{d}{dt}  I(p(t),q(t)) } &amp;=&amp;   &#92;displaystyle{  &#92;sum_{i &#92;in X} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{dp_i}{dt} +  &#92;frac{&#92;partial I}{&#92;partial q_i} &#92;frac{dq_i}{dt} }  &#92;end{array} " class="latex" /></p>
<p>We can separate this into a sum over states <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X+-+B%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X - B," class="latex" /> for which the time derivatives of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are given by the master equation, and boundary states <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+B%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in B," class="latex" /> for which they are not:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%28t%29%29+%7D+%26%3D%26+++%5Cdisplaystyle%7B++%5Csum_%7Bi+%5Cin+X-B%2C+%5C%3B+j+%5Cin+X%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+H_%7Bij%7D+p_j+%2B++++++++++++++++++++++++++++++++++++++++++++++++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+H_%7Bij%7D+q_j+%7D%5C%5C++%5C%5C++++%26%26+%2B+%5C%3B+%5C%3B+%5C%3B+%5Cdisplaystyle%7B++%5Csum_%7Bi+%5Cin+B%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7Bdp_i%7D%7Bdt%7D+%2B++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+%5Cfrac%7Bdq_i%7D%7Bdt%7D%7D++++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;frac{d}{dt}  I(p(t),q(t)) } &amp;=&amp;   &#92;displaystyle{  &#92;sum_{i &#92;in X-B, &#92;; j &#92;in X} &#92;frac{&#92;partial I}{&#92;partial p_i} H_{ij} p_j +                                                &#92;frac{&#92;partial I}{&#92;partial q_i} H_{ij} q_j }&#92;&#92;  &#92;&#92;    &amp;&amp; + &#92;; &#92;; &#92;; &#92;displaystyle{  &#92;sum_{i &#92;in B} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{dp_i}{dt} +  &#92;frac{&#92;partial I}{&#92;partial q_i} &#92;frac{dq_i}{dt}}    &#92;end{array} " class="latex" /></p>
<p>For boundary states we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bdp_i%7D%7Bdt%7D+%3D+%5Cfrac%7BDp_i%7D%7BDt%7D+%2B+%5Csum_%7Bj+%5Cin+X%7D+H_%7Bij%7Dp_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dp_i}{dt} = &#92;frac{Dp_i}{Dt} + &#92;sum_{j &#92;in X} H_{ij}p_j } " class="latex" /></p>
<p>and similarly for the time derivative of <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />    We thus obtain</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+++%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%28t%29%29+%7D+%26%3D%26+++%5Cdisplaystyle%7B++%5Csum_%7Bi%2Cj+%5Cin+X%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+H_%7Bij%7D+p_j+%2B+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+H_%7Bij%7D+q_j+%7D%5C%5C++%5C%5C++%26%26+%2B+%5C%3B+%5C%3B+%5Cdisplaystyle%7B++%5Csum_%7Bi+%5Cin+B%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7BDp_i%7D%7BDt%7D+%2B++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+%5Cfrac%7BDq_i%7D%7BDt%7D%7D++++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}   &#92;displaystyle{ &#92;frac{d}{dt}  I(p(t),q(t)) } &amp;=&amp;   &#92;displaystyle{  &#92;sum_{i,j &#92;in X} &#92;frac{&#92;partial I}{&#92;partial p_i} H_{ij} p_j + &#92;frac{&#92;partial I}{&#92;partial q_i} H_{ij} q_j }&#92;&#92;  &#92;&#92;  &amp;&amp; + &#92;; &#92;; &#92;displaystyle{  &#92;sum_{i &#92;in B} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{Dp_i}{Dt} +  &#92;frac{&#92;partial I}{&#92;partial q_i} &#92;frac{Dq_i}{Dt}}    &#92;end{array} " class="latex" /></p>
<p>To evaluate the first sum, recall that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++I%28p%2Cq%29+%3D+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5Cln+%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   I(p,q) = &#92;sum_{i &#92;in X} p_i &#92;ln (&#92;frac{p_i}{q_i})}  " class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D%7D+%3D%5Cdisplaystyle%7B1+%2B++%5Cln+%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29%7D+%2C++%5Cqquad++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D%7D%3D++%5Cdisplaystyle%7B-+%5Cfrac%7Bp_i%7D%7Bq_i%7D+++%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;partial I}{&#92;partial p_i}} =&#92;displaystyle{1 +  &#92;ln (&#92;frac{p_i}{q_i})} ,  &#92;qquad  &#92;displaystyle{ &#92;frac{&#92;partial I}{&#92;partial q_i}}=  &#92;displaystyle{- &#92;frac{p_i}{q_i}   }  " class="latex" /></p>
<p>Thus, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2Cj+%5Cin+X%7D++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+H_%7Bij%7D+p_j+%2B+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+H_%7Bij%7D+q_j++%3D++++%5Csum_%7Bi%2Cj%5Cin+X%7D+%281+%2B++%5Cln+%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29%29+H_%7Bij%7D+p_j+-+%5Cfrac%7Bp_i%7D%7Bq_i%7D+H_%7Bij%7D+q_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i,j &#92;in X}  &#92;frac{&#92;partial I}{&#92;partial p_i} H_{ij} p_j + &#92;frac{&#92;partial I}{&#92;partial q_i} H_{ij} q_j  =    &#92;sum_{i,j&#92;in X} (1 +  &#92;ln (&#92;frac{p_i}{q_i})) H_{ij} p_j - &#92;frac{p_i}{q_i} H_{ij} q_j } " class="latex" /></p>
<p>We can rewrite this as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++%5Csum_%7Bi%2Cj+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+1+%2B+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   &#92;sum_{i,j &#92;in X} H_{ij} p_j  &#92;left( 1 + &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right) } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij}" class="latex" /> is infinitesimal stochastic we have <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%7D+H_%7Bij%7D+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i} H_{ij} = 0," class="latex" /> so the first term drops out, and we are left with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++%5Csum_%7Bi%2Cj+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   &#92;sum_{i,j &#92;in X} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right) } " class="latex" /></p>
<p>as desired.    &nbsp;  &#9608;</p>
<p><b>Proof of Theorem 2.</b>  Thanks to Theorem 1, to prove</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%28t%29%29+%5C%3B+%5Cle+%5C%3B+++%5Csum_%7Bi+%5Cin+B%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7BDp_i%7D%7BDt%7D+%2B++%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+q_i%7D+%5Cfrac%7BDq_i%7D%7BDt%7D+++%7D+++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d}{dt}  I(p(t),q(t)) &#92;; &#92;le &#92;;   &#92;sum_{i &#92;in B} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{Dp_i}{Dt} +  &#92;frac{&#92;partial I}{&#92;partial q_i} &#92;frac{Dq_i}{Dt}   }   " class="latex" /></p>
<p>it suffices to show that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++%5Csum_%7Bi%2Cj+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%5Cle+0++%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   &#92;sum_{i,j &#92;in X} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right) &#92;le 0  }  " class="latex" /></p>
<p>or equivalently (recalling the proof of Theorem 1):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2Cj%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+%2B+1+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%5Cle+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i,j} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) + 1 - &#92;frac{p_i q_j}{p_j q_i} &#92;right) &#92;le 0 } " class="latex" /></p>
<p>The last two terms on the left hand side cancel when <img src="https://s0.wp.com/latex.php?latex=i+%3D+j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = j." class="latex" />  Thus, if we break the sum into an <img src="https://s0.wp.com/latex.php?latex=i+%5Cne+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;ne j" class="latex" /> part and an <img src="https://s0.wp.com/latex.php?latex=i+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = j" class="latex" /> part, the left side becomes</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++%5Csum_%7Bi+%5Cne+j%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+%2B+1+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%5C%3B+%2B+%5C%3B+%5Csum_j+H_%7Bjj%7D+p_j+%5Cln%28%5Cfrac%7Bp_j%7D%7Bq_j%7D%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   &#92;sum_{i &#92;ne j} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) + 1 - &#92;frac{p_i q_j}{p_j q_i} &#92;right) &#92;; + &#92;; &#92;sum_j H_{jj} p_j &#92;ln(&#92;frac{p_j}{q_j}) } " class="latex" /></p>
<p>Next we can use the infinitesimal stochastic property of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> to write <img src="https://s0.wp.com/latex.php?latex=H_%7Bjj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{jj}" class="latex" /> as the sum of <img src="https://s0.wp.com/latex.php?latex=-H_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-H_{ij}" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> not equal to <img src="https://s0.wp.com/latex.php?latex=j%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j," class="latex" /> obtaining</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cne+j%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+%2B+1+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+-+%5Csum_%7Bi+%5Cne+j%7D+H_%7Bij%7D+p_j+%5Cln%28%5Cfrac%7Bp_j%7D%7Bq_j%7D%29+%7D+%3D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;ne j} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) + 1 - &#92;frac{p_i q_j}{p_j q_i} &#92;right) - &#92;sum_{i &#92;ne j} H_{ij} p_j &#92;ln(&#92;frac{p_j}{q_j}) } = " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cne+j%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_iq_j%7D%7Bp_j+q_i%7D%29+%2B+1+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;ne j} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_iq_j}{p_j q_i}) + 1 - &#92;frac{p_i q_j}{p_j q_i} &#92;right) } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij} &#92;ge 0" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=i+%5Cne+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;ne j" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cln%28s%29+%2B+1+-+s+%5Cle+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(s) + 1 - s &#92;le 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=s+%3E+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s &gt; 0," class="latex" /> we conclude that this quantity is <img src="https://s0.wp.com/latex.php?latex=%5Cle+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;le 0." class="latex" />   &nbsp;  &#9608;</p>
<p><b>Proof of Theorem 3.</b>  Now suppose also that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an equilibrium solution of the master equation.  Then <img src="https://s0.wp.com/latex.php?latex=Dq_i%2FDt+%3D+dq_i%2Fdt+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Dq_i/Dt = dq_i/dt = 0" class="latex" /> for all states <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> so by Theorem 1 we need to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29++%5C%3B+%3D+%5C%3B++-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj+%5Cin+X%7D+J_%7Bij%7D+A_%7Bij%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right)  &#92;; = &#92;;  -&#92;frac{1}{2} &#92;sum_{i,j &#92;in X} J_{ij} A_{ij} }" class="latex" /></p>
<p>We also have <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bj+%5Cin+X%7D+H_%7Bij%7D+q_j+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{j &#92;in X} H_{ij} q_j = 0," class="latex" /> so the second<br />
term in the sum at left vanishes, and it suffices to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+%5C%3B+%3D+%5C%3B++-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj+%5Cin+X%7D+J_%7Bij%7D+A_%7Bij%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;ln(&#92;frac{p_i}{q_i}) &#92;; = &#92;;  - &#92;frac{1}{2} &#92;sum_{i,j &#92;in X} J_{ij} A_{ij} }" class="latex" /></p>
<p>By definition we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+J_%7Bij%7D+A_%7Bij%7D%7D++%3D++%5Cdisplaystyle%7B++%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D++%5Cleft%28+H_%7Bij%7D+p_j+-+H_%7Bji%7Dp_i+%5Cright%29++++%5Cln+%5Cleft%28+%5Cfrac%7Bp_j+q_i%7D%7Bp_i+q_j%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{1}{2} &#92;sum_{i,j} J_{ij} A_{ij}}  =  &#92;displaystyle{  &#92;frac{1}{2} &#92;sum_{i,j}  &#92;left( H_{ij} p_j - H_{ji}p_i &#92;right)    &#92;ln &#92;left( &#92;frac{p_j q_i}{p_i q_j} &#92;right) } " class="latex" /></p>
<p>This in turn equals</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D++H_%7Bij%7Dp_j+++++%5Cln+%5Cleft%28+%5Cfrac%7Bp_j+q_i%7D%7Bp_i+q_j%7D+%5Cright%29+-+++%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D++H_%7Bji%7Dp_i++%5Cln+%5Cleft%28+%5Cfrac%7Bp_j+q_i%7D%7Bp_i+q_j%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{1}{2} &#92;sum_{i,j}  H_{ij}p_j     &#92;ln &#92;left( &#92;frac{p_j q_i}{p_i q_j} &#92;right) -   &#92;frac{1}{2} &#92;sum_{i,j}  H_{ji}p_i  &#92;ln &#92;left( &#92;frac{p_j q_i}{p_i q_j} &#92;right) } " class="latex" /></p>
<p>and we can switch the dummy indices <img src="https://s0.wp.com/latex.php?latex=i%2Cj&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i,j" class="latex" /> in the second sum, obtaining</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D++H_%7Bij%7Dp_j+++++%5Cln+%5Cleft%28+%5Cfrac%7Bp_j+q_i%7D%7Bp_i+q_j%7D+%5Cright%29+-+++%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D++H_%7Bij%7Dp_j+++++%5Cln+%5Cleft%28+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{1}{2} &#92;sum_{i,j}  H_{ij}p_j     &#92;ln &#92;left( &#92;frac{p_j q_i}{p_i q_j} &#92;right) -   &#92;frac{1}{2} &#92;sum_{i,j}  H_{ij}p_j     &#92;ln &#92;left( &#92;frac{p_i q_j}{p_j q_i} &#92;right) } " class="latex" /></p>
<p>or simply</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2Cj%7D+H_%7Bij%7D+p_j+%5Cln+%5Cleft%28+%5Cfrac%7Bp_j+q_i%7D%7Bp_i+q_j%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i,j} H_{ij} p_j &#92;ln &#92;left( &#92;frac{p_j q_i}{p_i q_j} &#92;right) } " class="latex" /></p>
<p>But this is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Csum_%7Bi%2Cj%7D+H_%7Bij%7D+p_j+%5Cleft%28%5Cln+%28+%5Cfrac%7Bp_j%7D%7Bq_j%7D%29+%2B+%5Cln+%28%5Cfrac%7Bq_i%7D%7Bp_i%7D%29+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;sum_{i,j} H_{ij} p_j &#92;left(&#92;ln ( &#92;frac{p_j}{q_j}) + &#92;ln (&#92;frac{q_i}{p_i}) &#92;right) }" class="latex" /></p>
<p>and the first term vanishes because <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is infinitesimal stochastic: <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+H_%7Bij%7D+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i H_{ij} = 0." class="latex" /> We thus have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+J_%7Bij%7D+A_%7Bij%7D%7D+%3D+%5Csum_%7Bi%2Cj%7D+H_%7Bij%7D+p_j++%5Cln+%28%5Cfrac%7Bq_i%7D%7Bp_i%7D+%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{1}{2} &#92;sum_{i,j} J_{ij} A_{ij}} = &#92;sum_{i,j} H_{ij} p_j  &#92;ln (&#92;frac{q_i}{p_i} )  " class="latex" /></p>
<p>as desired.   &nbsp;  &#9608;</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2016/01/14/information-geometry-part-15/#comments">22 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2016/01/14/information-geometry-part-15/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;15)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-20695 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-physics" id="post-20695">
				<h2><a href="https://johncarlosbaez.wordpress.com/2016/01/11/information-geometry-part-14/" rel="bookmark">Information Geometry (Part&nbsp;14)</a></h2>
				<small>11 January, 2016</small><br />


				<div class="entry">
					<p><i>joint with <b><a href="http://www.azimuthproject.org/azimuth/show/Blake+Pollard">Blake Pollard</a></b></i></p>
<p>It&#8217;s been a long time since you&#8217;ve seen an installment of the <a href="http://math.ucr.edu/home/baez/information/">information geometry</a> series on this blog!  Before I took a long break, I was explaining relative entropy and how it changes in evolutionary games.  Much of what I said is summarized and carried further here:</p>
<p>&bull; John Baez and Blake Pollard, <a href="http://arxiv.org/abs/1512.02742">Relative entropy in biological systems</a>, <i>Entropy</i> <b>18</b> (2016), 46. (Blog article <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/">here</a>.)</p>
<p>But now Blake has a new paper, and I want to talk about that:</p>
<p>&bull; Blake Pollard, <a href="http://arxiv.org/abs/1601.00711">Open Markov processes: a compositional perspective on non-equilibrium steady states in biology</a>, <i>Entropy</i> <b>18</b> (2016), 140.</p>
<p>I&#8217;ll focus on just one aspect: the principle of minimum entropy production.  This is an exciting yet controversial principle in non-equilibrium thermodynamics.  Blake examines it in a situation where we can tell exactly what&#8217;s happening.</p>
<h3> Non-equilibrium steady states </h3>
<p>Life exists away from equilibrium.  Left isolated, systems will tend toward thermodynamic equilibrium.  However, biology is about <b>open systems</b>: physical systems that exchange matter or energy with their surroundings.  Open systems can be maintained away from equilibrium by this exchange.  This leads to the idea of a <b>non-equilibrium steady state</b>&#8212;a state of an open system that doesn&#8217;t change, but is not in equilibrium.</p>
<p>A simple example is a pan of water sitting on a stove.  Heat passes from the flame to the water and then to the air above.  If the flame is very low, the water doesn&#8217;t boil and nothing moves.  So, we have a steady state, at least approximately.  But this is not an equilibrium, because there is a constant flow of energy through the water.</p>
<p>Of course in reality the water will be slowly evaporating, so we don&#8217;t really have a steady state.  As always, models are approximations.  If the water is evaporating slowly enough, it can be useful to approximate the situation with a non-equilibrium steady state.</p>
<p>There is much more to biology than steady states.  However, to dip our toe into the chilly waters of non-equilibrium thermodynamics, it is nice to start with steady states.  And already here there are puzzles left to solve.</p>
<h3> Minimum entropy production </h3>
<p>Ilya Prigogine won the Nobel prize for his work on non-equilibrium thermodynamics.  One reason is that he had an interesting idea about steady states.  He claimed that under certain conditions, a non-equilibrium steady state will <em>minimize entropy production!</em></p>
<p>There has been a lot of work trying to make the <a href="http://www.scholarpedia.org/article/Minimum_entropy_production_principle">&#8216;principle of minimum entropy production&#8217;</a> precise and turn it into a theorem.  In this book:</p>
<p>&bull; G. Lebon and D. Jou, <i>Understanding Non-equilibrium Thermodynamics</i>, Springer, Berlin, 2008.</p>
<p>the authors give an argument for the principle of minimum entropy production based on four conditions:</p>
<p>&bull; <b>time-independent boundary conditions</b>: the surroundings of the system don&#8217;t change with time.</p>
<p>&bull; <b>linear phenomenological laws</b>: the laws governing the macroscopic behavior of the system are linear.</p>
<p>&bull; <b>constant phenomenological coefficients</b>: the laws  governing the macroscopic behavior of the system don&#8217;t change with time.</p>
<p>&bull; <b>symmetry of the phenomenological coefficients</b>: since they are linear, the laws governing the macroscopic behavior of the system can be described by a linear operator, and we demand that in a suitable basis the matrix for this operator is symmetric: <img src="https://s0.wp.com/latex.php?latex=T_%7Bij%7D+%3D+T_%7Bji%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_{ij} = T_{ji}." class="latex" /></p>
<p>The last condition is obviously the subtlest one; it&#8217;s sometimes called <b><a href="https://en.wikipedia.org/wiki/Onsager_reciprocal_relations">Onsager reciprocity</a></b>, and people have spent a lot of time trying to derive it from other conditions.</p>
<p>However, Blake goes in a different direction.  He considers a concrete class of open systems, a very large class called &#8216;open Markov processes&#8217;.  These systems obey the first three conditions listed above, and the &#8216;detailed balanced&#8217; open Markov processes also obey the last one.  But Blake shows that minimum entropy production holds only approximately&#8212;with the approximation being good for steady states that are <em>near equilibrium!</em></p>
<p>However, he shows that another minimum principle holds exactly, even for steady states that are far from equilibrium.  He calls this the &#8216;principle of minimum dissipation&#8217;.</p>
<p>We actually discussed the principle of minimum dissipation in an earlier paper:</p>
<p>&bull; John Baez, Brendan Fong and Blake Pollard, <a href="http://arxiv.org/abs/1504.05625">A compositional framework for Markov processes</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2015/09/04/a-compositional-framework-for-markov-processes/">here</a>.)</p>
<p>But one advantage of Blake&#8217;s new paper is that it presents the results with a minimum of category theory.  Of course I love category theory, and I think it&#8217;s the right way to formalize open systems, but it can be intimidating.</p>
<p>Another good thing about Blake&#8217;s new paper is that it explicitly compares the principle of minimum entropy to the principle of minimum dissipation.  He shows they agree in a certain limit&#8212;namely, the limit where the system is close to equilibrium.</p>
<p>Let me explain this.  I won&#8217;t include the nice example from biology that Blake discusses: a very simple model of membrane transport.  For that, read his paper!  I&#8217;ll just give the general results.</p>
<h3> The principle of minimum dissipation </h3>
<p>An <b>open Markov process</b> consists of a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> of <b>states</b>, a subset <img src="https://s0.wp.com/latex.php?latex=B+%5Csubseteq+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B &#92;subseteq X" class="latex" /> of <b>boundary states</b>, and an <b>infinitesimal stochastic</b> operator <img src="https://s0.wp.com/latex.php?latex=H%3A+%5Cmathbb%7BR%7D%5EX+%5Cto+%5Cmathbb%7BR%7D%5EX%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H: &#92;mathbb{R}^X &#92;to &#92;mathbb{R}^X," class="latex" /> meaning a linear operator with</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D+%5Cgeq+0+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i+%5Cneq+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij} &#92;geq 0 &#92; &#92;  &#92;text{for all} &#92; &#92; i &#92;neq j " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_i+H_%7Bij%7D+%3D+0+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i H_{ij} = 0 &#92; &#92;  &#92;text{for all} &#92; &#92; j " class="latex" /></p>
<p>I&#8217;ll explain these two conditions in a minute.</p>
<p>For each <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" /> we introduce a <b>population</b> <img src="https://s0.wp.com/latex.php?latex=p_i++%5Cin+%5B0%2C%5Cinfty%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i  &#92;in [0,&#92;infty)." class="latex" />  We call the resulting function <img src="https://s0.wp.com/latex.php?latex=p+%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : X &#92;to [0,&#92;infty)" class="latex" /> the <b>population distribution</b>. Populations evolve in time according to the <b>open master equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bdp_i%7D%7Bdt%7D+%3D+%5Csum_j+H_%7Bij%7Dp_j%7D+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i+%5Cin+X-B+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{dp_i}{dt} = &#92;sum_j H_{ij}p_j} &#92; &#92;  &#92;text{for all} &#92; &#92; i &#92;in X-B " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+%3D+b_i%28t%29+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i+%5Cin+B+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) = b_i(t) &#92; &#92;  &#92;text{for all} &#92; &#92; i &#92;in B " class="latex" /></p>
<p>So, the populations <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> obey a linear differential equation at states <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> that are not in the boundary, but they are specified &#8216;by the user&#8217; to be chosen functions <img src="https://s0.wp.com/latex.php?latex=b_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b_i" class="latex" /> at the boundary states.</p>
<p>The off-diagonal entries <img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D%2C+%5C+i+%5Cneq+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij}, &#92; i &#92;neq j" class="latex" /> are the rates at which population hops from the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state.  This lets us understand the definition of an infinitesimal stochastic operator.  The first condition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7Bij%7D+%5Cgeq+0+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i+%5Cneq+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ij} &#92;geq 0 &#92; &#92;  &#92;text{for all} &#92; &#92; i &#92;neq j " class="latex" /></p>
<p>says that the rate for population to transition from one state to another is non-negative.  The second:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_i+H_%7Bij%7D+%3D+0+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i H_{ij} = 0 &#92; &#92;  &#92;text{for all} &#92; &#92; j " class="latex" /></p>
<p>says that population is conserved, at least if there are no boundary states.  Population can flow in or out at boundary states, since the master equation doesn&#8217;t hold there.</p>
<p>A <b>steady state</b> is a solution of the open master equation that does not change with time.  A steady state for a closed Markov process is typically called an <b>equilibrium</b>.  So, an equilibrium obeys the master equation at all states, while for a steady state this may not be true at the boundary states.  Again, the reason is that population can flow in or out at the boundary.</p>
<p>We say an equilibrium <img src="https://s0.wp.com/latex.php?latex=q+%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q : X &#92;to [0,&#92;infty)" class="latex" /> of a Markov process is <b>detailed balanced</b> if the rate at which population flows from the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state to the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th state is equal to the rate at which it flows from the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th state to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7Bji%7Dq_i+%3D+H_%7Bij%7Dq_j+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i%2Cj+%5Cin+X+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{ji}q_i = H_{ij}q_j &#92; &#92;  &#92;text{for all} &#92; &#92; i,j &#92;in X " class="latex" /></p>
<p>Suppose we&#8217;ve got an open Markov process that has a detailed balanced equilibrium <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  Then a non-equilibrium steady state <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> will minimize a function called the &#8216;dissipation&#8217;, subject to constraints on its boundary populations.  There&#8217;s a nice formula for the dissipation in terms of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p><b>Definition.</b> Given an open Markov process with detailed balanced equilibrium <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> we define the <b>dissipation</b> for a population distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D%28p%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%2Cj%7D+H_%7Bij%7Dq_j+%5Cleft%28+%5Cfrac%7Bp_j%7D%7Bq_j%7D+-+%5Cfrac%7Bp_i%7D%7Bq_i%7D+%5Cright%29%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D(p) = &#92;frac{1}{2}&#92;sum_{i,j} H_{ij}q_j &#92;left( &#92;frac{p_j}{q_j} - &#92;frac{p_i}{q_i} &#92;right)^2 } " class="latex" /></p>
<p>This formula is a bit tricky, but you&#8217;ll notice it&#8217;s quadratic in <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and it vanishes when <img src="https://s0.wp.com/latex.php?latex=p+%3D+q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = q." class="latex" />  So, it&#8217;s pretty nice.</p>
<p>Using this concept we can formulate a principle of minimum dissipation, and prove that non-equilibrium steady states obey this principle:</p>
<p><b>Definition.</b> We say a population distribution <img src="https://s0.wp.com/latex.php?latex=p%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p: X &#92;to &#92;mathbb{R}" class="latex" /> obeys the <b>principle of minimum dissipation</b> with boundary population <img src="https://s0.wp.com/latex.php?latex=b%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b: X &#92;to &#92;mathbb{R}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> minimizes <img src="https://s0.wp.com/latex.php?latex=D%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D(p)" class="latex" /> subject to the constraint that</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+b_i+%5C+%5C++%5Ctext%7Bfor+all%7D+%5C+%5C+i+%5Cin+B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = b_i &#92; &#92;  &#92;text{for all} &#92; &#92; i &#92;in B" class="latex" /></p>
<p><b>Theorem 1.</b> A population distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a steady state with <img src="https://s0.wp.com/latex.php?latex=p_i+%3D+b_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = b_i" class="latex" /> for all boundary states <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> if and only if <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> obeys the principle of minimum dissipation with boundary population <img src="https://s0.wp.com/latex.php?latex=b.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b." class="latex" /></p>
<p><b>Proof</b>.  This follows from Theorem 28 in <a href="http://arxiv.org/abs/1504.05625">A compositional framework for Markov processes</a>.</p>
<h3> Minimum entropy production versus minimum dissipation </h3>
<p>How does dissipation compare with entropy production?  To answer this, first we must ask: what really is entropy production?  And: how does the equilibrium state <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> show up in the concept of entropy production?</p>
<p>The <b>relative entropy</b> of two population distributions <img src="https://s0.wp.com/latex.php?latex=p%2Cq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p,q" class="latex" /> is given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28p%2Cq%29+%3D+%5Csum_i+p_i+%5Cln+%5Cleft%28+%5Cfrac%7Bp_i%7D%7Bq_i%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(p,q) = &#92;sum_i p_i &#92;ln &#92;left( &#92;frac{p_i}{q_i} &#92;right) } " class="latex" /></p>
<p>It is well known that for a closed Markov process with <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as a detailed balanced equilibrium, the relative entropy is monotonically <em>decreasing</em> with time.   This is due to an annoying sign convention in the definition of relative entropy: while entropy is typically increasing, relative entropy typically decreases.  We could fix this by putting a minus sign in the above formula or giving this quantity <img src="https://s0.wp.com/latex.php?latex=I%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p,q)" class="latex" /> some other name.  A lot of people call it the <b>Kullback&ndash;Leibler divergence</b>, but I have taken to calling it <b>relative information</b>.  For more, see:</p>
<p>&bull; John Baez and Blake Pollard, <a href="http://arxiv.org/abs/1512.02742">Relative entropy in biological systems</a>.  (Blog article <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/">here</a>.)</p>
<p>We say &#8216;relative entropy&#8217; in the title, but then we explain why &#8216;relative information&#8217; is a better name, and use that.  More importantly, we explain why <img src="https://s0.wp.com/latex.php?latex=I%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p,q)" class="latex" /> has the physical meaning of <em>free energy</em>.  Free energy tends to decrease, so everything is okay.  For details, see Section 4.</p>
<p>Blake has a nice formula for how fast <img src="https://s0.wp.com/latex.php?latex=I%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p,q)" class="latex" /> decreases:</p>
<p><b>Theorem 2.</b> Consider an open Markov process with <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> as its set of states and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> as the set of boundary states.   Suppose <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> obeys the open master equation and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a detailed balanced equilibrium.  For any boundary state <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+B%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in B," class="latex" /> let</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BDp_i%7D%7BDt%7D+%3D+%5Cfrac%7Bdp_i%7D%7Bdt%7D+-+%5Csum_%7Bj+%5Cin+X%7D+H_%7Bij%7Dp_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{Dp_i}{Dt} = &#92;frac{dp_i}{dt} - &#92;sum_{j &#92;in X} H_{ij}p_j }" class="latex" /></p>
<p>measure how much <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> fails to obey the master equation.   Then we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+++%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bdt%7D++I%28p%28t%29%2Cq%29+%7D+%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29%7D+%5C%5C+%5C%5C++%26%26+%5C%3B+%2B+%5C%3B+%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+B%7D+%5Cfrac%7B%5Cpartial+I%7D%7B%5Cpartial+p_i%7D+%5Cfrac%7BDp_i%7D%7BDt%7D++%7D++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}   &#92;displaystyle{  &#92;frac{d}{dt}  I(p(t),q) } &amp;=&amp; &#92;displaystyle{ &#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right)} &#92;&#92; &#92;&#92;  &amp;&amp; &#92;; + &#92;; &#92;displaystyle{ &#92;sum_{i &#92;in B} &#92;frac{&#92;partial I}{&#92;partial p_i} &#92;frac{Dp_i}{Dt}  }  &#92;end{array} " class="latex" /></p>
<p>Moreover, the first term is less than or equal to zero.</p>
<p><b>Proof.</b> For a self-contained proof, see <a href="https://johncarlosbaez.wordpress.com/2016/01/14/information-geometry-part-15/">Information geometry (part 15)</a>, which is coming up soon.  It will be a special case of the theorems there.   &nbsp;  &#9608;</p>
<p>Blake compares this result to previous work by Schnakenberg:</p>
<p>&bull; J. Schnakenberg, Network theory of microscopic and macroscopic behavior of master equation systems, <i>Rev. Mod. Phys.</i> <b>48</b> (1976), 571&ndash;585.</p>
<p>The negative of Blake&#8217;s first term is this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+K%28p%29+%3D+-+%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ K(p) = - &#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right) } " class="latex" /></p>
<p>Under certain circumstances, this equals what Schnakenberg calls the <b>entropy production</b>.  But a better name for this quantity might be <b>free energy loss</b>, since for a closed Markov process that&#8217;s exactly what it is!  In this case there are no boundary states, so the theorem above says <img src="https://s0.wp.com/latex.php?latex=K%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K(p)" class="latex" /> is the rate at which relative entropy&#8212;or in other words, free energy&#8212;decreases.</p>
<p>For an open Markov process, things are more complicated.  The theorem above shows that free energy can also flow in or out at the boundary, thanks to the second term in the formula.</p>
<p>Anyway, the sensible thing is to compare a principle of &#8216;minimum free energy loss&#8217; to the principle of minimum dissipation.  The principle of minimum dissipation is true.  How about the principle of minimum free energy loss?  It turns out to be approximately true near equilibrium.</p>
<p>For this, consider the situation in which <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is near to the equilibrium distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in the sense that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bp_i%7D%7Bq_i%7D+%3D+1+%2B+%5Cepsilon_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{p_i}{q_i} = 1 + &#92;epsilon_i } " class="latex" /></p>
<p>for some small numbers <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_i." class="latex" />  We collect these numbers in a vector called <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon." class="latex" /></p>
<p><b>Theorem 3.</b>  Consider an open Markov process with <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> as its set of states and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> as the set of boundary states.   Suppose <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a detailed balanced equilibrium and let <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> be arbitrary.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=K%28p%29+%3D+D%28p%29+%2B+O%28%5Cepsilon%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K(p) = D(p) + O(&#92;epsilon^2) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=K%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K(p)" class="latex" /> is the free energy loss, <img src="https://s0.wp.com/latex.php?latex=D%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D(p)" class="latex" /> is the dissipation, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_i" class="latex" /> is defined as above, and by <img src="https://s0.wp.com/latex.php?latex=O%28%5Cepsilon%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(&#92;epsilon^2)" class="latex" /> we mean a sum of terms of order <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_i%5E2.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_i^2." class="latex" /></p>
<p><b>Proof.</b>  First take the free energy loss:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+K%28p%29+%3D+-%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cln%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%29+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ K(p) = -&#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;left( &#92;ln(&#92;frac{p_i}{q_i}) - &#92;frac{p_i q_j}{p_j q_i} &#92;right)} " class="latex" /></p>
<p>Expanding the logarithm to first order in <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon," class="latex" /> we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+K%28p%29+%3D++-%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cfrac%7Bp_i%7D%7Bq_i%7D+-+1+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%2B+O%28%5Cepsilon%5E2%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ K(p) =  -&#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;left( &#92;frac{p_i}{q_i} - 1 - &#92;frac{p_i q_j}{p_j q_i} &#92;right) + O(&#92;epsilon^2) } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is infinitesimal stochastic, <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+H_%7Bij%7D+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i H_{ij} = 0," class="latex" /> so the second term in the sum vanishes, leaving</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+K%28p%29+%3D++-%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+p_j++%5Cleft%28+%5Cfrac%7Bp_i%7D%7Bq_i%7D+-+%5Cfrac%7Bp_i+q_j%7D%7Bp_j+q_i%7D+%5Cright%29+%5C%3B+%2B+O%28%5Cepsilon%5E2%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ K(p) =  -&#92;sum_{i, j &#92;in X} H_{ij} p_j  &#92;left( &#92;frac{p_i}{q_i} - &#92;frac{p_i q_j}{p_j q_i} &#92;right) &#92;; + O(&#92;epsilon^2) } " class="latex" /></p>
<p>or</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+K%28p%29+%3D++-%5Csum_%7Bi%2C+j+%5Cin+X%7D+%5Cleft%28+H_%7Bij%7D+p_j++%5Cfrac%7Bp_i%7D%7Bq_i%7D+-+H_%7Bij%7D+q_j+%5Cfrac%7Bp_i%7D%7Bq_i%7D+%5Cright%29+%5C%3B+%2B+O%28%5Cepsilon%5E2%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ K(p) =  -&#92;sum_{i, j &#92;in X} &#92;left( H_{ij} p_j  &#92;frac{p_i}{q_i} - H_{ij} q_j &#92;frac{p_i}{q_i} &#92;right) &#92;; + O(&#92;epsilon^2) } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a equilibrium we have <img src="https://s0.wp.com/latex.php?latex=%5Csum_j+H_%7Bij%7D+q_j+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_j H_{ij} q_j = 0," class="latex" /> so now the last term in the sum vanishes, leaving</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+K%28p%29+%3D++-%5Csum_%7Bi%2C+j+%5Cin+X%7D+H_%7Bij%7D+%5Cfrac%7Bp_i+p_j%7D%7Bq_i%7D+%5C%3B+%2B+O%28%5Cepsilon%5E2%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ K(p) =  -&#92;sum_{i, j &#92;in X} H_{ij} &#92;frac{p_i p_j}{q_i} &#92;; + O(&#92;epsilon^2) } " class="latex" /></p>
<p>Next, take the dissipation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D%28p%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%2Cj%7D+H_%7Bij%7Dq_j+%5Cleft%28+%5Cfrac%7Bp_j%7D%7Bq_j%7D+-+%5Cfrac%7Bp_i%7D%7Bq_i%7D+%5Cright%29%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D(p) = &#92;frac{1}{2}&#92;sum_{i,j} H_{ij}q_j &#92;left( &#92;frac{p_j}{q_j} - &#92;frac{p_i}{q_i} &#92;right)^2 } " class="latex" /></p>
<p>and expand the square, getting</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D%28p%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%2Cj%7D+H_%7Bij%7Dq_j+%5Cleft%28+%5Cfrac%7Bp_j%5E2%7D%7Bq_j%5E2%7D+-+2%5Cfrac%7Bp_i+p_j%7D%7Bq_i+q_j%7D+%2B++%5Cfrac%7Bp_i%5E2%7D%7Bq_i%5E2%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D(p) = &#92;frac{1}{2}&#92;sum_{i,j} H_{ij}q_j &#92;left( &#92;frac{p_j^2}{q_j^2} - 2&#92;frac{p_i p_j}{q_i q_j} +  &#92;frac{p_i^2}{q_i^2} &#92;right) } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is infinitesimal stochastic, <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+H_%7Bij%7D+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i H_{ij} = 0." class="latex" />  The first term is just this times a function of <img src="https://s0.wp.com/latex.php?latex=j%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j," class="latex" /> summed over <img src="https://s0.wp.com/latex.php?latex=j%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j," class="latex" /> so it vanishes, leaving</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D%28p%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%2Cj%7D+H_%7Bij%7Dq_j+%5Cleft%28-+2%5Cfrac%7Bp_i+p_j%7D%7Bq_i+q_j%7D+%2B++%5Cfrac%7Bp_i%5E2%7D%7Bq_i%5E2%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D(p) = &#92;frac{1}{2}&#92;sum_{i,j} H_{ij}q_j &#92;left(- 2&#92;frac{p_i p_j}{q_i q_j} +  &#92;frac{p_i^2}{q_i^2} &#92;right) } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an equilibrium, <img src="https://s0.wp.com/latex.php?latex=%5Csum_j+H_%7Bij%7D+q_j+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_j H_{ij} q_j = 0." class="latex" />   The last term above is this times a function of <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> summed over <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> so it vanishes, leaving</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D%28p%29+%3D+-+%5Csum_%7Bi%2Cj%7D+H_%7Bij%7Dq_j++%5Cfrac%7Bp_i+p_j%7D%7Bq_i+q_j%7D+%3D+-+%5Csum_%7Bi%2Cj%7D+H_%7Bij%7D+%5Cfrac%7Bp_i+p_j%7D%7Bq_i%7D++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D(p) = - &#92;sum_{i,j} H_{ij}q_j  &#92;frac{p_i p_j}{q_i q_j} = - &#92;sum_{i,j} H_{ij} &#92;frac{p_i p_j}{q_i}  }" class="latex" /></p>
<p>This matches what we got for <img src="https://s0.wp.com/latex.php?latex=K%28p%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K(p)," class="latex" /> up to terms of order <img src="https://s0.wp.com/latex.php?latex=O%28%5Cepsilon%5E2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(&#92;epsilon^2)." class="latex" />     &nbsp;  &#9608;</p>
<p>In short: detailed balanced open Markov processes are governed by the principle of minimum dissipation, not minimum entropy production.  <i>Minimum dissipation agrees with minimum entropy production only near equilibrium.</i></p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2016/01/11/information-geometry-part-14/#comments">28 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2016/01/11/information-geometry-part-14/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;14)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/3/?s=information+and+entropy" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/?s=information+and+entropy" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/page/2/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jksOwjAMRC9EarWLSl0gzpImVuXQfLAd2t6eVAIJsWA1nrGePbAV43JSTApBwOOTHJa9C3KBr1Wspqx1oSRgfaRkZssQrShym4yydXf5hdq98KjIx1u6rbgcTeG8H4axZaIfhpJbq0c5oWYxzui79uhPkY38giogdRbHVJRyOjvc4rUfh2nqh2Hswwtq4FN0'></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZVeCxXTFdQUTZueWpTWF9xeWZyOV9iRS9FUUV1SmZSaV9VNDlkOHNVLEpvUVd2TGRqamlTXXVXJl9ZYVNIL3xNaFEya1pQfkhCblR0WlQ2bHdDQk5pRyZ6TWp5Ykh4bU95NlNhYUUlRUNKZHYlRWlZaGFNcURMTnUzQUxqK0Riai9fdHxEOFZ5Lnl5aH5yWmlyL2drcHF5WkxLUUNkZTlqdG5KfmI1QyxWVnJiWl9CZ1V4JXRFPWVINVVncWRZcDZFVC1i'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>