<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth | Page 8</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2014%2F03%2F16%2Fnetwork-theory-iii%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/page\/8\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fpage%2F8%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2014%2F03%2F16%2Fnetwork-theory-iii%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Page 8 &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results paged paged-8 search-paged-8 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-17693 post type-post status-publish format-standard hentry category-information-and-entropy category-networks category-probability" id="post-17693">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/" rel="bookmark">Network Theory III</a></h2>
				<small>16 March, 2014</small><br />


				<div class="entry">
					<p>&nbsp;</p>
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="450" height="315" src="https://www.youtube.com/embed/qX8fSYu7ors?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
<p>In the last of my <a href="https://johncarlosbaez.wordpress.com/2014/02/07/network-theory-talks-at-oxford/">Oxford talks</a> I explain how entropy and relative entropy can be understood using certain categories related to probability theory&#8230; and how these categories also let us understand Bayesian networks!  </p>
<p>The first two parts are explanations of these papers:</p>
<p>&bull; John Baez, Tobias Fritz and Tom Leinster, <a href="http://arxiv.org/abs/1106.1791">A characterization of entropy in terms of information loss</a></p>
<p>&bull; John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>.</p>
<p>Somewhere around here the talk was interrupted by a fire drill, waking up the entire audience!</p>
<p>By the way, in my talk I mistakenly said that relative entropy is a continuous functor; in fact it&#8217;s just lower semicontinuous.  I&#8217;ve fixed this in my slides.</p>
<p>The third part of my talk was my own interpretation of Brendan Fong&#8217;s master&#8217;s thesis:</p>
<p>&bull; Brendan Fong, <a href="http://arxiv.org/abs/1301.6201"><i>Causal Theories: a Categorical Perspective on Bayesian Networks</i></a>.</p>
<p>I took a slightly different approach, by saying that a causal theory <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D_G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{C}_G" class="latex" /> is the free category with products on certain objects and morphisms coming from a directed acyclic graph <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" />.  In his thesis he said <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D_G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{C}_G" class="latex" /> was the free symmetric monoidal category where each generating object is equipped with a cocommutative comonoid structure.  This is close to a category with finite products, though perhaps not quite the same: a symmetric monoidal category where every object is equipped with a cocommutative comonoid structure in a <i>natural</i> way (i.e., making a bunch of squares commute) is a category with finite products.  It would be interesting to see if this difference hurts or helps.</p>
<p>By making this slight change, I am claiming that causal theories can be seen as <a href="http://www.iti.cs.tu-bs.de/~adamek/algebraic.theories.pdf">algebraic theories</a> in the sense of Lawvere.  This would be a very good thing, since we know a lot about those.</p>
<p>You can also <a href="http://math.ucr.edu/home/baez/networks_oxford/networks_entropy.pdf">see the slides of this talk</a>.  Click on any picture in the slides, or any text in blue, and get more information!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/#comments">3 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/" rel="bookmark" title="Permanent Link to Network Theory III">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-17396 post type-post status-publish format-standard hentry category-mathematics category-physics" id="post-17396">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/02/06/categories-in-control/" rel="bookmark">Categories in Control &#8211; Erlangen&nbsp;Talk</a></h2>
				<small>6 February, 2014</small><br />


				<div class="entry">
					<p>&nbsp;</p>
<p><img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/diary/erlangen/erlangen_1916.jpg" /></p>
<p>I&#8217;m visiting Erlangen from now until the end of May, since my wife got a grant to do research here.  I&#8217;m trying to get a lot of papers finished.  But today I&#8217;m giving a talk in the math department of the university here, which with Germanic brevity is called the Friedrich-Alexander-Universität Erlangen-Nürnberg.</p>
<p>You can see my slides here, or maybe even come to my talk:</p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/control/control_talk_erlangen.pdf">Categories in control</a>, Thursday 6 February 2014, 16:15&#8211;18:00, Mathematics Department of the FAU, in  <a href="https://www.math.fau.de/kontakt/campus-map.html">&Uuml;bungsraum 1</a>.</p>
<p>The title is a pun.  It&#8217;s about categories in <a href="http://en.wikipedia.org/wiki/Control_theory"><b>control theory</b></a>, the branch of engineering that studies dynamical systems with inputs and outputs, and how to optimize their behavior.</p>
<p>Control theorists often describe these systems using <a href="http://en.wikipedia.org/wiki/Signal-flow_graph">signal-flow graphs</a>.  Here is a very rough schematic signal-flow graph, describing the all-important concept of a &#8216;feedback loop&#8217;:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/Signal-flow_graph"><img width="400" src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/24/Feedback_loop_with_descriptions.svg/500px-Feedback_loop_with_descriptions.svg.png" /></a></div>
<p>Here is a detailed one, describing a specific device called a servo:</p>
<p><a href="http://en.wikipedia.org/wiki/Signal-flow_graph#Example_4:_Position_servo_with_multi-loop_feedback"><img width="450" src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Position_servo_and_signal_flow_graph.png/640px-Position_servo_and_signal_flow_graph.png" /></a></p>
<p>The device is shown on top, and the signal-flow graph describing its behavior is at bottom.  For details, click on the picture.</p>
<p>Now, if you have a drop of category-theorist&#8217;s blood in your veins, you&#8217;ll look at this signal-flow graph and think <i>my god, that&#8217;s a string diagram for a morphism in a monoidal category!</i></p>
<p>And you&#8217;d be right.  But if you want to learn what that means, and why it matters, <a href="http://math.ucr.edu/home/baez/control/control_talk_erlangen.pdf">read my talk slides!</a></p>
<p>The slides should make sense if you&#8217;re a mathematician, but maybe not otherwise.  So, here&#8217;s the executive summary.  The same sort of super-abstract math that handles things like Feynman diagrams:</p>
<div align="center"><img width="350" src="https://i1.wp.com/math.ucr.edu/home/baez/networks/feynman_diagram_realistic.png" /></div>
<p>also handles signal-flow graphs.  The details are different in important and fascinating ways, and this is what I&#8217;m mainly concerned with.  But we now understand how signal-flow graphs fit into the general theory of networks.  This means we can proceed to use modern math to study them&#8212;and their relation to other kinds of networks, like electrical circuit diagrams:</p>
<p><img width="450" src="https://i1.wp.com/math.ucr.edu/home/baez/networks/electronics_circuit_diagram.jpg" /></p>
<h3> More talks </h3>
<p>Thanks to the <a href="http://www.azimuthproject.org/azimuth/show/HomePage">Azimuth Project</a> team, my graduate students and many other folks, the <a href="http://math.ucr.edu/home/baez/networks/networks_1.html">dream of network theory as a step toward &#8216;green mathematics&#8217;</a> seems to be coming true!  There&#8217;s a vast amount left to be done, so I&#8217;d have trouble convincing a skeptic, but I feel the project has turned a corner.  I now feel in my bones that it&#8217;s going to work: we&#8217;ll eventually develop a language for biology and ecology based in part on category theory.</p>
<p>So, I think it&#8217;s a good time to explain all the various aspects of this project that have been cooking away&#8212;some quite visibly, but others on secret back burners:</p>
<p>&bull; Jacob Biamonte and I have written a book on <a href="http://math.ucr.edu/home/baez/stoch_stable.pdf">Petri nets and chemical reaction networks</a>.  You may have seen parts of this <a href="http://math.ucr.edu/home/baez/networks/">on the blog</a>.  We started this project at the Centre for Quantum Technologies, but now he&#8217;s working at the Institute for Scientific Interchange, in Turin&#8212;and collaborating with people there on various aspects of network theory.</p>
<p>&bull; Brendan Fong is working with me on <a href="http://math.ucr.edu/home/baez/Brendan_Fong_Transfer_Report.pdf">electrical circuits</a>.  You may know him for his posts here on chemical reaction networks.  He&#8217;s now a grad student in computer science at Oxford.</p>
<p>&bull;  Jason Erbele, a math grad student at U.C. Riverside, is working with me on control theory.  This work is the main topic of my talk&#8212;but I also sketch how it ties together with what Brendan is doing.  There&#8217;s a lot more to say here.</p>
<p>&bull; Tobias Fritz, a postdoc at the Perimeter Institute, is working with me on category-theoretic aspects of information theory.  We published a <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/">paper on entropy</a> with Tom Leinster, and we&#8217;ve got a followup on relative entropy that&#8217;s almost done. I should be working on it <i>right this instant!</i>  <img src="https://i2.wp.com/math.ucr.edu/home/baez/emoticons/uhh.gif" />  But for now, read the series of posts here on Azimuth: <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy Part 1</a>, <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Part 2</a> and <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Part 3</a>.</p>
<p>&bull; Brendan Fong has also done some great work on <a href="http://arxiv.org/abs/1301.6201">Bayesian networks</a>, using ideas that connect nicely to what Tobias and I are doing.</p>
<p>&bull; Tu Pham and Franciscus Rebro are working on the math that underlies all these projects: bicategories of spans.</p>
<p>The computer science department at Oxford is a great place for category theory and diagrammatic reasoning, thanks to the presence of Samson Abramsky, Bob Coecke and others.  I&#8217;m going to visit them from February 21 to March 14.  It seems like a good time to give a series of talks on this stuff.   So, stay tuned!  I&#8217;ll try to make slides available here.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/06/categories-in-control/#comments">33 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/06/categories-in-control/" rel="bookmark" title="Permanent Link to Categories in Control &#8211; Erlangen&nbsp;Talk">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-17155 post type-post status-publish format-standard hentry category-azimuth category-climate category-mathematics" id="post-17155">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/12/31/2014-on-azimuth/" rel="bookmark">2014 on Azimuth</a></h2>
				<small>31 December, 2013</small><br />


				<div class="entry">
					<p>&nbsp; </p>
<div align="center">
<img src="https://i0.wp.com/math.ucr.edu/home/baez/champagne.jpg" />
</div>
<p>Happy New Year!  We&#8217;ve got some fun guest posts lined up for next year, including:</p>
<p>&bull; <b>Marc Harper, Relative entropy in evolutionary dynamics.</b></p>
<p><a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a> uses ideas from information theory in his work on bioinformatics and evolutionary game theory.  This article explains some of his new work.  And as a warmup, it explains how relative entropy can serve as a Lyapunov function in evolution!  </p>
<p>This includes answering the question: </p>
<p><i>“What is a Lyapunov function, and why should I care?&#8221;</i>  <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/sm_confused.gif" alt="" /></p>
<p>The brief answer, in case you&#8217;re eager to know, is this.   A Lyapunov function is something that <i>always increases</i>&#8212;or always decreases&#8212;as time goes on.  Examples include entropy and free energy.  So, a Lyapunov function can be a way of making the 2nd law of thermodynamics mathematically precise!  It&#8217;s also a way to show things are approaching equilibrium.  </p>
<p>The overall goal here is applying entropy and information theory to better understand the behavior of biological and ecological systems.  And in April 2015, Marc Harper and I are helping run a <a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/">workshop</a> on this topic!  We&#8217;re doing this with <a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a>, an ecologist who uses maximum entropy methods to predict the distribution, abundance and energy usage of species. It should be really interesting!</p>
<p>But back to blog articles:</p>
<p>&bull; <b>Manoj Gopalkrishnan, Lyapunov functions for complex-balanced systems.</b></p>
<p><a href="http://www.tcs.tifr.res.in/~manoj/">Manoj Gopalkrishnan</a> is a mathematician at the Tata Institute of Fundamental Research in Mumbai who works on problems coming from chemistry and biology.   This post will explain his recent paper on a Lyapunov function for chemical reactions.  This function is closely related to free energy, a concept from thermodynamics.  So again, one of the overall goals is to apply entropy to better understand living systems.</p>
<p>Since some evolutionary games are isomorphic to chemical reaction networks, this post should be connected to Marc&#8217;s. But there’s some mental work left to make the connection&#8212;for me, at least. It should be really cool when it all fits together!</p>
<p>&bull; <b>Alastair Jamieson-Lane, Stochastic cross impact balance analysis.</b></p>
<p><a href="http://www.azimuthproject.org/azimuth/show/Alastair+Jamieson-Lane">Alastair Jamieson-Lane</a> is a mathematician in the master’s program at the University of British Columbia.  Very roughly, this post is about a method for determining which economic scenarios are more likely. The likely scenarios get fed into things like the IPCC climate models, so this is important.</p>
<p>This blog article has an interesting origin.  <a href="https://uwaterloo.ca/knowledge-integration/people-profiles/vanessa-schweizer">Vanessa Schweizer</a> has a bachelor&#8217;s degree in physics, a masters in environmental studies, and a PhD in engineering and public policy.  She now works at the University of Waterloo on long-term decision-making problems.</p>
<p>A while back, I met Vanessa at a workshop called <a href="http://math.ucr.edu/home/baez/balsillie/">What Is Climate Change and What To Do About It?</a>, at the Balsillie School of International Affairs, which is in Waterloo. She described her work with Alastair Jamieson-Lane and the physicist <a href="http://perimeterinstitute.ca/people/matteo-smerlak">Matteo Smerlak</a> on stochastic cross impact balance analysis.  It sounded really interesting, something I&#8217;d like to work on.  So I solicited some blog articles from them.  I hope this is just the first!</p>
<p>So: <i>Happy New Year, and good reading!</i></p>
<p>Also: we&#8217;re always looking for good guest posts here on Azimuth, and we have a <a href="http://www.azimuthproject.org/azimuth/show/How+to#blog">system for helping you write them</a>.  So, if you know something interesting about environmental or energy issues, ecology, biology or chemistry, consider giving it a try!  </p>
<p>If you read some posts here, especially guest posts, you&#8217;ll get an idea of what we&#8217;re looking for.  <a href="http://www.azimuthproject.org/azimuth/show/David+Tanzer">David Tanzer</a>, a software developer in New York who is very active in the Azimuth Project these days, made an organized list of Azimuth blog posts here:</p>
<p>&bull; <a href="http://www.azimuthproject.org/azimuth/show/Azimuth+blog+overview">Azimuth Blog Overview</a>.</p>
<p>You can see the guest posts listed by author.   This overview is also great for catching up on old posts!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/12/31/2014-on-azimuth/#comments">2 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/azimuth/" rel="category tag">azimuth</a>, <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/12/31/2014-on-azimuth/" rel="bookmark" title="Permanent Link to 2014 on Azimuth">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16858 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-16858">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/11/11/quantropy-part-4/" rel="bookmark">Quantropy (Part 4)</a></h2>
				<small>11 November, 2013</small><br />


				<div class="entry">
					<p>There&#8217;s a new paper on the arXiv:</p>
<p>&bull; John Baez and Blake Pollard, <a href="http://arxiv.org/abs/1311.0813">Quantropy</a>.</p>
<p>Blake is a physics grad student at U. C. Riverside who plans to do his thesis with me.  </p>
<p>If you have carefully read all my previous posts on quantropy (<a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/">Part 1</a>, <a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/">Part 2</a> and <a href="https://johncarlosbaez.wordpress.com/2012/02/18/quantropy-part-3/">Part 3</a>), there&#8217;s only a little new stuff here.  But still, it&#8217;s better organized, and less chatty.</p>
<p>And in fact, Blake came up with a lot of new stuff for this paper!  He studied the quantropy of the harmonic oscillator, and tweaked the analogy between statistical mechanics and quantum mechanics in an interesting way.  Unfortunately, we needed to put a version of this paper on the arXiv by a deadline, and our writeup of this new work wasn&#8217;t quite ready (my fault). So, we&#8217;ll put that other stuff in a new version&#8212;or, I&#8217;m thinking now, a separate paper.</p>
<p>But here are two new things.  </p>
<p>First, putting this paper on the arXiv had the usual good effect of revealing some existing work on the same topic.  Joakim Munkhammar emailed me and pointed out this paper, which is free online:</p>
<p>&bull; Joakim Munkhammar, <a href="http://www.ejtp.com/articles/ejtpv8i25p93.pdf">Canonical relational quantum mechanics from information theory</a>, <i><a href="http://www.ejtp.com/ejtpv8i25">Electronic Journal of Theoretical Physics</a></i> <b>8</b> (2011), 93–108.</p>
<p>You&#8217;ll see it cites <a href="http://arxiv.org/abs/physics/0605068">Garrett Lisi&#8217;s paper</a> and pushes forward in various directions.  There seems to be a typo where he writes the path integral </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cdisplaystyle%7B+%5Cint+e%5E%7B-%5Calpha+S%28q%29+%7D+D+q%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;displaystyle{ &#92;int e^{-&#92;alpha S(q) } D q} " class="latex" /></p>
<p>and says </p>
<blockquote><p>
In order to fit the purpose Lisi concluded that the Lagrange multiplier value <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cequiv+1%2Fi+%5Chbar.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;equiv 1/i &#92;hbar." class="latex" />  In similarity with Lisi’s approach we shall also assume that the arbitrary scaling-part of the constant <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> is in fact <img src="https://s0.wp.com/latex.php?latex=1%2F%5Chbar.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/&#92;hbar." class="latex" />
</p></blockquote>
<p>I&#8217;m pretty sure he means <img src="https://s0.wp.com/latex.php?latex=1%2Fi%5Chbar%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/i&#92;hbar," class="latex" /> given what he writes later. However, he speaks of &#8216;maximizing entropy&#8217;, which is not quite right for a complex-valued quantity; Blake and I prefer to give this new quantity a new name, and speak of &#8216;finding a stationary point of quantropy&#8217;.</p>
<p>But in a way these are small issues; being a mathematician, I&#8217;m more quick to spot tiny technical defects than to absorb significant new ideas, and it will take a while to really understand Munkhammar&#8217;s paper.</p>
<p>Second, while writing our paper, Blake and I noticed another similarity between the partition function of a classical ideal gas and the partition function of a quantum free particle.  Both are given by an integral like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cdisplaystyle%7B%5Cint+e%5E%7B-%5Calpha+S%28q%29+%7D+D+q+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;displaystyle{&#92;int e^{-&#92;alpha S(q) } D q } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is a quadratic function of <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in &#92;mathbb{R}^n." class="latex" />  Here <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is the number of degrees of freedom for the particles in the ideal gas, or the number of time steps for a free particle on a line (where we are discretizing time).  The only big difference is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1%2FkT+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = 1/kT " class="latex" /></p>
<p>for the ideal gas, but</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1%2Fi+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = 1/i &#92;hbar" class="latex" /></p>
<p>for the free particle.  </p>
<p>In both cases there&#8217;s an ambiguity in the answer!  The reason is that to do this integral, we need to pick a measure <img src="https://s0.wp.com/latex.php?latex=D+q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D q." class="latex" />  The obvious guess is Lebesgue measure</p>
<p><img src="https://s0.wp.com/latex.php?latex=dq+%3D+dq_1+%5Ccdots+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq = dq_1 &#92;cdots dq_n " class="latex" /></p>
<p>on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n." class="latex" />  But this can&#8217;t be right, on physical grounds!</p>
<p>The reason is that the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> needs to be dimensionless, but <img src="https://s0.wp.com/latex.php?latex=d+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d q" class="latex" /> has units.  To correct this, we need to divide <img src="https://s0.wp.com/latex.php?latex=dq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq" class="latex" /> by some dimensionful quantity to get <img src="https://s0.wp.com/latex.php?latex=D+q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D q." class="latex" /></p>
<p>In the case of the ideal gas, this dimensionful quantity involves the <a href="http://en.wikipedia.org/wiki/Thermal_de_Broglie_wavelength">&#8216;thermal de Broglie wavelength&#8217;</a> of the particles in the gas.  And this brings Planck&#8217;s constant into the game, <i>even though we&#8217;re not doing quantum mechanics</i>: we&#8217;re studying the statistical mechanics of a <i>classical</i> ideal gas!  </p>
<p>That&#8217;s weird and interesting.  It&#8217;s not the only place where we see that classical statistical mechanics is incomplete or inconsistent, and we need to introduce some ideas from quantum physics to get sensible answers.  The most famous one is the <a href="http://en.wikipedia.org/wiki/Ultraviolet_catastrophe">ultraviolet catastrophe</a>.  What are all rest?</p>
<p>In the case of the free particle, we need to divide by a quantity with dimensions of length<sup><i>n</i></sup> to make</p>
<p><img src="https://s0.wp.com/latex.php?latex=dq+%3D+dq_1+%5Ccdots+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq = dq_1 &#92;cdots dq_n " class="latex" /></p>
<p>dimensionless, since each <img src="https://s0.wp.com/latex.php?latex=dq_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq_i" class="latex" /> has dimensions of length.  The easiest way is to introduce a length scale <img src="https://s0.wp.com/latex.php?latex=%5CDelta+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta x" class="latex" /> and divide each <img src="https://s0.wp.com/latex.php?latex=dq_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq_i" class="latex" /> by that.  This is commonly done when people study the free particle.  This length scale drops out of the final answer for the questions people usually care about&#8230; but <i>not</i> for the quantropy.  </p>
<p>Similarly, Planck&#8217;s constant drops out of the final answer for some questions about the classical ideal gas, but <i>not</i> for its entropy!</p>
<p>So there&#8217;s an interesting question here, about what this new length scale <img src="https://s0.wp.com/latex.php?latex=%5CDelta+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta x" class="latex" /> means, if anything.  One might argue that quantropy is a bad idea, and the need for this new length scale to make it unambiguous is just proof of that.  However, the mathematical analogy to quantum mechanics is so precise that I think it&#8217;s worth going a bit further out on this limb, and thinking a bit more about what&#8217;s going on.  </p>
<p>Some weird sort of <i>d&eacute;j&agrave; vu</i> phenomenon seems to be going on.  Once upon a time, people tried to calculate the partition functions of classical systems.  They discovered they were infinite or ambiguous until they introduced Planck&#8217;s constant, and eventually quantum mechanics.  Then Feynman introduced the path integral approach to quantum mechanics.  In this approach one is again computing partition functions, but now with a new meaning, and with complex rather than real exponentials.  But these partition functions are again infinite or ambiguous&#8230; <i>for very similar mathematical reasons!</i>  And at least in some cases, we can remove the ambiguity using the same trick as before: introducing a new constant.  But then&#8230; what?</p>
<p>Are we stuck in an infinite loop here?  What, if anything, is the meaning of this &#8216;second Planck&#8217;s constant&#8217;?  Does this have anything to do with <a href="http://en.wikipedia.org/wiki/Second_quantization"> second quantization</a>?  (I don&#8217;t see how, but I can&#8217;t resist asking.)</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/11/quantropy-part-4/#comments">70 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/11/quantropy-part-4/" rel="bookmark" title="Permanent Link to Quantropy (Part 4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-12832 post type-post status-publish format-standard hentry category-carbon-emissions category-climate category-economics category-energy category-information-and-entropy category-sustainability" id="post-12832">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/" rel="bookmark">John Harte</a></h2>
				<small>27 October, 2012</small><br />


				<div class="entry">
					<p>Earlier this week I gave a talk on the <a href="http://math.ucr.edu/home/baez/planet/planet_usc.pdf">Mathematics of Planet Earth</a> at the University of Southern California, and someone there recommended that I look into John Harte&#8217;s work on maximum entropy methods in ecology.  He works at U.C. Berkeley.  </p>
<div align="center"><a href="http://www.forbes.com/sites/michaeltobias/2011/08/29/climate-shock-uc-berkeley-scientist-dr-john-harte-puts-the-world-on-notice/"><img width="300" src="https://i0.wp.com/blogs-images.forbes.com/michaeltobias/files/2011/08/John-Harte1.jpg" /></a></div>
<p>I checked out <a href="http://socrates.berkeley.edu/~hartelab/">his website</a> and found that his goals resemble mine: save the planet and understand its ecosystems.  He&#8217;s a lot further along than I am, since he comes from a long background in ecology while I&#8217;ve just recently blundered in from mathematical physics.  I can&#8217;t really say what I think of his work since I&#8217;m just learning about it.  But I thought I should point out its existence.</p>
<p>This free book is something a lot of people would find interesting:</p>
<p>&bull; John and Mary Ellen Harte, <a href="http://www.cooltheearth.us/download.php"><i>Cool the Earth, Save the Economy: Solving the Climate Crisis Is EASY</i></a>, 2008.</p>
<p>EASY?  Well, it&#8217;s an acronym.  Here&#8217;s the basic idea of the US-based plan described in this book:</p>
<blockquote>
<p>Any proposed energy policy should include these two components:</p>
<p>&bull; <b>Technical/Behavioral</b>: What resources and technologies are to be used to supply energy? On the demand side, what technologies and lifestyle changes are being proposed to consumers?</p>
<p>&bull; <b>Incentives/Economic Policy</b>: How are the desired supply and demand options to be encouraged or forced? Here the options include taxes, subsidies, regulations, permits, research and development, and education.</p>
<p>And a successful energy policy should satisfy the AAA criteria:</p>
<p>&bull; <b>Availability</b>. The climate crisis will rapidly become costly to society if we do not take action expeditiously. We need to adopt now those technologies that are currently available, provided they meet the following two additional criteria:</p>
<p>&bull; <b>Affordability</b>. Because of the central role of energy in our society, its cost to consumers should not increase significantly. In fact, a successful energy policy could ultimately save consumers money.</p>
<p>&bull; <b>Acceptability</b>. All energy strategies have environmental, land use, and health and safety implications; these must be acceptable to the public. Moreover, while some interest groups will undoubtedly oppose any particular energy policy, political acceptability at a broad scale is necessary.</p>
<p>Our strategy for preventing climate catastrophe and achieving energy independence includes:</p>
<p>&bull; <b>Energy Efficient Technology</b> at home and at the workplace. Huge reductions in home energy use can be achieved with available technologies, including more efficient appliances such as refrigerators, water heaters, and light bulbs. Home retrofits and new home design features such as “smart” window coatings, lighter-colored roofs where there are hot summers, better home insulation, and passive solar designs can also reduce energy use. Together, energy efficiency in home and industry can save the U.S. up to approximately half of the energy currently consumed in those sectors, and at no net cost&#8212;just by making different choices. Sounds good, doesn’t it?</p>
<p>&bull; <b>Automobile Fuel Efficiency</b>. Phase in higher Corporate Average Fuel Economy (CAFE) standards for automobiles, SUVs and light trucks by requiring vehicles to go 35 miles per gallon of gas (mpg) by 2015, 45 mpg by 2020, and 60 mpg by 2030. This would rapidly wipe out our dependence on foreign oil and cut emissions from the vehicle sector by two-thirds. A combination of plug-in hybrid, lighter car body materials, re-design and other innovations could readily achieve these standards. This sounds good, too!</p>
<p>&bull; <b>Solar and Wind Energy</b>. Rooftop photovoltaic panels and solar water heating units should be phased in over the next 20 years, with the goal of solar installation on 75% of U.S. homes and commercial buildings by 2030. (Not all roofs receive sufficient sunlight to make solar panels practical for them.) Large wind farms, solar photovoltaic stations, and solar thermal stations should also be phased in so that by 2030, all U.S. electricity demand will be supplied by existing hydroelectric, existing and possibly some new nuclear, and, most importantly, new solar and wind units. This will require investment in expansion of the grid to bring the new supply to the demand, and in research and development to improve overnight storage systems. Achieving this goal would reduce our dependence on coal to practically zero. More good news!</p>
<p>&bull; <b>You</b> are part of the answer. Voting wisely for leaders who promote the first three components is one of the most important individual actions one can make. Other actions help, too. Just as molecules make up mountains, individual actions taken collectively have huge impacts. Improved driving skills, automobile maintenance, reusing and recycling, walking and biking, wearing sweaters in winter and light clothing in summer, installing timers on thermostats and insulating houses, carpooling, paying attention to energy efficiency labels on appliances, and many other simple practices and behaviors hugely influence energy consumption. A major education campaign, both in schools for youngsters and by the media for everyone, should be mounted to promote these consumer practices.</p>
<p>No part of EASY can be left out; all parts are closely integrated. Some parts might create much larger changes&#8212;for example, more efficient home appliances and automobiles&#8212;but all parts are essential. If, for example, we do not achieve the decrease in electricity demand that can be brought about with the E of EASY, then it is extremely doubtful that we could meet our electricity needs with the S of EASY.</p>
<p>It is equally urgent that once we start implementing the plan, we aggressively export it to other major emitting nations. We can reduce our own emissions all we want, but the planet will continue to warm if we can’t convince other major global emitters to reduce their emissions substantially, too.</p>
<p><b>What EASY will achieve.</b> If no actions are taken to reduce carbon dioxide emissions, in the year 2030 the U.S. will be emitting about 2.2 billion tons of carbon in the form of carbon dioxide. This will be an increase of 25% from today’s emission rate of about 1.75 billion tons per year of carbon. By following the EASY plan, the U.S. share in a global effort to solve the climate crisis (that is, prevent catastrophic warming) will result in U.S emissions of only about 0.4 billion tons of carbon by 2030, which represents a little less than 25% of 2007 carbon dioxide emissions.128 Stated differently, the plan provides a way to eliminate 1.8 billion tons per year of carbon by that date.</p>
<p>We must act urgently: in the 14 months it took us to write this book, atmospheric CO<sub>2</sub> levels rose by several billion tons of carbon, and more climatic consequences have been observed. Let’s assume that we conserve our forests and other natural carbon reservoirs at our current levels, as well as maintain our current nuclear and hydroelectric plants (or replace them with more solar and wind generators). Here’s what implementing EASY will achieve, as illustrated by Figure 3.1 on the next page.
</p></blockquote>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/ecological/harte_EASY.jpg" />
</div>
<p>Please check out this book and help me figure out if the numbers add up!  I could also use help understanding <a href="http://socrates.berkeley.edu/~hartelab/Publications.html">his research</a>, for example:</p>
<p>&bull; John Harte, <i>Maximum Entropy and Ecology: A Theory of Abundance, Distribution, and Energetics</i>, Oxford University Press, Oxford, 2011.</p>
<p>The book is not free but the <a href="http://fds.oup.com/www.oup.com/pdf/13/9780199593422_chapter1.pdf">first chapter</a> is.</p>
<p>This paper looks really interesting too:</p>
<p>&bull; J. Harte, T. Zillio, E. Conlisk and A. B. Smith, Maximum entropy and the state-variable approach to macroecology, <i><a href="http://www.esajournals.org/doi/abs/10.1890/07-1369.1">Ecology</a></i> <b>89</b> (2008), 2700–-2711.  </p>
<p>Again, it&#8217;s not freely available&#8212;tut tut. <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/shame_on_you.gif" />  Ecologists should follow physicists and make their work free online; if you&#8217;re serious about saving the planet you should let everyone know what you&#8217;re doing!  However, the abstract is visible to all, and of course I can use my academic superpowers to get ahold of the paper for myself:</p>
<blockquote><p>
<b>Abstract</b>: The biodiversity scaling metrics widely studied in macroecology include the species-area relationship (SAR), the scale-dependent species-abundance distribution (SAD), the distribution of masses or metabolic energies of individuals within and across species, the abundance-energy or abundance-mass relationship across species, and the species-level occupancy distributions across space. We propose a theoretical framework for predicting the scaling forms of these and other metrics based on the state-variable concept and an analytical method derived from information theory. In statistical physics, a method of inference based on information entropy results in a complete macro-scale description of classical thermodynamic systems in terms of the state variables volume, temperature, and number of molecules. In analogy, we take the state variables of an ecosystem to be its total area, the total number of species within any specified taxonomic group in that area, the total number of individuals across those species, and the summed metabolic energy rate for all those individuals. In terms solely of ratios of those state variables, and without invoking any specific ecological mechanisms, we show that realistic functional forms for the macroecological metrics listed above are inferred based on information entropy. The Fisher log series SAD emerges naturally from the theory. The SAR is predicted to have negative curvature on a log-log plot, but as the ratio of the number of species to the number of individuals decreases, the SAR becomes better and better approximated by a power law, with the predicted slope z in the range of 0.14-0.20. Using the 3/4 power mass-metabolism scaling relation to relate energy requirements and measured body sizes, the Damuth scaling rule relating mass and abundance is also predicted by the theory. We argue that the predicted forms of the macroecological metrics are in reasonable agreement with the patterns observed from plant census data across habitats and spatial scales. While this is encouraging, given the absence of adjustable fitting parameters in the theory, we further argue that even small discrepancies between data and predictions can help identify ecological mechanisms that influence macroecological patterns.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/#comments">14 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/" rel="category tag">carbon emissions</a>, <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/economics/" rel="category tag">economics</a>, <a href="https://johncarlosbaez.wordpress.com/category/energy/" rel="category tag">energy</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/sustainability/" rel="category tag">sustainability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/" rel="bookmark" title="Permanent Link to John Harte">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-12363 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-12363">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/10/08/the-mathematical-origin-of-irreversibility/" rel="bookmark">The Mathematical Origin of&nbsp;Irreversibility</a></h2>
				<small>8 October, 2012</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://aei-mpg.academia.edu/MatteoSmerlak/Papers">Matteo Smerlak</a></b></i></p>
<h3> Introduction </h3>
<p>Thermodynamical dissipation and adaptive evolution are two faces of the same Markovian coin!</p>
<p>Consider this. The <a href="http://en.wikipedia.org/wiki/Second_law_of_thermodynamics">Second Law of Thermodynamics</a> states that the entropy of an isolated thermodynamic system can never decrease; <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">Landauer&#8217;s principle</a> maintains that the erasure of information inevitably causes dissipation; <a href="http://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection">Fisher&#8217;s fundamental theorem of natural selection</a> asserts that any fitness difference within a population leads to adaptation in an evolution process governed by natural selection. Diverse as they are, these statements have two common characteristics: </p>
<p>1. they express the <i>irreversibility</i> of certain natural phenomena, and </p>
<p>2. the dynamical processes underlying these phenomena involve an element of <i>randomness</i>. </p>
<p>Doesn&#8217;t this suggest to you the following question: Could it be that thermal phenomena, forgetful information processing and adaptive evolution are governed by <i>the same stochastic mechanism?</i> </p>
<p>The answer is—yes! The key to this rather profound connection resides in a universal property of <a href="http://en.wikipedia.org/wiki/Markov_process">Markov processes</a> discovered recently in the context of non-equilibrium statistical mechanics, and known as the <a href="http://en.wikipedia.org/wiki/Fluctuation_theorem">&#8216;fluctuation theorem&#8217;</a>. Typically stated in terms of &#8216;dissipated work&#8217; or &#8216;entropy production&#8217;, this result can be seen as an extension of the Second Law of Thermodynamics to <i>small</i> systems, where thermal fluctuations cannot be neglected. But <i>it is actually much more than this</i>: it is the mathematical underpinning of irreversibility itself, be it thermodynamical, evolutionary, or else. To make this point clear, let me start by giving a general formulation of the fluctuation theorem that makes no reference to physics concepts such as &#8216;heat&#8217; or &#8216;work&#8217;.</p>
<h3> The mathematical fact </h3>
<p>Consider a system randomly jumping between states <img src="https://s0.wp.com/latex.php?latex=a%2C+b%2C%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a, b,&#92;dots" class="latex" /> with (possibly time-dependent) transition rates <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Ba+b%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{a b}(t)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> is the state prior to the jump, while <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b" class="latex" /> is the state after the jump. I&#8217;ll assume that this dynamics defines a (continuous-time) Markov process, namely that the numbers <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Ba+b%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{a b}" class="latex" /> are the matrix entries of an <a href="http://math.ucr.edu/home/baez/networks/networks_20.html">infinitesimal stochastic</a> matrix, which means that its off-diagonal entries are non-negative and that its columns sum up to zero. </p>
<p>Now, each possible history <img src="https://s0.wp.com/latex.php?latex=%5Comega%3D%28%5Comega_t%29_%7B0%5Cleq+t%5Cleq+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega=(&#92;omega_t)_{0&#92;leq t&#92;leq T}" class="latex" /> of this process can be characterized by the sequence of occupied states <img src="https://s0.wp.com/latex.php?latex=a_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j}" class="latex" /> and by the times <img src="https://s0.wp.com/latex.php?latex=%5Ctau_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau_{j}" class="latex" /> at which the transitions <img src="https://s0.wp.com/latex.php?latex=a_%7Bj-1%7D%5Clongrightarrow+a_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j-1}&#92;longrightarrow a_{j}" class="latex" /> occur <img src="https://s0.wp.com/latex.php?latex=%280%5Cleq+j%5Cleq+N%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0&#92;leq j&#92;leq N)" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega%3D%28%5Comega_%7B0%7D%3Da_%7B0%7D%5Coverset%7B%5Ctau_%7B0%7D%7D%7B%5Clongrightarrow%7D+a_%7B1%7D+%5Coverset%7B%5Ctau_%7B1%7D%7D%7B%5Clongrightarrow%7D%5Ccdots+%5Coverset%7B%5Ctau_%7BN%7D%7D%7B%5Clongrightarrow%7D+a_%7BN%7D%3D%5Comega_%7BT%7D%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega=(&#92;omega_{0}=a_{0}&#92;overset{&#92;tau_{0}}{&#92;longrightarrow} a_{1} &#92;overset{&#92;tau_{1}}{&#92;longrightarrow}&#92;cdots &#92;overset{&#92;tau_{N}}{&#92;longrightarrow} a_{N}=&#92;omega_{T})." class="latex" /></p>
<p>Define the <b>skewness</b> <img src="https://s0.wp.com/latex.php?latex=%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_{j}(&#92;tau_{j})" class="latex" /> of each of these transitions to be the logarithmic ratio of transition rates:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29%3A%3D%5Cln%5Cfrac%7B%5Cgamma_%7Ba_%7Bj%7Da_%7Bj-1%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7B%5Cgamma_%7Ba_%7Bj-1%7Da_%7Bj%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;sigma_{j}(&#92;tau_{j}):=&#92;ln&#92;frac{&#92;gamma_{a_{j}a_{j-1}}(&#92;tau_{j})}{&#92;gamma_{a_{j-1}a_{j}}(&#92;tau_{j})}}" class="latex" /></p>
<p>Also define the <a href="http://en.wikipedia.org/wiki/Self-information"><b>self-information</b></a> of the system in state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=i_a%28t%29%3A%3D+-%5Cln%5Cpi_%7Ba%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i_a(t):= -&#92;ln&#92;pi_{a}(t)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Ba%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_{a}(t)" class="latex" /> is the probability that the system is in state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />, given some prescribed initial distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Ba%7D%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_{a}(0)" class="latex" />.  This quantity is also sometimes called the <b>surprisal</b>, as it measures the &#8216;surprise&#8217; of finding out that the system is in state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.</p>
<p>Then the following identity&#8212;the <b>detailed fluctuation theorem</b>&#8212;holds:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BProb%7D%5B%5CDelta+i-%5CSigma%3D-A%5D+%3D+e%5E%7B-A%7D%5C%3B%5Cmathrm%7BProb%7D%5B%5CDelta+i-%5CSigma%3DA%5D+%5C%3B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Prob}[&#92;Delta i-&#92;Sigma=-A] = e^{-A}&#92;;&#92;mathrm{Prob}[&#92;Delta i-&#92;Sigma=A] &#92;;" class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5CSigma%3A%3D%5Csum_%7Bj%7D%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;Sigma:=&#92;sum_{j}&#92;sigma_{j}(&#92;tau_{j})}" class="latex" /></p>
<p>is the <b>cumulative skewness</b> along a trajectory of the system, and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+i%3D+i_%7Ba_N%7D%28T%29-i_%7Ba_0%7D%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta i= i_{a_N}(T)-i_{a_0}(0)" class="latex" /> </p>
<p>is the <b>variation of self-information</b> between the end points of this trajectory.  </p>
<p>This identity has an immediate consequence: if <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5C%2C%5Ccdot%5C%2C%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle&#92;,&#92;cdot&#92;,&#92;rangle" class="latex" /> denotes the average over all realizations of the process, then we have the <b>integral fluctuation theorem</b>: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+e%5E%7B-%5CDelta+i%2B%5CSigma%7D%5Crangle%3D1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle e^{-&#92;Delta i+&#92;Sigma}&#92;rangle=1," class="latex" /></p>
<p>which, by the convexity of the exponential and <a href="http://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen&#8217;s inequality</a>, implies:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5CDelta+i%5Crangle%3D%5CDelta+S%5Cgeq%5Clangle%5CSigma%5Crangle.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;Delta i&#92;rangle=&#92;Delta S&#92;geq&#92;langle&#92;Sigma&#92;rangle." class="latex" /></p>
<p>In short: <i>the mean variation of self-information, aka the variation of Shannon entropy</i> </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28t%29%3A%3D+%5Csum_%7Ba%7D%5Cpi_%7Ba%7D%28t%29i_a%28t%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(t):= &#92;sum_{a}&#92;pi_{a}(t)i_a(t) }" class="latex" /></p>
<p><i>is bounded from below by the mean cumulative skewness of the underlying stochastic trajectory.</i>  </p>
<p>This is the fundamental mathematical fact underlying irreversibility. To unravel its physical and biological consequences, it suffices to consider the origin and interpretation of the &#8216;skewness&#8217; term in different contexts. (By the way, people usually call <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> the &#8216;entropy production&#8217; or &#8216;dissipation function&#8217;&#8212;but how tautological is that?)</p>
<h3> The physical and biological consequences </h3>
<p>Consider first the standard stochastic-thermodynamic scenario where a physical system is kept in contact with a thermal reservoir at inverse temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> and undergoes thermally induced transitions between states <img src="https://s0.wp.com/latex.php?latex=a%2C+b%2C%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a, b,&#92;dots" class="latex" />. By virtue of the <a href="http://en.wikipedia.org/wiki/Detailed_balance"><b>detailed balance condition</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+e%5E%7B-%5Cbeta+E_%7Ba%7D%28t%29%7D%5Cgamma_%7Ba+b%7D%28t%29%3De%5E%7B-%5Cbeta+E_%7Bb%7D%28t%29%7D%5Cgamma_%7Bb+a%7D%28t%29%2C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ e^{-&#92;beta E_{a}(t)}&#92;gamma_{a b}(t)=e^{-&#92;beta E_{b}(t)}&#92;gamma_{b a}(t),}" class="latex" /></p>
<p>the skewness <img src="https://s0.wp.com/latex.php?latex=%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_{j}(&#92;tau_{j})" class="latex" /> of each such transition is <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta " class="latex" /> times the energy difference between the states <img src="https://s0.wp.com/latex.php?latex=a_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=a_%7Bj-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j-1}" class="latex" />, namely the <i>heat</i> received from the reservoir during the transition. Hence, the mean cumulative skewness <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5CSigma%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;Sigma&#92;rangle" class="latex" /> is nothing but <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Clangle+Q%5Crangle%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta&#92;langle Q&#92;rangle," class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> the total heat received by the system along the process. It follows from the detailed fluctuation theorem that </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+e%5E%7B-%5CDelta+i%2B%5Cbeta+Q%7D%5Crangle%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle e^{-&#92;Delta i+&#92;beta Q}&#92;rangle=1" class="latex" /></p>
<p>and therefore </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+S%5Cgeq%5Cbeta%5Clangle+Q%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta S&#92;geq&#92;beta&#92;langle Q&#92;rangle" class="latex" /> </p>
<p>which is of course <a href="http://en.wikipedia.org/wiki/Clausius_theorem">Clausius&#8217; inequality</a>. In a computational context where the control parameter is the entropy variation itself (such as in a bit-erasure protocol, where <img src="https://s0.wp.com/latex.php?latex=%5CDelta+S%3D-%5Cln+2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta S=-&#92;ln 2" class="latex" />), this inequality in turn expresses Landauer&#8217;s principle: it impossible to decrease the self-information of the system&#8217;s state without dissipating a minimal amount of heat into the environment (in this case <img src="https://s0.wp.com/latex.php?latex=-Q+%5Cgeq+k+T%5Cln2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-Q &#92;geq k T&#92;ln2" class="latex" />, the <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">&#8216;Landauer bound&#8217;</a>). More general situations (several types of reservoirs, <a href="http://en.wikipedia.org/wiki/Maxwell_demon">Maxwell-demon</a>-like feedback controls) can be treated along the same lines, and the various forms of the Second Law derived from the detailed fluctuation theorem. </p>
<p>Now, many would agree that evolutionary dynamics is a wholly different business from thermodynamics; in particular, notions such as &#8216;heat&#8217; or &#8216;temperature&#8217; are clearly irrelevant to Darwinian evolution. However, the stochastic framework of Markov processes <i>is</i> relevant to describe the genetic evolution of a population, and this fact alone has important consequences. As a simple example, consider the time evolution of mutant fixations <img src="https://s0.wp.com/latex.php?latex=x_%7Ba%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_{a}" class="latex" /> in a population, with <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> ranging over the possible genotypes. In a &#8216;symmetric mutation scheme&#8217;, which I understand is biological parlance for &#8216;reversible Markov process&#8217;, meaning one that obeys <a href="http://en.wikipedia.org/wiki/Detailed_balance">detailed balance</a>, the ratio between the <img src="https://s0.wp.com/latex.php?latex=a%5Cmapsto+b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a&#92;mapsto b" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=b%5Cmapsto+a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b&#92;mapsto a" class="latex" /> transition rates is completely determined by the <a href="http://en.wikipedia.org/wiki/Fitness_landscape"><b>fitnesses</b></a> <img src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{a}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f_b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_b" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b" class="latex" />, according to </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cgamma_%7Ba+b%7D%7D%7B%5Cgamma_%7Bb+a%7D%7D+%3D%5Cleft%28%5Cfrac%7Bf_%7Bb%7D%7D%7Bf_%7Ba%7D%7D%5Cright%29%5E%7B%5Cnu%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;gamma_{a b}}{&#92;gamma_{b a}} =&#92;left(&#92;frac{f_{b}}{f_{a}}&#92;right)^{&#92;nu} }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" /> is a model-dependent function of the effective population size [Sella2005]. Along a given history of mutant fixations, the cumulated skewness <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> is therefore given by minus the <b>fitness flux</b>: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5CPhi%3D%5Cnu%5Csum_%7Bj%7D%28%5Cln+f_%7Ba_j%7D-%5Cln+f_%7Ba_%7Bj-1%7D%7D%29.%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;Phi=&#92;nu&#92;sum_{j}(&#92;ln f_{a_j}-&#92;ln f_{a_{j-1}}).}" class="latex" /></p>
<p>The integral fluctuation theorem then becomes the <b>fitness flux theorem</b>: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+e%5E%7B-%5CDelta+i+-%5CPhi%7D%5Crangle%3D1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle e^{-&#92;Delta i -&#92;Phi}&#92;rangle=1}" class="latex" /></p>
<p>discussed recently by Mustonen and L&auml;ssig [Mustonen2010] and implying Fisher&#8217;s fundamental theorem of natural selection as a special case. (Incidentally, the &#8216;fitness flux theorem&#8217; derived in this reference is more general than this; for instance, it does not rely on the &#8216;symmetric mutation scheme&#8217; assumption above.) The ensuing inequality </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5CPhi%5Crangle%5Cgeq-%5CDelta+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;Phi&#92;rangle&#92;geq-&#92;Delta S " class="latex" /> </p>
<p>shows that a positive fitness flux is &#8220;an almost universal evolutionary principle of biological systems&#8221; [Mustonen2010], with negative contributions limited to time intervals with a systematic loss of adaptation (<img src="https://s0.wp.com/latex.php?latex=%5CDelta+S+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta S &gt; 0" class="latex" />). This statement may well be the closest thing to a version of the Second Law of Thermodynamics applying to evolutionary dynamics. </p>
<p>It is really quite remarkable that thermodynamical dissipation and Darwinian evolution can be reduced to the same stochastic mechanism, and that notions such as &#8216;fitness flux&#8217; and &#8216;heat&#8217; can arise as two faces of the same mathematical coin, namely the &#8216;skewness&#8217; of Markovian transitions. After all, the phenomenon of life is in itself a direct challenge to thermodynamics, isn&#8217;t it? When thermal phenomena tend to increase the world&#8217;s disorder, life strives to bring about and maintain exquisitely fine spatial and chemical structures&#8212;which is why Schr&ouml;dinger famously proposed to <i>define</i> life as <i>negative entropy</i>. Could there be a more striking confirmation of his intuition&#8212;and a reconciliation of evolution and thermodynamics in the same go&#8212;than the fundamental inequality of adaptive evolution <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5CPhi%5Crangle%5Cgeq-%5CDelta+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle&#92;Phi&#92;rangle&#92;geq-&#92;Delta S" class="latex" />?</p>
<p>Surely the detailed fluctuation theorem for Markov processes has other applications, pertaining neither to thermodynamics nor adaptive evolution. Can you think of any?</p>
<h3> Proof of the fluctuation theorem </h3>
<p>I am a physicist, but knowing that many readers of John&#8217;s blog are mathematicians, I&#8217;ll do my best to frame&#8212;and prove&#8212;the FT as an actual theorem. </p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C%5Cmathcal%7BT%7D%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega,&#92;mathcal{T},p)" class="latex" /> be a probability space and <img src="https://s0.wp.com/latex.php?latex=%28%5C%2C%5Ccdot%5C%2C%29%5E%7B%5Cdagger%7D%3D%5COmega%5Cto+%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;,&#92;cdot&#92;,)^{&#92;dagger}=&#92;Omega&#92;to &#92;Omega" class="latex" /> a measurable involution of <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />. Denote <img src="https://s0.wp.com/latex.php?latex=p%5E%7B%5Cdagger%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^{&#92;dagger}" class="latex" /> the pushforward probability measure through this involution, and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+R%3D%5Cln+%5Cfrac%7Bd+p%7D%7Bd+p%5E%5Cdagger%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ R=&#92;ln &#92;frac{d p}{d p^&#92;dagger} }" class="latex" /></p>
<p>the logarithm of the corresponding Radon-Nikodym derivative (we assume <img src="https://s0.wp.com/latex.php?latex=p%5E%5Cdagger&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^&#92;dagger" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> are mutually absolutely continuous). Then the following lemmas are true, with <img src="https://s0.wp.com/latex.php?latex=%281%29%5CRightarrow%282%29%5CRightarrow%283%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1)&#92;Rightarrow(2)&#92;Rightarrow(3)" class="latex" />:</p>
<p><b>Lemma 1.</b> The detailed fluctuation relation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cforall+A%5Cin%5Cmathbb%7BR%7D+%5Cquad++p%5Cbig%28R%5E%7B-1%7D%28-A%29+%5Cbig%29%3De%5E%7B-A%7Dp+%5Cbig%28R%5E%7B-1%7D%28A%29+%5Cbig%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;forall A&#92;in&#92;mathbb{R} &#92;quad  p&#92;big(R^{-1}(-A) &#92;big)=e^{-A}p &#92;big(R^{-1}(A) &#92;big)" class="latex" /></p>
<p><b>Lemma 2.</b>  The integral fluctuation relation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2Ce%5E%7B-R%28%5Comega%29%7D%3D1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,e^{-R(&#92;omega)}=1 }" class="latex" /></p>
<p><b>Lemma 3.</b>  The positivity of the Kullback-Leibler divergence:</p>
<p><img src="https://s0.wp.com/latex.php?latex=D%28p%5C%2C%5CVert%5C%2C+p%5E%7B%5Cdagger%7D%29%3A%3D%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2CR%28%5Comega%29%5Cgeq+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D(p&#92;,&#92;Vert&#92;, p^{&#92;dagger}):=&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,R(&#92;omega)&#92;geq 0." class="latex" /></p>
<p>These are basic facts which anyone can show: <img src="https://s0.wp.com/latex.php?latex=%282%29%5CRightarrow%283%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(2)&#92;Rightarrow(3)" class="latex" /> by Jensen&#8217;s inequality, <img src="https://s0.wp.com/latex.php?latex=%281%29%5CRightarrow%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1)&#92;Rightarrow(2)" class="latex" /> trivially, and <img src="https://s0.wp.com/latex.php?latex=%281%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1)" class="latex" /> follows from <img src="https://s0.wp.com/latex.php?latex=R%28%5Comega%5E%7B%5Cdagger%7D%29%3D-R%28%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(&#92;omega^{&#92;dagger})=-R(&#92;omega)" class="latex" /> and the change of variables theorem, as follows,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Cint_%7BR%5E%7B-1%7D%28-A%29%7D+d+p%28%5Comega%29%7D+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_%7BR%5E%7B-1%7D%28A%29%7Dd+p%5E%7B%5Cdagger%7D%28%5Comega%29+%7D+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_%7BR%5E%7B-1%7D%28A%29%7D+d+p%28%5Comega%29%5C%2C+e%5E%7B-R%28%5Comega%29%7D+%7D+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+e%5E%7B-A%7D+%5Cint_%7BR%5E%7B-1%7D%28A%29%7D+d+p%28%5Comega%29%7D+.%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;int_{R^{-1}(-A)} d p(&#92;omega)} &amp;=&amp; &#92;displaystyle{ &#92;int_{R^{-1}(A)}d p^{&#92;dagger}(&#92;omega) } &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ &#92;int_{R^{-1}(A)} d p(&#92;omega)&#92;, e^{-R(&#92;omega)} } &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ e^{-A} &#92;int_{R^{-1}(A)} d p(&#92;omega)} .&#92;end{array}" class="latex" /></p>
<p>But here is the beauty: if </p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C%5Cmathcal%7BT%7D%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega,&#92;mathcal{T},p)" class="latex" /> is actually a Markov process defined over some time interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2CT%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,T]" class="latex" /> and valued in some (say discrete) state space <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" />, with the instantaneous probability <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Ba%7D%28t%29%3Dp%5Cbig%28%5C%7B%5Comega_%7Bt%7D%3Da%5C%7D+%5Cbig%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_{a}(t)=p&#92;big(&#92;{&#92;omega_{t}=a&#92;} &#92;big)" class="latex" /> of each state <img src="https://s0.wp.com/latex.php?latex=a%5Cin%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a&#92;in&#92;Sigma" class="latex" /> satisfying the <b>master equation</b> (aka Kolmogorov equation)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%5Cpi_%7Ba%7D%28t%29%7D%7Bdt%7D%3D%5Csum_%7Bb%5Cneq+a%7D%5CBig%28%5Cgamma_%7Bb+a%7D%28t%29%5Cpi_%7Ba%7D%28t%29-%5Cgamma_%7Ba+b%7D%28t%29%5Cpi_%7Bb%7D%28t%29%5CBig%29%2C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d&#92;pi_{a}(t)}{dt}=&#92;sum_{b&#92;neq a}&#92;Big(&#92;gamma_{b a}(t)&#92;pi_{a}(t)-&#92;gamma_{a b}(t)&#92;pi_{b}(t)&#92;Big),} " class="latex" /></p>
<p>and</p>
<p>&bull;  the dagger involution is time-reversal, that is <img src="https://s0.wp.com/latex.php?latex=%5Comega%5E%7B%5Cdagger%7D_%7Bt%7D%3A%3D%5Comega_%7BT-t%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega^{&#92;dagger}_{t}:=&#92;omega_{T-t}," class="latex" /></p>
<p>then for a given path</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Comega%3D%28%5Comega_%7B0%7D%3Da_%7B0%7D%5Coverset%7B%5Ctau_%7B0%7D%7D%7B%5Clongrightarrow%7D+a_%7B1%7D+%5Coverset%7B%5Ctau_%7B1%7D%7D%7B%5Clongrightarrow%7D%5Ccdots+%5Coverset%7B%5Ctau_%7BN%7D%7D%7B%5Clongrightarrow%7D+a_%7BN%7D%3D%5Comega_%7BT%7D%29%5Cin%5COmega%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;omega=(&#92;omega_{0}=a_{0}&#92;overset{&#92;tau_{0}}{&#92;longrightarrow} a_{1} &#92;overset{&#92;tau_{1}}{&#92;longrightarrow}&#92;cdots &#92;overset{&#92;tau_{N}}{&#92;longrightarrow} a_{N}=&#92;omega_{T})&#92;in&#92;Omega}" class="latex" /></p>
<p>the logarithmic ratio <img src="https://s0.wp.com/latex.php?latex=R%28%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(&#92;omega)" class="latex" /> decomposes into &#8216;variation of self-information&#8217; and &#8216;cumulative skewness&#8217; along <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+R%28%5Comega%29%3D%5Cunderbrace%7B%5CBig%28%5Cln%5Cpi_%7Ba_0%7D%280%29-%5Cln%5Cpi_%7Ba_N%7D%28T%29+%5CBig%29%7D_%7B%5CDelta+i%28%5Comega%29%7D-%5Cunderbrace%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7D%5Cln%5Cfrac%7B%5Cgamma_%7Ba_%7Bj%7Da_%7Bj-1%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7B%5Cgamma_%7Ba_%7Bj-1%7Da_%7Bj%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7D_%7B%5CSigma%28%5Comega%29%7D.%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ R(&#92;omega)=&#92;underbrace{&#92;Big(&#92;ln&#92;pi_{a_0}(0)-&#92;ln&#92;pi_{a_N}(T) &#92;Big)}_{&#92;Delta i(&#92;omega)}-&#92;underbrace{&#92;sum_{j=1}^{N}&#92;ln&#92;frac{&#92;gamma_{a_{j}a_{j-1}}(&#92;tau_{j})}{&#92;gamma_{a_{j-1}a_{j}}(&#92;tau_{j})}}_{&#92;Sigma(&#92;omega)}.}" class="latex" /></p>
<p>This is easy to see if one writes the probability of a path explicitly as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7Bp%28%5Comega%29%3D%5Cpi_%7Ba_%7B0%7D%7D%280%29%5Cleft%5B%5Cprod_%7Bj%3D1%7D%5E%7BN%7D%5Cphi_%7Ba_%7Bj-1%7D%7D%28%5Ctau_%7Bj-1%7D%2C%5Ctau_%7Bj%7D%29%5Cgamma_%7Ba_%7Bj-1%7Da_%7Bj%7D%7D%28%5Ctau_%7Bj%7D%29%5Cright%5D%5Cphi_%7Ba_%7BN%7D%7D%28%5Ctau_%7BN%7D%2CT%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{p(&#92;omega)=&#92;pi_{a_{0}}(0)&#92;left[&#92;prod_{j=1}^{N}&#92;phi_{a_{j-1}}(&#92;tau_{j-1},&#92;tau_{j})&#92;gamma_{a_{j-1}a_{j}}(&#92;tau_{j})&#92;right]&#92;phi_{a_{N}}(&#92;tau_{N},T)}" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cphi_%7Ba%7D%28%5Ctau%2C%5Ctau%27%29%3D%5Cphi_%7Ba%7D%28%5Ctau%27%2C%5Ctau%29%3D%5Cexp%5CBig%28-%5Csum_%7Bb%5Cneq+a%7D%5Cint_%7B%5Ctau%7D%5E%7B%5Ctau%27%7Ddt%5C%2C+%5Cgamma_%7Ba+b%7D%28t%29%5CBig%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;phi_{a}(&#92;tau,&#92;tau&#039;)=&#92;phi_{a}(&#92;tau&#039;,&#92;tau)=&#92;exp&#92;Big(-&#92;sum_{b&#92;neq a}&#92;int_{&#92;tau}^{&#92;tau&#039;}dt&#92;, &#92;gamma_{a b}(t)&#92;Big)}" class="latex" /></p>
<p>is the probability that the process remains in the state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> between the times <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctau%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau&#039;" class="latex" />. It follows from the above lemma that</p>
<p><b>Theorem.</b> Let <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C%5Cmathcal%7BT%7D%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega,&#92;mathcal{T},p)" class="latex" /> be a Markov process and let <img src="https://s0.wp.com/latex.php?latex=i%2C%5CSigma%3A%5COmega%5Crightarrow+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i,&#92;Sigma:&#92;Omega&#92;rightarrow &#92;mathbb{R}" class="latex" /> be defined as above. Then we have</p>
<p>1. The detailed fluctuation theorem:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cforall+A%5Cin%5Cmathbb%7BR%7D%2C+p%5Cbig%28%28%5CDelta+i-%5CSigma%29%5E%7B-1%7D%28-A%29+%5Cbig%29%3De%5E%7B-A%7Dp+%5Cbig%28%28%5CDelta+i-%5CSigma%29%5E%7B-1%7D%28A%29+%5Cbig%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;forall A&#92;in&#92;mathbb{R}, p&#92;big((&#92;Delta i-&#92;Sigma)^{-1}(-A) &#92;big)=e^{-A}p &#92;big((&#92;Delta i-&#92;Sigma)^{-1}(A) &#92;big)" class="latex" /></p>
<p>2.  The integral fluctuation theorem:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2Ce%5E%7B-%5CDelta+i%28%5Comega%29%2B%5CSigma%28%5Comega%29%7D%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,e^{-&#92;Delta i(&#92;omega)+&#92;Sigma(&#92;omega)}=1" class="latex" /></p>
<p>3.  The &#8216;Second Law&#8217; inequality:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CDelta+S%3A%3D%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2C%5CDelta+i%28%5Comega%29%5Cgeq+%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2C%5CSigma%28%5Comega%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Delta S:=&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,&#92;Delta i(&#92;omega)&#92;geq &#92;int_{&#92;Omega} d p(&#92;omega)&#92;,&#92;Sigma(&#92;omega)} " class="latex" /></p>
<p>The same theorem can be formulated for other kinds of Markov processes as well, including diffusion processes (in which case it follows from the <a href="http://en.wikipedia.org/wiki/Girsanov_theorem">Girsanov theorem</a>).</p>
<h3> References </h3>
<p>Landauer&#8217;s principle was introduced here:</p>
<p>&bull; [Landauer1961]  R. Landauer, Irreversibility and heat generation in the computing process}, <i>IBM Journal of Research and Development</i> <b>5</b>, (1961) 183&#8211;191.</p>
<p>and is now being verified experimentally by various groups worldwide.</p>
<p>The &#8216;fundamental theorem of natural selection&#8217; was derived by Fisher in his book:</p>
<p>&bull; [Fisher1930]  R. Fisher, <i>The Genetical Theory of Natural Selection</i>, Clarendon Press, Oxford, 1930.</p>
<p>His derivation has long been considered obscure, even perhaps wrong, but apparently the theorem is now well accepted. I believe the first Markovian models of genetic evolution appeared here:</p>
<p>&bull; [Fisher1922]  R. A. Fisher, On the dominance ratio, <i>Proc. Roy. Soc. Edinb.</i> <b>42</b> (1922), 321&#8211;341.</p>
<p>&bull; [Wright1931]  S. Wright, Evolution in Mendelian populations, <i>Genetics</i> <b>16</b> (1931), 97&#8211;159.</p>
<p>Fluctuation theorems are reviewed here:</p>
<p>&bull; [Sevick2008]  E. Sevick, R. Prabhakar, S. R. Williams, and D. J. Searles, <a href="http://arxiv.org/abs/0709.3888">Fluctuation theorems</a>, <i>Ann. Rev. Phys. Chem.</i> <b>59</b> (2008), 603&#8211;633.</p>
<p>Two of the key ideas for the &#8216;detailed fluctuation theorem&#8217; discussed here are due to Crooks: </p>
<p>&bull; [Crooks1999]  Gavin Crooks, <a href="http://arxiv.org/abs/cond-mat/9901352">The entropy production fluctuation theorem and the nonequilibrium work relation for free energy differences</a>, <a href="http://dx.doi.org/10.1103/PhysRevE.60.2721"><i>Phys. Rev. E</i></a> <b>60</b> (1999), 2721&#8211;2726.</p>
<p>who identified <img src="https://s0.wp.com/latex.php?latex=%28E_%7Ba%7D%28%5Ctau_%7Bj%7D%29-E_%7Ba%7D%28%5Ctau_%7Bj-1%7D%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(E_{a}(&#92;tau_{j})-E_{a}(&#92;tau_{j-1}))" class="latex" /> as heat, and Seifert:</p>
<p>&bull; [Seifert2005]  Udo Seifert, <a href="http://arxiv.org/abs/cond-mat/0503686">Entropy production along a stochastic trajectory and an integral fluctuation theorem</a>, <a href="http://dx.doi.org/10.1103/PhysRevLett.95.040602"><i>Phys. Rev. Lett.</i></a> <b>95</b> (2005), 4.</p>
<p>who understood the relevance of the self-information in this context. </p>
<p>The connection between statistical physics and evolutionary biology is discussed here:</p>
<p>&bull; [Sella2005] G. Sella and A.E. Hirsh, <a href="http://www.pnas.org/content/102/27/9541.full.pdf+html">The application of statistical physics to evolutionary biology</a>, <a href="http://www.pnas.org/content/102/27/9541.short"><i>Proc. Nat. Acad. Sci. USA</i></a> <b>102</b> (2005), 9541&#8211;9546.</p>
<p>and the &#8216;fitness flux theorem&#8217; is derived in </p>
<p>&bull; [Mustonen2010]  V. Mustonen and M. L&auml;ssig, <a href="http://www.pnas.org/content/107/9/4248.full.pdf+html">Fitness flux and ubiquity of adaptive evolution</a>, <a href="http://www.pnas.org/content/107/9/4248.short"><i>Proc. Nat. Acad. Sci. USA</i></a> <b>107</b> (2010), 4248&#8211;4253.</p>
<p>Schr&ouml;dinger&#8217;s famous discussion of the physical nature of life was published here:</p>
<p>&bull; [Schr&ouml;dinger1944]  E. Schr&ouml;dinger, <i>What is Life?</i>, Cambridge University Press, Cambridge, 1944.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/08/the-mathematical-origin-of-irreversibility/#comments">57 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/08/the-mathematical-origin-of-irreversibility/" rel="bookmark" title="Permanent Link to The Mathematical Origin of&nbsp;Irreversibility">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10663 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy" id="post-10663">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/14/the-mathematics-of-biodiversity-part-8/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;8)</a></h2>
				<small>14 July, 2012</small><br />


				<div class="entry">
					<p>Last time I mentioned that estimating entropy from real-world data is important not just for measuring biodiversity, but also for another area of biology: <i>neurobiology!</i></p>
<p>When you look at something, neurons in your eye start firing.  But how, exactly, is their firing related to what you see?  Questions like this are hard!  Answering them&#8212; <a href="http://en.wikipedia.org/wiki/Neural_coding">&#8216;cracking the neural code&#8217;</a>&#8212;is a big challenge.  To make progress, neuroscientists are using information theory.    But as I explained <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/">last time</a>, estimating information from experimental data is tricky.</p>
<p><a href="http://www.anc.upmc.fr/rbrasselet.html">Romain Brasselet</a>, now a postdoc at the Max Planck Institute for Biological Cybernetics at Tübingen, is working on these topics.  He sent me a nice email explaining this area.</p>
<p>This is a bit of a digression, but the <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Research-Program-on-Mathematics-of-Biodiversity.aspx">Mathematics of Biodiversity program</a> in Barcelona has been extraordinarily multidisciplinary, with category theorists rubbing shoulders with ecologists, immunologists and geneticists.  One of the common themes is entropy and its role in biology, so I think it&#8217;s worth posting Romain&#8217;s comments here.  This is what he has to say&#8230;</p>
<h3> Information in neurobiology</h3>
<p>I will try to explain why neurobiologists are today very interested in reliable estimates of entropy/information and what are the techniques we use to obtain them.</p>
<p>The activity of sensory as well as more central neurons is known to be modulated by external stimulations. In 1926, in a seminal paper, Adrian observed that neurons in the sciatic nerve of the frog fire action potentials (or spikes) when some muscle in the hindlimb is stretched. In addition, he observed that the frequency of the spikes increases with the amplitude of the stretching.</p>
<p>• E.D. Adrian, <a href="http://jp.physoc.org/content/61/1/49.full.pdf"> The impulses produced by sensory nerve endings.</a> (1926).</p>
<p>For another very nice example, in 1962, Hubel and Wiesel found <a href="http://en.wikipedia.org/wiki/Simple_cell">neurons</a> in the cat visual cortex whose activity depends on the orientation of a visual stimulus, a simple black line over white background: some neurons fire preferentially for one orientation of the line (Hubel and Wiesel were awarded the 1981 Nobel Prize in Physiology for their work). This incidentally led to the concept of &#8220;receptive field&#8221; which is of tremendous importance in neurobiology&#8212;but though it&#8217;s fascinating, it&#8217;s a different topic.</p>
<p>Good, we are now able to define what makes a neuron tick. The problem is that neural activity is often very &#8220;noisy&#8221;: when the exact same stimulus is presented many times, the responses appear to be very different from trial to trial. Even careful observation cannot necessarily reveal correlations between the stimulations and the neural activity. So we would like a measure capable of capturing the statistical dependencies between the stimulation and the response of the neuron to know if we can say something about the stimulation just by observing the response of a neuron, which is essentially the task of the brain. In particular, we want a fundamental measure that does not rely on any assumption about the functioning of the brain. Information theory provides the tools to do this, that is why we like to use it: we often try to measure the mutual information between stimuli and responses.</p>
<p>To my knowledge, the first paper using information theory in neuroscience was by MacKay and McCulloch in 1952:</p>
<p>• Donald M. Mackay and Warren S. McCulloch, <a href="http://www.weizmann.ac.il/complex/tlusty/courses/InfoInBio/Papers/MacKayMcCulloch1952.pdf"> The limiting information capacity of a neuronal link</a>, <i>Bulletin of Mathematical Biophysics</i> <b>14</b> (1952), 127&#8211;135.</p>
<p>But information theory was not used in neuroscience much until the early 90&#8217;s. It started again with a paper by Bialek <i>et al.</i> in 1991:</p>
<p>• W. Bialek, F. Rieke, R. R. de Ruyter van Steveninck and D. Warland, Reading a neural code, <a href="http://www.sciencemag.org/content/252/5014/1854.short"> <i>Science</i></a> <b>252</b> (1991), 1854&#8211;1857.</p>
<p>However, when applying information-theoretic methods to biological data, we often have a limited sampling of the neural response, we are usually very happy when we have 50 trials for a given stimulus. Why is this limited sample a problem?</p>
<p>During the major part of the 20th century, following Adrian&#8217;s finding, the paradigm for the neural code was the frequency of the spikes or, equivalently, the number of spikes in a window of time. But in the early 90&#8217;s, it was observed that the exact timing of spikes is (in some cases) reliable across trials. So instead of considering the neural response as a single number (the number of spikes), the temporal patterns of spikes started to be taken into account.  But time is continuous, so to be able to do actual computations, time was discretized and a neural response became a binary string.</p>
<p>Now, if you consider relevant time-scales, say, a 100 millisecond time window with a 1 millisecond bin with a firing frequency of about 50 per second, then your response space is huge and the estimates of information with only 50 trials are not reliable anymore. That&#8217;s why a lot of efforts have been carried out to overcome the limited sampling bias.</p>
<p>Now, getting at the techniques developed in this field, John already mentioned the work by Liam Paninski, but here are other very interesting references:</p>
<p>• Stefano Panzeri and Alessandro Treves, <a href="http://digitallibrary.sissa.it/handle/1963/935"> Analytical estimates of limited sampling biases in different information measures</a>, <i><a href="http://www.ingentaconnect.com/content/apl/network/1996/00000007/00000001/art00006"> Network: Computation in Neural Systems</a></i> <b>7</b> (1996), 87&#8211;107.</p>
<p>They computed the first-order bias of the information (related to the <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/">Miller&#8211;Madow correction</a>) and then used a Bayesian technique to estimate the number of responses not included in the sample but that would be in an infinite sample (a goal similar to that of <a href="https://johncarlosbaez.wordpress.com/2012/06/21/the-mathematics-of-biodiversity-part-1/">Good&#8217;s rule of thumb</a>).</p>
<p>• S.P. Strong, R. Koberle, R.R. de Ruyter van Steveninck, and W. Bialek, Entropy and information in neural spike trains, <i><a href="http://prl.aps.org/abstract/PRL/v80/i1/p197_1"> Phys. Rev. Lett.</a></i> <b>80</b> (1998), 197&#8211;200.</p>
<p>The entropy (or if you prefer, information) estimate can be expanded in a power series in <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> (the sample size) around the true value. By computing the estimate for various values of <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> and fitting it with a parabola, it is possible to estimate the value of the entropy as <img src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N &#92;rightarrow &#92;infty." class="latex" /></p>
<p>These approaches are also well-known:</p>
<p>• Ilya Nemenman, Fariel Shafee and William Bialek, <a href="http://arxiv.org/abs/physics/0108025"> Entropy and inference, revisited</a>, 2002.</p>
<p>• Alexander Kraskov, Harald Stögbauer and Peter Grassberger, Estimating mutual information, <a href="http://pre.aps.org/abstract/PRE/v69/i6/e066138"> <i>Phys. Rev. E.</i> </a> <b>69</b> (2004), 066138.</p>
<p>Actually, Stefano Panzeri has quite a few impressive papers about this problem, and recently with colleagues he has made public a free Matlab toolbox for information theory (<a href="http://www.ibtb.org/">www.ibtb.org</a>) implementing various correction methods.</p>
<p>Finally, the work by Jonathan Victor is worth mentioning, since he provided (to my knowledge again) the first estimate of mutual information using geometry. This is of particular interest with respect to the work by <a href="http://www.maths.gla.ac.uk/~tl/mdiss.pdf">Christina Cobbold and Tom Leinster</a> on measures of biodiversity that take the distance between species into account:</p>
<p>• J. D. Victor and K. P. Purpura, Nature and precision of temporal coding in visual cortex: a metric-space analysis, <a href="http://jn.physiology.org/content/76/2/1310.short"><i>Journal of Neural Physiology</i></a> <b>76</b> (1996), 1310&#8211;1326.</p>
<p>He introduced a distance between sequences of spikes and from this, derived a lower bound on mutual information.</p>
<p>• Jonathan D. Victor, <a href="http://www-users.med.cornell.edu/~jdvicto/pdfs/vict03.pdf">Binless strategies for estimation of information from neural data</a>, <a href="http://pre.aps.org/abstract/PRE/v66/i5/e051903"><i>Phys. Rev. E.</i></a> <b>66</b> (2002), 051903.</p>
<p>Taking inspiration from work by Kozachenko and Leonenko, he obtained an estimate of the information based on the distances between the closest responses.</p>
<p>Without getting too technical, that&#8217;s what we do in neuroscience about the limited sampling bias. The incentive is that obtaining reliable estimates is crucial to understand the <a href="http://en.wikipedia.org/wiki/Neural_coding">&#8216;neural code&#8217;</a>, the holy grail of computational neuroscientists.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/14/the-mathematics-of-biodiversity-part-8/#comments">9 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/14/the-mathematics-of-biodiversity-part-8/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;8)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10580 post type-post status-publish format-standard hentry category-biodiversity category-information-and-entropy category-probability" id="post-10580">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;7)</a></h2>
				<small>12 July, 2012</small><br />


				<div class="entry">
					<p>How ignorant are you?  </p>
<p>Do you know?  </p>
<p><i>Do you know how much don&#8217;t you know?</i></p>
<p>It seems hard to accurately estimate your lack of knowledge.  It even seems hard to say precisely <i>how hard</i> it is.  But the cool thing is, we can actually extract an interesting math question from this problem.  And one answer to this question leads to the following conclusion:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate how ignorant you are.<br />
</b></p></blockquote>
<p>But the devil is in the details.  So let&#8217;s see the details!</p>
<p>The <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a> of a probability distribution is a way of measuring how ignorant we are when this probability distribution describes our knowledge. </p>
<p>For example, suppose all we care about is whether this ancient Roman coin will land heads up or tails up:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/File:PupienusSest.jpg"><img src="http://upload.wikimedia.org/wikipedia/commons/6/63/PupienusSest.jpg" /></a></div>
<p>If we know there&#8217;s a 50% chance of it landing heads up, that&#8217;s a Shannon entropy of 1 bit: we&#8217;re missing one bit of information.  </p>
<p>But suppose for some reason we know for sure it&#8217;s going to land heads up.  For example, suppose we know the guy on this coin is the emperor Pupienus Maximus, a egomaniac who had lead put on the back of all coins bearing his likeness, so his face would never hit the dirt!  Then the Shannon entropy is 0: we know what&#8217;s going to happen when we toss this coin.</p>
<p>Or suppose we know there&#8217;s a 90% it will land heads up, and a 10% chance it lands tails up.  Then the Shannon entropy is somewhere in between.  We can calculate it like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+0.9+%5Clog_2+%280.9%29+-+0.1+%5Clog_2+%280.1%29+%3D+0.46899...+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- 0.9 &#92;log_2 (0.9) - 0.1 &#92;log_2 (0.1) = 0.46899... " class="latex" /></p>
<p>so that&#8217;s how many bits of information we&#8217;re missing.</p>
<p>But now suppose we have no idea.  Suppose we just start flipping the coin over and over, and seeing what happens.  Can we <i>estimate</i> the Shannon entropy?</p>
<p>Here&#8217;s a naive way to do it.  First, use your experimental data to estimate the probability that that the coin lands heads-up.  Then, stick that probability into the formula for Shannon entropy.  For example, say we flip the coin 3 times and it lands head-up once.  Then we can <i>estimate</i> the probability of it landing heads-up as 1/3, and tails-up as 2/3.  So we can estimate that the Shannon entropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Cfrac%7B1%7D%7B3%7D+%5Clog_2+%28%5Cfrac%7B1%7D%7B3%7D%29+-%5Cfrac%7B2%7D%7B3%7D+%5Clog_2+%28%5Cfrac%7B2%7D%7B3%7D%29+%3D+0.918...+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;frac{1}{3} &#92;log_2 (&#92;frac{1}{3}) -&#92;frac{2}{3} &#92;log_2 (&#92;frac{2}{3}) = 0.918... } " class="latex" /></p>
<p>But it turns out that this approach systematically <i>underestimates</i> the Shannon entropy!  </p>
<p>Say we have a coin that lands up a certain fraction of the time, say <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />   And say we play this game: we flip our coin <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> times, see what we get, and estimate the Shannon entropy using the simple recipe I just illustrated.  </p>
<p>Of course, our estimate will depend on the luck of the game.  But on average, it will be <i>less</i> than the <i>actual</i> Shannon entropy, which is </p>
<p><img src="https://s0.wp.com/latex.php?latex=-+p+%5Clog_2+%28p%29+-+%281-p%29+%5Clog_2+%281-p%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- p &#92;log_2 (p) - (1-p) &#92;log_2 (1-p)  " class="latex" /></p>
<p>We can prove this mathematically.  But it shouldn&#8217;t be surprising.  After all, if <img src="https://s0.wp.com/latex.php?latex=n+%3D+1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 1," class="latex" /> we&#8217;re playing a game where we flip the coin just <i>once</i>.  And with this game, our naive estimate of the Shannon entropy will always be <i>zero!</i>  Each time we play the game, the coin will either land heads up 100% of the time, or tails up 100% of the time!  </p>
<p>If we play the game with more coin flips, the error gets less severe.  In fact it approaches zero as the number of coin flips gets ever larger, so that <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty." class="latex" />  The case where you flip the coin just once is an extreme case&#8212;but extreme cases can be good to think about, because they can indicate what may happen in less extreme cases.</p>
<p>One moral here is that naively generalizing on the basis of limited data can make you feel more sure you know what&#8217;s going on than you actually are.  </p>
<p>I hope you knew <i>that</i> already!</p>
<p>But we can also say, in a more technical way, that the naive way of estimating Shannon entropy is a <a href="http://en.wikipedia.org/wiki/Bias_of_an_estimator"><b>biased estimator</b></a>: the average value of the estimator is different from the value of the quantity being estimated.   </p>
<p>Here&#8217;s an example of an unbiased estimator.  Say you&#8217;re trying to estimate the probability that the coin will land heads up.  You flip it <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> times and see that it lands up <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m" class="latex" /> times.  You estimate that the probability is <img src="https://s0.wp.com/latex.php?latex=m%2Fn.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m/n." class="latex" />  That&#8217;s the obvious thing to do, and it turns out to be unbiased.  </p>
<p>Statisticians like to think about <a href="http://en.wikipedia.org/wiki/Estimator">estimators</a>, and being unbiased is one way an estimator can be &#8216;good&#8217;.  Beware: it&#8217;s not the only way!  There are estimators that are unbiased, but whose standard deviation is so huge that they&#8217;re almost useless.  It can be better to have an estimate of something that&#8217;s more accurate, even though on average it&#8217;s a bit too low.  So sometimes, a biased estimator can be more useful than an unbiased estimator.  </p>
<p>Nonetheless, my ears perked up when Lou Jost mentioned that there is no unbiased estimator for Shannon entropy.  In rough terms, the moral is that:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate how ignorant you are.</b>
</p></blockquote>
<p>I think this is important.  For example, it&#8217;s important because Shannon entropy is also used as a measure of <i>biodiversity</i>.  Instead of flipping a coin repeatedly and seeing which side lands up, now we go out and collect plants or animals, and see which species we find.  The relative abundance of different species defines a  probability distribution on the set of species.  In this language, the moral is:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate biodiversity.<br />
</b></p></blockquote>
<p>But of course, this doesn&#8217;t mean we should give up.  We may just have to settle for an estimator that&#8217;s a bit biased!  And people have spent a bunch of time looking for estimators that are less biased than the naive one I just described.  </p>
<p>By the way, equating &#8216;biodiversity&#8217; with &#8216;Shannon entropy&#8217; is sloppy: there are <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">many measures of biodiversity</a>.  The Shannon entropy is just a special case of the <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a>, which depends on a parameter <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />: we get Shannon entropy when <img src="https://s0.wp.com/latex.php?latex=q+%3D+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 1." class="latex" />  </p>
<p>As <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gets smaller, the R&eacute;nyi entropy gets more and more sensitive to rare species&#8212;or shifting back to the language of probability theory, rare events.  It&#8217;s the rare events that make Shannon entropy hard to estimate, so I imagine there should be theorems about estimators for R&eacute;nyi entropy, which say it gets harder to estimate as <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gets smaller.  Do you know such theorems?</p>
<p>Also, I should add that biodiversity is better captured by the &#8216;Hill numbers&#8217;, which are functions of the R&eacute;nyi entropy, than by the R&eacute;nyi entropy itself.  (See <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">here</a> for the formulas.)  Since these functions are nonlinear, the lack of an unbiased estimator for R&eacute;nyi entropy doesn&#8217;t instantly imply the same for the Hill numbers.  So there are also some obvious questions about unbiased estimators for Hill numbers.  Do you know answers to those?</p>
<p>Here are some papers on estimators for entropy.  Most of these focus on estimating the Shannon entropy of a probability distribution on a finite set.  </p>
<p>This old classic has a proof that the &#8216;naive&#8217; estimator of Shannon entropy is biased, and estimates on the bias:</p>
<p>&bull; Bernard Harris, <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a020217.pdf">The statistical estimation of entropy in the non-parametric case</a>, Army Research Office, 1975.</p>
<p>He shows the bias goes to zero as we increase the number of samples: the number I was calling <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> in my coin flip example.  In fact he shows the bias goes to zero like <img src="https://s0.wp.com/latex.php?latex=O%281%2Fn%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(1/n)." class="latex" />  This is big <a href="http://en.wikipedia.org/wiki/Big_O_notation">big O notation</a> which means that as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%2B%5Cinfty%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to +&#92;infty," class="latex" /> the bias is bounded by some constant times <img src="https://s0.wp.com/latex.php?latex=1%2Fn.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/n." class="latex" /> This constant depends on the size of our finite set&#8212;or, if you want to do better, the <b>class number</b>, which is the number of elements on which our probability distribution is nonzero. </p>
<p>Using this idea, he shows that you can find a less biased estimator if you have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> on a finite set and you know that exactly <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> of these probabilities are nonzero.   To do this, just take the &#8216;naive&#8217; estimator I described earlier and add <img src="https://s0.wp.com/latex.php?latex=%28k-1%29%2F2n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(k-1)/2n." class="latex" />  This is called the <b>Miller&#8211;Madow bias correction</b>.  The bias of this improved estimator goes to zero like <img src="https://s0.wp.com/latex.php?latex=O%281%2Fn%5E2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(1/n^2)." class="latex" /></p>
<p>The problem is that in practice you don&#8217;t know ahead of time how many probabilities are nonzero!  In applications to biodiversity this would amount to knowing ahead of time how many species exist, before you go out looking for them. </p>
<p>But what about the theorem that there&#8217;s no unbiased estimator for Shannon entropy?  The best reference I&#8217;ve found is this:</p>
<p>&bull; Liam Paninski, <a href="http://www.stat.columbia.edu/~liam/research/abstracts/info_est-nc-abs.html">Estimation of entropy and mutual information</a>, <i>Neural Computation</i> <b>15</b> (2003) 1191-1254. </p>
<p>In Proposition 8 of Appendix A, Paninski gives a quick proof that there is no unbiased estimator of Shannon entropy for probability distributions on a finite set.  But his paper goes far beyond this.  Indeed, it seems like a pretty definitive modern discussion of the whole subject of estimating entropy.  Interestingly, this subject is dominated by neurobiologists studying entropy of signals in the brain!  So, lots of his examples involve brain signals.</p>
<p>Another overview, with tons of references, is this:</p>
<p>&bull; J. Beirlant, E. J. Dudewicz, L. Gy&ouml;rfi, and E. C. van der Meulen, <a href="http://www.its.caltech.edu/~jimbeck/summerlectures/references/Entropy%20estimation.pdf">Nonparametric entropy estimation: an overview</a>.  </p>
<p>This paper focuses on the situation where don&#8217;t know ahead of time how many probabilities are nonzero:</p>
<p>&bull; Anne Chao and T.-J. Shen, <a href="http://wayback.archive.org/web/20110715000000*/http://chao.stat.nthu.edu.tw/paper/2003_eest_10_p429.pdf">Nonparametric estimation of Shannon&#8217;s index of diversity when there are unseen species in sample</a>, <i><a href="http://www.springerlink.com/content/j23110l474087421/">Environmental and Ecological Statistics</a></i> <b>10</b> (2003), 429&amp;&#8211;443.</p>
<p>In 2003 there was a conference on the problem of estimating entropy, whose webpage has useful information.  As you can see, it was dominated by neurobiologists:</p>
<p>&bull; <a href="http://menem.com/~ilya/pages/NIPS03/">Estimation of entropy and information of undersampled probability distributions: theory, algorithms, and applications to the neural code</a>, Whistler, British Columbia, Canada, 12 December 2003.</p>
<p>By the way, I was very confused for a while, because these guys claim to have found an unbiased estimator of Shannon entropy:</p>
<p>&bull; Stephen Montgomery Smith and Thomas Sch&uuml;rmann, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.6882&amp;rep=rep1&amp;type=pdf">Unbiased estimators for entropy and class number</a>.</p>
<p>However, their way of estimating entropy has a funny property: in the language of biodiversity, it&#8217;s only well-defined if our samples include at least one species of each organism.   So, we cannot compute this estimate for an <i>arbitary</i> list of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> samples.  This means it&#8217;s not <a href="http://en.wikipedia.org/wiki/Estimator">estimator</a> in the usual sense&#8212;the sense that Paninski is using!  So it doesn&#8217;t really contradict Paninski&#8217;s result.</p>
<p>To wrap up, let me state Paninski&#8217;s result in a mathematically precise way. Suppose <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a probability distribution on a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  Suppose <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is any number we can compute from <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />: that is, any real-valued function on the set of probability distributions.   We&#8217;ll be interested in the case where <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the <b>Shannon entropy</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-%5Csum_%7Bx+%5Cin+X%7D+p%28x%29+%5C%2C+%5Clog+p%28x%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = -&#92;sum_{x &#92;in X} p(x) &#92;, &#92;log p(x) }" class="latex" /></p>
<p>Here we can use whatever base for the logarithm we like: earlier I was using base 2, but that&#8217;s not sacred.  Define an <b>estimator</b> to be any function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D%3A+X%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}: X^n &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>The idea is that given <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> <b>samples</b> from the set <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> meaning points <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cdots%2C+x_n+%5Cin+X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1, &#92;dots, x_n &#92;in X," class="latex" /> the estimator gives a number <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D%28x_1%2C+%5Cdots%2C+x_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}(x_1, &#92;dots, x_n)" class="latex" />.   This number is supposed to estimate some feature of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />: for example, its entropy.   </p>
<p>If the samples are independent and distributed according to the distribution <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> the <b>sample mean of the estimator</b> will be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Chat%7BS%7D+%5Crangle+%3D+%5Csum_%7Bx_1%2C+%5Cdots%2C+x_n+%5Cin+X%7D+%5Chat%7BS%7D%28x_1%2C+%5Cdots%2C+x_n%29+%5C%2C+p%28x_1%29+%5Ccdots+p%28x_n%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;hat{S} &#92;rangle = &#92;sum_{x_1, &#92;dots, x_n &#92;in X} &#92;hat{S}(x_1, &#92;dots, x_n) &#92;, p(x_1) &#92;cdots p(x_n) } " class="latex" /></p>
<p>The <b>bias</b> of the estimator is the difference between the sample mean of the estimator and actual value of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Chat%7BS%7D+%5Crangle+-+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;hat{S} &#92;rangle - S " class="latex" /></p>
<p>The estimator <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}" class="latex" /> is <b>unbiased</b> if this bias is zero for all <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>Proposition 8 of Paninski&#8217;s paper says there exists no unbiased estimator for entropy!  The proof is very short&#8230; </p>
<p>Okay, that&#8217;s all for today.</p>
<p>I&#8217;m back in Singapore now; I learned so much at the <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx">Mathematics of Biodiversity</a> conference that there&#8217;s no way I&#8217;ll be able to tell you all that information.   I&#8217;ll try to write a few more blog posts, but please be aware that my posts so far give a hopelessly biased and idiosyncratic view of the conference, which would be almost unrecognizable to most of the participants.  There are a lot of important themes I haven&#8217;t touched on at all&#8230; while this business of entropy estimation barely came up: I just find it interesting!</p>
<p>If more of you blogged more, we wouldn&#8217;t have this problem. </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/#comments">26 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;7)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10541 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics category-physics category-probability" id="post-10541">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;5)</a></h2>
				<small>3 July, 2012</small><br />


				<div class="entry">
					<p>I&#8217;d be happy to get your feedback on these slides of the talk I&#8217;m giving the day after tomorrow:</p>
<p>&bull; John Baez, <a href="http://math.ucr.edu/home/baez/biodiversity/">Diversity, entropy and thermodynamics</a>, 6 July 2012, Exploratory Conference on the Mathematics of Biodiversity, Centre de Recerca Matemàtica, Barcelona.</p>
<blockquote><p>
<b>Abstract:</b> As is well known, some popular measures of biodiversity are formally identical to measures of entropy developed by Shannon, Rényi and others. This fact is part of a larger analogy between thermodynamics and the mathematics of biodiversity, which we explore here. Any probability distribution can be extended to a 1-parameter family of probability distributions where the parameter has the physical meaning of &#8216;temperature&#8217;. This allows us to introduce thermodynamic concepts such as energy, entropy, free energy and the partition function in any situation where a probability distribution is present&#8212;for example, the probability distribution describing the relative abundances of different species in an ecosystem. The Rényi entropy of this probability distribution is closely related to the change in free energy with temperature. We give one application of thermodynamic ideas to population dynamics, coming from the work of Marc Harper: as a population approaches an &#8216;evolutionary optimum&#8217;, the amount of Shannon information it has &#8216;left to learn&#8217; is nonincreasing. This fact is closely related to the Second Law of Thermodynamics.
</p></blockquote>
<p>This talk is rather different than the one I&#8217;d envisaged giving!  There was a lot of interest in my work on R&eacute;nyi entropy and thermodynamics, because R&eacute;nyi entropies&#8212;and their exponentials, called the Hill numbers&#8212;are an <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">important measure of biodiversity</a>.  So, I decided to spend a lot of time talking about that.</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/biodiversity/"><img src="https://i2.wp.com/math.ucr.edu/home/baez/biodiversity/408px-Forest_fruits_from_Barro_Colorado.jpg" /></a></div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/#comments">12 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10481 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics" id="post-10481">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;4)</a></h2>
				<small>2 July, 2012</small><br />


				<div class="entry">
					<div align="center">
<a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/barcelona_biodiversity_poster.jpg" /><br />
</a>
</div>
<p>Today the conference part of this program is starting:</p>
<p>&bull; <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Research-Program-on-Mathematics-of-Biodiversity.aspx">Research Program on the Mathematics of Biodiversity</a>, June-July 2012, Centre de Recerca Matemàtica, Barcelona, Spain.  Organized by Ben Allen, Silvia Cuadrado, Tom Leinster, Richard Reeve and John Woolliams.</p>
<p>Lou Jost kicked off the proceedings with an impassioned call to think harder about fundamental concepts:</p>
<p>&bull; Lou Jost,  <a href="http://math.ucr.edu/home/baez/biodiversity/JostBarcelonaPublicGeneticsEcology.pdf">Why biologists should care about the mathematics of biodiversity</a>. </p>
<p>Then Tom Leinster gave an introduction to some of these concepts, and Lou explained how they show up in ecology, genetics, economics and physics.  </p>
<p>Suppose we have <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different species on an island.  Suppose a fraction <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> of the organisms belong to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  So,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i+%3D+1%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i = 1} " class="latex" /></p>
<p>and mathematically we can treat these numbers as probabilities.</p>
<p>People have many ways to compute the &#8216;biodiversity&#8217; from these numbers.   Some of these can be wildly misleading when applied incorrectly, and this has led to shocking errors.  For example, in genetics, a commonly used formula for determining when plants or animals on a bunch of islands will split into separate species is completely wrong.</p>
<p>In fact, if we&#8217;re not careful, some measures of biodiversity can fool us into thinking we&#8217;re <i>saving</i> most of the biodiversity when we&#8217;re actually <i>losing</i> almost all of it!   </p>
<p>One good example involves measures of similarity between tropical butterflies in the canopy (the top of the forest) and the understory (the bottom).  According to Lou Just, some published studies say the similarity is about 95%.  That sounds like the two communities are almost the same.  However, <i>almost no</i> butterflies living in the canopy live in the understory, and vice versa!  The problem is that mathematics is being used inappropriately.</p>
<p>Here are four famous measures of biodiversity:</p>
<p>&bull; <b>Species richness</b>.  This is just the number of species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /></p>
<p>&bull; <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Shannon_index">Shannon entropy</a></b>.  This is the expected amount of information you gain when someone tells you which species an organism belongs to:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Csum_%7Bi%3D1%7D%5En+p_i+%5Cln%28p_i%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;sum_{i=1}^n p_i &#92;ln(p_i) }" class="latex" /></p>
<p>&bull; The <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Inverse_Simpson_index">inverse Simpson index</a></b>.  This is the reciprocal of the probability that two randomly chosen organisms belong to the same species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+1+%5Cbig%2F+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ 1 &#92;big/ &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>The probability that two organisms belong to the same species is called the <a href="http://en.wikipedia.org/wiki/Diversity_index#Simpson_index"><b>Simpson index</b></a>:  </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>This is used in economics as a measure of the concentration of wealth, where <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the fraction of wealth owned by the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th individual.  Be careful: there&#8217;s a lot of different jargon in different fields, so it&#8217;s easy to get confused at first!  For example, the probability that two organisms belong to <i>different</i> species is often called the <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Gini-Simpson_index">Gini&#8211;Simpson index</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+1+-+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ 1 - &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>It was introduced by the statistician <a href="http://en.wikipedia.org/wiki/Corrado_Gini">Corrado Gini</a> a century ago, in 1912 and the ecologist <a href="http://en.wikipedia.org/wiki/Edward_H._Simpson">Edward H. Simpson</a> in 1949.  It&#8217;s also called the <b><a href="http://www.uwyo.edu/dbmcd/molmark/lect04/lect4.html">heterozygosity</a></b> in genetics.</p>
<p>&bull; The <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Berger-Parker_index">Berger&#8211;Parker index</a></b>.   This is the fraction of organisms that belong to the most common species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bmax%7D+%5C%2C+p_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{max} &#92;, p_i " class="latex" /></p>
<p>So, unlike the other main ones I&#8217;ve listed, this quantity tends to go <i>down</i> when biodiversity goes up.  To fix this we could take its reciprocal, as we did with the Simpson index.</p>
<p>What a mess, eh?  But here&#8217;s some good news: all these quantities are functions of a single quantity, the <b><a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_q%28p%29+%3D+%5Cfrac%7B1%7D%7B1+-q%7D+%5Cln+%5Csum_%7Bi%3D1%7D%5En+p_i%5Eq++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_q(p) = &#92;frac{1}{1 -q} &#92;ln &#92;sum_{i=1}^n p_i^q  } " class="latex" /></p>
<p>for various values of the parameter <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  </p>
<p>I&#8217;ve written about the R&eacute;nyi entropies and their role in thermodynamics before <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">on this blog</a>.  I&#8217;ll also talk about it later in this conference, and I&#8217;ll show you my slides.  So, I won&#8217;t repeat that story here.  Suffice it to say that R&eacute;nyi entropies are fascinating but still a bit mysterious to me.</p>
<p>But one of Lou Jost&#8217;s main points is that we can make bad mistakes if we work with R&eacute;nyi entropies when we should be working with their exponentials, which are called <b>Hill numbers</b> and denoted by a <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />, for &#8216;diversity&#8217;:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%7B%7D%5EqD%28p%29+%3D+e%5E%7BH_q%28p%29%7D+%3D+++%5Cleft%28%5Csum_%7Bi%3D1%7D%5En+p_i%5Eq+%5Cright%29%5E%7B%5Cfrac%7B1%7D%7B1-q%7D%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ {}^qD(p) = e^{H_q(p)} =   &#92;left(&#92;sum_{i=1}^n p_i^q &#92;right)^{&#92;frac{1}{1-q}}  } " class="latex" /></p>
<p>These were introduced by <a href="http://intranet.catie.ac.cr/intranet/posgrado/Agrof-Cult-AyP/Curso%20SAF%20A%20y%20P%202011/Propedeutico%20Agroforestal/Lecturas%20optativas/Diversity%20and%20evenness%20a%20unifying%20notation%20and%20consequences.pdf">M. O. Hill</a> in 1973.  One reason they&#8217;re good is that they are <b>effective numbers</b>.  This means that if all the species are equally common, the Hill number equals the number of species, regardless of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5C%3B+%5CLongrightarrow+%5C%3B+%7B%7D%5EqD%28p%29+%3D+n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;frac{1}{n} &#92;; &#92;Longrightarrow &#92;; {}^qD(p) = n " class="latex" /></p>
<p>So, they&#8217;re a way of measuring an &#8216;effective&#8217; number of species in situations where species are <i>not</i> all equally common.  </p>
<p>A closely related fact is that the Hill numbers obey the <b>replication principle</b>.  This means that if we have probability distributions on two finite sets, each with Hill number <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> for some choice of <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> and we combine them with equal weights to get a probability distribution on the disjoint union of those sets, the resulting distribution has Hill number <img src="https://s0.wp.com/latex.php?latex=2X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2X." class="latex" /></p>
<p>Another good fact is that the Hill numbers are as large as possible when all the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are equal.  They&#8217;re as small as possible, namely 1, when one of the <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> equals 1 and the rest are zero.</p>
<p>Let&#8217;s see how all the measures of biodiversity I listed are either Hill numbers or can easily be converted to Hill numbers.  We&#8217;ll also see that at <img src="https://s0.wp.com/latex.php?latex=q+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 0," class="latex" /> the Hill number treats all species that are present in an equal way, regardless of their abundance.  As <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> increases, it counts more abundant species more heavily, since we&#8217;re raising the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> to a bigger power.  And when <img src="https://s0.wp.com/latex.php?latex=q+%3D+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = &#92;infty" class="latex" />, we only care about the <i>most</i> abundant species: none of the others matter at all!</p>
<p>Here goes:</p>
<p>&bull; The species richness is the limit of the Hill numbers as <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to 0" class="latex" /> from above:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clim_%7Bq+%5Cto+0%5E%2B%7D+%7B%7D%5EqD+%28p%29+%3D+n+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;lim_{q &#92;to 0^+} {}^qD (p) = n } " class="latex" /></p>
<p>So, we can just call this <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E0D%28p%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^0D(p)." class="latex" /></p>
<p>&bull; The exponential of the Shannon entropy is the limit of the Hill numbers as <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to 1" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clim_%7Bq+%5Cto+1%7D+%7B%7D%5EqD%28p%29+%3D+%5Cexp%5Cleft%28-+%5Csum_%7Bi%3D1%7D%5En+p_i+%5Cln%28p_i%29%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;lim_{q &#92;to 1} {}^qD(p) = &#92;exp&#92;left(- &#92;sum_{i=1}^n p_i &#92;ln(p_i)&#92;right) } " class="latex" /></p>
<p>So, we can just call this <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E1D%28p%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^1D(p)." class="latex" /></p>
<p>&bull; The inverse Simpson index is the Hill number at <img src="https://s0.wp.com/latex.php?latex=q+%3D+2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 2" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%7B%7D%5E2D%28p%29+%3D++1+%5Cbig%2F+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  {}^2D(p) =  1 &#92;big/ &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>&bull;  The reciprocal of the Berger&#8211;Parker index is the limit of Hill numbers as <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to +&#92;infty" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clim_%7Bq+%5Cto+%2B%5Cinfty%7D+%7B%7D%5EqD%28p%29+%3D+1+%5Cbig%2F+%5Cmathrm%7Bmax%7D+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;lim_{q &#92;to +&#92;infty} {}^qD(p) = 1 &#92;big/ &#92;mathrm{max} &#92;, p_i } " class="latex" /></p>
<p>so we can call this quantity <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E%5Cinfty+D%28p%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^&#92;infty D(p)." class="latex" /></p>
<p>These facts mean that understanding Hill numbers will help us understand lots of measures of biodiversity!  And the good properties of Hill numbers will help us avoid dangerous mistakes.</p>
<p>For mathematicians, a good challenge is to find theorems uniquely characterizing the Hill numbers&#8230;. preferably with assumptions that biologists will accept as plausible facts about &#8216;diversity&#8217;.  Some theorems like this already exist for specific choices of <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> but it will be better to characterize the function <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5Eq+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^q D" class="latex" /> for all values of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in one blow.  Tom Leinster is working on such a theorem now.</p>
<p>Another important task is to generalize Hill numbers to take into account things like:</p>
<p>&bull; &#8216;distances&#8217; between species, measured either genetically, phylogenetically or functionally,</p>
<p>&bull; &#8216;values&#8217; for species, measured either economically or<br />
any other way.</p>
<p>There&#8217;s a lot of work on this, and many of the talks here conference will discuss these generalizations.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/#comments">13 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/9/?s=information+and+entropy" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/page/7/?s=information+and+entropy" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/page/8/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jksOwjAMRC9EarWLSl0gzpImVuXQfLAd2t6eVAIJsWA1nrGePbAV43JSTApBwOOTHJa9C3KBr1Wspqx1oSRgfaRkZssQrShym4yydXf5hdq98KjIx1u6rbgcTeG8H4axZaIfhpJbq0c5oWYxzui79uhPkY38giogdRbHVJRyOjvc4rUfh2nqh2Hswwtq4FN0'></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTY5flpCdmFjdjg1VU9zMXpVcXg4b1hYYW01dmVwWTBkQnpHOC98M1EmQW5pdlkrJUNnZVY5LXRwbFhjUFRqWlhCeHNmMi1qQWVNLlpwb0Q9YmhOL1hUej8mc2FkcVc4VHRfTCt0SnQ0V2I1Jjd3T01jNi5UeW13emNoLHhIb1pLbkROWU5vNFpJbk5YMGh4Lzk4Qk80Y0lfOTRiVD8/WV1IY0h0Ml90ejJRL1t8UWJtY0t3W2pjOFA1cFh1eUdydFVzalJ2'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>