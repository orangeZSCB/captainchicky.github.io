<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth | Page 6</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2014%2F02%2F07%2Fnetwork-theory-talks-at-oxford%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/page\/6\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fpage%2F6%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2014%2F02%2F07%2Fnetwork-theory-talks-at-oxford%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Page 6 &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results paged paged-6 search-paged-6 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-17426 post type-post status-publish format-standard hentry category-computer-science category-conferences category-mathematics category-networks" id="post-17426">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/02/07/network-theory-talks-at-oxford/" rel="bookmark">Network Theory Talks at&nbsp;Oxford</a></h2>
				<small>7 February, 2014</small><br />


				<div class="entry">
					<p>I&#8217;m giving some talks at Oxford:</p>
<blockquote>
<h3>Network Theory</h3>
<p>Nature and the world of human technology are full of networks.  People like to draw diagrams of networks: flow charts, electrical circuit diagrams, signal-flow graphs, Bayesian networks, Feynman diagrams and the like.  Mathematically minded people know that in principle these diagrams fit into a common framework: category theory.  But we are still far from a unified theory of networks. After an overview, we will look at three portions of the jigsaw puzzle in three separate talks:</p>
<p>I. Electrical circuits and signal-flow graphs.</p>
<p>II. Stochastic Petri nets, chemical reaction networks and Feynman diagrams.</p>
<p>III.  Bayesian networks, information and entropy.
</p></blockquote>
<p>If you&#8217;re nearby I hope you can come!  All these talks will take place in Lecture Theatre B in the Computer Science Department&#8212;see the map below.  Here are the times:</p>
<p>&bull; Friday 21 February 2014, 2 pm: <b>Network Theory: overview</b>.  <a href="http://math.ucr.edu/home/baez/networks_oxford/networks_overview.pdf">See the slides</a> or <a href="http://www.youtube.com/watch?v=bd3wTrRh2O0">watch a video</a>.</p>
<p>&bull; Tuesday 25 February, 3:30 pm: <b>Network Theory I: electrical circuits and signal-flow graphs</b>.  <a href="http://math.ucr.edu/home/baez/networks_oxford/networks_control.pdf">See the slides</a> or <a href="http://www.youtube.com/watch?v=p3CUOwuqmm0&amp;feature=youtu.be">watch a video</a>.</p>
<p>&bull; Tuesday 4 March, 3:30 pm:  <b>Network Theory II: stochastic Petri nets, chemical reaction networks and Feynman diagrams</b>.  <a href="http://math.ucr.edu/home/baez/networks_oxford/networks_stochastic.pdf">See the slides</a> or <a href="http://www.youtube.com/watch?v=bd3wTrRh2O0">watch a video</a>.</p>
<p>&bull;  Tuesday 11 March, 3:30 pm: <b>Network Theory III: Bayesian networks, information and entropy</b>.   <a href="http://math.ucr.edu/home/baez/networks_oxford/networks_entropy.pdf">See the slides</a> or <a href="http://www.youtube.com/watch?v=qX8fSYu7ors">watch a video</a><a></p>
<p>The first talk will be part of the </a><a href="http://www.cs.ox.ac.uk/seminars/oasis/">OASIS</a> series, meaning  the &#8220;Oxford Advanced Seminar on Informatic Structures&#8221;.  </p>
<p>I thank <a href="http://www.cs.ox.ac.uk/samson.abramsky/">Samson Abramsky</a>, <a href="http://www.cs.ox.ac.uk/bob.coecke/">Bob Coecke</a> and <a href="http://www.cs.ox.ac.uk/people/jamie.vicary/">Jamie Vicary</a> of the Computer Science Department for inviting me, and <a href="http://www.maths.ox.ac.uk/people/profiles/ulrike.tillmann">Ulrike Tillmann</a> and <a href="http://www.maths.ox.ac.uk/people/profiles/minhyong.kim">Minhyong Kim</a> of the Mathematical Institute for helping me get set up.  I also thank all the people who helped do the work I&#8217;ll be talking about, most notably <a href="http://www.thequantumnetwork.org/team/jacob-biamonte/">Jacob Biamonte, </a><a href="http://mathdept.ucr.edu/gradwebpages/erebele.html">Jason Erbele</a>, <a href="http://www.thequantumnetwork.org/team/jacob-biamonte/">Brendan Fong</a>, <a href="http://perimeterinstitute.ca/personal/tfritz/">Tobias Fritz</a>, <a href="http://www.maths.ed.ac.uk/~tl/">Tom Leinster</a>, <a href="http://mathdept.ucr.edu/gradwebpages/pham.html">Tu Pham</a>, and <a href="http://mathdept.ucr.edu/gradwebpages/rebro.html">Franciscus Rebro</a>.</p>
<p>Ulrike Tillmann has also kindly invited me to give a topology seminar:</p>
<blockquote>
<h3> Operads and the Tree of Life</h3>
<p>Trees are not just combinatorial structures: they are also biological structures, both in the obvious way but also in the study of evolution. Starting from DNA samples from living species, biologists use increasingly sophisticated mathematical techniques to reconstruct the most likely &#8220;phylogenetic tree&#8221; describing how these species evolved from earlier ones. In their work on this subject, they have encountered an interesting example of an operad, which is obtained by applying a variant of the Boardmann&#8211;Vogt &#8220;W construction&#8221; to the operad for commutative monoids. The operations in this operad are labelled trees of a certain sort, and it plays a universal role in the study of stochastic processes that involve branching.  It also shows up in tropical algebra.  This talk is based on work in progress with <a href="https://ethz.academia.edu/NinaOtter">Nina Otter</a>.
</p></blockquote>
<p>I&#8217;m not sure exactly where this will take place, but surely somewhere in the Mathematical Institute building:</p>
<p>&bull; Monday 24 February, 3:30 pm, Operads and the Tree of Life. <a href="http://math.ucr.edu/home/baez/tree_of_life/tree_of_life.pdf">See the slides</a>.</p>
<p>The Computer Science Department is shown in the map here:</p>
<div class="googlemaps">
				<iframe width="450" height="700" frameborder="0" scrolling="no" marginheight="0" marginwidth="0"  src="https://maps.google.com/maps?f=q&#038;source=s_q&#038;hl=en&#038;geocode=&#038;q=Department%20of%20Computer%20Science%20oxford%20&#038;aq=&#038;sll=51.757261,-1.256501&#038;sspn=0.01214,0.028839&#038;t=m&#038;ie=UTF8&#038;cid=10072800072332159494&#038;hnear=&#038;ll=51.759832,-1.25845&#038;spn=0.009297,0.009141&#038;z=16&#038;iwloc=A&#038;output=embed"></iframe>
			</div>
<p>&nbsp;</p>
<p>The Mathematical Institute is a bit to the west:</p>
<div class="googlemaps">
				<iframe width="450" height="700" frameborder="0" scrolling="no" marginheight="0" marginwidth="0"  src="https://maps.google.com/maps?oe=utf-8&#038;client=firefox-a&#038;ie=UTF8&#038;q=Mathematical%20Institute&#038;fb=1&#038;hnear=0x48713380adc41faf:0xc820dba8cb547402,Oxford,%20UK&#038;cid=1512324427709889058&#038;t=m&#038;ll=51.76035,-1.262977&#038;spn=0.006641,0.009656&#038;z=16&#038;iwloc=A&#038;output=embed"></iframe>
			</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/07/network-theory-talks-at-oxford/#comments">18 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/computer-science/" rel="category tag">computer science</a>, <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/02/07/network-theory-talks-at-oxford/" rel="bookmark" title="Permanent Link to Network Theory Talks at&nbsp;Oxford">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-14360 post type-post status-publish format-standard hentry category-astronomy category-climate category-mathematics" id="post-14360">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/01/30/milankovich-vs-the-ice-ages/" rel="bookmark">Milankovich vs the Ice&nbsp;Ages</a></h2>
				<small>30 January, 2013</small><br />


				<div class="entry">
					<p><i>guest post by <b>Blake Pollard</b></i></p>
<p>Hi!  My name is Blake S. Pollard. I am a physics graduate student working under Professor Baez at the University of California, Riverside. I studied Applied Physics as an undergraduate at Columbia University. As an undergraduate my research was more on the environmental side; working as a researcher at the Water Center, a part of the Earth Institute at Columbia University, I developed methods using time-series satellite data to keep track of irrigated agriculture over northwestern India for the past decade. </p>
<p>I am passionate about physics, but have the desire to apply my skills in more terrestrial settings. That is why I decided to come to UC Riverside and work with Professor Baez on some potentially more practical cross-disciplinary problems. Before starting work on my PhD I spent a year surfing in Hawaii, where I also worked in experimental particle physics at the University of Hawaii at Manoa. My current interests (besides passing my classes) lie in exploring potential applications of the analogy between information and entropy, as well as in understanding parallels between statistical, stochastic, and quantum mechanics.</p>
<p>Glacial cycles are one essential feature of Earth&#8217;s climate dynamics over timescales on the order of 100&#8217;s of kiloyears (kyr). It is often accepted as common knowledge that these glacial cycles are in some way forced by variations in the Earth&#8217;s orbit. In particular many have argued that the approximate 100 kyr period of glacial cycles corresponds to variations in the Earth&#8217;s eccentricity. As we saw in Professor Baez&#8217;s <a href="https://johncarlosbaez.wordpress.com/2011/08/21/this-weeks-finds-week-318/">earlier posts</a>, while the variation of eccentricity does affect the total insolation arriving to Earth, this variation is small. Thus many have proposed the existence of a nonlinear mechanism by which such small variations become amplified enough to drive the glacial cycles. Others have proposed that eccentricity is not primarily responsible for the 100 kyr period of the glacial cycles.  </p>
<p>Here is a brief summary of some time series analysis I performed in order to better understand the relationship between the Earth&#8217;s Ice Ages and the Milankovich cycles.</p>
<p>I used publicly available data on the Earth&#8217;s orbital parameters computed by Andr&eacute; Berger (see below for all references).  This data includes an estimate of the insolation derived from these parameters, which is plotted below against the Earth&#8217;s temperature, as estimated using deuterium concentrations in an ice core from a site in the Antarctic called EPICA Dome C:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/1InsolTempPlot.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/1InsolTempPlot.png" /></a></div>
<p>As you can see, it&#8217;s a complicated mess, even when you click to enlarge it!   However, I&#8217;m going to focus on the orbital parameters themselves, which behave more simply.  Below you can see graphs of three important parameters:</p>
<p>&bull; obliquity (tilt of the Earth&#8217;s axis),<br />
&bull; precession (direction the tilted axis is pointing),<br />
&bull; eccentricity (how much the Earth&#8217;s orbit deviates from being circular).</p>
<p>You can click on any of the graphs here to enlarge them:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/2OBL.png"><img width="450" src="https://i1.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/2OBL.png" /></a></div>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/3PREC.png"><img width="450" src="https://i1.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/3PREC.png" /></a></div>
<div align="center"><a href="//math.ucr.edu/home/baez/ecological/pollard_gabor/4ECC.png"><img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/4ECC.png" /></a></div>
<p>Richard Muller and Gordon MacDonald have argued that another astronomical parameter is important: the angle between the plane Earth&#8217;s orbit and the &#8216;invariant plane&#8217; of the solar system. This invariant plane of the solar system depends on the angular momenta of the planets, but roughly coincides with the plane of Jupiter&#8217;s orbit, from what I understand. Here is a plot of the orbital plane inclination for the past 800 kyr: </p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/5Oplane.png"><img width="450" src="https://i1.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/5Oplane.png" /></a></div>
<p>One can see from these plots, or from some spectral analysis, that the main periodicities of the orbital parameters are: </p>
<p>&bull; Obliquity ~ 42 kyr<br />
&bull; Precession ~ 21 kyr<br />
&bull; Eccentricity ~100 kyr<br />
&bull; Orbital plane ~ 100 kyr </p>
<p>Of course the curves clearly are not simple sine waves with those frequencies. Fourier transforms give information regarding the relative power of different frequencies occurring in a time series, but there is no information left regarding the time dependence of these frequencies as the time dependence is integrated out in the Fourier transform.</p>
<p>The Gabor transform is a generalization of the Fourier transform, sometimes referred to as the &#8216;windowed&#8217; Fourier transform. For the <b>Fourier transform</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F%28w%29+%3D+%5Cdfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D+%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+f%28t%29+e%5E%7B-iwt%7D+%5C%2C+dt%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F(w) = &#92;dfrac{1}{&#92;sqrt{2&#92;pi}} &#92;int_{-&#92;infty}^{&#92;infty} f(t) e^{-iwt} &#92;, dt}" class="latex" /></p>
<p>one may think of <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-iwt%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-iwt} " class="latex" />, the &#8216;kernel function&#8217;, as the guy acting as your basis element in both spaces. For the Gabor transform instead of <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-iwt%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-iwt}" class="latex" /> one defines a family of functions,</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7B%28b%2C%5Comega%29%7D%28t%29+%3D+e%5E%7Bi%5Comega%28t-b%29%7Dg%28t-b%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{(b,&#92;omega)}(t) = e^{i&#92;omega(t-b)}g(t-b) " class="latex" /> </p>
<p>where <img src="https://s0.wp.com/latex.php?latex=g+%5Cin+L%5E%7B2%7D%28%5Cmathbb%7BR%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;in L^{2}(&#92;mathbb{R})" class="latex" /> is called the <b>window function</b>. Typical windows are square windows and triangular (Bartlett) windows, but the most common is the Gaussian:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+g%28t%29%3D+e%5E%7B-kt%5E2%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ g(t)= e^{-kt^2} }" class="latex" /></p>
<p>which is used in the analysis below. The <b>Gabor transform</b> of a function <img src="https://s0.wp.com/latex.php?latex=f%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(t)" class="latex" /> is then given by </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+G_%7Bf%7D%28b%2Cw%29+%3D+%5Cint_%7B-%5Cinfty%7D%5E%5Cinfty+f%28t%29+%5Coverline%7Bg%28t-b%29%7D+e%5E%7B-iw%28t-b%29%7D+%5C%2C+dt+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ G_{f}(b,w) = &#92;int_{-&#92;infty}^&#92;infty f(t) &#92;overline{g(t-b)} e^{-iw(t-b)} &#92;, dt }" class="latex" /></p>
<p>Note the output of a Gabor transform, like the Fourier transform, is a complex function. The modulus of this function indicates the strength of a particular frequency in the signal, while the phase carries information about the&#8230; well, phase.</p>
<p>For example the modulus of the Gabor transform of </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28t%29%3D%5Csin%28%5Cdfrac%7B2%5Cpi+t%7D%7B100%7D%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(t)=&#92;sin(&#92;dfrac{2&#92;pi t}{100}) }" class="latex" /></p>
<p>is shown below. For these I used the package <a href="http://cran.r-project.org/web/packages/Rwave/">Rwave</a>, originally written in S by Rene Carmona and Bruno Torresani; R port by Brandon Whitcher.</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/6cgtSIN.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/6cgtSIN.png" /></a></div>
<p>You can see that the line centered at a frequency of .01 corresponds to the function&#8217;s period of 100 time units.</p>
<p>A Fourier transform would do okay for such a function, but consider now a sine wave whose frequency increases linearly. As you can see below, the Gabor transform of such a function shows the linear increase of frequency with time:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/7cgtSINsqr.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/7cgtSINsqr.png" /></a></div>
<p>The window parameter in both of the above Gabor transforms is 100 time units. Adjusting this parameter effects the vertical blurriness of the Gabor transform. For example here is the same plot as a above, but with window parameters of 300, 200, 100, and 50 time units:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/8window300.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/8window300.png" /></a></div>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/9window200.png"><img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/9window200.png" /></a></div>
<div align="center"><a href="//math.ucr.edu/home/baez/ecological/pollard_gabor/10window100.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/10window100.png" /></a></div>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/11window50.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/11window50.png" /></a></div>
<p>You can see as you make the window smaller the line gets sharper, but only to a point. When the window becomes approximately smaller than a given period of the signal the line starts to blur again.  This makes sense, because you can&#8217;t know the frequency of a signal precisely at a precise moment in time&#8230; just like you can&#8217;t precisely know both the momentum and position of a particle in quantum mechanics!  The math is related, in fact.</p>
<p>Now let&#8217;s look at the Earth&#8217;s temperature over the past 800 kyr, estimated from the EPICA ice core deuterium concentrations:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/12TEMP.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/12TEMP.png" /></a></div>
<p>When you look at this, first you notice spikes occurring about every 100 kyr. You can also see that the last 5 of these spikes appear to be bigger and more dramatic than the ones occurring before 500 kyr ago.  Roughly speaking, each of these spikes corresponds to rapid warming of the Earth, after which occurs slightly less rapid cooling, and then a slow decrease in temperature until the next spike occurs. These are the Earth&#8217;s glacial cycles. </p>
<p>At the bottom of the curve, where the temperature is about about 4 &deg;C cooler than the mean of this curve, glaciers are forming and extending down across the northern hemisphere. The relatively warm periods on the top of the spikes, about 10 &deg;C hotter than the glacial periods. are called the <b>interglacials</b>. You can see that we are currently in the middle of an interglacial, so the Earth is relatively warm compared to rest of the glacial cycles.</p>
<p>Now we&#8217;ll take a look at the windowed Fourier transform, or the Gabor transform, of this data. The window size for these plots is 300 kyr.</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/13GTMtemp.png"><img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/13GTMtemp.png" /></a></div>
<p>Zooming in a bit, one can see a few interesting features in this plot:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/14GTMtempzoom.png"><img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/14GTMtempzoom.png" /></a></div>
<p>We see one line at a frequency of about .024, with a sampling rate of 1 kyr, corresponds to a period of about 42 kyr, close to the period of obliquity. We also see a few things going on around a frequency of .01, corresponding to a 100 kyr period.</p>
<p>The band at .024 appears to be relatively horizontal, indicating an approximately constant frequency. Around the 100 kyr periods there is more going on. At a slightly higher frequency, about .015, there appears to be a band of slowly increasing frequency. Also, around .01 it&#8217;s hard to say what is really going on. It is possible that we see a combination of two frequency elements, one increasing, one decreasing, but almost symmetric. This may just be an artifact of the Gabor transform or the window and frequency parameters.</p>
<p>The window size for the plots below is slightly smaller, about 250 kyr.  If we put the temperature and obliquity Gabor Transforms side by side, we see this:</p>
<div align="center"><a href="//math.ucr.edu/home/baez/ecological/pollard_gabor/15cgtOBLtemp.png"><img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/15cgtOBLtemp.png" /></a></div>
<p>It&#8217;s clear the lines at .024 line up pretty well.</p>
<p>Doing the same with eccentricity:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/16cgtECCtemp.png"><img width="450" src="https://i1.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/16cgtECCtemp.png" /></a></div>
<p>Eccentricity does not line up well with temperature in this exercise though both have bright bands above and below .01 . </p>
<p>Now for temperature and orbital inclination:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/17cgtOPlanetemp.png"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/ecological/pollard_gabor/17cgtOPlanetemp.png" /></a></div>
<p>One sees that the frequencies line up better for this than for eccentricity, but one has to keep in mind that there is a nonlinear transformation performed on the &#8216;raw&#8217; orbital plane data to project this down into the &#8216;invariant plane&#8217; of the solar system. While this is physically motivated, it surely nudges the spectrum.</p>
<p>The temperature data clearly has a component with a period of approximately 42 kyr, matching well with obliquity. If you tilt your head a bit you can also see an indication of a fainter response at a frequency a bit above .04, corresponding roughly to period just below 25 kyrs, close to that of precession.</p>
<p>As far as the 100 kyr period goes, which is the periodicity of the glacial cycles, this analysis confirms much of what is known, namely that we can&#8217;t say for sure. Eccentricity seems to line up well with a periodicity of approximately 100 kyr, but on closer inspection there seems to be some discrepancies if you try to understand the glacial cycles as being forced by variations in eccentricity. The orbital plane inclination has a more similar Gabor transform modulus than does eccentricity. </p>
<p>A good next step would be to look the relative phases of the orbital parameters versus the temperature, but that&#8217;s all for now.  </p>
<p>If you have any questions or comments or suggestions, please let me know!</p>
<h3> References </h3>
<p>The orbital data used above is due to Andr&eacute; Berger <i>et al</i> and can be obtained here:</p>
<p>&bull; <a href="http://gcmd.nasa.gov/records/GCMD_EARTH_LAND_NGDC_PALEOCLIM_INSOL.html">Orbital variations and insolation database</a>, NOAA/NCDC/WDC Paleoclimatology.</p>
<p>The temperature proxy is due to J. Jouzel <i>et al</i>, and it&#8217;s based on changes in deuterium concentrations from the EPICA Antarctic ice core dating back over 800 kyr.  This data can be found here:</p>
<p>&bull; <a href="http://www.ncdc.noaa.gov/paleo/metadata/noaa-icecore-6080.html">EPICA Dome C &#8211; 800 kyr deuterium data and temperature estimates</a>, NOAA Paleoclimatology.</p>
<p>Here are the papers by Muller and Macdonald that I mentioned:</p>
<p>&bull; Richard Muller and Gordan MacDonald, <a href="http://muller.lbl.gov/papers/sciencespectra.htm">Glacial cycles and astronomical forcing</a>, <i>Science</i> <b>277</b> (1997), 215&#8211;218.</p>
<p>&bull; Richard Muller and Gordan MacDonald, <a href="http://www.pnas.org/content/94/16/8329.full">Spectrum of 100-kyr glacial cycle: orbital inclination, not eccentricity</a>, <i>PNAS</i> <b>1997</b>, 8329&#8211;8334.</p>
<p>They also have a book:</p>
<p>&bull; Richard Muller and Gordan MacDonald, <i>Ice Ages and Astronomical Causes</i>, Springer, Berlin, 2002.</p>
<p>You can also get files of the data I used here:</p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/orbit91Berger.txt">Berger <i>et al</i> orbital parameter data</a>, with explanatory text <a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/orbit91HEADER.txt">here</a>.</p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/tempDeut.txt">Jouzel <i>et al</i> EPICA Dome C temperature data</a>, with explanatory text <a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/tempDeutHEADER.txt">here</a>.</p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/ecological/pollard_gabor/OrbitalPlaneInclination.txt">Muller and Macdonald&#8217;s orbital plane inclination data</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/01/30/milankovich-vs-the-ice-ages/#comments">37 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/astronomy/" rel="category tag">astronomy</a>, <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/01/30/milankovich-vs-the-ice-ages/" rel="bookmark" title="Permanent Link to Milankovich vs the Ice&nbsp;Ages">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-11087 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-probability" id="post-11087">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/" rel="bookmark">The Noisy Channel Coding&nbsp;Theorem</a></h2>
				<small>28 July, 2012</small><br />


				<div class="entry">
					<div align="center"><a href="http://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf;jsessionid=639715502BCB1045260D9FA914505CCE?sequence=1"><img src="https://lh5.googleusercontent.com/-v-pRcD4u6yc/T8v91X48mII/AAAAAAAAK-8/mxthvAIQMTs/w497-h373/shannon.jpg" /></a></div>
<p>Here&#8217;s a charming, easily readable tale of Claude Shannon and how he came up with information theory:</p>
<p>• Erico Guizzo, <i><a href="http://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf;jsessionid=639715502BCB1045260D9FA914505CCE?sequence=1">The Essential Message: Claude Shannon and the Making of Information Theory</a></i>.</p>
<p>I hadn&#8217;t known his PhD thesis was on genetics!  His master&#8217;s thesis introduced Boolean logic to circuit design.  And as a kid,  he once set up a telegraph line to a friend&#8217;s house half a mile away.</p>
<p>So, he was perfectly placed to turn <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">information</a> into a mathematical quantity, deeply related to entropy, and prove some now-famous theorems about it.</p>
<p>These theorems set limits on how much information we can transmit through a noisy channel.   More excitingly, they say we can cook up coding schemes that let us come <i>as close as we want</i> to this limit, with an arbitrarily low probability of error.</p>
<p>As Erico Guizzo points out, these results are fundamental to the &#8216;information age&#8217; we live in today:</p>
<blockquote><p>
Can we transmit, say, a high-resolution picture over a telephone line? How long will that take? Is there a best way to do it?</p>
<p>Before Shannon, engineers had no clear answers to these questions. At that time, a wild zoo of technologies was in operation, each with a life of its own&#8212;telephone, telegraph, radio, television, radar, and a number of other systems developed during the war. Shannon came up with a unifying, general  theory of communication. It didn&#8217;t matter whether you transmitted signals using a copper wire, an optical fiber, or a parabolic dish. It didn&#8217;t matter if you were transmitting text, voice, or images. Shannon envisioned communication in abstract, mathematical terms; he defined what the once fuzzy concept of &#8220;information&#8221; meant for communication engineers and proposed a precise way to quantify it. According to him, the information content of any kind of message could be measured in binary digits, or just <b>bits</b>&#8212;a name suggested by a colleague at Bell Labs. Shannon took the bit as the fundamental unit in information theory. It was the first time that the term appeared in print.
</p></blockquote>
<p>So, I want to understand Shannon&#8217;s theorems and their proofs&#8212;especially because they clarify the relation between <i>information</i> and <i>entropy</i>, two concepts I&#8217;d like to be an expert on.  It&#8217;s sort of embarrassing that I don&#8217;t already know this stuff!  But I thought I&#8217;d post some preliminary remarks anyway, in case you too are trying to learn this stuff, or in case you can help me.</p>
<p>There are various different theorems I should learn.  For example:</p>
<p>• The <a href="http://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">source coding theorem</a> says it&#8217;s impossible to compress a stream of data to make the average number of bits per symbol in the compressed data less than the <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon information</a> of the source, without some of the data almost certainly getting lost.  However, you can make the number of bits per symbols arbitrarily close to the Shannon entropy with a probability of error as small as you like.</p>
<p>• the <a href="http://en.wikipedia.org/wiki/Noisy_channel_coding_theorem">noisy channel coding theorem</a> is a generalization to data sent over a noisy channel.</p>
<p>The proof of the noisy channel coding theorem seems not so bad&#8212;there&#8217;s a sketch of a proof in <a href="http://en.wikipedia.org/wiki/Noisy_channel_coding_theorem">the Wikipedia article on this theorem</a>.   But many theorems have a hard lemma at their heart, and for this one it&#8217;s a result in probability theory called the <a href="http://en.wikipedia.org/wiki/Asymptotic_equipartition_property">asymptotic equipartition property</a>.</p>
<p>You should not try to <i>dodge</i> the hard lemma at the heart of the theorem you&#8217;re trying to understand: there&#8217;s a reason it&#8217;s there.    So what&#8217;s the asymptotic equipartition property?</p>
<p>Here&#8217;s a somewhat watered-down statement that gets the basic idea across.   Suppose you have a method of randomly generating letters&#8212;for example, a probability distribution on the set of letters.  Suppose you randomly generate a string of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> letters, and compute <img src="https://s0.wp.com/latex.php?latex=-%281%2Fn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-(1/n)" class="latex" /> times the logarithm of the probability that you got that string.   Then as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty" class="latex" /> this number &#8216;almost surely&#8217; approaches some number <img src="https://s0.wp.com/latex.php?latex=S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S." class="latex" />  What&#8217;s this number <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />?  It&#8217;s the <i>entropy</i> of the probability distribution you used to generate those letters!</p>
<p>(<b><a href="http://en.wikipedia.org/wiki/Almost_surely">Almost surely</a></b> is probability jargon for &#8216;with probability 100%&#8217;, which is not the same as &#8216;always&#8217;.)</p>
<p>This result is really cool&#8212;definitely worth understanding in its own right!  It says that while many strings are possible, the ones you&#8217;re most likely to see lie in a certain &#8216;typical set&#8217;.  The &#8216;typical&#8217; strings are the ones where when you compute <img src="https://s0.wp.com/latex.php?latex=-%281%2Fn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-(1/n)" class="latex" /> times the log of their probability, the result is close to <img src="https://s0.wp.com/latex.php?latex=S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S." class="latex" />  How close?  Well, you get to pick that.</p>
<p>The typical strings are not individually the most probable strings!  But if you randomly generate a string, it&#8217;s very probable that it lies in the typical set.  That sounds a bit paradoxical, but if you think about it, you&#8217;ll see it&#8217;s not.  Think of repeatedly flipping a coin that has a 90% chance of landing heads up.    The most probable single outcome is that it lands heads up every time.  But the <i>typical</i> outcome is that it lands up close to 90% of the time.  And, there are lots of ways this can happen.   So, if you flip the coin a bunch of times, there&#8217;s a very high chance that the outcome is typical.</p>
<p>It&#8217;s easy to see how this result is the key to the noisy channel coding theorem.  In general, the &#8216;typical set&#8217; has few elements compared to the whole set of strings with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> letters.  So, you can make short codes for the strings in this set, and compress your message that way, and this works almost all the time.  How much you can compress your message depends on the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />.</p>
<p>(I said &#8216;in general&#8217; because there&#8217;s one exception: when every <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-letter string is equally probable, every string is in the typical set.  In this very special case, no compression is possible.)</p>
<p>So, we&#8217;re seeing the link between information and entropy!</p>
<p>The actual coding schemes that people use are a lot trickier than the simple scheme I&#8217;m hinting at here.  When you read about them, you see scary things like this:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/Turbo_code"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Turbo_decoder.svg/300px-Turbo_decoder.svg.png" /></a></div>
<p>But presumably they&#8217;re faster to implement, hence more practical.</p>
<p>The first coding schemes that come really close to the Shannon limit are the <a href="http://en.wikipedia.org/wiki/Turbo_code">turbo codes.</a>   Surprisingly, these codes were developed only in 1993!  They&#8217;re used in 3G mobile communications and deep space satellite communications.</p>
<p>One key trick is to use, not <i>one</i> decoder, but <i>two</i>.   These two decoders keep communicating with each other and improving their guesses about the signal they&#8217;re received, until they agree:</p>
<blockquote><p>
This iterative process continues until the two decoders come up with the same hypothesis for the m-bit pattern of the payload, typically in 15 to 18 cycles. An analogy can be drawn between this process and that of solving cross-reference puzzles like crossword or sudoku. Consider a partially completed, possibly garbled crossword puzzle. Two puzzle solvers (decoders) are trying to solve it: one possessing only the &#8220;down&#8221; clues (parity bits), and the other possessing only the &#8220;across&#8221; clues. To start, both solvers guess the answers (hypotheses) to their own clues, noting down how confident they are in each letter (payload bit). Then, they compare notes, by exchanging answers and confidence ratings with each other, noticing where and how they differ. Based on this new knowledge, they both come up with updated answers and confidence ratings, repeating the whole process until they converge to the same solution.
</p></blockquote>
<p>This can be seen as &#8220;an instance of loopy belief propagation in Bayesian networks.&#8221;</p>
<p>By the way, the picture I showed you above is a flowchart of the decoding scheme for a simple turbo code.  You can see the two decoders, and maybe the loop where data gets fed back to the decoders.</p>
<p>While I said this picture is &#8220;scary&#8221;, I actually like it because it&#8217;s an example of <a href="http://math.ucr.edu/home/baez/networks/">network theory</a> applied to real-life problems.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comments">18 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/" rel="bookmark" title="Permanent Link to The Noisy Channel Coding&nbsp;Theorem">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1238 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-1238">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/" rel="bookmark">Algorithmic Thermodynamics (Part&nbsp;1)</a></h2>
				<small>12 October, 2010</small><br />


				<div class="entry">
					<p>My grad student <a href="http://www.cs.auckland.ac.nz/~msta039/">Mike Stay</a> and I have published this paper:</p>
<p>• John Baez and Mike Stay, <a href="http://arxiv.org/abs/1010.2067">Algorithmic thermodynamics</a>, <i>Mathematical Structures in Computer Science</i> <b>22</b> (2012), 771&ndash;787.</p>
<p>Mike has a masters degree in computer science, and he&#8217;s working for Google on a project called <a href="http://en.wikipedia.org/wiki/Caja_project">Caja</a>.  This is a system for letting people write programs in JavaScript while protecting the end users from dirty tricks the programmers might have tried.  With me, he&#8217;s mainly working on the intersection of <a href="http://arxiv.org/abs/0903.0340">computer science and category theory</a>, trying to bring 2-categories into the game.  But Mike also knows a lot about <a href="http://en.wikipedia.org/wiki/Algorithmic_information_theory">algorithmic information theory</a>, a subject with fascinating connections to thermodynamics.  So, it was natural for us to work on that too.</p>
<p>Let me just tell you a little about what we did.</p>
<div align="center">
<a href="http://mediaresetproject.blogspot.com/2009/11/information-man.html"><br />
<img src="https://johncarlosbaez.files.wordpress.com/2010/10/shannon2.jpg?w=234" /><br />
</a>
</div>
<p>Around 1948, the electrical engineer <a href="http://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> came up with a mathematical theory of information.  Here is a quote that gives a flavor of his thoughts:</p>
<blockquote><p>
The whole point of a message is that it should contain something new.
</p></blockquote>
<p>Say you have a source of information, for example a mysterious radio transmission from an extraterrestrial civilization.  Suppose every day you get a signal sort of like this:</p>
<div align="center">
00101101011111101010101011011101110
</div>
<p>How much information are you getting?  If the message always looks like this, presumably not much:</p>
<div align="center">
00000000000000000000000000000000000
</div>
<p>Shannon came up with a precise formula for the information.  But beware: it&#8217;s not really a formula for the information of a <i>particular</i> message. It&#8217;s a formula for the <i>average</i> information of a message chosen from some probability distribution.  It&#8217;s this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Csum_i+p_i+%5C%3B%5Cmathrm%7Blog%7D%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;sum_i p_i &#92;;&#92;mathrm{log}(p_i) " class="latex" /></p>
<p>where we sum over all possible messages, and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the probability of the <i>i </i>th message.</p>
<p>So, for example, suppose you keep getting the same message.  Then every message has probability 0 except for one message, which has probability 1.  Then either <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Cln%28p_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(p_i)" class="latex" /> is zero, so the information is zero.</p>
<p>That seems vaguely plausible in the example where every day we get this message:</p>
<div align="center">
000000000000000000000000000000000000000
</div>
<p>It may seem less plausible if every day we get this message:</p>
<div align="center">
011010100010100010100010000010100000100
</div>
<p>It looks like the aliens are trying to tell us which numbers are prime!  1 is not prime, 2 is, 3 is, 4 is not, 5 is, and so on.  Aren&#8217;t we getting some information?</p>
<p>Maybe so: this could be considered a defect of <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon information</a>.  On the other hand, you might be willing to admit that if we keep getting the same message every day, the second time we get it we&#8217;re not getting any <i>new</i> information.  Or, you might <i>not</i> be willing to admit this — it&#8217;s actually a subtle issue, and I don&#8217;t feel like arguing.</p>
<p>But at the very least, you&#8217;ll probably admit that the second time you get the same message, you get less <i>new</i> information than the first time.  The third time you get even less, and so on.  So it&#8217;s quite believable that in the long run, the average amount of new information per message approaches 0 in this case.  For Shannon, information means <i>new</i> information.</p>
<p>On the other hand, suppose we are absolutely unable to to predict each new bit we get from the aliens.  Suppose our ability to predict the next bit is no better than our ability to predict whether a fair coin comes up heads or tails.  Then Shannon&#8217;s formula says we are getting the same amount of new information with every bit: namely, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Blog%7D%282%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{log}(2)." class="latex" /></p>
<p>If we take the logarithm using base 2 here, we get 1 — so we say we&#8217;re getting <i>one bit of information</i>.  If we take it using base e, as physicists prefer, we get <img src="https://s0.wp.com/latex.php?latex=%5Cln%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(2)" class="latex" /> — and we say we&#8217;re getting <img src="https://s0.wp.com/latex.php?latex=%5Cln%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(2)" class="latex" /> <a>nats</a> of information.   One bit equals <img src="https://s0.wp.com/latex.php?latex=%5Cln%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(2)" class="latex" /> nats.</p>
<p>There&#8217;s a long road from these reflections to a full justification of the specific formula for Shannon information!  To begin marching down that road you can read his original paper, <a href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">A mathematical theory of communication</a>.</p>
<p>Anyway: it soon became clear that Shannon&#8217;s formula was almost the same as the usual formula for &#8220;entropy&#8221;, which goes back to <a href="http://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">Josiah Willard Gibbs</a>.  Gibbs was actually the first engineer to get a Ph.D. in the United States, back in 1863&#8230; but he&#8217;s mainly famous as a mathematician, physicist and chemist.</p>
<div align="center">
<a href="http://en.wikipedia.org/wiki/Josiah_Willard_Gibbs"><br />
<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Josiah_Willard_Gibbs_-from_MMS-.jpg/235px-Josiah_Willard_Gibbs_-from_MMS-.jpg" /><br />
</a>
</div>
<p>Entropy is a measure of disorder. Suppose we have a box of stuff — solid, liquid, gas, whatever.   There are many possible states this stuff can be in: for example, the atoms can be in different positions, and have different velocities.  Suppose we only know the <i>probability</i> that the stuff is in any one of the allowed states.  If the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state is occupied with probability <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> Gibbs said the <a href="http://en.wikipedia.org/wiki/Entropy">entropy</a> of our box of stuff is</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+k+%5Csum_i+p_i+%5C%3B+%5Cmathrm%7Blog%7D%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- k &#92;sum_i p_i &#92;; &#92;mathrm{log}(p_i) " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> is a constant called <a href="http://en.wikipedia.org/wiki/Boltzmann_constant">Boltzmann&#8217;s constant</a>.</p>
<p>There&#8217;s a wonderful story here, which I don&#8217;t have time to tell in detail.  The way I wrote Shannon&#8217;s formula for information and Gibbs&#8217; formula for entropy, you&#8217;d think only a moron would fail to instantly grasp that they&#8217;re basically the same.  But historically, it took some work.</p>
<p>The appearance of Bolzmann&#8217;s constant hints at why.  It shows up because people had ideas about entropy, and the closely related concept of <i>temperature</i>, long before they realized the full  mathematical meaning of these concepts!   So entropy traditionally came in units of &#8220;joules/kelvin&#8221;, and physicists had a visceral understanding of it.   But dividing by Boltzmann&#8217;s constant, we can translate that notion of entropy into the modern, more abstract way of thinking of entropy as information!</p>
<p>Henceforth I&#8217;ll work in units where <img src="https://s0.wp.com/latex.php?latex=k+%3D+1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k = 1," class="latex" /> as modern mathematical physicists do, and treat entropy and information as the same concept.</p>
<p>Closely related to information and entropy is a third concept, which I will call <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a>, though it was developed by many people — including Martin-Löf, Solomonoff, Kolmogorov, Levin, and Chaitin — and it has many names, including descriptive complexity, Kolmogorov-Chaitin complexity, algorithmic entropy, and program-size complexity.  You may be intimidated by all these names, but you shouldn&#8217;t be: when lots of people keep rediscovering something and giving it new names, it&#8217;s usually because this thing is pathetically simple.</p>
<p>So, what is Kolmogorov complexity?   It&#8217;s a way of measuring the information in a <i>single</i> message rather than a probability distribution of messages.  And it&#8217;s just <i>the length of the shortest computer program that prints out this message</i>.</p>
<p>I suppose some clarification may be needed here:</p>
<p>1) I&#8217;m only talking about programs without any input, that either calculate, print out a message, and halt&#8230; or calculate forever and never halt.</p>
<p>2) Of course the length of the shortest program that prints out the desired message depends on the programming language.  But there are theorems saying it doesn&#8217;t depend &#8220;too much&#8221; on the language.  So don&#8217;t worry about it: just pick your favorite language and stick with that.</p>
<p>If you think about it, Kolmogorov complexity is a really nice concept.  The Kolmogorov complexity of a string of a million 0&#8217;s is a lot less than a million: you can write a short program that says &#8220;print a million 0&#8217;s&#8221;.  But the Kolmogov complexity of a highly unpredictable string of a million 0&#8217;s and 1&#8217;s is about a million: you basically need to include that string in your program and then say &#8220;print this string&#8221;.  Those are the two extremes, but in general the complexity will be somewhere in between.</p>
<p>Various people — the bigshots listed above, and others too — soon realized that Kolmogorov complexity is deeply connected to Shannon information.  They have similar properties, but they&#8217;re also directly related.   It&#8217;s another great story, and I urge you to learn it.  For that, I recommend:</p>
<p>•  Ming Li and Paul Vitanyi, <i>An Introduction to Kolmogorov Complexity and Its Applications</i>, Springer, Berlin, 2008.</p>
<p>To understand the relation a bit better, Mike and I started thinking about <i>probability measures on the set of programs</i>.  People had already thought about this — but we thought about it a bit more the way physicists do.</p>
<p>Physicists like to talk about something called a <a href="http://en.wikipedia.org/wiki/Canonical_ensemble">Gibbs ensemble</a>.  Suppose we have a set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F: X &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>Then the Gibbs ensemble is the probability distribution on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> that maximizes entropy subject to the condition that <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> has some specified average, or &#8220;expected value&#8221;.</p>
<p>So, to find the Gibbs ensemble, we need to find a probability distribution <img src="https://s0.wp.com/latex.php?latex=p+%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : X &#92;to &#92;mathbb{R}" class="latex" /> that maximizes</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%3B+%5Cmathrm%7Blog%7D%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;sum_{i &#92;in X} p_i &#92;; &#92;mathrm{log}(p_i) " class="latex" /></p>
<p>subject to the constraint that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%3B+F%28i%29+%3D+%5Clangle+F+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i &#92;in X} p_i &#92;; F(i) = &#92;langle F &#92;rangle " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clangle+F+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle F &#92;rangle " class="latex" /> is some number, the expected value of <img src="https://s0.wp.com/latex.php?latex=F.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F." class="latex" />  Finding the probability distribution that does the job is an exercise in <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">Lagrange multipliers</a>.  I won&#8217;t do it.    There&#8217;s a nice formula for the answer, but we won&#8217;t need it here.</p>
<p>What you really need to know is something more important: why Gibbs invented the Gibbs ensemble!  He invented it to solve some puzzles that sound completely impossible at first.</p>
<p>For example, suppose you have a box of stuff and you don&#8217;t know which state it&#8217;s in.  Suppose you only know the expected value of its energy, say <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle." class="latex" /> What&#8217;s the probability that this stuff is in its <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state?</p>
<p>This may sound like an insoluble puzzle: how can we possibly know?   But Gibbs proposed an answer!  He said, basically: find the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> that maximizes entropy subject to the constraint that the mean value of energy is <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle." class="latex" />  Then the answer is <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /></p>
<p>In other words: <i>use the Gibbs ensemble.</i></p>
<p>Now let&#8217;s come back to Kolmogorov complexity.</p>
<p>Imagine randomly picking a program out of a hat.  What&#8217;s the probability that you pick a certain program?   Again, this sounds like an impossible puzzle.  But you can answer it if you know the expected value of the length of this program!   Then you just use the Gibbs ensemble.</p>
<p>What does this have to do with Kolmogorov complexity?  Here I&#8217;ll be a bit fuzzy, because the details are in our paper, and I want you to read that.</p>
<p>Suppose we start out with the Gibbs ensemble I just mentioned.  In other words, we have a program in a hat, but all you know is the expected value of its length.</p>
<p>But now suppose I tell you the message that this program prints out.  Now you know more.  How much more information do you have now?  <i>The Kolmogorov complexity of the message</i> — that&#8217;s how much!</p>
<p>(Well, at least this is correct up to some error bounded by a constant.)</p>
<p>The main fuzzy thing in what I just said is &#8220;how much more information do you have?&#8221;  You see, I&#8217;ve explained information, but I haven&#8217;t explained <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">&#8220;information gain&#8221;</a>.  Information is a quantity you compute from <i>one</i> probability distribution.  Information gain is a quantity you compute from <i>two</i>.  More precisely,</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Csum_i+p_i+%5C%3B+%5Cmathrm%7Blog%7D%28p_i%2Fq_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;sum_i p_i &#92;; &#92;mathrm{log}(p_i/q_i) " class="latex" /></p>
<p>is the information you gain when you <i>thought</i> the probability distribution was <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> but then someone comes along and tells you it&#8217;s <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>In fact, we argue that information gain is more fundamental than information.   This is a Bayesian idea: <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is your <a href="http://en.wikipedia.org/wiki/Prior_probability">&#8220;prior&#8221;</a>, the probability distribution you <i>thought</i> was true, and the information you get upon hearing the distribution is <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> should be defined relative to this prior.  When people think they&#8217;re talking about information without any prior, they are really using a prior that&#8217;s so bland that they don&#8217;t notice it&#8217;s there: a so-called <a href="http://en.wikipedia.org/wiki/Uninformative_prior#Uninformative_priors">&#8220;uninformative prior&#8221;</a>.</p>
<p>But I digress.  To help you remember the story so far, let me repeat myself.  Up to a bounded error, <i>the Kolmogorov complexity of a message is the information gained when you start out only knowing the expected length of a program, and then learn which message the program prints out.</i></p>
<p>But this is just the beginning of the story.  We&#8217;ve seen how Kolmogorov complexity is related to Gibbs ensembles.  Now that we have Gibbs ensembles running around, we can go ahead and do thermodynamics!  We can talk about quantities analogous to temperature, pressure, and so on&#8230; and all the usual thermodynamic relations hold!  We can even take the ideas about steam engines and apply them to programs!</p>
<div align="center">
<img src="https://i2.wp.com/math.ucr.edu/home/baez/piston_tiny.jpg" />
</div>
<p>But for that, please read <a href="http://arxiv.org/abs/1010.2067">our paper</a>.  Here&#8217;s the abstract, which says what we really do:</p>
<blockquote><p>
Algorithmic entropy can be seen as a special case of entropy as studied in statistical mechanics. This viewpoint allows us to apply many techniques developed for use in thermodynamics to the subject of algorithmic information theory. In particular, suppose we fix a universal prefix-free Turing machine and let X be the set of programs that halt for this machine. Then we can regard X as a set of &#8216;microstates&#8217;, and treat any function on X as an &#8216;observable&#8217;. For any collection of observables, we can study the Gibbs ensemble that maximizes entropy subject to constraints on expected values of these observables. We illustrate this by taking the log runtime, length, and output of a program as observables analogous to the energy E, volume V and number of molecules N in a container of gas. The conjugate variables of these observables allow us to define quantities which we call the &#8216;algorithmic temperature&#8217; T, &#8216;algorithmic pressure&#8217; P and `algorithmic potential&#8217; μ, since they are analogous to the temperature, pressure and chemical potential. We derive an analogue of the fundamental thermodynamic relation dE = T dS &#8211; P d V + μ dN, and use it to study thermodynamic cycles analogous to those for heat engines. We also investigate the values of T, P and μ for which the partition function converges. At some points on the boundary of this domain of convergence, the partition function becomes uncomputable. Indeed, at these points the partition function itself has nontrivial algorithmic entropy.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comments">24 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/" rel="bookmark" title="Permanent Link to Algorithmic Thermodynamics (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31143 post type-post status-publish format-standard hentry category-biology category-chemistry category-information-and-entropy category-physics" id="post-31143">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/06/16/nonequilibrium-thermodynamics-in-biology-part-2/" rel="bookmark">Nonequilibrium Thermodynamics in Biology (Part&nbsp;2)</a></h2>
				<small>16 June, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg"><img loading="lazy" data-attachment-id="30978" data-permalink="https://johncarlosbaez.wordpress.com/2021/05/11/non-equilibrium-thermodynamics-in-biology/attachment/30978/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg" data-orig-size="3761,1577" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450" src="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450&#038;h=189" alt="" class="aligncenter size-full wp-image-30978" width="450" height="189" srcset="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450&amp;h=189 450w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=900&amp;h=378 900w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=150&amp;h=63 150w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=300&amp;h=126 300w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=768&amp;h=322 768w" sizes="(max-width: 450px) 100vw, 450px" /></a></p>
<p><a>Larry Li</a>, <a href="https://biology.pnnl.gov/people/bill-cannon">Bill Cannon</a> and I ran a session on non-equilibrium thermodynamics in biology at <a href="https://www.smb2021.org/home">SMB2021</a>, the annual meeting of the Society for Mathematical Biology.  You can see talk slides here!</p>
<p>Here&#8217;s the basic idea:</p>
<blockquote><p>
  Since Lotka, physical scientists have argued that living things belong to a class of complex and orderly systems that exist not despite the second law of thermodynamics, but because of it. Life and evolution, through natural selection of dissipative structures, are based on non-equilibrium thermodynamics. The challenge is to develop an understanding of what the respective physical laws can tell us about flows of energy and matter in living systems, and about growth, death and selection. This session addresses current challenges including understanding emergence, regulation and control across scales, and entropy production, from metabolism in microbes to evolving ecosystems.</p></blockquote>
<p>Click on the links to see slides for most of the talks:</p>
<p>• <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_craciun.pdf">Persistence, permanence, and global stability in reaction network models: some results inspired by thermodynamic principles</a></b><br />
Gheorghe Craciun, University of Wisconsin–Madison</p>
<blockquote><p>
The standard mathematical model for the dynamics of concentrations in biochemical networks is called mass-action kinetics. We describe mass-action kinetics and discuss the connection between special classes of mass-action systems (such as detailed balanced and complex balanced systems) and the Boltzmann equation. We also discuss the connection between the &#8216;global attractor conjecture&#8217; for complex balanced mass-action systems and Boltzmann&#8217;s H-theorem. We also describe some implications for biochemical mechanisms that implement noise filtering and cellular homeostasis.</p></blockquote>
<p>• <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_dill.pdf">The  principle of maximum caliber of nonequilibria</a></b><br />
Ken Dill, Stony Brook University</p>
<blockquote><p>
Maximum Caliber is a principle for inferring pathways and rate distributions of kinetic processes. The structure and foundations of MaxCal are much like those of Maximum Entropy for static distributions. We have explored how MaxCal may serve as a general variational principle for nonequilibrium statistical physics&#8212;giving well-known results, such as the Green-Kubo relations, Onsager&#8217;s reciprocal relations and Prigogine&#8217;s Minimum Entropy Production principle near equilibrium, but is also applicable far from equilibrium. I will also discuss some applications, such as finding reaction coordinates in molecular simulations non-linear dynamics in gene circuits, power-law-tail distributions in &#8216;social-physics&#8217; networks, and others.</p></blockquote>
<p>• <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_gaspard.pdf">Nonequilibrium biomolecular information processes</a></b><br />
Pierre Gaspard, Université libre de Bruxelles</p>
<blockquote><p>
Nearly 70 years have passed since the discovery of DNA structure and its role in coding genetic information. Yet, the kinetics and thermodynamics of genetic information processing in DNA replication, transcription, and translation remain poorly understood. These template-directed copolymerization processes are running away from equilibrium, being powered by extracellular energy sources. Recent advances show that their kinetic equations can be exactly solved in terms of so-called iterated function systems. Remarkably, iterated function systems can determine the effects of genome sequence on replication errors, up to a million times faster than kinetic Monte Carlo algorithms. With these new methods, fundamental links can be established between molecular information processing and the second law of thermodynamics, shedding a new light on genetic drift, mutations, and evolution.</p></blockquote>
<p>• <b> <a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_harte.pdf">Nonequilibrium dynamics of  disturbed ecosystems</a></b><br />
John Harte, University of California, Berkeley</p>
<blockquote><p>
The Maximum Entropy Theory of Ecology (METE) predicts the shapes of macroecological metrics in relatively static ecosystems, across spatial scales, taxonomic categories, and habitats, using constraints imposed by static state variables. In disturbed ecosystems, however, with time-varying state variables, its predictions often fail. We extend macroecological theory from static to dynamic, by combining the MaxEnt inference procedure with explicit mechanisms governing disturbance. In the static limit, the resulting theory, DynaMETE, reduces to METE but also predicts a new scaling relationship among static state variables. Under disturbances, expressed as shifts in demographic, ontogenic growth, or migration rates, DynaMETE predicts the time trajectories of the state variables as well as the time-varying shapes of macroecological metrics such as the species abundance distribution and the distribution of metabolic rates over<br />
individuals. An iterative procedure for solving the dynamic theory is presented. Characteristic signatures of the deviation from static predictions of macroecological patterns are shown to result from different kinds of disturbance. By combining MaxEnt inference with explicit dynamical mechanisms of disturbance, DynaMETE is a candidate theory of macroecology for ecosystems responding to anthropogenic or natural disturbances.</p></blockquote>
<p>• <b> <a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_krishnamurthy.pdf">Stochastic chemical reaction networks</a> </b><br />
Supriya Krishnamurthy, Stockholm University</p>
<blockquote><p>
The study of chemical reaction networks (CRN&#8217;s) is a very active field. Earlier well-known results (Feinberg Chem. Enc. Sci. 42 2229 (1987), Anderson et al Bull. Math. Biol. 72 1947 (2010)) identify a topological quantity called deficiency, easy to compute for CRNs of any size, which, when exactly equal to zero, leads to a unique factorized (non-equilibrium) steady-state for these networks. No general results exist however for the steady states of non-zero-deficiency networks. In recent work, we show how to write the full moment-hierarchy for any non-zero-deficiency CRN obeying mass-action kinetics, in terms of equations for the factorial moments. Using these, we can recursively predict values for lower moments from higher moments, reversing the procedure usually used to solve moment hierarchies. We show, for non-trivial examples, that in this manner we can predict any moment of interest, for CRN&#8217;s with non-zero deficiency and non-factorizable steady states. It is however an open question how scalable these techniques are for large networks.</p></blockquote>
<p>• <b> <a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_mast.pdf">Heat flows adjust local ion concentrations in favor of prebiotic chemistry</a></b><br />
Christof Mast, Ludwig-Maximilians-Universität München</p>
<blockquote><p>
Prebiotic reactions often require certain initial concentrations of ions. For example, the activity of RNA enzymes requires a lot of divalent magnesium salt, whereas too much monovalent sodium salt leads to a reduction in enzyme function. However, it is known from leaching experiments that prebiotically relevant geomaterial such as basalt releases mainly a lot of sodium and only little magnesium. A natural solution to this problem is heat fluxes through thin rock fractures, through which magnesium is actively enriched and sodium is depleted by thermogravitational convection and thermophoresis. This process establishes suitable conditions for ribozyme function from a basaltic leach. It can take place in a spatially distributed system of rock cracks and is therefore particularly stable to natural fluctuations and disturbances.</p></blockquote>
<p>• <b>Deficiency of chemical reaction networks and thermodynamics</b><br />
Matteo Polettini, University of Luxembourg</p>
<blockquote><p>
Deficiency is a topological property of a Chemical Reaction Network linked to important dynamical features, in particular of deterministic fixed points and of stochastic stationary states. Here we link it to thermodynamics: in particular we discuss the validity of a strong vs. weak zeroth law, the existence of time-reversed mass-action kinetics, and the possibility to formulate marginal fluctuation relations. Finally we illustrate some subtleties of the Python module we created for MCMC stochastic simulation of CRNs, soon to be made public.</p></blockquote>
<p>• <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_qian.pdf">Large deviations theory and emergent landscapes in biological dynamics</a></b><br />
Hong Qian, University of Washington</p>
<blockquote><p>
The mathematical theory of large deviations provides a nonequilibrium thermodynamic description of complex biological systems that consist of heterogeneous individuals. In terms of the notions of stochastic elementary reactions and pure kinetic species, the continuous-time, integer-valued Markov process dictates a thermodynamic structure that generalizes (i) Gibbs&#8217; microscopic chemical thermodynamics of equilibrium matters to nonequilibrium small systems such as living cells and tissues; and (ii) Gibbs&#8217; potential function to the landscapes for biological dynamics, such as that of C. H. Waddington and S. Wright.</p></blockquote>
<p>• <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_vallino.pdf">Using the maximum entropy production  principle to understand and predict microbial biogeochemistry</a></b><br />
Joseph Vallino, Marine Biological Laboratory, Woods Hole</p>
<blockquote><p>
Natural microbial communities contain billions of individuals per liter and can exceed a trillion cells per liter in sediments, as well as harbor thousands of species in the same volume. The high species diversity contributes to extensive metabolic functional capabilities to extract chemical energy from the environment, such as methanogenesis, sulfate reduction, anaerobic photosynthesis, chemoautotrophy, and many others, most of which are only expressed by bacteria and archaea. Reductionist modeling of natural communities is problematic, as we lack knowledge on growth kinetics for most organisms and have even less understanding on the mechanisms governing predation, viral lysis, and predator avoidance in these systems. As a result, existing models that describe microbial communities contain dozens to hundreds of parameters, and state variables are extensively aggregated. Overall, the models are little more than non-linear parameter fitting exercises that have limited, to no, extrapolation potential, as there are few principles governing organization and function of complex self-assembling systems. Over the last decade, we have been developing a systems approach that models microbial communities as a distributed metabolic network that focuses on metabolic function rather than describing individuals or species. We use an optimization approach to determine which metabolic functions in the network should be up regulated versus those that should be down regulated based on the non-equilibrium thermodynamics principle of maximum entropy production (MEP). Derived from statistical mechanics, MEP proposes that steady state systems will likely organize to maximize free energy dissipation rate. We have extended this conjecture to apply to non-steady state systems and have proposed that living systems maximize entropy production integrated over time and space, while non-living systems maximize instantaneous entropy production. Our presentation will provide a brief overview of the theory and approach, as well as present several examples of applying MEP to describe the biogeochemistry of microbial systems in laboratory experiments and natural ecosystems.</p></blockquote>
<p>• <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_wiuf.pdf">Reduction and the quasi-steady state approximation</a></b><br />
Carsten Wiuf, University of Copenhagen</p>
<blockquote><p>
Chemical reactions often occur at different time-scales. In applications of chemical reaction network theory it is often desirable to reduce a reaction network to a smaller reaction network by elimination of fast species or fast reactions. There exist various techniques for doing so, e.g. the Quasi-Steady-State Approximation or the Rapid Equilibrium Approximation. However, these methods are not always mathematically justifiable. Here, a method is presented for which (so-called) non-interacting species are eliminated by means of QSSA. It is argued that this method is mathematically sound. Various examples are given (Michaelis-Menten mechanism, two-substrate mechanism, &#8230;) and older related techniques from the 50s and 60s are briefly discussed.</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/06/16/nonequilibrium-thermodynamics-in-biology-part-2/#respond">Leave a Comment &#187;</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/chemistry/" rel="category tag">chemistry</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/06/16/nonequilibrium-thermodynamics-in-biology-part-2/" rel="bookmark" title="Permanent Link to Nonequilibrium Thermodynamics in Biology (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-30955 post type-post status-publish format-standard hentry category-biology category-chemistry category-mathematics category-networks" id="post-30955">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/05/11/non-equilibrium-thermodynamics-in-biology/" rel="bookmark">Non-Equilibrium Thermodynamics in Biology (Part&nbsp;1)</a></h2>
				<small>11 May, 2021</small><br />


				<div class="entry">
					<p><img loading="lazy" data-attachment-id="30978" data-permalink="https://johncarlosbaez.wordpress.com/2021/05/11/non-equilibrium-thermodynamics-in-biology/attachment/30978/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg" data-orig-size="3761,1577" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450" src="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450&#038;h=189" alt="" class="aligncenter size-full wp-image-30978" width="450" height="189" srcset="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450&amp;h=189 450w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=900&amp;h=378 900w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=150&amp;h=63 150w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=300&amp;h=126 300w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=768&amp;h=322 768w" sizes="(max-width: 450px) 100vw, 450px" /></p>
<p>Together with <a href="">William Cannon</a> and <a href="https://iigb.ucr.edu/blli/">Larry Li</a>, I&#8217;m helping run a minisymposium as part of <a href="https://www.smb2021.org/home">SMB2021</a>, the annual meeting of the Society for Mathematical Biology:</p>
<p>• Non-equilibrium Thermodynamics in Biology: from Chemical Reaction Networks to Natural Selection, Monday June 14, 2021, beginning 9:30 am Pacific Time.</p>
<p>You can register for free <a href="https://www.smb2021.org/registration">here</a> before May 31st, 11:59 pm Pacific Time.  You need to register to watch the talks live on Zoom.  I think the talks will be recorded.</p>
<p>Here&#8217;s the idea:</p>
<blockquote><p>
  <strong>Abstract:</strong> Since Lotka, ﻿physical scientists have argued that living things belong to a class of complex and orderly systems that exist not despite the second law of thermodynamics, but because of it. Life and evolution, through natural selection of dissipative structures, are based on non-equilibrium thermodynamics. The challenge is to develop an understanding of what the respective physical laws can tell us about flows of energy and matter in living systems, and about growth, death and selection. This session will address current challenges including understanding emergence, regulation and control across scales, and entropy production, from metabolism in microbes to evolving ecosystems.</p></blockquote>
<p>It&#8217;s exciting to me because I want to get back into work on thermodynamics and reaction networks, and we&#8217;ll have some excellent speakers on these topics.  I think the talks will be in this order&#8230; later I will learn the exact schedule.</p>
<h4> <a href="https://scholar.google.com/citations?user=E0o4qtUAAAAJ&amp;hl=de">Christof Mast, Ludwig-Maximilians-Universität München</a></h4>
<p><strong>Coauthors:</strong> T. Matreux, K. LeVay, A. Schmid, P. Aikkila, L. Belohlavek, Z. Caliskanoglu, E. Salibi, A. Kühnlein, C. Springsklee, B. Scheu, D. B. Dingwell, D. Braun, H. Mutschler.</p>
<p><strong>Title:</strong> Heat flows adjust local ion concentrations in favor of prebiotic chemistry</p>
<blockquote><p>
  <strong>Abstract:</strong> Prebiotic reactions often require certain initial concentrations of ions. For example, the activity of RNA enzymes requires a lot of divalent magnesium salt, whereas too much monovalent sodium salt leads to a reduction in enzyme function. However, it is known from leaching experiments that prebiotically relevant geomaterial such as basalt releases mainly a lot of sodium and only little magnesium. A natural solution to this problem is heat fluxes through thin rock fractures, through which magnesium is actively enriched and sodium is depleted by thermogravitational convection and thermophoresis. This process establishes suitable conditions for ribozyme function from a basaltic leach. It can take place in a spatially distributed system of rock cracks and is therefore particularly stable to natural fluctuations and disturbances.</p></blockquote>
<h4> <a href="https://www.researchgate.net/profile/Supriya-Krishnamurthy">Supriya Krishnamurthy, Stockholm University</a></h4>
<p><strong>Coauthors:</strong> Eric Smith</p>
<p><strong>Title:</strong> Stochastic chemical reaction networks</p>
<blockquote><p>
  <strong>Abstract:</strong> The study of chemical reaction networks (CRNs) is a very active field. Earlier well-known results (Feinberg Chem. Enc. Sci. 42 2229 (1987), Anderson et al Bull. Math. Biol. 72 1947 (2010)) identify a topological quantity called deficiency, easy to compute for CRNs of any size, which, when exactly equal to zero, leads to a unique factorized (non-equilibrium) steady-state for these networks. No general results exist however for the steady states of non-zero-deficiency networks. In recent work, we show how to write the full moment-hierarchy for any non-zero-deficiency CRN obeying mass-action kinetics, in terms of equations for the factorial moments. Using these, we can recursively predict values for lower moments from higher moments, reversing the procedure usually used to solve moment hierarchies. We show, for non-trivial examples, that in this manner we can predict any moment of interest, for CRNs with non-zero deficiency and non-factorizable steady states. It is however an open question how scalable these techniques are for large networks.</p></blockquote>
<h4> <a href="http://homepages.ulb.ac.be/~gaspard/">Pierre Gaspard, Université libre de Bruxelles</a></h4>
<p><strong>Title:</strong> Nonequilibrium biomolecular information processes</p>
<blockquote><p>
  <strong>Abstract:</strong> Nearly 70 years have passed since the discovery of DNA structure and its role in coding genetic information. Yet, the kinetics and thermodynamics of genetic information processing in DNA replication, transcription, and translation remain poorly understood. These template-directed copolymerization processes are running away from equilibrium, being powered by extracellular energy sources. Recent advances show that their kinetic equations can be exactly solved in terms of so-called iterated function systems. Remarkably, iterated function systems can determine the effects of genome sequence on replication errors, up to a million times faster than kinetic Monte Carlo algorithms. With these new methods, fundamental links can be established between molecular information processing and the second law of thermodynamics, shedding a new light on genetic drift, mutations, and evolution.</p></blockquote>
<h4> <a href="http://web.math.ku.dk/~pbx512/">Carsten Wiuf, University of Copenhagen</a></h4>
<p><strong>Coauthors:</strong> Elisenda Feliu, Sebastian Walcher, Meritxell Sáez</p>
<p><strong>Title:</strong> Reduction and the Quasi-Steady State Approximation</p>
<blockquote><p>
  <strong>Abstract:</strong> Chemical reactions often occur at different time-scales. In applications of chemical reaction network theory it is often desirable to reduce a reaction network to a smaller reaction network by elimination of fast species or fast reactions. There exist various techniques for doing so, e.g. the Quasi-Steady-State Approximation or the Rapid Equilibrium Approximation. However, these methods are not always mathematically justifiable.  Here, a method is presented for which (so-called) non-interacting species are eliminated by means of QSSA. It is argued that this method is mathematically sound. Various examples are given (Michaelis-Menten mechanism, two-substrate mechanism, &#8230;) and older related techniques from the 50-60ies are briefly discussed.</p></blockquote>
<h4> <a href="https://scholar.google.com/citations?user=0Y1jhiMAAAAJ&amp;hl=en">Matteo Polettini, University of Luxembourg</a></h4>
<p><strong>Coauthor:</strong> Tobias Fishback</p>
<p><strong>Title:</strong> Deficiency of chemical reaction networks and thermodynamics</p>
<blockquote><p>
  <strong>Abstract:</strong> Deficiency is a topological property of a Chemical Reaction Network linked to important dynamical features, in particular of deterministic fixed points and of stochastic stationary states. Here we link it to thermodynamics: in particular we discuss the validity of a strong vs. weak zeroth law, the existence of time-reversed mass-action kinetics, and the possibility to formulate marginal fluctuation relations. Finally we illustrate some subtleties of the Python module we created for MCMC stochastic simulation of CRNs, soon to be made public.</p></blockquote>
<h4> <a href="http://dillgroup.stonybrook.edu/#/home">Ken Dill, Stony Brook University</a></h4>
<p><strong>Title:</strong> The principle of maximum caliber of nonequilibria</p>
<blockquote><p>
  <strong>Abstract:</strong> Maximum Caliber is a principle for inferring pathways and rate distributions of kinetic processes.  The structure and foundations of MaxCal are much like those of Maximum Entropy for static distributions.  We have explored how MaxCal may serve as a general variational principle for nonequilibrium statistical physics &#8211; giving well-known results, such as the Green-Kubo relations, Onsager&#8217;s reciprocal relations and Prigogine&#8217;s Minimum Entropy Production principle near equilibrium, but is also applicable far from equilibrium.  I will also discuss some applications, such as finding reaction coordinates in molecular simulations non-linear dynamics in gene circuits, power-law-tail distributions in &#8220;social-physics&#8221; networks, and others.</p></blockquote>
<h4> <a href="http://eco37.mbl.edu/">Joseph Vallino, Marine Biological Laboratory, Woods Hole</a></h4>
<p><strong>Coauthors:</strong>  Ioannis Tsakalakis, Julie A. Huber</p>
<p><strong>Title:</strong> Using the maximum entropy production principle to understand and predict microbial biogeochemistry</p>
<blockquote><p>
  <strong>Abstract:</strong> Natural microbial communities contain billions of individuals per liter and can exceed a trillion cells per liter in sediments, as well as harbor thousands of species in the same volume. The high species diversity contributes to extensive metabolic functional capabilities to extract chemical energy from the environment, such as methanogenesis, sulfate reduction, anaerobic photosynthesis, chemoautotrophy, and many others, most of which are only expressed by bacteria and archaea. Reductionist modeling of natural communities is problematic, as we lack knowledge on growth kinetics for most organisms and have even less understanding on the mechanisms governing predation, viral lysis, and predator avoidance in these systems. As a result, existing models that describe microbial communities contain dozens to hundreds of parameters, and state variables are extensively aggregated. Overall, the models are little more than non-linear parameter fitting exercises that have limited, to no, extrapolation potential, as there are few principles governing organization and function of complex self-assembling systems. Over the last decade, we have been developing a systems approach that models microbial communities as a distributed metabolic network that focuses on metabolic function rather than describing individuals or species. We use an optimization approach to determine which metabolic functions in the network should be up regulated versus those that should be down regulated based on the non-equilibrium thermodynamics principle of maximum entropy production (MEP). Derived from statistical mechanics, MEP proposes that steady state systems will likely organize to maximize free energy dissipation rate. We have extended this conjecture to apply to non-steady state systems and have proposed that living systems maximize entropy production integrated over time and space, while non-living systems maximize instantaneous entropy production. Our presentation will provide a brief overview of the theory and approach, as well as present several examples of applying MEP to describe the biogeochemistry of microbial systems in laboratory experiments and natural ecosystems.</p></blockquote>
<h4> <a href="https://people.math.wisc.edu/~craciun/">Gheorge Craciun, University of Wisconsin-Madison</a></h4>
<p><strong>Title:</strong> Persistence, permanence, and global stability in reaction network models: some results inspired by thermodynamic principles</p>
<blockquote><p>
  <strong>Abstract:</strong> The standard mathematical model for the dynamics of concentrations in biochemical networks is called mass-action kinetics. We describe mass-action kinetics and discuss the connection between special classes of mass-action systems (such as detailed balanced and complex balanced systems) and the Boltzmann equation. We also discuss the connection between the &#8220;global attractor conjecture&#8221; for complex balanced mass-action systems and Boltzmann&#8217;s H-theorem. We also describe some implications for biochemical mechanisms that implement noise filtering and cellular homeostasis.</p></blockquote>
<h4> <a href="http://faculty.washington.edu/hqian/">Hong Qian, University of Washington</a></h4>
<p><strong>Title:</strong> Large deviations theory and emergent landscapes in biological dynamics</p>
<blockquote><p>
  <strong>Abstract:</strong> The mathematical theory of large deviations provides a nonequilibrium thermodynamic description of complex biological systems that consist of heterogeneous individuals. In terms of the notions of stochastic elementary reactions and pure kinetic species, the continuous-time, integer-valued Markov process dictates a thermodynamic structure that generalizes (i) Gibbs’ macroscopic chemical thermodynamics of equilibrium matters to nonequilibrium small systems such as living cells and tissues; and (ii) Gibbs’ potential function to the landscapes for biological dynamics, such as that of C. H. Waddington’s and S. Wright’s.</p></blockquote>
<h4> <a href="https://hartelab.weebly.com/">John Harte, University of Berkeley</a></h4>
<p><strong>Coauthors:</strong> Micah Brush, Kaito Umemura</p>
<p><strong>Title:</strong> Nonequilibrium dynamics of disturbed ecosystems</p>
<blockquote><p>
  <strong>Abstract:</strong> The Maximum Entropy Theory of Ecology (METE) predicts the shapes of macroecological metrics in relatively static ecosystems, across spatial scales, taxonomic categories, and habitats, using constraints imposed by static state variables. In disturbed ecosystems, however, with time-varying state variables, its predictions often fail. We extend macroecological theory from static to dynamic, by combining the MaxEnt inference procedure with explicit mechanisms governing disturbance. In the static limit, the resulting theory, DynaMETE, reduces to METE but also predicts a new scaling relationship among static state variables. Under disturbances, expressed as shifts in demographic, ontogenic growth, or migration rates, DynaMETE predicts the time trajectories of the state variables as well as the time-varying shapes of macroecological metrics such as the species abundance distribution and the distribution of metabolic rates over individuals. An iterative procedure for solving the dynamic theory is presented. Characteristic signatures of the deviation from static predictions of macroecological patterns are shown to result from different kinds of disturbance. By combining MaxEnt inference with explicit dynamical mechanisms of disturbance, DynaMETE is a candidate theory of macroecology for ecosystems responding to anthropogenic or natural disturbances.</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/05/11/non-equilibrium-thermodynamics-in-biology/#comments">3 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/chemistry/" rel="category tag">chemistry</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/05/11/non-equilibrium-thermodynamics-in-biology/" rel="bookmark" title="Permanent Link to Non-Equilibrium Thermodynamics in Biology (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-30259 post type-post status-publish format-standard hentry category-physics" id="post-30259">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/03/24/the-joy-of-condensed-matter/" rel="bookmark">The Joy of Condensed&nbsp;Matter</a></h2>
				<small>24 March, 2021</small><br />


				<div class="entry">
					<p>I published a slightly different version of this article in <i><a href="https://nautil.us/issue/97/wonder/the-joy-of-condensed-matter">Nautilus</a></i> on February 24, 2021.</p>
<hr />
<p>Everyone seems to be talking about the problems with physics: Woit&#8217;s book <i>Not Even Wrong</i>, Smolin&#8217;s <i>The Trouble With Physics</i> and Hossenfelder&#8217;s <i>Lost in Math</i> leap to mind, and they have started a wider conversation. But is all of physics really in trouble, or just some of it?</p>
<p>If you actually read these books, you&#8217;ll see they&#8217;re about so-called &#8220;fundamental physics&#8221;. Some other parts of physics are doing just fine, and I want to tell you about one. It&#8217;s called &#8220;condensed matter physics&#8221;, and it&#8217;s the study of solids and liquids. We are living in the golden age of condensed matter physics.</p>
<p>But first, what is &#8220;fundamental&#8221; physics? It&#8217;s a tricky term. You might think any truly revolutionary development in physics counts as fundamental. But in fact physicists use this term in a more precise, narrowly delimited way. One of the goals of physics is to figure out some laws that, at least in principle, we could use to predict everything that can be predicted about the physical universe. The search for these laws is fundamental physics.</p>
<p>The fine print is crucial. First: &#8220;at least in principle&#8221;. In principle we can use the fundamental physics we know to calculate the boiling point of water to immense accuracy&#8212;but nobody has done it yet, because the calculation is hard. Second: &#8220;everything that can be predicted&#8221;. As far we can tell, quantum mechanics says there&#8217;s inherent randomness in things, which makes some predictions <i>impossible</i>, not just impractical, to carry out with certainty. And this inherent quantum randomness sometimes gets amplified over time, by a phenomenon called chaos. For this reason, even if we knew everything about the universe now, we couldn&#8217;t predict the weather precisely a year from now.</p>
<p>So even if fundamental physics succeeded perfectly, it would be far from giving the answer to all our questions about the physical world. But it&#8217;s important nonetheless, because it gives us the basic framework in which we can try to answer these questions.</p>
<p>As of now, research in fundamental physics has given us the Standard Model (which seeks to describe matter and all the forces <i>except</i> gravity) and General Relativity (which describes gravity). These theories are tremendously successful, but we know they are not the last word. Big questions remain unanswered&#8212;like the nature of dark matter, or whatever is fooling us into thinking there&#8217;s dark matter.<br />
Unfortunately, progress on these questions has been very slow since the 1990s.</p>
<p>Luckily fundamental physics is not all of physics, and today it is no longer the most exciting part of physics. There is still plenty of mind-blowing new physics being done. And lot of it&#8212;though by no means all&#8212;is condensed matter physics.</p>
<p>Traditionally, the job of condensed matter physics was to predict the properties of solids and liquids found in nature. Sometimes this can be very hard: for example, computing the boiling point of water. But now we know enough fundamental physics to design strange new materials&#8212;and then actually <i>make</i> these materials, and probe their properties with experiments, testing our theories of how they should work. Even better, these experiments can often be done on a table top. There&#8217;s no need for enormous particle accelerators here.</p>
<p>Let&#8217;s look at an example. We&#8217;ll start with the humble &#8220;hole&#8221;. A crystal is a regular array of atoms, each with some electrons orbiting it. When one of these electrons gets knocked off somehow, we get a &#8220;hole&#8221;: an atom with a missing electron. And this hole can actually move around like a particle! When an electron from some neighboring atom moves to fill the hole, the hole moves to the neighboring atom. Imagine a line of people all wearing hats except for one whose head is bare: if their neighbor lends them their hat, the bare head moves to the neighbor. If this keeps happening, the bare head will move down the line of people. The absence of a thing can act like a thing!</p>
<p>The famous physicist Paul Dirac came up with the idea of holes in 1930. He correctly predicted that since electrons have negative electric charge, holes should have positive charge. Dirac was working on fundamental physics: he hoped the proton could be explained as a hole. That turned out not to be true. Later physicists found another particle that could: the &#8220;positron&#8221;. It&#8217;s just like an electron with the opposite charge. And thus antimatter—particles like ordinary matter particles, with the same mass but with the opposite charge—was born. But that&#8217;s another story.</p>
<p>In 1931, Heisenberg applied the idea of holes to condensed matter physics. He realized that just as electrons create an electrical current as they move along, so do holes&#8212;but because they&#8217;re positively charged, their electrical current goes in the other direction! It became clear that holes carry electrical current in some but of the materials called &#8220;semiconductors&#8221;: for example, silicon with a bit of aluminum added to it. After many further developments, in 1948 the physicist William Schockley patented transistors that use both holes and electrons to form a kind of switch. He later won the Nobel prize for this, and now they&#8217;re widely used in computer chips.</p>
<p>Holes in semiconductors are not really particles in the sense of fundamental physics. They are really just a convenient way of thinking about the motion of electrons. But any sufficiently convenient abstraction takes on a life of its own. The equations that describe the behavior of holes are just like the equations that describe the behavior of particles. So, we can treat holes <i>as if</i> they were particles. We&#8217;ve already seen that a hole is positively charged. But because it takes energy to get a hole moving, a hole also acts like it has a mass. And so on: the properties we normally attribute to particles also make sense for holes.</p>
<p>Physicists have a name for things that act like particles even though they&#8217;re really not: &#8220;quasiparticles&#8221;. There are many kinds: holes are just one of the simplest. The beauty of quasiparticles is that we can practically make them to order, having a vast variety of properties. As Michael Nielsen put it, we now live in the era of &#8220;designer matter&#8221;.</p>
<p>For example, consider the &#8220;exciton&#8221;. Since an electron is negatively charged and a hole is positively charged, they attract each other. And if the hole is much heavier than the electron&#8212;remember, a hole has a mass&#8212;an electron can orbit a hole much as an electron orbits a proton in a hydrogen atom. Thus, they form a kind of artificial atom called an exciton. It&#8217;s a ghostly dance of presence and absence!</p>
<div align="center"><a href="https://commons.wikimedia.org/wiki/File:Moving_Wannier_exciton.svg"><img loading="lazy" data-attachment-id="30622" data-permalink="https://johncarlosbaez.wordpress.com/2021/03/24/the-joy-of-condensed-matter/wannier-mott_exciton/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png" data-orig-size="1920,873" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="wannier-mott_exciton" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=450" class="aligncenter size-full wp-image-30622" src="https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=450&#038;h=205" alt="" width="450" height="205" srcset="https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=450&amp;h=205 450w, https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=900&amp;h=410 900w, https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=150&amp;h=68 150w, https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=300&amp;h=136 300w, https://johncarlosbaez.files.wordpress.com/2021/06/wannier-mott_exciton.png?w=768&amp;h=349 768w" sizes="(max-width: 450px) 100vw, 450px" /><br />
This is how an exciton moves through a crystal.</a></div>
<p>The idea of excitons goes back all the way to 1931. By now we can make excitons in large quantities in certain semiconductors. They don&#8217;t last for long: the electron quickly falls back into the hole. It can take between 1 and 10 trillonths of a second for this to happen. But that&#8217;s enough time to do some interesting things.</p>
<p>For example: if you can make an artificial atom, can you make an artificial molecule? Sure! Just as two atoms of hydrogen can stick together and form a molecule, two excitons can stick together and form a &#8220;biexciton&#8221;. An exciton can stick to another hole and form a &#8220;trion&#8221;. An exciton can even stick to a <i>photon</i>&#8212;a particle of light&#8212;and form something called a &#8220;polariton&#8221;. It&#8217;s a blend of matter and light!</p>
<p>Can you make a gas of artificial atoms? Yes! At low densities and high temperatures, excitons zip around very much like atoms in a gas. Can you make a liquid? Again, yes: at higher densities, and colder temperatures, excitons bump into each other enough to act like a liquid. At even colder temperatures, excitons can even form a &#8220;superfluid&#8221;, with almost zero viscosity: if you could somehow get it swirling around, it would go on practically forever.</p>
<p>This is just a small taste of what researchers in condensed matter physics are doing these days. Besides excitons, they are studying a host of other quasiparticles. A &#8220;phonon&#8221; is a quasiparticle of sound formed from vibrations moving through a crystal. A &#8220;magnon&#8221; is a quasiparticle of magnetization: a pulse of electrons in a crystal whose spins have flipped. The list goes on, and becomes ever more esoteric.</p>
<p>But there is also much more to the field than quasiparticles. Physicists can now create materials in which the speed of light is much slower than usual, say 40 miles an hour. They can create materials called &#8220;hyperbolic metamaterials&#8221; in which light moves as if there were two space dimensions and two time dimensions, instead of the usual three dimensions of space and one of time! Normally we think that time can go forward in just one direction, but in these substances light acts as if there&#8217;s a whole circle of directions that count as &#8220;forward in time&#8221;. The possibilities are limited only by our imagination and the fundamental laws of physics.</p>
<p>At this point, usually some skeptic comes along and questions whether these things are <i>useful</i>. Indeed, some of these new materials are likely to be useful. In fact a lot of condensed matter physics, while less glamorous than what I have just described, is carried out precisely to develop new improved computer chips&#8212;and also technologies like &#8220;photonics,&#8221; which uses light instead of electrons. The fruits of photonics are ubiquitous&#8212;it saturates modern technology, like flat-screen TVs&#8212;but physicists are now aiming for more radical applications, like computers that process information using light.</p>
<p>Then typically some other kind of skeptic comes along and asks if condensed matter physics is &#8220;just engineering&#8221;. Of course the very premise of this question is insulting: there is nothing wrong with engineering! Trying to build useful things is not only important in itself, it&#8217;s a great way to raise deep new questions about physics. For example the whole field of thermodynamics, and the idea of entropy, arose in part from trying to build better steam engines. But condensed matter physics is not just engineering. Large portions of it are blue-sky research into the possibilities of matter, like I&#8217;ve been talking about here.</p>
<p>These days, the field of condensed matter physics is just as full of rewarding new insights as the study of elementary particles or black holes. And unlike fundamental physics, progress in condensed matter physics is rapid&#8212;in part because experiments are comparatively cheap and easy, and in part because there is more new territory to explore.</p>
<p>So, when you see someone bemoaning the woes of fundamental physics, take them seriously&#8212;but don&#8217;t let it get you down. Just find a good article on condensed matter physics and read that. You&#8217;ll cheer up immediately.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/03/24/the-joy-of-condensed-matter/#comments">23 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/03/24/the-joy-of-condensed-matter/" rel="bookmark" title="Permanent Link to The Joy of Condensed&nbsp;Matter">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-28874 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-probability" id="post-28874">
				<h2><a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/" rel="bookmark">Fisher&#8217;s Fundamental Theorem (Part&nbsp;3)</a></h2>
				<small>8 October, 2020</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Last time</a> we stated and proved a simple version of Fisher&#8217;s fundamental theorem of natural selection, which says that <i>under some conditions</i>, the rate of increase of the mean fitness equals the variance of the fitness.   But the conditions we gave were very restrictive: namely, that the fitness of each species of replicator is constant, not depending on how many of these replicators there are, or any other replicators.</p>
<p>To broaden the scope of Fisher&#8217;s fundamental theorem we need to do one of two things:</p>
<p>1) change the left side of the equation: talk about some other quantity other than rate of change of mean fitness.</p>
<p>2) change the right side of the question: talk about some other quantity than the variance in fitness.</p>
<p>Or we could do both!   <img src="https://i1.wp.com/math.ucr.edu/home/baez/emoticons/tongue2.gif" />  People have spent a lot of time generalizing Fisher&#8217;s fundamental theorem.   I don&#8217;t think there are, or should be, any hard rules on what counts as a generalization.</p>
<p>But today we&#8217;ll take alternative 1).  We&#8217;ll show the square of something called the &#8216;Fisher speed&#8217; <i>always</i> equals the variance in fitness.  One nice thing about this result is that we can drop the restrictive condition I mentioned.   Another nice thing is that the Fisher speed is a concept from information theory!  It&#8217;s defined using the <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">Fisher metric</a> on the space of probability distributions.</p>
<p>And yes&#8212;that metric is named after the same guy who proved Fisher&#8217;s fundamental theorem!  So, arguably, <i>Fisher</i> should have proved this generalization of Fisher&#8217;s fundamental theorem.  But in fact it seems that I was the first to prove it, around <a href="https://math.ucr.edu/home/baez/information/information_geometry_16.html">February 1st, 2017</a>.   Some similar results were already known, and I will discuss those someday.  But they&#8217;re a bit different.</p>
<p>A good way to think about the Fisher speed is that it&#8217;s &#8216;the rate at which information is being updated&#8217;.   A population of replicators of different species gives a probability distribution.  Like any probability distribution, this has information in it.   As the populations of our replicators change, the Fisher speed says the rate at which this information is being updated.  So, in simple terms, we&#8217;ll show</p>
<blockquote><p>
  The square of the rate at which information is updated is equal to the variance in fitness.</p></blockquote>
<p>This is quite a change from Fisher&#8217;s original idea, namely:</p>
<blockquote><p>
  The rate of increase of mean fitness is equal to the variance in fitness.</p></blockquote>
<p>But it has the advantage of always being true&#8230; as long the population dynamics are described by the general framework we introduced last time.  So let me remind you of the general setup, and then prove the result!</p>
<h3> The setup</h3>
<p>We start out with population functions <img src="https://s0.wp.com/latex.php?latex=P_i+%5Ccolon+%5Cmathbb%7BR%7D+%5Cto+%280%2C%5Cinfty%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i &#92;colon &#92;mathbb{R} &#92;to (0,&#92;infty)," class="latex" /> one for each species of replicator <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C%5Cdots%2Cn%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,&#92;dots,n," class="latex" /> obeying the <b>Lotka–Volterra equation</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) P_i } " class="latex" /></p>
<p>for some differentiable functions <img src="https://s0.wp.com/latex.php?latex=f_i+%5Ccolon+%280%2C%5Cinfty%29+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i &#92;colon (0,&#92;infty) &#92;to &#92;mathbb{R}" class="latex" /> called <b>fitness functions</b>.   The probability of a replicator being in the <i>i</i>th species is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Csum_j+P_j%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  p_i(t) = &#92;frac{P_i(t)}{&#92;sum_j P_j(t)} } " class="latex" /></p>
<p>Using the Lotka–Volterra equation we showed last time that these probabilities obey the <b>replicator equation</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5Cleft%28+f_i%28P%29+-+%5Coverline+f%28P%29+%5Cright%29++p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;left( f_i(P) - &#92;overline f(P) &#92;right)  p_i } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> is short for the whole list of populations <img src="https://s0.wp.com/latex.php?latex=%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(P_1(t), &#92;dots, P_n(t))," class="latex" /> and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Coverline+f%28P%29+%3D+%5Csum_j+f_j%28P%29+p_j++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;overline f(P) = &#92;sum_j f_j(P) p_j  } " class="latex" /></p>
<p>is the <b>mean fitness</b>.</p>
<h3>The Fisher metric</h3>
<p>The space of probability distributions on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}" class="latex" /> is called the <b>(n-1)-simplex</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D+%3D+%5C%7B+%28x_1%2C+%5Cdots%2C+x_n%29+%3A+%5C%3B+x_i+%5Cge+0%2C+%5C%3B+%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+x_i+%3D+1+%7D+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1} = &#92;{ (x_1, &#92;dots, x_n) : &#92;; x_i &#92;ge 0, &#92;; &#92;displaystyle{ &#92;sum_{i=1}^n x_i = 1 } &#92;} " class="latex" /></p>
<p>It&#8217;s called <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1}" class="latex" /> because it&#8217;s (n-1)-dimensional.  When <img src="https://s0.wp.com/latex.php?latex=n+%3D+3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 3" class="latex" /> it looks like the letter <img src="https://s0.wp.com/latex.php?latex=%5CDelta%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta:" class="latex" /></p>
<div align="center">
<a href="https://johncarlosbaez.files.wordpress.com/2020/10/800px-2d-simplex.svg_.png"><img src="https://johncarlosbaez.files.wordpress.com/2020/10/800px-2d-simplex.svg_.png?w=150" alt="" width="150" /></a></div>
<p>The <b>Fisher metric</b> is a Riemannian metric on the interior of the (n-1)-simplex.  That is, given a point <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in the interior of <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1}" class="latex" /> and two tangent vectors <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> at this point the Fisher metric gives a number</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28v%2Cw%29+%3D+%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7Bv_i+w_i%7D%7Bp_i%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(v,w) = &#92;displaystyle{ &#92;sum_{i=1}^n &#92;frac{v_i w_i}{p_i}  } " class="latex" /></p>
<p>Here we are describing the tangent vectors <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> as vectors in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> with the property that the sum of their components is zero: that&#8217;s what makes them tangent to the (n-1)-simplex.  And we&#8217;re demanding that <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> be in the interior of the simplex to avoid dividing by zero, since on the boundary of the simplex we have <img src="https://s0.wp.com/latex.php?latex=p_i+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = 0" class="latex" /> for at least one choice of $i.$</p>
<p>If we have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moving around in the interior of the (n-1)-simplex as a function of time, its <b>Fisher speed</b> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csqrt%7Bg%28%5Cdot%7Bp%7D%28t%29%2C+%5Cdot%7Bp%7D%28t%29%29%7D+%3D+%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%5Cdot%7Bp%7D_i%28t%29%5E2%7D%7Bp_i%28t%29%7D%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sqrt{g(&#92;dot{p}(t), &#92;dot{p}(t))} = &#92;sqrt{&#92;sum_{i=1}^n &#92;frac{&#92;dot{p}_i(t)^2}{p_i(t)}} } " class="latex" /></p>
<p>if the derivative <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}(t)" class="latex" /> exists.   This is the usual formula for the speed of a curve moving in a Riemannian manifold, specialized to the case at hand.</p>
<p>Now we&#8217;ve got all the formulas we&#8217;ll need to prove the result we want.  But for those who don&#8217;t already know and love it, it&#8217;s worthwhile saying a bit more about the Fisher metric.</p>
<p>The factor of <img src="https://s0.wp.com/latex.php?latex=1%2Fx_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/x_i" class="latex" /> in the Fisher metric changes the geometry of the simplex so that it becomes <i>round</i>, like a portion of a sphere:</p>
<div align="center">
<a href="https://johncarlosbaez.files.wordpress.com/2020/10/fisher_metric.jpg"><img src="https://johncarlosbaez.files.wordpress.com/2020/10/fisher_metric.jpg?w=250" alt="" width="250" /></a></div>
<p>But the reason the Fisher metric is important, I think, is its connection to relative information.  Given two probability distributions <img src="https://s0.wp.com/latex.php?latex=p%2C+q+%5Cin+%5CDelta%5E%7Bn-1%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q &#92;in &#92;Delta^{n-1}," class="latex" /> the <b>information of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /></b> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28q%2Cp%29+%3D+%5Csum_%7Bi+%3D+1%7D%5En+q_i+%5Cln%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i%7D%5Cright%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(q,p) = &#92;sum_{i = 1}^n q_i &#92;ln&#92;left(&#92;frac{q_i}{p_i}&#92;right)   } " class="latex" /></p>
<p>You can show this is the expected amount of information gained if <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> was your prior distribution and you receive information that causes you to update your prior to <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   So, sometimes it&#8217;s called the <b>information gain</b>.  It&#8217;s also called <b>relative entropy</b> or&#8212;my least favorite, since it sounds so mysterious&#8212;the <b><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a></b>.</p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> is a smooth curve in the interior of the (n-1)-simplex.  We can ask the rate at which information is gained as time passes.  Perhaps surprisingly, a calculation gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28p%28t%29%2C+p%28t_0%29%29%5CBig%7C_%7Bt+%3D+t_0%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(p(t), p(t_0))&#92;Big|_{t = t_0} = 0 } " class="latex" /></p>
<p>That is, in some sense &#8216;to first order&#8217; no information is being gained at any moment <img src="https://s0.wp.com/latex.php?latex=t_0+%5Cin+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_0 &#92;in &#92;mathbb{R}." class="latex" />   However, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7D+I%28p%28t%29%2C+p%28t_0%29%29%5CBig%7C_%7Bt+%3D+t_0%7D+%3D++g%28%5Cdot%7Bp%7D%28t_0%29%2C+%5Cdot%7Bp%7D%28t_0%29%29%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d^2}{dt^2} I(p(t), p(t_0))&#92;Big|_{t = t_0} =  g(&#92;dot{p}(t_0), &#92;dot{p}(t_0))}  " class="latex" /></p>
<p>So, the square of the Fisher speed has a nice interpretation in terms of relative entropy!</p>
<p>For a derivation of these last two equations, see <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/">Part 7</a> of my posts on information geometry.  For more on the meaning of relative entropy, see <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">Part 6</a>.</p>
<h3> The result</h3>
<p>It&#8217;s now extremely easy to show what we want, but let me state it formally so all the assumptions are crystal clear.</p>
<p><b>Theorem.</b>  Suppose the functions <img src="https://s0.wp.com/latex.php?latex=P_i+%5Ccolon+%5Cmathbb%7BR%7D+%5Cto+%280%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i &#92;colon &#92;mathbb{R} &#92;to (0,&#92;infty)" class="latex" /> obey the Lotka&#8211;Volterra equations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot+P_i+%3D+f_i%28P%29+P_i%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot P_i = f_i(P) P_i}  " class="latex" /></p>
<p>for some differentiable functions <img src="https://s0.wp.com/latex.php?latex=f_i+%5Ccolon+%280%2C%5Cinfty%29%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i &#92;colon (0,&#92;infty)^n &#92;to &#92;mathbb{R}" class="latex" /> called fitness functions.   Define probabilities and the mean fitness as above, and define the <b>variance of the fitness</b> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cmathrm%7BVar%7D%28f%28P%29%29+%3D++%5Csum_j+%28+f_j%28P%29+-+%5Coverline+f%28P%29%29%5E2+%5C%2C+p_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;mathrm{Var}(f(P)) =  &#92;sum_j ( f_j(P) - &#92;overline f(P))^2 &#92;, p_j } " class="latex" /></p>
<p>Then if none of the populations <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> are zero, the square of the Fisher speed of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29+%3D+%28p_1%28t%29%2C+%5Cdots+%2C+p_n%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t) = (p_1(t), &#92;dots , p_n(t))" class="latex" /> is the variance of the fitness:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29++%3D+%5Cmathrm%7BVar%7D%28f%28P%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(&#92;dot{p}, &#92;dot{p})  = &#92;mathrm{Var}(f(P)) " class="latex" /></p>
<p><b>Proof.</b> The proof is near-instantaneous.  We take the square of the Fisher speed:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%5Cdot%7Bp%7D_i%28t%29%5E2%7D%7Bp_i%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ g(&#92;dot{p}, &#92;dot{p}) = &#92;sum_{i=1}^n &#92;frac{&#92;dot{p}_i(t)^2}{p_i(t)} } " class="latex" /></p>
<p>and plug in the replicator equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%28f_i%28P%29+-+%5Coverline+f%28P%29%29+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = (f_i(P) - &#92;overline f(P)) p_i } " class="latex" /></p>
<p>We obtain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29%7D+%26%3D%26++%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%28+f_i%28P%29+-+%5Coverline+f%28P%29+%5Cright%29%5E2+p_i+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cmathrm%7BVar%7D%28f%28P%29%29++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ g(&#92;dot{p}, &#92;dot{p})} &amp;=&amp;  &#92;displaystyle{ &#92;sum_{i=1}^n &#92;left( f_i(P) - &#92;overline f(P) &#92;right)^2 p_i } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;mathrm{Var}(f(P))  &#92;end{array} " class="latex" /></p>
<p>as desired.   &nbsp; █</p>
<p>It&#8217;s hard to imagine anything simpler than this.  We see that given the Lotka–Volterra equation, what causes information to be updated is nothing more and nothing less than variance in fitness!</p>
<hr />
<p>The whole series:</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-1/">Part 1</a>: the obscurity of Fisher&#8217;s original paper.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Part 2</a>: a precise statement of Fisher&#8217;s fundamental theorem of natural selection, and conditions under which it holds.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/">Part 3</a>: a modified version of the fundamental theorem of natural selection, which holds much more generally.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/">Part 4</a>: my paper on the fundamental theorem of natural selection.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comments">10 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/" rel="bookmark" title="Permanent Link to Fisher&#8217;s Fundamental Theorem (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-28650 post type-post status-publish format-standard hentry category-biodiversity category-information-and-entropy" id="post-28650">
				<h2><a href="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/" rel="bookmark">Ascendancy vs. Reserve</a></h2>
				<small>22 September, 2020</small><br />


				<div class="entry">
					<p>Why is biodiversity &#8216;good&#8217;?    To what extent is this sort of goodness even relevant to ecosystems&#8212;as opposed to us humans?  I&#8217;d like to study this mathematically.</p>
<p>To do this, we&#8217;d need to extract some answerable questions out of the morass of subtlety and complexity.    For example: what role does biodiversity play in the ability of ecosystems to be robust under sudden changes of external conditions?   This is already plenty hard to study mathematically, since it requires understanding &#8216;biodiversity&#8217; and &#8216;robustness&#8217;.</p>
<p>Luckily there has already been a lot of work on the mathematics of biodiversity and its connection to entropy.  For example:</p>
<p>• Tom Leinster, <a href="https://johncarlosbaez.wordpress.com/2011/11/07/measuring-biodiversity/">Measuring biodiversity</a>, <em>Azimuth</em>, 7 November 2011.</p>
<p>But how does biodiversity help robustness?</p>
<p>There&#8217;s been a lot of work on this.   This paper has some inspiring passages:</p>
<p>• Robert E. Ulanowicz,, Sally J. Goerner, Bernard Lietaer and Rocio Gomez, <a href="http://wtf.tw/ref/ulanowicz.pdf">Quantifying sustainability: Resilience, efficiency and the return of information theory</a>, <em>Ecological Complexity</em> <strong>6</strong> (2009), 27&#8211;36.</p>
<p>I&#8217;m not sure the math lives up to their claims, but I like these lines:</p>
<blockquote><p>
  In other words, (14) says that the capacity for a system to undergo evolutionary change or self-organization consists of two aspects: It must be capable of exercising sufficient directed power (ascendancy) to maintain its integrity over time. Simultaneously, it must possess a reserve of flexible actions that can be used to meet the exigencies of novel disturbances. According to (14) these two aspects are literally complementary.</p></blockquote>
<p>The two aspects are &#8216;ascendancy&#8217;, which is something like efficiency or being optimized, and &#8216;reserve capacity&#8217;, which is something like random junk that might come in handy if something unexpected comes up.</p>
<p>You know those gadgets you kept in the back of your kitchen drawer and never needed&#8230; until you did?   If you&#8217;re aiming for &#8216;ascendancy&#8217; you&#8217;d clear out those drawers.   But if you keep that stuff, you&#8217;ve got more &#8216;reserve capacity&#8217;.  They both have their good points.   Ideally you want to strike a wise balance.  You&#8217;ve probably sensed this every time you clean out your house: should I keep this thing because I might need it, or should I get rid of it?</p>
<p>I think it would be great to make these concepts precise.  The paper at hand attempts this by taking a matrix of nonnegative numbers <img src="https://s0.wp.com/latex.php?latex=T_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_{i j}" class="latex" /> to describe flows in an ecological network.   They define a kind of entropy for this matrix, very similar in look to <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>. Then they write this as a sum of two parts: a part closely analogous to <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a>, and a part closely analogous to <a href="https://en.wikipedia.org/wiki/Conditional_entropy">conditional entropy</a>.   This decomposition is standard in information theory.   This is their equation (14).</p>
<p>If you want to learn more about the underlying math, click on this picture:</p>
<p><a href="http://www.info612.ece.mcgill.ca/lecture_02.pdf"><img loading="lazy" data-attachment-id="28656" data-permalink="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/entropy-mutual-information-relative-entropy-relation-diagram/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png" data-orig-size="500,308" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Entropy-mutual-information-relative-entropy-relation-diagram" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=450" src="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=450&#038;h=277" alt="" class="aligncenter size-full wp-image-28656" width="450" height="277" srcset="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=450&amp;h=277 450w, https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=150&amp;h=92 150w, https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=300&amp;h=185 300w, https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png 500w" sizes="(max-width: 450px) 100vw, 450px"/></a></p>
<p>The new idea of these authors is that in the context of an ecological network, the mutual information can be understood as &#8216;ascendancy&#8217;, while the conditional entropy can be understood as &#8216;reserve capacity&#8217;.</p>
<p>I don&#8217;t know if I believe this!   But I like the general idea of a balance between ascendancy and reserve capacity.</p>
<p>They write:</p>
<blockquote><p>
  While the dynamics of this dialectic interaction can be quite subtle and highly complex, one thing is boldly clear—systems with either vanishingly small ascendancy or insignificant reserves are destined to perish before long. A system lacking ascendancy has neither the extent of activity nor the internal organization needed to survive. By contrast, systems that are so tightly constrained and honed to a particular environment appear ‘‘brittle’’ in the sense of Holling (1986) or ‘‘senescent’’ in the sense of Salthe (1993) and are prone to collapse in the face of even minor novel disturbances. Systems that endure&#8212;that is, are sustainable&#8212;lie somewhere between these extremes.  But, where?</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/#comments">11 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/" rel="bookmark" title="Permanent Link to Ascendancy vs. Reserve">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-24795 post type-post status-publish format-standard hentry category-chemistry category-information-and-entropy category-physics" id="post-24795">
				<h2><a href="https://johncarlosbaez.wordpress.com/2018/05/08/effective-thermodynamics-for-a-marginal-observer/" rel="bookmark">Effective Thermodynamics for a Marginal&nbsp;Observer</a></h2>
				<small>8 May, 2018</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="https://tomate.wordpress.com/">Matteo Polettini</a></b></i></p>
<p>Suppose you receive an email from someone who claims &#8220;here is the project of a machine that runs forever and ever and produces energy for free!&#8221;  Obviously he must be a crackpot. But he may be well-intentioned. You opt for not being rude, roll your sleeves, and put your hands into the dirt, holding the Second Law as lodestar.</p>
<p>Keep in mind that there are two fundamental sources of error: either he is not considering certain input currents (&#8220;hey, what about that tiny hidden cable entering your machine from the electrical power line?!&#8221;, &#8220;uh, ah, that&#8217;s just to power the &#8220;ON&#8221; LED&#8221;, &#8220;mmmhh, you sure?&#8221;), or else he is not measuring the energy input correctly (&#8220;hey, why are you using a Geiger counter to measure input voltages?!&#8221;, &#8220;well, sir, I ran out of voltmeters&#8230;&#8221;).</p>
<p>In other words, the observer might only have partial information about the setup, either in quantity or quality. Because he has been marginalized by society (most crackpots believe they are misunderstood geniuses) we will call such observer &#8220;marginal,&#8221; which incidentally is also the word that mathematicians use when they focus on the probability of a subset of stochastic variables.</p>
<p>In fact, our modern understanding of thermodynamics as embodied in statistical mechanics and stochastic processes is founded (and funded) on ignorance: we never really have &#8220;complete&#8221; information.  If we actually had, all energy would look alike, it would not come in &#8220;more refined&#8221; and &#8220;less refined&#8221; forms, there would not be a differentials of order/disorder (using Paul Valery&#8217;s <a href="https://tomate.wordpress.com/2018/03/23/lidee-fixe/">beautiful words</a>), and that would end thermodynamic reasoning, the energy problem, and generous research grants altogether.</p>
<p>Even worse, within this statistical approach we might be missing chunks of information because some parts of the system are invisible to us. But then, what warrants that <em> we </em>are doing things right, and <em>he</em> (our correspondent) is the crackpot? Couldn&#8217;t it be the other way around? Here I would like to present some <a href="#refs">recent ideas</a> I&#8217;ve been working on together with some collaborators on how to deal with incomplete information about the sources of dissipation of a thermodynamic system. I will do this in a quite theoretical manner, but somehow I will mimic the guidelines suggested above for debunking crackpots. My three buzzwords will be: <strong>marginal</strong>, <strong>effective</strong>, and <strong>operational</strong>.</p>
<h3>&#8220;Complete&#8221; thermodynamics: an out-of-the-box view</h3>
<p>The laws of thermodynamics that I address are:</p>
<p>&bull; The good ol&#8217; Second Law (2nd)</p>
<p>&bull; The <a href="https://en.wikipedia.org/wiki/Fluctuation-dissipation_theorem">Fluctuation-Dissipation Relation</a> (FDR), and the <a href="https://en.wikipedia.org/wiki/Onsager_reciprocal_relations">Reciprocal Relation</a> (RR) close to equilibrium.</p>
<p>&bull; The more recent <a href="https://en.wikipedia.org/wiki/Fluctuation_theorem">Fluctuation Relation</a> (FR)<sup>1</sup> and its corollary the Integral Fluctuation Relation (IFR), which have been discussed on this blog in a <a href="https://johncarlosbaez.wordpress.com/2012/10/08/the-mathematical-origin-of-irreversibility/">remarkable post</a> by Matteo Smerlak.</p>
<p>The list above is all in the &#8220;area of the second law&#8221;. How about the other laws? Well, thermodynamics has for long been a phenomenological science, a patchwork.  So-called <a href="https://tomate.wordpress.com/2017/11/21/thermodynamics-done-right-lack-of-pedagogical-material/">stochastic thermodynamics</a> is trying to put some order in it by systematically grounding thermodynamic claims in (mostly Markov) stochastic processes. But it&#8217;s not an easy task, because the different laws of thermodynamics live in somewhat different conceptual planes. And it&#8217;s not even clear if they are theorems, prescriptions, or habits (a bit like in jurisprudence<sup>2</sup>).</p>
<p>Within stochastic thermodynamics, the Zeroth Law is so easy nobody cares to formulate it (I do, so stay tuned&#8230;). The Third Law: no idea, let me know. As regards the First Law (or, better, &#8220;laws&#8221;, as many as there are conserved quantities across the system/environment interface&#8230;), we will assume that all related symmetries have been exploited from the offset to boil down the description to a minimum.</p>
<div align="center">
<img src="https://tomate.files.wordpress.com/2018/05/1.png?w=440" alt="1" width="440" /></div>
<p>This minimum is as follows. We identify a system that is well separated from its environment. The system evolves in time, the environment is so large that its state does not evolve within the timescales of the system<sup>3</sup>. When tracing out the environment from the description, an uncertainty falls upon the system&#8217;s evolution. We assume the system&#8217;s dynamics to be described by a stochastic Markovian process.</p>
<p>How exactly the system evolves and what is the relationship between system and environment will be described in more detail below. Here let us take an &#8220;out of the box&#8221; view. We resolve the environment into several reservoirs labeled by index <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />. Each of these reservoirs is &#8220;at equilibrium&#8221; on its own (whatever that means<sup>4</sup>). Now, the idea is that each reservoir tries to impose &#8220;its own equilibrium&#8221; on the system, and that their competition leads to a flow of currents across the system/environment interface. Each time an amount of the reservoir&#8217;s resource crosses the interface, a &#8220;thermodynamic cost&#8221; has to be to be paid or gained (be it a chemical potential difference for a molecule to go through a membrane, or a temperature gradient for photons to be emitted/absorbed, etc.).</p>
<p>The fundamental quantities of stochastic thermodynamic modeling thus are:</p>
<p>&bull; On the &#8220;-dynamic&#8221; side: the time-integrated currents <img src="https://s0.wp.com/latex.php?latex=%5CPhi%5Et_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Phi^t_&#92;alpha" class="latex" />, <em>independent</em> among themselves<sup>5</sup>. Currents are stochastic variables distributed with joint probability density</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=P%28%5C%7B%5CPhi_%5Calpha%5C%7D_%5Calpha%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;{&#92;Phi_&#92;alpha&#92;}_&#92;alpha)" class="latex" /></p>
<p>&bull; On the &#8220;thermo-&#8221; side: The so-called thermodynamic forces or &#8220;affinities&#8221;<sup>6</sup> <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BA%7D_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{A}_&#92;alpha" class="latex" />  (collectively denoted <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BA%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{A}" class="latex" />). These are tunable parameters that characterize reservoir-to-reservoir gradients, and they are not stochastic. For convenience, we conventionally take them all positive.</p>
<p>Dissipation is quantified by the <b>entropy production</b>:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cmathcal%7BA%7D_%5Calpha+%5CPhi%5Et_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum &#92;mathcal{A}_&#92;alpha &#92;Phi^t_&#92;alpha" class="latex" /></p>
<p>We are finally in the position to state the main results. Be warned that in the following expressions the exact treatment of time and its scaling would require a lot of specifications, but keep in mind that all these relations hold true in the long-time limit, and that all cumulants scale linearly with time.</p>
<p>&bull; <strong>FR</strong>: The probability of observing positive currents is exponentially favoured with respect to negative currents according to</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=P%28%5C%7B%5CPhi_%5Calpha%5C%7D_%5Calpha%29+%2F+P%28%5C%7B-%5CPhi_%5Calpha%5C%7D_%5Calpha%29+%3D+%5Cexp+%5Csum+%5Cmathcal%7BA%7D_%5Calpha+%5CPhi%5Et_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;{&#92;Phi_&#92;alpha&#92;}_&#92;alpha) / P(&#92;{-&#92;Phi_&#92;alpha&#92;}_&#92;alpha) = &#92;exp &#92;sum &#92;mathcal{A}_&#92;alpha &#92;Phi^t_&#92;alpha" class="latex" /></p>
<p style="padding-left:30px;">
<p><em>Comment</em>: This is not trivial, it follows from the explicit expression of the path integral, see below.</p>
<p>&bull; <strong>IFR</strong>: The exponential of minus the entropy production is unity</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cbig%5Clangle++%5Cexp+-+%5Csum+%5Cmathcal%7BA%7D_%5Calpha+%5CPhi%5Et_%5Calpha++%5Cbig%5Crangle_%7B%5Cmathcal%7BA%7D%7D+%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;big&#92;langle  &#92;exp - &#92;sum &#92;mathcal{A}_&#92;alpha &#92;Phi^t_&#92;alpha  &#92;big&#92;rangle_{&#92;mathcal{A}} =1" class="latex" /></p>
<p style="padding-left:30px;">
<p><em>Homework</em>: Derive this relation from the FR in one line.</p>
<p>&bull; <strong>2nd Law</strong>: The average entropy production is not negative</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cmathcal%7BA%7D_%5Calpha+%5Cleft%5Clangle+%5CPhi%5Et_%5Calpha+%5Cright%5Crangle_%7B%5Cmathcal%7BA%7D%7D+%5Cgeq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum &#92;mathcal{A}_&#92;alpha &#92;left&#92;langle &#92;Phi^t_&#92;alpha &#92;right&#92;rangle_{&#92;mathcal{A}} &#92;geq 0" class="latex" /></p>
<p style="padding-left:30px;">
<p><em>Homework</em>: Derive this relation using Jensen&#8217;s inequality.</p>
<p>&bull; <strong>Equilibrium</strong>: Average currents vanish if and only if affinities vanish:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5Clangle+%5CPhi%5Et_%5Calpha+%5Cright%5Crangle_%7B%5Cmathcal%7BA%7D%7D+%5Cequiv+0%2C+%5Cforall+%5Calpha+%5Ciff++%5Cmathcal%7BA%7D_%5Calpha+%5Cequiv+0%2C+%5Cforall+%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left&#92;langle &#92;Phi^t_&#92;alpha &#92;right&#92;rangle_{&#92;mathcal{A}} &#92;equiv 0, &#92;forall &#92;alpha &#92;iff  &#92;mathcal{A}_&#92;alpha &#92;equiv 0, &#92;forall &#92;alpha" class="latex" /></p>
<p style="padding-left:30px;">
<p><em>Homework</em>: Derive this relation taking the first derivative w.r.t.  <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BA%7D_%5Calpha%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{&#92;mathcal{A}_&#92;alpha}" class="latex" /> of the IFR. Notice that also the average depends on the affinities.</p>
<p>&bull; <strong>S-FDR</strong>: At equilibrium, it is impossible to tell whether a current is due to a spontaneous fluctuation (quantified by its variance) or to an external perturbation (quantified by<em> </em>the response of its mean). In a symmetrized (S-) version:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft.++%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cmathcal%7BA%7D_%5Calpha%7D%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Calpha%27%7D+%5Cright%5Crangle+%5Cright%7C_%7B0%7D+%2B+%5Cleft.++%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cmathcal%7BA%7D_%7B%5Calpha%27%7D%7D%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Calpha%7D+%5Cright%5Crangle+%5Cright%7C_%7B0%7D+%3D+%5Cleft.+%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Calpha%7D+%5CPhi%5Et_%7B%5Calpha%27%7D+%5Cright%5Crangle+%5Cright%7C_%7B0%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left.  &#92;frac{&#92;partial}{&#92;partial &#92;mathcal{A}_&#92;alpha}&#92;left&#92;langle &#92;Phi^t_{&#92;alpha&#039;} &#92;right&#92;rangle &#92;right|_{0} + &#92;left.  &#92;frac{&#92;partial}{&#92;partial &#92;mathcal{A}_{&#92;alpha&#039;}}&#92;left&#92;langle &#92;Phi^t_{&#92;alpha} &#92;right&#92;rangle &#92;right|_{0} = &#92;left. &#92;left&#92;langle &#92;Phi^t_{&#92;alpha} &#92;Phi^t_{&#92;alpha&#039;} &#92;right&#92;rangle &#92;right|_{0}" class="latex" /></p>
<p style="padding-left:30px;">
<p><em>Homework</em>: Derive this relation taking the mixed second derivatives w.r.t.  <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BA%7D_%5Calpha%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{&#92;mathcal{A}_&#92;alpha}" class="latex" /> of the IFR.</p>
<p>&bull; <strong>RR</strong>: The reciprocal response of two different currents to a perturbation of the reciprocal affinities close to equilibrium is symmetrical:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft.++%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cmathcal%7BA%7D_%5Calpha%7D%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Calpha%27%7D+%5Cright%5Crangle+%5Cright%7C_%7B0%7D+-+%5Cleft.++%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cmathcal%7BA%7D_%7B%5Calpha%27%7D%7D%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Calpha%7D+%5Cright%5Crangle+%5Cright%7C_%7B0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left.  &#92;frac{&#92;partial}{&#92;partial &#92;mathcal{A}_&#92;alpha}&#92;left&#92;langle &#92;Phi^t_{&#92;alpha&#039;} &#92;right&#92;rangle &#92;right|_{0} - &#92;left.  &#92;frac{&#92;partial}{&#92;partial &#92;mathcal{A}_{&#92;alpha&#039;}}&#92;left&#92;langle &#92;Phi^t_{&#92;alpha} &#92;right&#92;rangle &#92;right|_{0} = 0" class="latex" /></p>
<p style="padding-left:30px;">
<p><em>Homework</em>: Derive this relation taking the mixed second derivatives w.r.t.  <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BA%7D_%5Calpha%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{&#92;mathcal{A}_&#92;alpha}" class="latex" /> of the FR.</p>
<p>Notice the implication scheme: FR &rArr; IFR &rArr; 2nd, IFR &rArr; S-FDR, FR &rArr; RR.</p>
<h3>&#8220;Marginal&#8221; thermodynamics (still out-of-the-box)</h3>
<p>Now we assume that we can only measure a marginal subset of currents <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5CPhi_%5Cmu%5Et%5C%7D_%5Cmu+%5Csubset+%5C%7B%5CPhi_%5Calpha%5Et%5C%7D_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{&#92;Phi_&#92;mu^t&#92;}_&#92;mu &#92;subset &#92;{&#92;Phi_&#92;alpha^t&#92;}_&#92;alpha" class="latex" /> (index <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> always has a smaller range than <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" />), distributed with joint marginal probability</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=P%28%5C%7B%5CPhi_%5Cmu%5C%7D_%5Cmu%29+%3D+%5Cint+%5Cprod_%7B%5Calpha+%5Cneq+%5Cmu%7D+d%5CPhi_%5Calpha+%5C%2C+P%28%5C%7B%5CPhi_%5Calpha%5C%7D_%5Calpha%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;{&#92;Phi_&#92;mu&#92;}_&#92;mu) = &#92;int &#92;prod_{&#92;alpha &#92;neq &#92;mu} d&#92;Phi_&#92;alpha &#92;, P(&#92;{&#92;Phi_&#92;alpha&#92;}_&#92;alpha) " class="latex" /></p>
<div align="center">
<img src="https://tomate.files.wordpress.com/2018/05/21.png?w=440" alt="2" width="440" /></div>
<p>Notice that a state where these marginal currents vanish might not be an equilibrium, because other currents might still be whirling around. We call this a <em>stalling</em> state.</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bstalling%3A%7D+%5Cqquad+%5Clangle+%5CPhi_%5Cmu+%5Crangle+%5Cequiv+0%2C++%5Cquad+%5Cforall+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{stalling:} &#92;qquad &#92;langle &#92;Phi_&#92;mu &#92;rangle &#92;equiv 0,  &#92;quad &#92;forall &#92;mu" class="latex" /></p>
<p>My central question is: <i>can we associate to these currents some effective affinity <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{Q}_&#92;mu" class="latex" /> in such a way that at least some of the results above still hold true? And, are all definitions involved just a fancy mathematical construct, or are they operational?</i></p>
<p>First the bad news: In general the FR is violated for all choices of effective affinities:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=P%28%5C%7B%5CPhi_%5Cmu%5C%7D_%5Cmu%29+%2F+P%28%5C%7B-%5CPhi_%5Cmu%5C%7D_%5Cmu%29+%5Cneq+%5Cexp+%5Csum+%5Cmathcal%7BQ%7D_%5Cmu+%5CPhi%5Et_%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;{&#92;Phi_&#92;mu&#92;}_&#92;mu) / P(&#92;{-&#92;Phi_&#92;mu&#92;}_&#92;mu) &#92;neq &#92;exp &#92;sum &#92;mathcal{Q}_&#92;mu &#92;Phi^t_&#92;mu" class="latex" /></p>
<p>This is not surprising and nobody would expect that. How about the IFR?</p>
<p>&bull; <strong>Marginal IFR</strong>: There are effective affinities such that</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5Clangle+%5Cexp+-+%5Csum+%5Cmathcal%7BQ%7D_%5Cmu+%5CPhi%5Et_%5Cmu+%5Cright%5Crangle_%7B%5Cmathcal%7BA%7D%7D+%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left&#92;langle &#92;exp - &#92;sum &#92;mathcal{Q}_&#92;mu &#92;Phi^t_&#92;mu &#92;right&#92;rangle_{&#92;mathcal{A}} =1" class="latex" /></p>
<p>Mmmhh. Yeah. Take a closer look this expression: can you see why there actually exists an infinite choice of &#8220;effective affinities&#8221; that would make that average cross 1? Which on the other hand is just a number, so who even cares? So this can&#8217;t be the point.</p>
<p>The fact is, the IFR per se is hardly of any practical interest, as are all &#8220;absolutes&#8221; in physics. What matters is &#8220;relatives&#8221;: in our case, response. But then we need to specify how the effective affinities depend on the &#8220;real&#8221; affinities. And here steps in a crucial technicality, whose precise argumentation is a pain. Basing on reasonable assumptions<sup>7</sup>, we demonstrate that the IFR holds for the following choice of effective affinities:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%5Cmu+%3D+%5Cmathcal%7BA%7D_%5Cmu+-+%5Cmathcal%7BA%7D%5E%7B%5Cmathrm%7Bstalling%7D%7D_%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{Q}_&#92;mu = &#92;mathcal{A}_&#92;mu - &#92;mathcal{A}^{&#92;mathrm{stalling}}_&#92;mu" class="latex" />,</p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BA%7D%5E%7B%5Cmathrm%7Bstalling%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{A}^{&#92;mathrm{stalling}}" class="latex" /> is the set of values of the affinities that make marginal currents stall. Notice that this latter formula gives an <em>operational</em> definition of the effective affinities that could in principle be reproduced in laboratory (just go out there and tune the tunable until everything stalls, and measure the difference). Obviously:</p>
<p>&bull; <strong>Stalling</strong>: Marginal currents vanish  if and only if effective affinities vanish:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5Clangle+%5CPhi%5Et_%5Cmu+%5Cright%5Crangle_%7B%5Cmathcal%7BA%7D%7D+%5Cequiv+0%2C+%5Cforall+%5Cmu+%5Ciff+%5Cmathcal%7BA%7D_%5Cmu+%5Cequiv+0%2C+%5Cforall+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left&#92;langle &#92;Phi^t_&#92;mu &#92;right&#92;rangle_{&#92;mathcal{A}} &#92;equiv 0, &#92;forall &#92;mu &#92;iff &#92;mathcal{A}_&#92;mu &#92;equiv 0, &#92;forall &#92;mu" class="latex" /></p>
<p>Now, according to the inference scheme illustrated above, we can also prove that:</p>
<p>&bull; <strong>Effective 2nd Law</strong>: The average marginal entropy production is not negative</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cmathcal%7BQ%7D_%5Cmu+%5Cleft%5Clangle+%5CPhi%5Et_%5Cmu+%5Cright%5Crangle_%7B%5Cmathcal%7BA%7D%7D+%5Cgeq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum &#92;mathcal{Q}_&#92;mu &#92;left&#92;langle &#92;Phi^t_&#92;mu &#92;right&#92;rangle_{&#92;mathcal{A}} &#92;geq 0" class="latex" /></p>
<p>&bull; <strong>S-FDR at stalling</strong>:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cmathcal%7BA%7D_%5Cmu%7D%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Cmu%27%7D+%5Cright%5Crangle+%5Cright%7C_%7B%5Cmathcal%7BA%7D%5E%7B%5Cmathrm%7Bstalling%7D%7D%7D+%2B+%5Cleft.+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cmathcal%7BA%7D_%7B%5Cmu%27%7D%7D%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Cmu%7D+%5Cright%5Crangle+%5Cright%7C_%7B%5Cmathcal%7BA%7D%5E%7B%5Cmathrm%7Bstalling%7D%7D%7D+%3D+%5Cleft.+%5Cleft%5Clangle+%5CPhi%5Et_%7B%5Cmu%7D+%5CPhi%5Et_%7B%5Cmu%27%7D+%5Cright%5Crangle+%5Cright%7C_%7B%5Cmathcal%7BA%7D%5E%7B%5Cmathrm%7Bstalling%7D%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;frac{&#92;partial}{&#92;partial &#92;mathcal{A}_&#92;mu}&#92;left&#92;langle &#92;Phi^t_{&#92;mu&#039;} &#92;right&#92;rangle &#92;right|_{&#92;mathcal{A}^{&#92;mathrm{stalling}}} + &#92;left. &#92;frac{&#92;partial}{&#92;partial &#92;mathcal{A}_{&#92;mu&#039;}}&#92;left&#92;langle &#92;Phi^t_{&#92;mu} &#92;right&#92;rangle &#92;right|_{&#92;mathcal{A}^{&#92;mathrm{stalling}}} = &#92;left. &#92;left&#92;langle &#92;Phi^t_{&#92;mu} &#92;Phi^t_{&#92;mu&#039;} &#92;right&#92;rangle &#92;right|_{&#92;mathcal{A}^{&#92;mathrm{stalling}}}" class="latex" /></p>
<p>Notice instead that the RR is gone at stalling. This is a clear-cut prediction of the theory that can be experimented with basically the same apparatus with which response theory has been experimentally studied so far (not that I actually know what these apparatus are&#8230;): <i>at stalling states, differing from equilibrium states, the S-FDR still holds, but the RR does not</i>.</p>
<h3>Into the box</h3>
<p>You&#8217;ve definitely gotten enough at this point, and you can give up here. Please <a href="#refs">exit through the gift shop</a>.</p>
<p>If you&#8217;re stubborn, let me tell you what&#8217;s inside the box. The system&#8217;s dynamics is modeled as a <strong>continuous-time, discrete configuration-space Markov &#8220;jump&#8221; process</strong>. The state space can be described by a graph <img src="https://s0.wp.com/latex.php?latex=G%3D%28I%2C+E%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G=(I, E)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=I&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I" class="latex" /> is the set of configurations, <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> is the set of possible transitions or &#8220;edges&#8221;, and there exists some incidence relation between edges and couples of configurations. The process is determined by the rates <img src="https://s0.wp.com/latex.php?latex=w_%7Bi+%5Cgets+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w_{i &#92;gets j}" class="latex" /> of jumping from one configuration to another.</p>
<p>We choose these processes because they allow some nice network analysis and because the path integral is well defined! A single realization of such a process is a <b>trajectory</b></p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Comega%5Et+%3D+%28i_0%2C%5Ctau_0%29+%5Cto+%28i_1%2C%5Ctau_1%29+%5Cto+%5Cldots+%5Cto+%28i_N%2C%5Ctau_N%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega^t = (i_0,&#92;tau_0) &#92;to (i_1,&#92;tau_1) &#92;to &#92;ldots &#92;to (i_N,&#92;tau_N)" class="latex" /></p>
<p>A &#8220;Markovian jumper&#8221; waits at some configuration <img src="https://s0.wp.com/latex.php?latex=i_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i_n" class="latex" /> for some time <img src="https://s0.wp.com/latex.php?latex=%5Ctau_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau_n" class="latex" /> with an exponentially decaying probability <img src="https://s0.wp.com/latex.php?latex=w_%7Bi_n%7D+%5Cexp+-+w_%7Bi_n%7D+%5Ctau_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w_{i_n} &#92;exp - w_{i_n} &#92;tau_n" class="latex" /> with exit rate <img src="https://s0.wp.com/latex.php?latex=w_i+%3D+%5Csum_k+w_%7Bk+%5Cgets+i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w_i = &#92;sum_k w_{k &#92;gets i}" class="latex" />, then instantaneously jumps to a new configuration <img src="https://s0.wp.com/latex.php?latex=i_%7Bn%2B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i_{n+1}" class="latex" /> with transition probability <img src="https://s0.wp.com/latex.php?latex=w_%7Bi_%7Bn%2B1%7D+%5Cgets+%7Bi_n%7D%7D%2Fw_%7Bi_n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w_{i_{n+1} &#92;gets {i_n}}/w_{i_n}" class="latex" />. The overall probability density of a single trajectory is given by</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=P%28%5Comega%5Et%29+%3D+%5Cdelta+%5Cleft%28t+-+%5Csum_n+%5Ctau_n+%5Cright%29+e%5E%7B-+w_%7Bi_N%7D%5Ctau_%7Bi_N%7D%7D+%5Cprod_%7Bn%3D0%7D%5E%7BN-1%7D+w_%7Bj_n+%5Cgets+i_n%7D+e%5E%7B-+w_%7Bi_n%7D+%5Ctau_%7Bi_n%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;omega^t) = &#92;delta &#92;left(t - &#92;sum_n &#92;tau_n &#92;right) e^{- w_{i_N}&#92;tau_{i_N}} &#92;prod_{n=0}^{N-1} w_{j_n &#92;gets i_n} e^{- w_{i_n} &#92;tau_{i_n}}" class="latex" /></p>
<p>One can in principle obtain the probability distribution function of any observable defined along the trajectory by taking the marginal of this measure (though in most cases this is technically impossible). Where does this expression come from? For a formal derivation, see the very beautiful review paper by Weber and Frey, but be aware that this is what one would intuitively come up with if one had to simulate with the Gillespie algorithm.</p>
<p>The dynamics of the Markov process can also be described by the probability of being at some configuration <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />, which evolves via the <b>master equation</b></p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i%28t%29+%3D+%5Csum_j+%5Cleft%5B+w_%7Bij%7D+p_j%28t%29+-+w_%7Bji%7D+p_i%28t%29+%5Cright%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i(t) = &#92;sum_j &#92;left[ w_{ij} p_j(t) - w_{ji} p_i(t) &#92;right]" class="latex" />.</p>
<p>We call such probability the system&#8217;s <b>state</b>, and we assume that the system relaxes to a uniquely defined steady state <img src="https://s0.wp.com/latex.php?latex=p+%3D+%5Cmathrm%7Blim%7D_%7Bt+%5Cto+%5Cinfty%7D+p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = &#92;mathrm{lim}_{t &#92;to &#92;infty} p(t)" class="latex" />.</p>
<p>A time-integrated current along a single trajectory is a linear combination of the net number of jumps <img src="https://s0.wp.com/latex.php?latex=%5C%23%5Et&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;#^t" class="latex" /> between configurations in the network:</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5CPhi%5Et_%5Calpha+%3D+%5Csum_%7Bij%7D+C%5E%7Bij%7D_%5Calpha+%5Cleft%5B+%5C%23%5Et%28i+%5Cgets+j%29+-+%5C%23%5Et%28j%5Cgets+i%29+%5Cright%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Phi^t_&#92;alpha = &#92;sum_{ij} C^{ij}_&#92;alpha &#92;left[ &#92;#^t(i &#92;gets j) - &#92;#^t(j&#92;gets i) &#92;right]" class="latex" /></p>
<p>The idea here is that one or several transitions within the system occur because of the &#8220;absorption&#8221; or the &#8220;emission&#8221; of some environmental degrees of freedom, each with different intensity. However, for the moment let us simplify the picture and require that only one transition contributes to a current, that is that there exist <img src="https://s0.wp.com/latex.php?latex=i_%5Calpha%2Cj_%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i_&#92;alpha,j_&#92;alpha" class="latex" /> such that</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=C%5E%7Bij%7D_%5Calpha+%3D+%5Cdelta%5Ei_%7Bi_%5Calpha%7D+%5Cdelta%5Ej_%7Bj_%5Calpha%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C^{ij}_&#92;alpha = &#92;delta^i_{i_&#92;alpha} &#92;delta^j_{j_&#92;alpha}" class="latex" />.</p>
<p>Now, what does it mean for such a set of currents to be &#8220;complete&#8221;? Here we get inspiration from Kirchhoff&#8217;s Current Law in electrical circuits: the continuity of the trajectory at each configuration of the network implies that after a sufficiently long time, <em>cycle </em>or <em>loop </em> or <em>mesh </em>currents completely describe the steady state. There is a standard procedure to identify a set of cycle currents: take a spanning tree <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> of the network; then the currents flowing along the edges <img src="https://s0.wp.com/latex.php?latex=E%5Csetminus+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E&#92;setminus T" class="latex" /> left out from the spanning tree form a complete set.</p>
<p>The last ingredient you need to know are the affinities. They can be constructed as follows. Consider the Markov process on the network where the observable edges are removed <img src="https://s0.wp.com/latex.php?latex=G%27+%3D+%28I%2CT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G&#039; = (I,T)" class="latex" />. Calculate the steady state of its associated master equation <img src="https://s0.wp.com/latex.php?latex=%28p%5E%7B%5Cmathrm%7Beq%7D%7D_i%29_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p^{&#92;mathrm{eq}}_i)_i" class="latex" />, which is necessarily an equilibrium (since there cannot be cycle currents in a tree&#8230;). Then the affinities are given by</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BA%7D_%5Calpha+%3D+%5Clog++w_%7Bi_%5Calpha+j_%5Calpha%7D+p%5E%7B%5Cmathrm%7Beq%7D%7D_%7Bj_%5Calpha%7D+%2F+w_%7Bj_%5Calpha+i_%5Calpha%7D+p%5E%7B%5Cmathrm%7Beq%7D%7D_%7Bi_%5Calpha%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{A}_&#92;alpha = &#92;log  w_{i_&#92;alpha j_&#92;alpha} p^{&#92;mathrm{eq}}_{j_&#92;alpha} / w_{j_&#92;alpha i_&#92;alpha} p^{&#92;mathrm{eq}}_{i_&#92;alpha}" class="latex" />.</p>
<p>Now you have all that is needed to formulate the complete theory and prove the FR.</p>
<p><em>Homework</em>: (Difficult!) With the above definitions, prove the FR.</p>
<p>How about the marginal theory? To define the effective affinities, take the set <img src="https://s0.wp.com/latex.php?latex=E_%7B%5Cmathrm%7Bmar%7D%7D+%3D+%5C%7Bi_%5Cmu+j_%5Cmu%2C+%5Cforall+%5Cmu%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_{&#92;mathrm{mar}} = &#92;{i_&#92;mu j_&#92;mu, &#92;forall &#92;mu&#92;}" class="latex" /> of edges where there run observable currents. Notice that now its complement obtained by removing the observable edges, the <b>hidden</b> edge set <img src="https://s0.wp.com/latex.php?latex=E_%7B%5Cmathrm%7Bhid%7D%7D+%3D+E+%5Csetminus+E_%7B%5Cmathrm%7Bmar%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_{&#92;mathrm{hid}} = E &#92;setminus E_{&#92;mathrm{mar}}" class="latex" />, is not in general a spanning tree: there might be cycles that are not accounted for by our observations. However, we can still consider the Markov process on the hidden space, and calculate its <b>stalling</b> steady state <img src="https://s0.wp.com/latex.php?latex=p%5E%7B%5Cmathrm%7Bst%7D%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^{&#92;mathrm{st}}_i" class="latex" />, and ta-taaa: The effective affinities are given by</p>
<p style="padding-left:90px;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%5Cmu+%3D+%5Clog+w_%7Bi_%5Cmu+j_%5Cmu%7D+p%5E%7B%5Cmathrm%7Bst%7D%7D_%7Bj_%5Cmu%7D+%2F+w_%7Bj_%5Cmu+i_%5Cmu%7D+p%5E%7B%5Cmathrm%7Bst%7D%7D_%7Bi_%5Cmu%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{Q}_&#92;mu = &#92;log w_{i_&#92;mu j_&#92;mu} p^{&#92;mathrm{st}}_{j_&#92;mu} / w_{j_&#92;mu i_&#92;mu} p^{&#92;mathrm{st}}_{i_&#92;mu}" class="latex" />.</p>
<p>Proving the marginal IFR is far more complicated than the complete FR. In fact, very often in my field we will not work with the current&#8217; probability density itself,  but we prefer to take its bidirectional Laplace transform and work with the currents&#8217; cumulant generating function. There things take a quite different and more elegant look.</p>
<p>Many other questions and possibilities open up now. The most important one left open is: Can we generalize the theory the (physically relevant) case where the current is supported on several edges? For example, for a current defined like <img src="https://s0.wp.com/latex.php?latex=%5CPhi%5Et+%3D+5+%5CPhi%5Et_%7B12%7D+%2B+7+%5CPhi%5Et_%7B34%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Phi^t = 5 &#92;Phi^t_{12} + 7 &#92;Phi^t_{34}" class="latex" />? Well, it depends: the theory holds provided that the stalling state is not &#8220;internally alive&#8221;, meaning that if the observable current vanishes on average, then also should <img src="https://s0.wp.com/latex.php?latex=%5CPhi%5Et_%7B12%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Phi^t_{12}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5CPhi%5Et_%7B34%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Phi^t_{34}" class="latex" /> separately. This turns out to be a physically meaningful but quite strict condition.</p>
<h3>Is all of thermodynamics &#8220;effective&#8221;?</h3>
<p>Let me conclude with some more of those philosophical considerations that sadly I have to leave out of papers&#8230;</p>
<p>Stochastic thermodynamics strongly depends on the identification of physical and information-theoretic entropies â€” something that I did not openly talk about, but that lurks behind the whole construction. Throughout my short experience as researcher I have been pursuing a program of &#8220;relativization&#8221; of thermodynamics, by making the role of the observer more and more evident and movable. Inspired by Einstein&#8217;s <em>Gedankenexperimenten</em>, I also tried to make the theory operational. This program may raise eyebrows here and there: Many thermodynamicians embrace a naive materialistic world-view whereby what only matters are &#8220;real&#8221; physical quantities like temperature, pressure, and all the rest of the information-theoretic discourse is at best mathematical speculation or a fascinating analog with no fundamental bearings.  According to some, information as a physical concept lingers alarmingly close to certain extreme postmodern claims in the social sciences that &#8220;reality&#8221; does not exist unless observed, a position deemed dangerous at times when the authoritativeness of science is threatened by all sorts of anti-scientific waves.</p>
<p>I think, on the contrary, that making concepts relative and effective and by summoning the observer explicitly is a laic and prudent position that serves as an antidote to radical subjectivity. The other way around&mdash;clinging to the objectivity of a preferred observer, which is implied in any materialistic interpretation of thermodynamics, e.g. by assuming that the most fundamental degrees of freedom are the positions and velocities of gas&#8217;s molecules&mdash;is the dangerous position, expecially when the role of such preferred observer is passed around from the scientist to the technician and eventually to the technocrat, who would be induced to believe there are<a href="https://www.amazon.com/Save-Everything-Click-Here-Technological/dp/1610393708"> simple technological fixes</a> to <a href="https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815">complex social problems</a>&#8230;</p>
<p>How do we reconcile observer-dependency and the laws of physics? The object and the subject? On the one hand, much like the position of an object depends on the reference frame, so much so entropy and entropy production do depend on the observer and the particular apparatus that he controls or experiment he is involved with. On the other hand, much like motion is ultimately independent of position and it is agreed upon by all observers that share compatible measurement protocols, so much so the laws of thermodynamics are independent of that particular observer&#8217;s quantification of entropy and entropy production (e.g., the effective Second Law holds independently of how much the marginal observer knows of the system, if he operates according to our phenomenological protocol&#8230;). This is the case even in the every-day thermodynamics as practiced by energetic engineers <i>et al.</i>, where there are lots of choices to gauge upon, and there is no other external warrant that the amount of dissipation being quantified is the &#8220;true&#8221; one (whatever that means&#8230;)&mdash;there can only be trust in one&#8217;s own good practices and methodology.</p>
<p>So in this sense, I like to think that all observers are marginal, that this effective theory  serves as a dictionary by which different observers practice and communicate thermodynamics, and that we should not revere the laws of thermodynamics as &#8220;true&#8221; idols,  but rather as tools of good scientific practice.</p>
<p><a name="refs"></p>
<h3>References</h3>
<p></a></p>
<p>&bull; M. Polettini and M. Esposito, Effective fluctuation and response theory, arXiv:<a href="https://arxiv.org/abs/1803.03552">1803.03552</a>.</p>
<p>In this work we give the complete theory and numerous references to work of other people that was along the same lines. We employ a &#8220;spiral&#8221; approach to the presentation of the results, inspired by the pedagogical principle of <a href="https://johncarlosbaez.wordpress.com/2016/03/18/interview-part-1/">Albert Baez</a>.</p>
<p>&bull; M. Polettini and M. Esposito, Effective thermodynamics for a marginal observer, <i>Phys. Rev. Lett.</i> <strong>119</strong> (2017), 240601, arXiv:<a href="https://arxiv.org/abs/1703.05715">1703.05715</a>.</p>
<p>This is a shorter version of the story.</p>
<p>&bull; B. Altaner, M. Polettini and M. Esposito, Fluctuation-dissipation relations far from equilibrium, <i>Phys. Rev. Lett.</i> <strong>117</strong> (2016), 180601, <a href="http://arxiv.org/abs/1604.08832">arXiv:1604.0883</a>.</p>
<p>An early version of the story, containing the FDR results but not the full-fledged FR.</p>
<p>&bull; G. Bisker, M. Polettini, T. R. Gingrich and J. M. Horowitz, Hierarchical bounds on entropy production inferred from partial information, <i>J. Stat. Mech.</i> (2017), 093210, arXiv:<a href="https://arxiv.org/abs/1708.06769">1708.06769</a>.</p>
<p>Some extras.</p>
<p>&bull; M. F. Weber and E. Frey, Master equations and the theory of stochastic path integrals, <i>Rep. Progr. Phys.</i> <strong>80</strong> (2017), 046601, arXiv:<a href="https://arxiv.org/abs/1609.02849">1609.02849</a>.</p>
<p>Great reference if one wishes to learn about path integrals for master equation systems.</p>
<h3> Footnotes</h3>
<p><sup>1</sup> There are as many so-called &#8220;Fluctuation Theorems&#8221; as there are authors working on them, so I decided not to call them by any name. Furthermore, notice I prefer to distinguish between a relation (a formula) and a theorem (a line of reasoning). I lingered more on this <a href="https://tomate.wordpress.com/2017/09/21/is-the-fluctuation-theorem-a-theorem/">here</a>.</p>
<p><sup>2</sup> <i>&#8220;Just so you know, nobody knows what energy is.&#8221;</i>&mdash;Richard Feynman.</p>
<p>I cannot help but mention here the beautiful book by Shapin and Schaffer, <em>Leviathan and the Air-Pump</em>, about the Boyle vs. Hobbes diatribe about what constitutes a  &#8220;matter of fact,&#8221; and Bruno Latour&#8217;s interpretation of it in <em>We Have Never Been Modern</em>. Latour argues that &#8220;modernity&#8221; is a process of separation of the human and natural spheres, and within each of these spheres a process of purification of the unit facts of knowledge and the unit facts of politics, of the object and the subject. At the same time we live in a world where these two spheres are never truly separated, a world of &#8220;hybrids&#8221; that are at the same time necessary &#8220;for all practical purposes&#8221; and unconceivable according to the myths that sustain the narration of science, of the State, and even of religion. In fact, despite these myths, we cannot conceive a scientific fact out of the contextual &#8220;network&#8221; where this fact is produced and replicated, and neither we can conceive society out of the material needs that shape it: so in  this sense &#8220;we have never been modern&#8221;, we are not quite different from all those societies that we take pleasure of studying with the tools of anthropology. Within the scientific community Latour is widely despised; probably he is also misread. While it is really difficult to see how his analysis applies to, say, high-energy physics, I find that thermodynamics and its ties to the industrial revolution perfectly embodies this tension between the natural and the artificial, the matter of fact and the matter of concern. Such great thinkers as Einstein and Ehrenfest thought of the Second Law as the only physical law that would never be replaced, and I believe this is revelatory. A second thought on the Second Law, a systematic and precise definition of all its terms and circumstances, reveals that the only formulations that make sense are those phenomenological statements such as Kelvin-Planck&#8217;s or similar, which require a lot of contingent definitions regarding the operation of the engine, while fetishized and universal statements are nonsensical (such as that masterwork of confusion that is &#8220;the entropy of the Universe cannot decrease&#8221;). In this respect, it is neither a purely <em>natural</em> law&mdash;as the moderns argue, nor a purely <em>social</em> construct&mdash;as the postmodern argue. One simply has to renounce to operate this separation. While I do not have a definite answer on this problem, I like to think of the Second Law as a <em>practice</em>, a consistency check of the thermodynamic discourse.</p>
<p><sup>3</sup> This assumption really belongs to a time, the XIXth century, when resources were virtually infinite on planet Earth&#8230;</p>
<p><sup>4</sup> As we will see shortly, we <em>define</em> equilibrium as that state where there are no currents at the interface between the system and the environment, so what is the environment&#8217;s own definition of equilibrium?!</p>
<p><sup>5</sup> This because we have already exploited the First Law.</p>
<p><sup>6</sup> This nomenclature comes from alchemy, via chemistry (think of Goethe&#8217;s <em>The elective affinities</em>&#8230;), it propagated in the XXth century via De Donder and Prigogine, and eventually it is still present in language in Luxembourg because in some way we come from the &#8220;late Brussels school&#8221;.</p>
<p><sup>7</sup> Basically, we ask that the tunable parameters are environmental properties, such as temperatures, chemical potentials, etc. and <em>not</em> internal properties, such as the energy landscape or the activation barriers between configurations.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2018/05/08/effective-thermodynamics-for-a-marginal-observer/#comments">1 Comment</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/chemistry/" rel="category tag">chemistry</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2018/05/08/effective-thermodynamics-for-a-marginal-observer/" rel="bookmark" title="Permanent Link to Effective Thermodynamics for a Marginal&nbsp;Observer">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/7/?s=information+and+entropy" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/page/5/?s=information+and+entropy" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/page/6/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s0.wp.com/_static/??/wp-content/mu-plugins/carousel/swiper-bundle.css,/wp-content/mu-plugins/carousel/jetpack-carousel.css?m=1630955947j&cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F06%2F16%2Fnonequilibrium-thermodynamics-in-biology-part-2%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jt0OgjAMhV/I0sCFCRfGZxlbQzbZj2sn8PZOI4lBw1V7TvP1HJwT6BiEgqBjNPSwmtLSOD7h18kXSFMZbWBUxtsAg8roFQvluoFkpW+8h+o/dy+U189o5qSjh5TjskKm6rFsjA16Kob4BVVJfiDT1KCDIlrlWJgmdCSp5sNmHDCzNSMJI5eBdbZJbAw/vf9kwLv6Tlbu6i/tuev7tuvOrXsCIKR8xw=='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZneCxJZkphaSVQRzVpTk58eltoaSZkdG9XaVkudTNYTU4xW1NOMnZOeGpaaXRRLHxuLnZtOVlnTiYuWXNPKzFzd214Nk0uZmpTZzlQVEJoMTB8UUMtMTFHU1AxP2cyOVBiMDlPNWNMcHFGMVo9ZTcuMm4rWFM2Jmp3MGhXL29VdGlxRnp2azdxY2R8PSwscytoOWF3RUNiLT1STVViUDdXZXRlMHx1P35zb3hsVmxETyZbMFdKfm5hV1JjWHpsV1g3UE8s'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>