<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth | Page 9</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F02%2F10%2Fquantropy-part-2%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/page\/9\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fpage%2F9%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F02%2F10%2Fquantropy-part-2%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Page 9 &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results paged paged-9 search-paged-9 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-7944 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-7944">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/" rel="bookmark">Quantropy (Part 2)</a></h2>
				<small>10 February, 2012</small><br />


				<div class="entry">
					<p>In my <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/">first post in this series</a>, we saw that filling in a well-known analogy between statistical mechanics and quantum mechanics requires a new concept: &#8216;quantropy&#8217;.  To get some feeling for this concept, we should look at some examples.  But to do that, we need to develop some tools to compute quantropy.  That&#8217;s what we&#8217;ll do today.</p>
<p>All these tools will be borrowed from statistical mechanics.  So, let me remind you how to compute the entropy of a system in thermal equilibrium starting if we know the energy of every state.  Then we&#8217;ll copy this and get a formula for the quantropy of a system if we know the action of every history.</p>
<h3> Computing entropy </h3>
<p>Everything in this section is bog-standard.  In case you don&#8217;t know, that&#8217;s British slang for &#8216;extremely, perhaps even depressingly, familiar&#8217;.  Apparently it rains so much in England that bogs are not only standard, they&#8217;re the <i>standard</i> of what counts as standard!</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be a measure space: physically, the set of states of some system.  In statistical mechanics we suppose the system occupies states with probabilities given by some probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3A+X+%5Cto+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : X &#92;to [0,&#92;infty) " class="latex" /></p>
<p>where of course </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_X+p%28x%29+%5C%2C+dx+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_X p(x) &#92;, dx = 1 " class="latex" /></p>
<p>The <b>entropy</b> of this probability distribution is</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Cint_X+p%28x%29+%5Cln%28p%28x%29%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;int_X p(x) &#92;ln(p(x)) &#92;, dx " class="latex" /></p>
<p>There&#8217;s a nice way to compute the entropy when our system is in thermal equilibrium.  This idea makes sense when we have a function </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3A+X+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H : X &#92;to &#92;mathbb{R} " class="latex" /> </p>
<p>saying the <b>energy</b> of each state.  Our system is in <b>thermal equilibrium</b> when <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> maximizes entropy subject to a constraint on the expected value of energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+%5Cint_X+H%28x%29+p%28x%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = &#92;int_X H(x) p(x) &#92;, dx " class="latex" /></p>
<p>A famous calculation shows that thermal equilibrium occurs precisely when <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is the so-called <b>Gibbs state</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p%28x%29+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta+H%28x%29%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p(x) = &#92;frac{e^{-&#92;beta H(x)}}{Z} } " class="latex" /></p>
<p>for some real number <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalization factor called the <b>partition function</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cint_X+e%5E%7B-%5Cbeta+H%28x%29%7D+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;int_X e^{-&#92;beta H(x)} &#92;, dx " class="latex" /></p>
<p>The number <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is called the <b>coolness</b>, since physical considerations say that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cbeta+%3D+%5Cfrac%7B1%7D%7BT%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;beta = &#92;frac{1}{T} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the <b>temperature</b> in units where Boltzmann&#8217;s constant is 1.</p>
<p>There&#8217;s a famous way to compute the entropy of the Gibbs state; I don&#8217;t know who did it first, but it&#8217;s both straightforward and tremendously useful.  We take the formula for entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Cint_X+p%28x%29+%5Cln%28p%28x%29%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;int_X p(x) &#92;ln(p(x)) &#92;, dx " class="latex" /></p>
<p>and substitute the Gibbs state</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p%28x%29+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta+H%28x%29%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p(x) = &#92;frac{e^{-&#92;beta H(x)}}{Z} } " class="latex" /></p>
<p>getting</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+S+%26%3D%26+%5Cint_X+p%28x%29+%5Cleft%28+%5Cbeta+H%28x%29+-+%5Cln+Z+%5Cright%29%5C%2C+dx+%5C%5C+++%5C%5C++%26%3D%26+%5Cbeta+%5C%2C+%5Clangle+H+%5Crangle+-+%5Cln+Z+%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} S &amp;=&amp; &#92;int_X p(x) &#92;left( &#92;beta H(x) - &#92;ln Z &#92;right)&#92;, dx &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;beta &#92;, &#92;langle H &#92;rangle - &#92;ln Z &#92;end{array}  " class="latex" /></p>
<p>Reshuffling this a little bit, we obtain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+T+%5Cln+Z+%3D+%5Clangle+H+%5Crangle+-+T+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- T &#92;ln Z = &#92;langle H &#92;rangle - T S" class="latex" /></p>
<p>If we define the <b>free energy</b> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-+T+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = - T &#92;ln Z" class="latex" /></p>
<p>then we&#8217;ve shown that</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+H+%5Crangle+-+T+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle H &#92;rangle - T S " class="latex" /></p>
<p>This justifies the term &#8216;free energy&#8217;: it&#8217;s the expected energy minus the energy in the form of heat, namely <img src="https://s0.wp.com/latex.php?latex=T+S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T S." class="latex" /></p>
<p>It&#8217;s nice that we can compute the free energy purely in terms of the partition function and the temperature, or equivalently the coolness <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F+%3D+-+%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F = - &#92;frac{1}{&#92;beta} &#92;ln Z }" class="latex" /></p>
<p>Can we also do this for the entropy?  Yes!  First we&#8217;ll do it for the expected energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Clangle+H+%5Crangle+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_X+H%28x%29+p%28x%29+%5C%2C+dx+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%7D+%5Cint_X+H%28x%29+e%5E%7B-%5Cbeta+H%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cint_X+e%5E%7B-%5Cbeta+H%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7BdZ%7D%7Bd+%5Cbeta%7D+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;langle H &#92;rangle &amp;=&amp; &#92;displaystyle{ &#92;int_X H(x) p(x) &#92;, dx } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{1}{Z} &#92;int_X H(x) e^{-&#92;beta H(x)} &#92;, dx } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{d}{d &#92;beta} &#92;int_X e^{-&#92;beta H(x)} &#92;, dx } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{dZ}{d &#92;beta} } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;frac{d}{d &#92;beta} &#92;ln Z } &#92;end{array} " class="latex" /></p>
<p>This gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+S+%26%3D%26+%5Cbeta+%5C%2C+%5Clangle+H+%5Crangle+-+%5Cln+Z+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+-+%5Cbeta+%5C%2C+%5Cfrac%7Bd+%5Cln+Z%7D%7Bd+%5Cbeta%7D+-+%5Cln+Z+%7D%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} S &amp;=&amp; &#92;beta &#92;, &#92;langle H &#92;rangle - &#92;ln Z &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ - &#92;beta &#92;, &#92;frac{d &#92;ln Z}{d &#92;beta} - &#92;ln Z }&#92;end{array} " class="latex" /></p>
<p>So, if we know the partition function of a system in thermal equilibrium as a function of the temperature, we can work out its entropy, expected energy and free energy.  </p>
<h3> Computing quantropy </h3>
<p>Now we&#8217;ll repeat everything for quantropy!  The idea is simply to replace the energy by action and the temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=i+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;hbar" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is Planck&#8217;s constant.  It&#8217;s harder to get the integrals to converge in interesting examples.  But we&#8217;ll worry about that next time, that when we actually do an example!  </p>
<p>It&#8217;s annoying that in physics <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> stands for both entropy and action, since in this article we need to think about both.  People also use <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> to stand for entropy, but that&#8217;s no better, since that letter also stands for &#8216;Hamiltonian&#8217;!  To avoid this let&#8217;s use <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> to stand for action.  This letter is also used to mean &#8216;Helmholtz free energy&#8217;, but we&#8217;ll just have to live with that.  It would be real bummer if we failed to unify physics just because we ran out of letters.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be a measure space: physically, the set of histories of some system.  In quantum mechanics we suppose the system carries out histories with amplitudes given by some function</p>
<p><img src="https://s0.wp.com/latex.php?latex=a+%3A+X+%5Cto+%5Cmathbb%7BC%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a : X &#92;to &#92;mathbb{C} " class="latex" /></p>
<p>where perhaps surprisingly</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_X+a%28x%29+%5C%2C+dx+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_X a(x) &#92;, dx = 1 " class="latex" /></p>
<p>The <b>quantropy</b> of this function is</p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+%3D+-+%5Cint_X+a%28x%29+%5Cln%28a%28x%29%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = - &#92;int_X a(x) &#92;ln(a(x)) &#92;, dx " class="latex" /></p>
<p>There&#8217;s a nice way to compute the entropy in Feynman&#8217;s path integral formalism.  This formalism makes sense when we have a function </p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%3A+X+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A : X &#92;to &#92;mathbb{R} " class="latex" /> </p>
<p>saying the <b>action</b> of each history.  Feynman proclaimed that in this case we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a%28x%29+%3D+%5Cfrac%7Be%5E%7Bi+A%28x%29%2F%5Chbar%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a(x) = &#92;frac{e^{i A(x)/&#92;hbar}}{Z} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is Planck&#8217;s constant and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalization factor called the <b>partition function</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cint_X+e%5E%7Bi+A%28x%29%2F%5Chbar%7D+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;int_X e^{i A(x)/&#92;hbar} &#92;, dx " class="latex" /></p>
<p>Last time I showed that we obtain Feynman&#8217;s prescription for <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> by demanding that it&#8217;s a stationary point for the <b>quantropy</b> </p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+%3D+-+%5Cint_X+a%28x%29+%5C%2C+%5Cln+%28a%28x%29%29+%5C%2C+dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = - &#92;int_X a(x) &#92;, &#92;ln (a(x)) &#92;, dx" class="latex" /></p>
<p>subject to a constraint on the <b>expected action</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Cint_X+A%28x%29+a%28x%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;int_X A(x) a(x) &#92;, dx " class="latex" /></p>
<p>As I mentioned <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/">last time</a>, the formula for quantropy is dangerous, since we&#8217;re taking the logarithm of a complex-valued function.  There&#8217;s not really a &#8216;best&#8217; logarithm for a complex number: if we have one choice we can add any multiple of <img src="https://s0.wp.com/latex.php?latex=2+%5Cpi+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2 &#92;pi i" class="latex" /> and get another.  So in general, to define quantropy we need to pick a choice of <img src="https://s0.wp.com/latex.php?latex=%5Cln+%28a%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (a(x))" class="latex" /> for each point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X." class="latex" />  That&#8217;s a lot of ambiguity! </p>
<p>Luckily, the ambiguity is much less when we use Feynman&#8217;s prescription for <img src="https://s0.wp.com/latex.php?latex=a.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a." class="latex" />  Why?  Because then <img src="https://s0.wp.com/latex.php?latex=a%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a(x)" class="latex" /> is defined in terms of an exponential, and it&#8217;s easy to take the logarithm of an exponential!  So, we can declare that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cln+%28a%28x%29%29+%3D+%5Cdisplaystyle%7B+%5Cln+%5Cleft%28+%5Cfrac%7Be%5E%7BiA%28x%29%2F%5Chbar%7D%7D%7BZ%7D%5Cright%29+%7D+%3D+%5Cfrac%7Bi%7D%7B%5Chbar%7D+A%28x%29+-+%5Cln+Z++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (a(x)) = &#92;displaystyle{ &#92;ln &#92;left( &#92;frac{e^{iA(x)/&#92;hbar}}{Z}&#92;right) } = &#92;frac{i}{&#92;hbar} A(x) - &#92;ln Z  " class="latex" /></p>
<p>Once we choose a logarithm for <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />, this formula will let us define <img src="https://s0.wp.com/latex.php?latex=%5Cln+%28a%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (a(x))" class="latex" /> and thus the quantropy.</p>
<p>So let&#8217;s do this, and say the quantropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+-+%5Cint_X+a%28x%29+%5Cleft%28+%5Cfrac%7Bi%7D%7B%5Chbar%7D+A%28x%29+-+%5Cln+Z+%5Cright%29%5C%2C+dx+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = - &#92;int_X a(x) &#92;left( &#92;frac{i}{&#92;hbar} A(x) - &#92;ln Z &#92;right)&#92;, dx } " class="latex" /></p>
<p>We can simplify this a bit, since the integral of <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> is 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+%5Cfrac%7B1%7D%7Bi+%5Chbar%7D+%5Clangle+A+%5Crangle+%2B+%5Cln+Z+%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = &#92;frac{1}{i &#92;hbar} &#92;langle A &#92;rangle + &#92;ln Z }  " class="latex" /></p>
<p>Reshuffling this a little bit, we obtain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+i+%5Chbar+%5Cln+Z+%3D+%5Clangle+A+%5Crangle+-+i+%5Chbar+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- i &#92;hbar &#92;ln Z = &#92;langle A &#92;rangle - i &#92;hbar Q" class="latex" /></p>
<p>By analogy to free energy in statistical mechanics, let&#8217;s define the <b>free action</b> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-+i+%5Chbar+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = - i &#92;hbar &#92;ln Z" class="latex" /></p>
<p>I&#8217;m using the same letter for free energy and free action, but they play exactly analogous roles, so it&#8217;s not so bad.  Indeed we now have</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+A+%5Crangle+-+i+%5Chbar+Q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle A &#92;rangle - i &#92;hbar Q " class="latex" /></p>
<p>which is the analogue of a formula we saw for free energy in thermodynamics.  </p>
<p>It&#8217;s nice that we can compute the free action purely in terms of the partition function and Planck&#8217;s constant. Can we also do this for the quantropy?  Yes!  </p>
<p>It&#8217;ll be convenient to introduce a parameter</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cbeta+%3D+%5Cfrac%7B1%7D%7Bi+%5Chbar%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;beta = &#92;frac{1}{i &#92;hbar} }" class="latex" /></p>
<p>which is analogous to &#8216;coolness&#8217;.  We could call it &#8216;quantum coolness&#8217;, but a better name might be <b>classicality</b>, since it&#8217;s big when our system is close to classical.  Whatever we call it, the main thing is that unlike ordinary coolness, it&#8217;s imaginary!    </p>
<p>In terms of classicality, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta+A%28x%29%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a(x) = &#92;frac{e^{- &#92;beta A(x)}}{Z} } " class="latex" /></p>
<p>Now we can compute the expected action just as we computed the expected energy in thermodynamics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Clangle+A+%5Crangle+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_X+A%28x%29+a%28x%29+%5C%2C+dx+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%7D+%5Cint_X+A%28x%29+e%5E%7B-%5Cbeta+A%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cint_X+e%5E%7B-%5Cbeta+A%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7BdZ%7D%7Bd+%5Cbeta%7D+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z+%7D+%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;langle A &#92;rangle &amp;=&amp; &#92;displaystyle{ &#92;int_X A(x) a(x) &#92;, dx } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{1}{Z} &#92;int_X A(x) e^{-&#92;beta A(x)} &#92;, dx } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{d}{d &#92;beta} &#92;int_X e^{-&#92;beta A(x)} &#92;, dx } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{dZ}{d &#92;beta} } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;frac{d}{d &#92;beta} &#92;ln Z } &#92;end{array}" class="latex" /></p>
<p>This gives:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+Q+%26%3D%26+%5Cbeta+%5C%2C%5Clangle+A+%5Crangle+-+%5Cln+Z+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+-+%5Cbeta+%5C%2C%5Cfrac%7Bd+%5Cln+Z%7D%7Bd+%5Cbeta%7D+-+%5Cln+Z+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} Q &amp;=&amp; &#92;beta &#92;,&#92;langle A &#92;rangle - &#92;ln Z &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ - &#92;beta &#92;,&#92;frac{d &#92;ln Z}{d &#92;beta} - &#92;ln Z } &#92;end{array} " class="latex" /></p>
<p>So, if we can compute the partition function in the path integral approach to quantum mechanics, we can also work out the quantropy, expected action and free action!   </p>
<p>Next time I&#8217;ll use these formulas to compute quantropy in an example: the free particle.  We&#8217;ll see some strange and interesting things.</p>
<p><a name="summary"></p>
<h3> Summary </h3>
<p></a></p>
<p>Here&#8217;s where our analogy stands now:</p>
<table border="1">
<tr>
<td><b>Statistical Mechanics</b></td>
<td><b>Quantum Mechanics</b></td>
</tr>
<tr>
<td>states: <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /></td>
<td>histories: <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /></td>
</tr>
<tr>
<td>probabilities: <img src="https://s0.wp.com/latex.php?latex=p%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p: X &#92;to [0,&#92;infty)" class="latex" /></td>
<td>amplitudes: <img src="https://s0.wp.com/latex.php?latex=a%3A+X+%5Cto+%5Cmathbb%7BC%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a: X &#92;to &#92;mathbb{C} " class="latex" /></td>
</tr>
<tr>
<td>energy: <img src="https://s0.wp.com/latex.php?latex=H%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H: X &#92;to &#92;mathbb{R}" class="latex" /></td>
<td>action: <img src="https://s0.wp.com/latex.php?latex=A%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A: X &#92;to &#92;mathbb{R}" class="latex" /> </td>
</tr>
<tr>
<td>temperature: <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /></td>
<td>Planck&#8217;s constant times <i>i</i>: <img src="https://s0.wp.com/latex.php?latex=i+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;hbar" class="latex" /></td>
</tr>
<tr>
<td>coolness: <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" /></td>
<td>classicality: <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2Fi+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/i &#92;hbar" class="latex" /></td>
</tr>
<tr>
<td>partition function: <img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-%5Cbeta+H%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{-&#92;beta H(x)}" class="latex" /></td>
<td>partition function: <img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-%5Cbeta+A%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{-&#92;beta A(x)}" class="latex" /></td>
</tr>
<tr>
<td>Boltzmann distribution: <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+e%5E%7B-%5Cbeta+H%28x%29%7D%2FZ&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x) = e^{-&#92;beta H(x)}/Z" class="latex" /></td>
<td>Feynman sum over histories: <img src="https://s0.wp.com/latex.php?latex=a%28x%29+%3D+e%5E%7B-%5Cbeta+A%28x%29%7D%2FZ&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a(x) = e^{-&#92;beta A(x)}/Z" class="latex" /></td>
</tr>
<tr>
<td>entropy: <img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Csum_%7Bx+%5Cin+X%7D+p%28x%29+%5Cln%28p%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;sum_{x &#92;in X} p(x) &#92;ln(p(x))" class="latex" /></td>
<td>quantropy: <img src="https://s0.wp.com/latex.php?latex=Q+%3D+-+%5Csum_%7Bx+%5Cin+X%7D+a%28x%29+%5Cln%28a%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = - &#92;sum_{x &#92;in X} a(x) &#92;ln(a(x))" class="latex" /></td>
</tr>
<tr>
<td>expected energy: <img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+%5Csum_%7Bx+%5Cin+X%7D+p%28x%29+H%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = &#92;sum_{x &#92;in X} p(x) H(x) " class="latex" /></td>
<td>expected action: <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Csum_%7Bx+%5Cin+X%7D+a%28x%29+A%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;sum_{x &#92;in X} a(x) A(x) " class="latex" /></td>
</tr>
<tr>
<td>free energy: <img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+H+%5Crangle+-+TS&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle H &#92;rangle - TS" class="latex" /></td>
<td>free action: <img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+A+%5Crangle+-+i+%5Chbar+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle A &#92;rangle - i &#92;hbar Q" class="latex" /></td>
</tr>
<tr>
<td>  <img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = - &#92;frac{d}{d &#92;beta} &#92;ln Z" class="latex" />  </td>
<td> <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = - &#92;frac{d}{d &#92;beta} &#92;ln Z" class="latex" /></td>
</tr>
<tr>
<td> <img src="https://s0.wp.com/latex.php?latex=F+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = -&#92;frac{1}{&#92;beta} &#92;ln Z" class="latex" /> </td>
<td> <img src="https://s0.wp.com/latex.php?latex=F+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = -&#92;frac{1}{&#92;beta} &#92;ln Z" class="latex" />    </td>
</tr>
<tr>
<td>  <img src="https://s0.wp.com/latex.php?latex=S+%3D++%5Cln+Z+-+%5Cbeta+%5C%2C%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S =  &#92;ln Z - &#92;beta &#92;,&#92;frac{d}{d &#92;beta}&#92;ln Z " class="latex" /> </td>
<td> <img src="https://s0.wp.com/latex.php?latex=Q+%3D+%5Cln+Z+-+%5Cbeta+%5C%2C%5Cfrac%7Bd+%7D%7Bd+%5Cbeta%7D%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = &#92;ln Z - &#92;beta &#92;,&#92;frac{d }{d &#92;beta}&#92;ln Z " class="latex" />
</td>
</tr>
</table>
<p>I should also say a word about units and dimensional analysis.  There&#8217;s enormous flexibility in how we do dimensional analysis.  Amateurs often don&#8217;t realize this, because they&#8217;ve just learned one system, but experts take full advantage of this flexibility to pick a setup that&#8217;s convenient for what they&#8217;re doing.  The fewer independent units you use, the fewer dimensionful constants like the speed of light, Planck&#8217;s constant and Boltzmann&#8217;s constant you see in your formulas.  That&#8217;s often good.  But here I don&#8217;t want to set Planck&#8217;s constant equal to 1 because I&#8217;m treating it as analogous to temperature&#8212;so it&#8217;s important, and I want to <i>see</i> it.  I&#8217;m also finding dimensional analysis useful to check my formulas. </p>
<p>So, I&#8217;m using units where mass, length and time count as independent dimensions in the sense of dimensional analysis.  On the other hand, I&#8217;m not treating temperature as an independent dimension: instead, I&#8217;m setting Boltzmann&#8217;s constant to 1 and using that to translate from temperature into energy.   This is fairly common in some circles. And for me, treating temperature as an independent dimension would be analogous to treating Planck&#8217;s constant as having its own independent dimension!  I don&#8217;t feel like doing that.</p>
<p>So, here&#8217;s how the dimensional analysis works in my setup:</p>
<table border="1">
<tr>
<td><b>Statistical Mechanics</b></td>
<td><b>Quantum Mechanics</b></td>
</tr>
<tr>
<td>probabilities: dimensionless</td>
<td>amplitudes: dimensionless </td>
</tr>
<tr>
<td>energy: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /> </td>
<td>action: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /> </td>
</tr>
<tr>
<td>temperature: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /></td>
<td>Planck&#8217;s constant: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /></td>
</tr>
<tr>
<td>coolness: <img src="https://s0.wp.com/latex.php?latex=T%5E2%2FML&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^2/ML" class="latex" /></td>
<td>classicality: <img src="https://s0.wp.com/latex.php?latex=T%2FML+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T/ML " class="latex" /></td>
</tr>
<tr>
<td>partition function: dimensionless </td>
<td>partition function: dimensionless </td>
</tr>
<td>entropy: dimensionless </td>
<td>quantropy: dimensionless </td>
<tr>
<td>expected energy: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /></td>
<td>expected action: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /></td>
</tr>
<tr>
<td>free energy: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /></td>
<td>free action: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /></td>
</tr>
</table>
<p>I like this setup because I often think of entropy as closely allied to information, measured in bits or nats depending on whether I&#8217;m using base 2 or base <i>e</i>.  From this viewpoint, it should be dimensionless.  </p>
<p>Of course, in thermodynamics it&#8217;s common to put a factor of Boltzmann&#8217;s constant in front of the formula for entropy.  Then entropy has units of energy/temperature.  But I&#8217;m using units where Boltzmann&#8217;s constant is 1 and temperature has the same units as energy!  So for me, entropy is dimensionless.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/#comments">19 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/" rel="bookmark" title="Permanent Link to Quantropy (Part 2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-7696 post type-post status-publish format-standard hentry category-information-and-entropy category-physics category-quantum-technologies" id="post-7696">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/01/29/a-quantum-hammersley-clifford-theorem/" rel="bookmark">A Quantum Hammersley&#8211;Clifford Theorem</a></h2>
				<small>29 January, 2012</small><br />


				<div class="entry">
					<p>I&#8217;m at this workshop:</p>
<p>&bull; <a href="http://www.physics.usyd.edu.au/quantum/Coogee2012/">Sydney Quantum Information Theory Workshop: Coogee 2012</a>, 30 January &#8211; 2 February 2012, Coogee Bay Hotel, Coogee, Sydney, organized by Stephen Bartlett, Gavin Brennen, Andrew Doherty and Tom Stace.</p>
<p>Right now David Poulin is speaking about a quantum version of the Hammersley&#8211;Clifford theorem, which is a theorem about Markov networks.   Let me quickly say a bit about what he proved!   This will be a bit rough, since I&#8217;m doing it live&#8230;</p>
<p>The <a href="http://en.wikipedia.org/wiki/Mutual_information"><b>mutual information</b></a> between two random variables is </p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%29+%3D+S%28A%29+%2B+S%28B%29+-+S%28A%2CB%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B) = S(A) + S(B) - S(A,B)" class="latex" /></p>
<p>The <a href="http://en.wikipedia.org/wiki/Conditional_mutual_information"><b>conditional mutual information</b></a> between three random variables <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+%5Csum_c+p%28C%3Dc%29+I%28A%3AB%7CC%3Dc%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = &#92;sum_c p(C=c) I(A:B|C=c)" class="latex" /></p>
<p>It&#8217;s the average amount of information about <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> learned by measuring <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> when you already knew <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /> </p>
<p>All this works for both classical (Shannon) and quantum (von Neumann) entropy. So, when we say &#8216;random variable&#8217; above, we<br />
could mean it in the traditional classical sense or in the quantum sense.</p>
<p>If <img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = 0" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=A%2C+C%2C+B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A, C, B" class="latex" /> has the following <b>Markov property</b>: if you know <img src="https://s0.wp.com/latex.php?latex=C%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C," class="latex" /> learning <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> tells you nothing new about <img src="https://s0.wp.com/latex.php?latex=B.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B." class="latex" />  In condensed matter physics, say a spin system, we get (quantum) random variables from measuring what&#8217;s going on in regions, and we have <b>short range entanglement</b> if <img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = 0" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> corresponds to some sufficiently thick region separating the regions <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B." class="latex" />   We&#8217;ll get this in any Gibbs state of a spin chain with a local Hamiltonian.</p>
<p>A <b>Markov network</b> is a graph with random variables at vertices (and thus subsets of vertices) such that  <img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = 0" class="latex" /> whenever <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is a subset of vertices that completely &#8216;shields&#8217; the subset <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> from the subset <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />: any path from <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> goes through a vertex in a <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /></p>
<p>The <a href="http://en.wikipedia.org/wiki/Hammersley%E2%80%93Clifford_theorem"><b>Hammersley&#8211;Clifford theorem</b></a> says that in the classical case we can get any Markov network from the Gibbs state </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-&#92;beta H)" class="latex" /></p>
<p>of a local Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H," class="latex" /> and vice versa.   Here a Hamiltonian is <b>local</b> if it is a sum of terms, one depending on the degrees of freedom in each <a href="http://en.wikipedia.org/wiki/Clique_%28graph_theory%29">clique</a> in the graph:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Csum_%7BC+%5Cin+%5Cmathrm%7Bcliques%7D%7D+h_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;sum_{C &#92;in &#92;mathrm{cliques}} h_C" class="latex" /></p>
<p>Hayden, Jozsa, Petz and Winter gave a quantum generalization of one direction of this result to graphs that are just &#8216;chains&#8217;, like this:</p>
<p>o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o</p>
<p>Namely: for such graphs, any quantum Markov network is the Gibbs state of some local Hamiltonian.  Now Poulin has shown the same for <i>all</i> graphs.  But the converse is, in general, <i>false</i>.  If the different terms <img src="https://s0.wp.com/latex.php?latex=h_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h_C" class="latex" /> in a local Hamiltonian all commute, its Gibbs state will have the Markov property.  But otherwise, it may not.  </p>
<p>For some related material, see:</p>
<p>&bull; David Poulin, <a href="http://cnls.lanl.gov/CQIT/poulin.pdf">Quantum graphical models and belief propagation</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/01/29/a-quantum-hammersley-clifford-theorem/#comments">1 Comment</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/" rel="category tag">quantum technologies</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/01/29/a-quantum-hammersley-clifford-theorem/" rel="bookmark" title="Permanent Link to A Quantum Hammersley&#8211;Clifford Theorem">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-6940 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-6940">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/" rel="bookmark">Quantropy (Part 1)</a></h2>
				<small>22 December, 2011</small><br />


				<div class="entry">
					<p>I wish you all happy holidays!  My wife Lisa and I are going to Bangkok on Christmas Eve, and thence to Luang Prabang, a town in Laos where the Nam Khan river joins the Mekong.  We&#8217;ll return to Singapore on the 30th.  See you then!  And in the meantime, here&#8217;s a little present&#8212;something to mull over.</p>
<div align="center"><img height="200" src="https://i1.wp.com/math.ucr.edu/home/baez/ludwig_boltzmann_and_richard_feynman.jpg" /></div>
<h4>Statistical mechanics versus quantum mechanics</h4>
<p>There&#8217;s a famous analogy between statistical mechanics and quantum mechanics.   In statistical mechanics, a system can be in any state, but its probability of being in a state with energy <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> is proportional to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-E%2FT%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-E/T) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the temperature in units where Boltzmann&#8217;s constant is 1.  In quantum mechanics, a system can move along any path, but its amplitude for moving along a path with action <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is proportional to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28i+S%2F%5Chbar%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(i S/&#92;hbar)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is Planck&#8217;s constant.  So, we have an analogy where Planck&#8217;s constant is like an imaginary temperature:</p>
<div align="center">
<table border="1">
<tr>
<td><b>Statistical Mechanics</b></td>
<td><b>Quantum Mechanics</b></td>
</tr>
<tr>
<td>probabilities</td>
<td>amplitudes</td>
</tr>
<tr>
<td>energy</td>
<td>action</td>
</tr>
<tr>
<td>temperature</td>
<td>Planck&#8217;s constant times <i>i</i></td>
</tr>
</table>
</div>
<p>In other words, making the replacements</p>
<p><img src="https://s0.wp.com/latex.php?latex=E+%5Cmapsto+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E &#92;mapsto S" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=T+%5Cmapsto+i+%5Chbar+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;mapsto i &#92;hbar " class="latex" /></p>
<p>formally turns the probabilities for states in statistical mechanics into the amplitudes for paths, or &#8216;histories&#8217;, in quantum mechanics.</p>
<p>But the probabilities <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-E%2FT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-E/T)" class="latex" /> arise naturally from maximizing entropy subject to a constraint on the expected energy.  So what about the amplitudes <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28i+S%2F%5Chbar%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(i S/&#92;hbar)" class="latex" />?</p>
<p>Following the analogy without thinking too hard, we&#8217;d guess it arises from minimizing something subject to a constraint on the expected action.</p>
<p>But now we&#8217;re dealing with complex numbers, so &#8216;minimizing&#8217; doesn&#8217;t sound right.  It&#8217;s better talk about finding a &#8216;stationary point&#8217;: a place where the derivative of something is zero.</p>
<p>More importantly, what is this something?  We&#8217;ll have to see&#8212;indeed, we&#8217;ll have to see if this whole idea makes sense!  But for now, let&#8217;s just call it &#8216;quantropy&#8217;.  This is a goofy word whose only virtue is that it quickly gets the idea across: just as the main ideas in statistical mechanics follow from the idea of maximizing entropy, we&#8217;d like the main ideas in quantum mechanics to follow from maximizing&#8230; err, well, finding a stationary point&#8230; of &#8216;quantropy&#8217;.</p>
<p>I don&#8217;t know how well this idea works, but there&#8217;s no way to know except by trying, so I&#8217;ll try it here.  I got this idea thanks to a nudge from Uwe Stroinski and WebHubTel, who started talking about <a href="https://johncarlosbaez.wordpress.com/2011/12/05/probabilities-versus-amplitudes/#comment-11558">the principle of least action and the principle of maximum entropy</a> at a moment when I was thinking hard about <a href="http://math.ucr.edu/home/baez/prob/">probabilities versus amplitudes</a>.</p>
<p>Of course, if this idea makes sense, someone probably had it already.  If you know where, please tell me.</p>
<p>Here&#8217;s the story&#8230;</p>
<h4>Statics </h4>
<p>Static systems at temperature zero obey the <a href="http://en.wikipedia.org/wiki/Principle_of_minimum_energy"><b>principle of minimum energy</b></a>.  <a href="http://en.wikipedia.org/wiki/Energy"><b>Energy</b></a> is typically the sum of kinetic and potential energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=E+%3D+K+%2B+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E = K + V" class="latex" /></p>
<p>where the <a href="http://en.wikipedia.org/wiki/Potential_energy"><b>potential energy</b></a> <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> depends only on the system&#8217;s position, while the <a href="http://en.wikipedia.org/wiki/Kinetic_energy"><b>kinetic energy</b></a> <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> also depends on its velocity.  The kinetic energy is often (but not always) a quadratic function of velocity with a minimum at velocity zero.  In classical physics this lets our system minimize energy in a two-step way.  First it will minimize kinetic energy, <img src="https://s0.wp.com/latex.php?latex=K%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K," class="latex" />  by staying still.  Then it will go on to minimize potential energy, <img src="https://s0.wp.com/latex.php?latex=V%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V," class="latex" /> by choosing the right place to stay still.</p>
<p>This is actually somewhat surprising: usually minimizing the sum of two things involves an interesting tradeoff.  But sometimes it doesn&#8217;t!</p>
<p>In quantum physics, a tradeoff <i>is</i> required, thanks to the uncertainty principle.  We can&#8217;t know the position and velocity of a particle simultaneously, so we can&#8217;t simultaneously minimize potential and kinetic energy.  This makes minimizing their sum much more interesting, as you&#8217;ll know if you&#8217;ve ever worked out the lowest-energy state of a harmonic oscillator or hydrogen atom.</p>
<p>But in classical physics, minimizing energy often forces us into &#8216;statics&#8217;: the boring part of physics, the part that studies things that don&#8217;t move.  And people usually say statics at temperature zero is governed by the <a href="http://en.wikipedia.org/wiki/Minimum_total_potential_energy_principle" rel="nofollow"><b>principle of minimum potential energy</b></a>.</p>
<p>Next let&#8217;s turn up the heat.  What about static systems at nonzero temperature?  This is what people study in the subject called &#8216;thermostatics&#8217;, or more often, &#8216;equilibrium thermodynamics&#8217;.</p>
<p>In classical or quantum thermostatics at any fixed temperature, a closed system will obey the <b>principle of minimum free energy</b>.   Now it will minimize</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+E+-+T+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = E - T S" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the temperature and <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the entropy.  Note that this principle reduces to the principle of minimum energy when <img src="https://s0.wp.com/latex.php?latex=T+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = 0." class="latex" />  But as <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> gets bigger, the second term in the above formula becomes more important, so the system gets more interested in having lots of entropy.  That&#8217;s why water forms orderly ice crystals at low temperatures (more or less minimizing energy despite low entropy) and a wild random gas at high temperatures (more or less maximizing entropy despite high energy).</p>
<p>But where does the principle of minimum free energy come from?</p>
<p>One nice way to understand it uses probability theory.  Suppose for simplicity that our system has a finite set of states, say <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> and the energy of the state <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=E_x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_x." class="latex" />   Instead of our system occupying a single definite state, let&#8217;s suppose it can be in <i>any</i> state, with a probability <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> of being in the state <img src="https://s0.wp.com/latex.php?latex=x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x." class="latex" />  Then its <b><a href="http://en.wikipedia.org/wiki/Entropy_in_thermodynamics_and_information_theory">entropy</a></b> is, by definition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Csum_x+p_x+%5Cln%28p_x%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;sum_x p_x &#92;ln(p_x) }" class="latex" /></p>
<p>The expected value of the energy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+E+%3D+%5Csum_x+p_x+E_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ E = &#92;sum_x p_x E_x } " class="latex" /></p>
<p>Now suppose our system maximizes entropy subject to a constraint on the expected value of energy.   Thanks to the Lagrange multiplier trick, this is the same as maximizing</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+-+%5Cbeta+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S - &#92;beta E " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is a Lagrange multiplier.  When we go ahead and maximize this, we see the system chooses a <a href="http://en.wikipedia.org/wiki/Boltzmann_distribution"><b>Boltzmann distribution</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Cfrac%7B%5Cexp%28-%5Cbeta+E_x%29%7D%7B%5Csum_x+%5Cexp%28-%5Cbeta+E_x%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;frac{&#92;exp(-&#92;beta E_x)}{&#92;sum_x &#92;exp(-&#92;beta E_x)}} " class="latex" /></p>
<p>This is just a calculation; you must do it for yourself someday, and I will not rob you of that joy.</p>
<p>But what does this mean?   We could call <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Thermodynamic_beta"><b>coolness</b></a>, since its inverse is the <b><a href="http://en.wikipedia.org/wiki/Temperature">temperature</a></b>, <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /> at least in units where Boltzmann&#8217;s constant is set to 1.   So, when the temperature is positive, maximizing <img src="https://s0.wp.com/latex.php?latex=S+-+%5Cbeta+E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S - &#92;beta E" class="latex" /> is the same as minimizing the <a href="http://en.wikipedia.org/wiki/Helmholtz_free_energy"><b>free energy</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+E+-+T+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = E - T S " class="latex" /></p>
<p>(For negative temperatures, maximizing <img src="https://s0.wp.com/latex.php?latex=S+-+%5Cbeta+E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S - &#92;beta E" class="latex" /> would amount to <i>maximizing</i> free energy.)</p>
<p>So, every minimum or maximum principle described so far can be seen as a special case or limiting case of the <a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy"><b>principle of maximum entropy</b></a>, as long as we admit that sometimes we need to maximize entropy <i>subject to constraints</i>.</p>
<p>Why &#8216;limiting case&#8217;?  Because the principle of least energy only shows up as the low-temperature limit, or <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to &#92;infty" class="latex" /> limit, of the idea of maximizing entropy subject to a constraint on expected energy.  But that&#8217;s good enough for me.</p>
<h4>Dynamics </h4>
<p>Now suppose things are changing as time passes, so we&#8217;re doing &#8216;dynamics&#8217; instead of mere &#8216;statics&#8217;.  In classical mechanics we can imagine a system tracing out a path <img src="https://s0.wp.com/latex.php?latex=%5Cgamma%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma(t)" class="latex" /> as time passes from one time to another, for example from <img src="https://s0.wp.com/latex.php?latex=t+%3D+t_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = t_0" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=t+%3D+t_1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = t_1." class="latex" />   The <a href="http://en.wikipedia.org/wiki/Action_%28physics%290"><b>action</b></a> of this path is typically the integral of the kinetic minus potential energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A%28%5Cgamma%29+%3D+%5Cdisplaystyle%7B+%5Cint_%7Bt_0%7D%5E%7Bt_1%7D++%28K%28t%29+-+V%28t%29%29+%5C%2C+dt+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A(&#92;gamma) = &#92;displaystyle{ &#92;int_{t_0}^{t_1}  (K(t) - V(t)) &#92;, dt }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=K%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K(t)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=V%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(t)" class="latex" /> depend on the path <img src="https://s0.wp.com/latex.php?latex=%5Cgamma.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma." class="latex" />  Note that now I&#8217;m calling action <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> instead of the more usual <img src="https://s0.wp.com/latex.php?latex=S%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S," class="latex" /> since we&#8217;re already using <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> for entropy and I don&#8217;t want things to get any more confusing than necessary.</p>
<p>The <a href="http://en.wikipedia.org/wiki/Principle_of_least_action"><b>principle of least action</b></a> says that if we fix the endpoints of this path, that is the points <img src="https://s0.wp.com/latex.php?latex=%5Cgamma%28t_0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma(t_0)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cgamma%28t_1%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma(t_1)," class="latex" /> the system will follow the path that minimizes the action subject to these constraints.</p>
<p>Why is there a minus sign in the definition of action?  How did people come up with principle of least action?   How is it related to the principle of least energy in statics?   These are all fascinating questions.  But I have a half-written book that tackles these questions, so I won&#8217;t delve into them here:</p>
<p>• John Baez and Derek Wise, <i><a href="http://math.ucr.edu/home/baez/classical/texfiles/2005/book/classical.pdf">Lectures on Classical Mechanics</a></i>.</p>
<p>Instead, let&#8217;s go straight to dynamics in quantum mechanics.  Here Feynman proposed that instead of our following a single definite path, it can follow <i>any</i> path, with an amplitude <img src="https://s0.wp.com/latex.php?latex=a%28%5Cgamma%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a(&#92;gamma)" class="latex" /> of following the path <img src="https://s0.wp.com/latex.php?latex=%5Cgamma.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma." class="latex" />   And he proposed this prescription for the amplitude:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a%28%5Cgamma%29+%3D+%5Cfrac%7B%5Cexp%28i+A%28%5Cgamma%29%2F%5Chbar%29%7D%7B%5Cint++%5Cexp%28i+A%28%5Cgamma%29%2F%5Chbar%29+%5C%2C+d+%5Cgamma%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a(&#92;gamma) = &#92;frac{&#92;exp(i A(&#92;gamma)/&#92;hbar)}{&#92;int  &#92;exp(i A(&#92;gamma)/&#92;hbar) &#92;, d &#92;gamma}} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is <a href="http://en.wikipedia.org/wiki/Planck_constant"><b>Planck&#8217;s constant</b></a>.  He also gave a heuristic argument showing that as <img src="https://s0.wp.com/latex.php?latex=%5Chbar+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar &#92;to 0" class="latex" />, this prescription reduces to the principle of least action!</p>
<p>Unfortunately the integral over all paths&#8212;called a &#8216;path integral&#8217;&#8212;is hard to make rigorous except in certain special cases.   And it&#8217;s a bit of a distraction for what I&#8217;m talking about now.  So let&#8217;s talk more abstractly about &#8216;histories&#8217; instead of paths with fixed endpoints, and consider a system whose possible &#8216;histories&#8217; form a finite set, say <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  Systems of this sort frequently show up as discrete approximations to continuous ones, but they also show up in other contexts, like quantum cellular automata and topological quantum field theories.  Don&#8217;t worry if you don&#8217;t know what those things are.  I&#8217;d just prefer to write sums instead of integrals now, to make everything easier.</p>
<p>Suppose the action of the history <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=A_x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_x." class="latex" />  Then Feynman&#8217;s <a href="http://en.wikipedia.org/wiki/Path_integral_formulation"><b>sum over histories formulation</b></a> of quantum mechanics says the amplitude of the history <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28i+A_x+%2F%5Chbar%29%7D%7B%5Csum_x++%5Cexp%28i+A_x+%2F%5Chbar%29+%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(i A_x /&#92;hbar)}{&#92;sum_x  &#92;exp(i A_x /&#92;hbar) }} " class="latex" /></p>
<p>This looks very much like the Boltzmann distribution:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Cfrac%7B%5Cexp%28-E_x%2FT%29%7D%7B%5Csum_x+%5Cexp%28-+E_x%2FT%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;frac{&#92;exp(-E_x/T)}{&#92;sum_x &#92;exp(- E_x/T)}} " class="latex" /></p>
<p>Indeed, the only serious difference is that we&#8217;re taking the exponential of an imaginary quantity instead of a real one.</p>
<p>So far everything has been a review of very standard stuff.  Now comes something weird and new&#8212;at least, new to me.</p>
<h4> Quantropy </h4>
<p>I&#8217;ve described statics and dynamics, and a famous analogy between them, but there are some missing items in the analogy, which would be good to fill in:</p>
<div align="center">
<table border="1">
<tr>
<td><b>Statics</b></td>
<td><b>Dynamics</b></td>
</tr>
<tr>
<td>statistical mechanics</td>
<td>quantum mechanics</td>
</tr>
<tr>
<td>probabilities</td>
<td>amplitudes</td>
</tr>
<tr>
<td>Boltzmann distribution</td>
<td>Feynman sum over histories</td>
</tr>
<tr>
<td>energy</td>
<td>action</td>
</tr>
<tr>
<td>temperature</td>
<td>Planck&#8217;s constant times <i>i</i></td>
</tr>
<tr>
<td>entropy</td>
<td>???</td>
</tr>
<tr>
<td>free energy</td>
<td>???</td>
</tr>
</table>
</div>
<p>Since the Boltzmann distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Cfrac%7B%5Cexp%28-E_x%2FT%29%7D%7B%5Csum_x+%5Cexp%28-+E_x%2FT%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;frac{&#92;exp(-E_x/T)}{&#92;sum_x &#92;exp(- E_x/T)}} " class="latex" /></p>
<p>comes from the principle of maximum entropy, you might hope Feynman&#8217;s sum over histories formulation of quantum mechanics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28i+A_x+%2F%5Chbar%29%7D%7B%5Csum_x++%5Cexp%28i+A_x+%2F%5Chbar%29+%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(i A_x /&#92;hbar)}{&#92;sum_x  &#92;exp(i A_x /&#92;hbar) }} " class="latex" /></p>
<p>comes from a maximum principle too!</p>
<p>Unfortunately Feynman&#8217;s sum over histories involves complex numbers, and it doesn&#8217;t make sense to maximize a complex function.   However, when we say nature likes to minimize or maximize something, it often behaves like a bad freshman who applies the first derivative test and quits there: it just finds a <a href="http://en.wikipedia.org/wiki/Stationary_point"><b>stationary point</b></a>, where the first derivative is zero.  For example, in statics we have &#8216;stable&#8217; equilibria, which are local minima of the energy, but also &#8216;unstable&#8217; equilibria, which are still stationary points of the energy, but not local minima.  This is good for us, because stationary points still make sense for complex functions.</p>
<p>So let&#8217;s try to derive Feynman&#8217;s prescription from some sort of &#8216;principle of stationary quantropy&#8217;.</p>
<p>Suppose we have a finite set of <b>histories</b>, <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> and each history <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> has a complex <b>amplitude</b> <img src="https://s0.wp.com/latex.php?latex=a_x++%5Cin+%5Cmathbb%7BC%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x  &#92;in &#92;mathbb{C}." class="latex" />  We&#8217;ll assume these amplitudes are normalized so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_x+a_x+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_x a_x = 1 " class="latex" /></p>
<p>since that&#8217;s what Feynman&#8217;s normalization actually achieves.  We can try to define the <b>quantropy</b> of <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+-+%5Csum_x+a_x+%5Cln%28a_x%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = - &#92;sum_x a_x &#92;ln(a_x) }" class="latex" /></p>
<p>You might fear this is ill-defined when <img src="https://s0.wp.com/latex.php?latex=a_x+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x = 0," class="latex" /> but that&#8217;s not the worst problem; in the study of entropy we typically set</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%5Cln+0+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;ln 0 = 0" class="latex" /></p>
<p>and everything works fine.   The worst problem is that the logarithm has different branches: we can add any multiple of <img src="https://s0.wp.com/latex.php?latex=2+%5Cpi+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2 &#92;pi i" class="latex" /> to our logarithm and get another equally good logarithm.  For now suppose we&#8217;ve chosen a specific logarithm for each number <img src="https://s0.wp.com/latex.php?latex=a_x%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x," class="latex" /> and suppose that when we vary them they don&#8217;t go through zero, so we can smoothly change the logarithm as we move them.  This should let us march ahead for now, but clearly it&#8217;s a disturbing issue which we should revisit someday.</p>
<p>Next, suppose each history <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> has an <b>action</b> <img src="https://s0.wp.com/latex.php?latex=A_x+%5Cin+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_x &#92;in &#92;mathbb{R}." class="latex" />  Let&#8217;s seek amplitudes <img src="https://s0.wp.com/latex.php?latex=a_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x" class="latex" /> that give a stationary point of the quantropy <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> subject to a constraint on the <b>expected action</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+A+%3D+%5Csum_x+a_x+A_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ A = &#92;sum_x a_x A_x } " class="latex" /></p>
<p>The term &#8216;expected action&#8217; is a bit odd, since the numbers <img src="https://s0.wp.com/latex.php?latex=a_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x" class="latex" /> are amplitudes rather than probabilities.   While I could try to justify it from how expected values are computed in Feynman&#8217;s formalism, I&#8217;m mainly using this term because <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is analogous to the expected value of the energy, which we saw earlier.  We can worry later what all this stuff really means; right now I&#8217;m just trying to push forwards with an analogy and do a calculation.</p>
<p>So, let&#8217;s look for a stationary point of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> subject to a constraint on <img src="https://s0.wp.com/latex.php?latex=A.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A." class="latex" />  To do this, I&#8217;d be inclined to use <a href="http://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a> and look for a stationary point of</p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A" class="latex" /></p>
<p>But there&#8217;s another constraint, too, namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_x+a_x+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_x a_x = 1 " class="latex" /></p>
<p>So let&#8217;s write</p>
<p><img src="https://s0.wp.com/latex.php?latex=B+%3D+%5Csum_x+a_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B = &#92;sum_x a_x " class="latex" /></p>
<p>and look for stationary points of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> subject to the constraints</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%3D+%5Calpha+%2C+%5Cqquad+B+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A = &#92;alpha , &#92;qquad B = 1" class="latex" /></p>
<p>To do this, the Lagrange multiplier recipe says we should find stationary points of</p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A+-+%5Cmu+B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A - &#92;mu B" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> are Lagrange multipliers.  The Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> is really interesting.  It&#8217;s analogous to &#8216;coolness&#8217;, <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T," class="latex" /> so our analogy chart suggests that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1%2Fi%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = 1/i&#92;hbar" class="latex" /></p>
<p>This says that when <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> gets big our system becomes close to classical.  So, we could call <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> the <b>classicality</b> of our system.   The Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> is less interesting&#8212;or at least I haven&#8217;t thought about it much.</p>
<p>So, we&#8217;ll follow the usual Lagrange multiplier recipe and look for amplitudes for which</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%3D+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+%5Cleft%28Q+-+%5Clambda+A+-+%5Cmu+B+%5Cright%29+%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 = &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} &#92;left(Q - &#92;lambda A - &#92;mu B &#92;right) }  " class="latex" /></p>
<p>holds, along with the constraint equations.  We begin by computing the derivatives we need:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bcclcl%7D+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+Q++%7D++%26%3D%26+-+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+%5C%3B+a_x+%5Cln%28a_x%29%7D++%26%3D%26+-+%5Cln%28a_x%29+-+1+%5C%5C++++%5C%5C++++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D%5C%3B+A++%7D++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+a_x+A_x%7D+%26%3D%26+A_x+%5C%5C++++%5C%5C+++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+B++%7D++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D%5C%3B+a_x+%7D+%26%3D%26+1+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{cclcl} &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} Q  }  &amp;=&amp; - &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} &#92;; a_x &#92;ln(a_x)}  &amp;=&amp; - &#92;ln(a_x) - 1 &#92;&#92;    &#92;&#92;    &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x}&#92;; A  }  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} a_x A_x} &amp;=&amp; A_x &#92;&#92;    &#92;&#92;   &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} B  }  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x}&#92;; a_x } &amp;=&amp; 1 &#92;end{array} " class="latex" /></p>
<p>Thus, we need</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%3D+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+%5Cleft%28Q+-+%5Clambda+A+-+%5Cmu+B+%5Cright%29+%3D+-%5Cln%28a_x%29+-+1-+%5Clambda+A_x+-+%5Cmu+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 = &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} &#92;left(Q - &#92;lambda A - &#92;mu B &#92;right) = -&#92;ln(a_x) - 1- &#92;lambda A_x - &#92;mu } " class="latex" /></p>
<p>or</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28-%5Clambda+A_x%29%7D%7B%5Cexp%28%5Cmu+%2B+1%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(-&#92;lambda A_x)}{&#92;exp(&#92;mu + 1)} } " class="latex" /></p>
<p>The constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_x+a_x+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_x a_x = 1" class="latex" /></p>
<p>then forces us to choose:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cexp%28%5Cmu+%2B+1%29+%3D+%5Csum_x+%5Cexp%28-%5Clambda+A_x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;exp(&#92;mu + 1) = &#92;sum_x &#92;exp(-&#92;lambda A_x) } " class="latex" /></p>
<p>so we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28-%5Clambda+A_x%29%7D%7B%5Csum_x+%5Cexp%28-%5Clambda+A_x%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(-&#92;lambda A_x)}{&#92;sum_x &#92;exp(-&#92;lambda A_x)} } " class="latex" /></p>
<p>Hurrah!  This is precisely Feynman&#8217;s sum over histories formulation of quantum mechanics if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1%2Fi%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = 1/i&#92;hbar" class="latex" /></p>
<p>We could go further with the calculation, but this is the punchline, so I&#8217;ll stop here.    I&#8217;ll just note that the final answer:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28iA_x%2F%5Chbar%29%7D%7B%5Csum_x+%5Cexp%28iA_x%2F%5Chbar%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(iA_x/&#92;hbar)}{&#92;sum_x &#92;exp(iA_x/&#92;hbar)} } " class="latex" /></p>
<p>does two equivalent things in one blow:</p>
<p>• It gives a stationary point of quantropy subject to the constraints that the amplitudes sum to 1 and the expected action takes some fixed value.</p>
<p>• It gives a stationary point of the <b>free action</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+-+i+%5Chbar+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A - i &#92;hbar Q" class="latex" /></p>
<p>subject to the constraint that the amplitudes sum to 1.</p>
<p>In case the second point is puzzling, note that the &#8216;free action&#8217; is the quantum analogue of &#8216;free energy&#8217;, <img src="https://s0.wp.com/latex.php?latex=E+-+T+S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E - T S." class="latex" />  It&#8217;s also just <img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A" class="latex" /> times <img src="https://s0.wp.com/latex.php?latex=-i+%5Chbar%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-i &#92;hbar," class="latex" /> and we already saw that finding stationary points of <img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A" class="latex" /> is another way of finding stationary points of quantropy with a constraint on the expected action.</p>
<p>Note also that when <img src="https://s0.wp.com/latex.php?latex=%5Chbar+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar &#92;to 0" class="latex" />, free action reduces to action, so we recover the principle of least action&#8212;or at least <i>stationary</i> action&#8212;in classical mechanics.</p>
<blockquote><p>
<b>Summary.</b>  We recover Feynman&#8217;s sum over histories formulation of quantum mechanics from assuming that all histories have complex amplitudes, that these amplitudes sum to one, and that the amplitudes give a stationary point of quantropy subject to a constraint on the expected action.  Alternatively, we can assume the amplitudes sum to one and that they give a stationary point of free action.
</p></blockquote>
<p>That&#8217;s sort of nice!  So, here&#8217;s our analogy chart, all filled in:</p>
<div align="center">
<table border="1">
<tr>
<td><b>Statics</b></td>
<td><b>Dynamics</b></td>
</tr>
<tr>
<td>statistical mechanics</td>
<td>quantum mechanics</td>
</tr>
<tr>
<td>probabilities</td>
<td>amplitudes</td>
</tr>
<tr>
<td>Boltzmann distribution</td>
<td>Feynman sum over histories</td>
</tr>
<tr>
<td>energy</td>
<td>action</td>
</tr>
<tr>
<td>temperature</td>
<td>Planck&#8217;s constant times <i>i</i></td>
</tr>
<tr>
<td>entropy</td>
<td>quantropy</td>
</tr>
<tr>
<td>free energy</td>
<td>free action</td>
</tr>
</table>
</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/#comments">133 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/" rel="bookmark" title="Permanent Link to Quantropy (Part 1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-6147 post type-post status-publish format-standard hentry category-biodiversity category-biology category-mathematics" id="post-6147">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/11/07/measuring-biodiversity/" rel="bookmark">Measuring Biodiversity</a></h2>
				<small>7 November, 2011</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://www.maths.ed.ac.uk/~tl/">Tom Leinster</a></b></i></p>
<p>Even if there weren&#8217;t a <a href="http://www.azimuthproject.org/azimuth/show/Biodiversity">global biodiversity crisis</a>, we&#8217;d  want to know how to put a number on biodiversity. As Lord Kelvin famously put it:</p>
<blockquote>
<p>When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, your knowledge is of a meagre and unsatisfactory kind: it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced to the stage of <i>science</i>.</p>
</blockquote>
<p>In this post, I&#8217;ll talk about what happens when you take a mass of biological data and try to turn it into a single <i>number</i>, intended to measure biodiversity.</p>
<p>There have been more than 50 years of debate about how to measure diversity.  While the idea of putting a number on biological diversity goes back to the 1940s at least, the debate really seems to have got going in the wake of  pioneering work by the great ecologist <a href="http://en.wikipedia.org/wiki/Robert_Whittaker">Robert Whittaker</a> in the 1960s.</p>
<p>There followed several decades in which progress was made&#8230; but there was a lot of talking at cross-purposes. In fact, there was so much confusion that some people gave up on the diversity concept altogether. The mood is summed up by the title of an excellent and much-cited paper of <a href="http://www.bio.sdsu.edu/pub/stuart/stuart.html">Stuart Hurlbert</a>:</p>
<p>&bull; S. H. Hurlbert, The nonconcept of species diversity: A critique and alternative parameters. <i>Ecology</i> <b>52</b>:577&ndash;586, 1971.</p>
<p>So why all the confusion?</p>
<p>One reason is that the word &#8220;diversity&#8221; is used by different people in many different ways.  We all know that diversity is important: so if you found a quantity that seemed to measure biological variation in a sensible way, you might be tempted to call it &#8220;diversity&#8221; and publish a paper promoting your quantity over all other quantities that have ever been given that name.  There are literally dozens of measures of diversity in the literature.  Here are two simple ones:</p>
<ul>
<li>
<i>Species richness</i> is simply the number of species in the community concerned.
</li>
<li>
The <i>Shannon entropy</i> is <img src="https://s0.wp.com/latex.php?latex=-%5Csum_%7Bi+%3D+1%7D%5ES+p_i+%5Clog%28p_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;sum_{i = 1}^S p_i &#92;log(p_i)" class="latex" />, where our community consists of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> species in proportions <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cldots%2C+p_S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;ldots, p_S" class="latex" />.
</li>
</ul>
<p>Which quantity should we call &#8220;diversity&#8221;?  Do all these quantities really measure the same kind of thing?  If community A has greater than species richness than community B, but lower Shannon entropy, what does it <i>mean</i>?</p>
<p>Another cause for confusion is a blurring between the questions</p>
<blockquote>
<p><i>Which quantities deserve to be called diversity?</i></p>
</blockquote>
<p>and</p>
<blockquote>
<p><i>Which quantities are we capable of measuring experimentally?</i></p>
</blockquote>
<p>For example, we might all agree that species richness is an important quantity, but that doesn&#8217;t mean that species richness is easy to measure in practice.  (In fact, it&#8217;s not, more on which below.)  My own view is that the two questions should be kept separate:</p>
<blockquote>
<p>The statistical problem of designing appropriate estimators becomes relevant only after the measure to be estimated is accepted to be meaningful.</p>
</blockquote>
<p>(Hans-Rolf Gregorius, Elizabeth M. Gillet, Generalized Simpson-diversity, <i>Ecological Modelling</i> <b>211</b>:90&ndash;96, 2008.)</p>
<p>The problems involved in quantifying diversity are of three types: <b>practical</b>, <b>statistical</b> and <b>conceptual</b>. I&#8217;ll say a little about the first two, and rather more about the third.</p>
<p><b>Practical</b>&nbsp; Suppose that you&#8217;re doing a survey of the vertebrates in a forest.  Perhaps one important species is brightly coloured and noisy, while another is silent, shy, and well-camouflaged.  How do you prevent the first from being recorded disproportionately?</p>
<p>Or suppose that you&#8217;re carrying out a survey, with multiple people doing the fieldwork.  Different people have a tendency to spot different things: for example, one person might be short-sighted and another long-sighted.  How do you ensure that this doesn&#8217;t affect your results?</p>
<p><b>Statistical</b>&nbsp; Imagine that you want to know how many distinct species of insect live in a particular area &mdash; the &#8220;species richness&#8221;, in the terminology introduced above.  You go out collecting, and you come back with 100 specimens representing 10 species.</p>
<p>But your survey might have missed some species altogether, so you go out and get a bigger sample.  This time, you get 200 specimens representing 15 species.  Does this help you discover how many species there <i>really</i> are?  </p>
<p>Logically, not at all.  The only certainty is that there are at least 15 species.  Maybe there are thousands of species, but almost all of them are extremely rare.  Or maybe there are really only 15.  Unless you collect <i>all</i> the insects, you&#8217;ll never know for sure exactly how many species there are.</p>
<p>However, it may be that you can make reasonable assumptions about the frequency distribution of the species.  People sometimes do exactly this, to try to overcome the difficulty of estimating species richness.</p>
<p><b>Conceptual</b>&nbsp; This is what I really want to talk about.</p>
<p>I mentioned earlier that different people mean different things by &#8220;diversity&#8221;.  Here&#8217;s an example. </p>
<p>Consider two bird communities.  The first looks like this:</p>
<p><img src="https://i0.wp.com/www.maths.ed.ac.uk/~tl/birds_top.png" alt="" /></p>
<p>It contains four species, one of which is responsible for most of the population, and three of which are quite rare.  The second looks like this:</p>
<p><img src="https://i1.wp.com/www.maths.ed.ac.uk/~tl/birds_bottom.png" alt="" /></p>
<p>It has only three species, but they&#8217;re evenly balanced.</p>
<p>Which community is the more diverse?  It&#8217;s a matter of opinion. Mostly in the press, and in many scholarly articles too, &#8220;biodiversity&#8221; is used as a synonym for &#8220;species richness&#8221;.  On this count, the first community is more diverse.  But if you&#8217;re more concerned with the healthy functioning of the whole community, the presence of rare species might not be particularly important: it&#8217;s <i>balance</i> that matters, and the second community has more of that.</p>
<p>Different people using the word &#8220;diversity&#8221; attach different amounts of significance to rare species.  There&#8217;s a spectrum of points of view, ranging from those who give rare species the same weight as common ones (as in the definition of species richness) to those who are only interested in the most common species of all.  Every point on this spectrum of viewpoints is reasonable.  None should have a monopoly on the word &#8220;diversity&#8221;. </p>
<p>At least, that&#8217;s what <a href="http://www.maths.gla.ac.uk/~cc">Christina Cobbold</a> and I argue in our new paper:</p>
<p>&bull; Tom Leinster, Christina A. Cobbold, <a href="http://www.maths.ed.ac.uk/~tl/mdiss.pdf">Measuring diversity: the importance of species similarity</a>, <i>Ecology</i>, in press (<a href="http://www.esajournals.org/doi/abs/10.1890/10-2402.1">doi:10.1890/10-2402.1</a>).</p>
<p>But that&#8217;s not actually our main point.  As the title suggests, the real purpose of our paper is to show how to measure diversity in a way that reflects <i>the varying differences between species</i>.  I&#8217;ll explain. </p>
<p>Most of the existing approaches to measuring biodiversity go like this.</p>
<p>We have a &#8220;community&#8221; of organisms &mdash; the fish in a lake, the fungi in a forest, or the bacteria on your skin.  This community is divided into <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> groups, conventionally called <b>species</b>, though they needn&#8217;t be species in the ordinary sense.</p>
<p>We assume that we know the <b>relative abundances</b>, or relative frequencies, of the species.  Write them as <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cldots%2C+p_S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;ldots, p_S" class="latex" />.  Thus, <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the proportion of the total population that belongs to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, where &#8220;proportion&#8221; is measured in any way you think sensible (number of individuals, total mass, etc).</p>
<p>We only care about <i>relative</i> abundances here, not <i>absolute</i> abundances: so <img src="https://s0.wp.com/latex.php?latex=p_1+%2B+%5Ccdots+%2B+p_S+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 + &#92;cdots + p_S = 1" class="latex" />. If half of a forest is destroyed, it might be a catastrophe, but on the (unrealistic) assumption that all the flora and fauna in the forest were distributed homogeneously, it won&#8217;t actually change the biodiversity.  (That&#8217;s not a statement about what&#8217;s important in life; it&#8217;s only a statement about the usage of a word.)  </p>
<p>This model is common but crude.  It can&#8217;t detect the difference between a community of six dramatically different species and a community consisting of six species of barnacle.</p>
<p>So, Christina and I use a refined model, as follows.  We assume that we also have a measure of the <b>similarity</b> between each pair of species.  This is a real number between 0 and 1, with 0 indicating that the species are as dissimilar as could be, and 1 indicating that they&#8217;re identical.  Writing the similarity between the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th and <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th species as <img src="https://s0.wp.com/latex.php?latex=Z_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z_{ij}" class="latex" />, this gives an <img src="https://s0.wp.com/latex.php?latex=S+%5Ctimes+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S &#92;times S" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Z}" class="latex" />.  Our only assumption on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Z}" class="latex" /> is that its diagonal entries are all 1: every species is identical to itself.</p>
<p>There are many ways of measuring inter-species similarity. Probably the most familiar approach is genetic, as in &#8220;you share 98% of your DNA with a chimpanzee&#8221;.  But there are many other possibilities: functional, phylogenetic, morphological, taxonomic, &#8230;.  Diversity is a measure of the variety of life; having to choose a measure of similarity forces you to get clear exactly what you mean by &#8220;variety&#8221;.</p>
<p>Christina and I are by no means the first people to incorporate species similarity into the model of an ecological community.  The main new thing in our paper is this measure of the community&#8217;s diversity:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%7D%5Eq+D%5E%7B%5Cmathbf%7BZ%7D%7D%28%5Cmathbf%7Bp%7D%29+%3D+%28+%5Csum_i+p_i+%28%5Cmathbf%7BZ%7D%5Cmathbf%7Bp%7D%29_i%5E%7Bq+-+1%7D+%29%5E%7B1%2F%281+-+q%29%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^q D^{&#92;mathbf{Z}}(&#92;mathbf{p}) = ( &#92;sum_i p_i (&#92;mathbf{Z}&#92;mathbf{p})_i^{q - 1} )^{1/(1 - q)}." class="latex" /></p>
<p>What does this mean?</p>
<ul>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7B%7D%5Eq+D%5E%7B%5Cmathbf%7BZ%7D%7D%28%5Cmathbf%7Bp%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^q D^{&#92;mathbf{Z}}(&#92;mathbf{p})" class="latex" /> is what we call the <b>diversity of order <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /></b> of the community.  Here <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a parameter between <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;infty" class="latex" />, which you get to choose.  Different values of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> represent different points on the spectrum of viewpoints described above.  Small values of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> give high importance to rare species; large values of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> give high importance to common species.
</li>
<li>
<img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bp%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{p}" class="latex" /> is shorthand for the relative abundances <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cldots%2C+p_S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;ldots, p_S" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Z}" class="latex" /> is the matrix of similarities.
</li>
<li>
<img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BZ%7D%5Cmathbf%7Bp%7D%29_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;mathbf{Z}&#92;mathbf{p})_i" class="latex" /> means <img src="https://s0.wp.com/latex.php?latex=%5Csum_j+Z_%7Bij%7D+p_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_j Z_{ij} p_j" class="latex" />.
</li>
</ul>
<p>The expression doesn&#8217;t make sense if <img src="https://s0.wp.com/latex.php?latex=q+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 1" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=q+%3D+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = &#92;infty" class="latex" />, but can be made sense of by taking limits.  For <img src="https://s0.wp.com/latex.php?latex=q+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 1" class="latex" />, this gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E1+D%5E%7B%5Cmathbf%7BZ%7D%7D%28%5Cmathbf%7Bp%7D%29+%3D+1%2F%28%5Cmathbf%7BZ+p%7D%29_1%5E%7Bp_1%7D+%28%5Cmathbf%7BZ+p%7D%29_2%5E%7Bp_2%7D+%5Ccdots+%28%5Cmathbf%7BZ+p%7D%29_S%5E%7Bp_S%7D+%3D+%5Cexp%28-%5Csum_i+p_i+%5Clog%28%5Cmathbf%7BZ+p%7D%29_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^1 D^{&#92;mathbf{Z}}(&#92;mathbf{p}) = 1/(&#92;mathbf{Z p})_1^{p_1} (&#92;mathbf{Z p})_2^{p_2} &#92;cdots (&#92;mathbf{Z p})_S^{p_S} = &#92;exp(-&#92;sum_i p_i &#92;log(&#92;mathbf{Z p})_i)" class="latex" /></p>
<p>If you want to know the value at <img src="https://s0.wp.com/latex.php?latex=q+%3D+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = &#92;infty" class="latex" />, or any of the other mathematical details, you can read <a href="http://golem.ph.utexas.edu/category/2011/10/measuring_diversity.html">this post at the <i>n</i>-Category Caf&eacute;</a>, or of course our paper.  In both places, you&#8217;ll also find an explanation of what motivates this formula.  What&#8217;s more, you&#8217;ll see that many existing measures of diversity are special cases of ours, obtained by taking particular values for <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and/or <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Z}" class="latex" />.</p>
<p>But I won&#8217;t talk about any of that here.  Instead, I&#8217;ll tell you  how taking species similarity into account can radically alter the assessment of diversity.</p>
<p>I&#8217;ll do this using an example: butterflies of subfamily Charaxinae at a site in an Ecuadorian rainforest. The data is from here:</p>
<p>&bull; P. J. DeVries, D. Murray, R. Lande, Species diversity in vertical, horizontal and temporal dimensions of a fruit-feeding butterfly community in an Ecuadorian rainforest.  <i>Biological Journal of the Linnean Society</i> <b>62</b>:343&ndash;364, 1997.</p>
<p>They measured the butterfly abundances in both the canopy (top level) and understorey (lower level) at this site, with the following results:</p>
<table cellpadding="1" cellspacing="0">
<tr>
<td><b>Species</b></td>
<td><b>Canopy&nbsp;&nbsp;</b></td>
<td><b>Understorey</b></td>
</tr>
<tr>
<td><i>Prepona laertes</i></td>
<td>15</td>
<td>0</td>
</tr>
<tr>
<td><i>Archaeoprepona demophon&nbsp;&nbsp;</i></td>
<td>14</td>
<td>37</td>
</tr>
<tr>
<td><i>Zaretis itys</i></td>
<td>25</td>
<td>11</td>
</tr>
<tr>
<td><i>Memphis arachne</i></td>
<td>89</td>
<td>23</td>
</tr>
<tr>
<td><i>Memphis offa</i></td>
<td>21</td>
<td>3</td>
</tr>
<tr>
<td><i>Memphis xenocles</i></td>
<td>32</td>
<td>8</td>
</tr>
</table>
<p>Which is more diverse: canopy or understorey?</p>
<p>We&#8217;ve already seen that the answer is going to depend on what exactly we mean by &#8220;diverse&#8221;.</p>
<p>First let&#8217;s answer the question under the (crude!) assumption that different species have nothing whatsoever in common.  This means taking our similarity matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Z}" class="latex" /> to be the identity matrix: if <img src="https://s0.wp.com/latex.php?latex=i+%5Cneq+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;neq j" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=Z_%7Bij%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z_{ij} = 0" class="latex" /> (totally dissimilar), and if <img src="https://s0.wp.com/latex.php?latex=i+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = j" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=Z_%7Bii%7D+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z_{ii} = 1" class="latex" /> (totally identical).</p>
<p>Now, remember that there&#8217;s a spectrum of viewpoints on how much importance to give to rare species when measuring diversity. Rather than choosing a particular viewpoint, we&#8217;ll calculate the diversity from <i>all</i> viewpoints, and display it on a graph. In other words, we&#8217;ll draw the graph of <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5Eq+D%5E%7B%5Cmathbf%7BZ%7D%7D%28%5Cmathbf%7Bp%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^q D^{&#92;mathbf{Z}}(&#92;mathbf{p})" class="latex" /> (the diversity of order <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />) against <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> (the viewpoint).  Here&#8217;s what we get:</p>
<p><img width="450" src="https://i1.wp.com/www.maths.ed.ac.uk/~tl/naive_bflies.jpg" alt="" /></p>
<p>(the horizontal axis should be labelled with a <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.)</p>
<p>Conclusion: from <i>all</i> viewpoints, the butterfly population in the canopy is at least as diverse as that in the understorey.</p>
<p>Now let&#8217;s do it again, but this time taking account of the varying <i>similarities</i> between species of butterflies.  We don&#8217;t have much to go on: how do we know whether <i>Prepona laertes</i> is very similar to, or very different from, <i>Archaeoprepona demophon</i>?   With only the data above, we don&#8217;t.  So what can we do?</p>
<p>All we have to go on is the taxonomy.  Remember your high school biology: for the butterfly <i>Prepona laertes</i>, the genus is <i>Prepona</i> and the species is <i>laertes</i>.  We&#8217;d expect species in the same genus to have more in common than species in different genera.  So let&#8217;s define the similarity between two species as follows:</p>
<ul>
<li>
the similarity is 1 if the species are the same
</li>
<li>
the similarity is 0.5 if the species are different but in the same genus
</li>
<li>
the similarity is 0 if they are not even in the same genus.
</li>
</ul>
<p>This is still crude, but in the absence of further information, it&#8217;s about the best we can do.  And it&#8217;s better than the first approach, where we ignored the taxonomy entirely.  Throwing away biologically relevant information is unlikely to lead to a better assessment of diversity.</p>
<p>Using this taxonomic matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Z}" class="latex" />, and the same abundances, the diversity graphs become:</p>
<p><img width="450" src="https://i0.wp.com/www.maths.ed.ac.uk/~tl/taxo_bflies.jpg" alt="" /></p>
<p>This is more interesting!  For <img src="https://s0.wp.com/latex.php?latex=q+%3E+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &gt; 1" class="latex" />, the understorey looks <i>more</i> diverse than the canopy &mdash; the opposite conclusion to our first approach.</p>
<p>It&#8217;s not hard to see why.  Look again at the table of abundances, but paying attention to the <i>genera</i> of the butterflies.  In the canopy, nearly three-quarters of the butterflies are of genus <i>Memphis</i>.  So when we take into account the fact that species in the same genus tend to be somewhat similar, the canopy looks much less diverse than it did before.  In the understorey, however, the species are spread more evenly between genera, so taking similarity into account leaves its diversity relatively unchanged.</p>
<p>Taking account of species similarity opens up a world of uncertainty.  How should we measure similarity?  There are as many possibilities as there are quantifiable characteristics of living organisms.  It&#8217;s much more reassuring to stay in the black-and-white world where distinct species are always assigned a similarity of 0, no matter how similar they might actually be.  (This is, effectively, what most existing measures do.)  But that&#8217;s just hiding from reality. </p>
<p>Maybe you disagree!  If so, try the the Discussion section of our <a href="http://www.maths.ed.ac.uk/~tl/mdiss.pdf">paper</a>, where we lay out our arguments in more detail.  Or let me know by leaving a comment.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/11/07/measuring-biodiversity/#comments">34 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/11/07/measuring-biodiversity/" rel="bookmark" title="Permanent Link to Measuring Biodiversity">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-4668 post type-post status-publish format-standard hentry category-climate category-physics" id="post-4668">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/08/25/eddy-who/" rel="bookmark">Eddy Who?</a></h2>
				<small>25 August, 2011</small><br />


				<div class="entry">
					<h3>Or: A very short introduction to turbulence</h3>
<p><i>guest post by <b><a href="http://www.azimuthproject.org/azimuth/show/Tim+van+Beek">Tim van Beek</a></b></i></p>
<p>Have a look at this picture:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/www.azimuthproject.org/azimuth/files/billow_clouds_snipped.png" alt="" />
</div>
<p>Then look at this one:</p>
<div align="center">
<img width="450" src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/KH_snipped.png" alt="" />
</div>
<p>Do they look similar?</p>
<p>They should!  They are both examples of a <a href="http://en.wikipedia.org/wiki/Kelvin-Helmholtz_instability">Kelvin-Helmoltz instability</a>.</p>
<p>The first graphic is a picture of billow clouds (the fancier name is <a href="http://en.wikipedia.org/wiki/Altostratus_undulatus_cloud">altostratus undulatus clouds</a>):</p>
<div align="center">
<img width="450" src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/billow_clouds.png" alt="" />
</div>
<p>The picture is taken from:</p>
<p>&bull; C. Donald Ahrens: <i>Meteorology Today</i>, 9th edition, Brooks/Cole, Kentucky, 2009.</p>
<p>The second graphic:</p>
<div align="center">
<img width="450" src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/Kelvin_Helmholtz_lab_flow.png" alt="" />
</div>
<p>shows a lab experiment and is taken from:</p>
<p>&bull; G.L. Brown and A. Roshko, online available <a href="http://authors.library.caltech.edu/10104/">Density effects and large structure in turbulent mixing layers</a>, <i>Journal of Fluid Mechanics</i> <b>64</b> (1974), 775-816.</p>
<p>Isn&#8217;t it strange that clouds in the sky would show the same pattern as some gases in a small laboratory experiment? The reason for this is not quite understood today. In this post, I would like to talk a little bit about what is known. </p>
<h4>Matter that tries to get out of its own way</h4>
<p>Fluids like water and air can be well described by Newton&#8217;s laws of classical mechanics. When you start learning about classical mechanics, you consider discrete masses, most of the time.  Billiard balls, for example. But it is possible to formulate Newton&#8217;s laws of motion for fluids by treating them as &#8216;infinitely many infinitesimally small&#8217; billiard balls, all pushing and rubbing against each other and therefore trying to get out of the way of each other.</p>
<p>If we do this, we get some equations describing fluid flow: the <a href="http://en.wikipedia.org/wiki/Navier-Stokes">Navier-Stokes equations</a>.  </p>
<p>The Navier-Stokes equations are a complicated set of nonlinear partial differential equations. A lot of mathematical questions about them are still unanswered, like: under what conditions is there a smooth solution to these equations? If you can answer that question, you will win one of the <a href="http://www.claymath.org/millennium/Navier-Stokes_Equations">a million dollars</a> from the Clay Mathematics Institute.</p>
<p>If you completed the standard curriculum of physics as I did, chances are that you never attended a class on fluid dynamics. At least I never did. When you take a first look at the field, you will notice: the literature about the Navier-Stokes equations alone is <i>huge!</i> Not to mention all the special aspects of numerical simulations, special aspects of the climate and so on. </p>
<p>So it is nice to find a pedagogical introduction to the subject for people who have some background knowledge in partial differential equations, for the mathematical theory:</p>
<p>&bull; C. Foias, R. Rosa, O. Manley and R. Temam, <i>Navier-Stokes Equations and Turbulence</i>, Cambridge U. Press, Cambridge, 2001.  </p>
<p>So, there is a lot of fun to be had for the mathematically inclined. But today I would like to talk about an aspect of fluid flows that also has a tremendous <i>practical</i> importance, especially for the climate of the Earth: <i>turbulence!</i></p>
<p>There is no precise definition of turbulence, but people know it when they see it. A fluid can flow in layers, with one layer above the other, maybe slightly slower or faster. Material of one layer does hardly mix with material of another layer. These flows are called <b><a href="http://en.wikipedia.org/wiki/Laminar_flow">laminar flows</a></b>. When a laminar flow gets faster and faster, it turns into a turbulent flow at some point:</p>
<div align="center">
<img width="450" src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/laminar_to_turbulent_flow.png" alt="" />
</div>
<p>This is a fluid flow inside a circular pipe, with a layer of some darker fluid in the middle.</p>
<p>As a first guess we could say that a characteristic property of turbulent flow is the presence of circular flows, commonly called <b><a href="http://en.wikipedia.org/wiki/Eddy_%28fluid_dynamics%29">eddies</a></b>.</p>
<h4>Tempests in Teapots</h4>
<p>A funny aspect of the Navier-Stokes equations is that they don&#8217;t come with any recommended length scale. Properties of the fluid flow like velocity and pressure are modelled as smooth functions of continuous time and space. Of course we know that this model does not work on a atomic length scale, where we have to consider individual atoms. Pressure and velocity of a fluid flow don&#8217;t make any sense on a length scale that is smaller than the average distance between electrons and the atom nucleus. </p>
<p>We know this, but this is a fact that is not present in the model comprised by the Navier-Stokes equations! </p>
<p>But let us look at bigger length scales.  An interesting feature of the solutions of the Navier-Stokes equations is that there are fluid flows that stretch over hundreds of meters that look like fluid flows that stretch over centimeters only. And it is really astonishing that this phenomenon can be observed in nature. This is another example of the unreasonable effectiveness of mathematical models.</p>
<p>You have seen an example of this in the introduction already. That was a boundary layer instability. Here is a full blown turbulent example: </p>
<div align="center">
<img width="450" src="https://i2.wp.com/www.azimuthproject.org/azimuth/files/turbulence_at_different_length_scales.png" alt="" />
</div>
<p>The last two pictures are from the book:</p>
<p>&bull; Arkady Tsinober: <i>An Informal Conceptual Introduction to Turbulence</i>, 2nd edition, Springer, Fluid Mechanics and Its Applications Volume 92, Berlin, 2009. </p>
<p>This is a nice introduction to the subject, especially if you are more interested in phenomenology than mathematical details.</p>
<p>Maybe you noticed the &#8220;Reynolds number&#8221; in the label text of the last picture. What is that?</p>
<p>People in business administration like management ratios; they throw all the confusing information they have about a company into a big mixer and extract one or two numbers that tell them where they stand, like business volume and earnings. People in hydrodynamics are somewhat similar; they define all kinds of &#8220;numbers&#8221; that condense a lot of information about fluid flows. </p>
<p>A CEO would want to know if the earnings of his company are positive or negative. We would like to know a number that tells us if a fluid flow is laminar or turbulent. Luckily, such a number already exists. It is the <a href="http://en.wikipedia.org/wiki/Reynolds_number"><b>Reynolds number</b></a>! A low number indicates a laminar flow, a high number a turbulent flow. Like the calculation of the revenue of a company, the calculation of the Reynolds number of a given fluid flow is not an exact science. Instead there is some measure of estimation necessary. The definition involves, for example, a &#8220;characteristic length scale&#8221;. This is a fuzzy concept that usually involves some object that interacts with &#8211; in our case &#8211; the fluid flow. The characteristic length scale in this case is the physical dimension of the object. While there is usually no objectively correct way to assing a &#8220;characteristic length&#8221; to a three dimensional object, this concept allows us nevertheless to distinguish the scattering of water waves on a ocean liner (length scale &asymp; 10<sup></sup><sup>3</sup> meter) from their scattering on a peanut (length scale &asymp; 10<sup></sup><sup>-2</sup> meter).</p>
<p>The following graphic shows laminar and turbulent flows and their characteristic Reynolds numbers:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/www.azimuthproject.org/azimuth/files/schematic_turbulence_Reynoldsnumber.png" alt="schematic display of laminar and turbulent flow for different Reynolds numbers" />
</div>
<p>This graphic is from the book</p>
<p>&bull; Thomas Bohr, Mogens H. Jensen, Giovanni Paladin and Angelo Vulpiani, <i>Dynamical Systems Approach to Turbulence</i>, Cambridge U. Press, Cambridge, 1998 </p>
<p>But let us leave the Reynolds number for now and turn to one of its ingredients: <i>viscosity</i>. Understanding viscosity is important for understanding how eddies in a fluid flow are connected to energy dissipation.</p>
<h4>Eddies dissipate kinetic energy</h4>
<blockquote><p>
&#8220;Eddies,&#8221; said Ford, &#8220;in the space-time continuum.&#8221;</p>
<p>
&#8220;Ah,&#8221; nodded Arthur, &#8220;is he? Is he?&#8221; He pushed his hands into the pocket of his dressing gown and looked knowledgeably into the distance.
</p>
<p>
&#8220;What?&#8221; said Ford.
</p>
<p>&#8220;Er, who,&#8221; said Arthur, &#8220;is Eddy, then, exactly, then?&#8221;</p>
<p>&#8211; from Douglas Adams: <i>Life, the Universe and Everything</i>
</p></blockquote>
<p>A fluid flow can be pictured as consisting of a lot of small fluid packages that move alongside each other. In many situations, there will be some friction between these packages. In the case of fluids, this friction is called <b><a href="http://en.wikipedia.org/wiki/Viscosity">viscosity</a></b>. </p>
<p>It is an empirical fact that at small velocities fluid flows are laminar: there are layers upon layers, with one layer moving at a constant speed, and almost no mixing. At the boundaries, the fluid will attach to the surrounding material, and the relative fluid flow will be zero. If you picture such a flow between a plate that is at rest, and a plate that is moving forward, you will see that due to friction between the layers a force needs to be exerted to keep the moving plate moving:</p>
<div align="center">
<img width="450" src="https://i1.wp.com/www.azimuthproject.org/azimuth/files/definition_of_viscosity.png" alt="" />
</div>
<p>In the simplest approximation, you will have to exert some force <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> per unit area <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, in order to sustain a linear increase of the velocity of the upper plate along the y-axis, <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+u+%2F+%5Cpartial+y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial u / &#92;partial y." class="latex" /> The constant of proportionality is called the viscosity <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7BF%7D%7BA%7D+%3D+%5Cmu+%5Cfrac%7B%5Cpartial+u%7D%7B%5Cpartial+y%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{F}{A} = &#92;mu &#92;frac{&#92;partial u}{&#92;partial y}} " class="latex" /></p>
<p>More friction means a bigger viscosity: honey has a bigger viscosity than water.</p>
<p>If you stir honey, the fluid flow will come to a halt rather fast. The energy that you put in to start the fluid flow is turned to heat by dissipation. This mechanism is of course related to friction and therefore to viscosity.</p>
<p>It is possible to formulate an exact formula for this dissipation process using the Navier-Stokes equations. It is not hard to prove it, but I will only explain the involved gadgets.</p>
<p>A fluid flow in three dimensions can be described by stating the velocity of the fluid flow at a certain time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Cin+%5Cmathbb%7BR%7D%5E3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;vec{x} &#92;in &#92;mathbb{R}^3" class="latex" /> (I&#8217;m not specifying the region of the fluid flow or any boundary or initial conditions). Let&#8217;s call the velocity <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bu%7D%28t%2C+%5Cvec%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;vec{u}(t, &#92;vec{x})" class="latex" />.</p>
<p>Let&#8217;s assume that the fluid has a constant density <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />. Such a fluid is called <b>incompressible</b>. For convenience let us assume that the density is 1: <img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = 1" class="latex" />. Then the <b>kinetic energy</b> <img src="https://s0.wp.com/latex.php?latex=E%28%5Cvec%7Bu%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(&#92;vec{u})" class="latex" /> of a fluid flow at a fixed time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> is given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+E%28%5Cvec%7Bu%7D%29+%3D+%5Cint+%5C%7C+%5Cvec%7Bu%7D%28t%2C+%5Cvec%7Bx%7D%29+%5C%7C%5E2+%5C%3B+d%5E3+x+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ E(&#92;vec{u}) = &#92;int &#92;| &#92;vec{u}(t, &#92;vec{x}) &#92;|^2 &#92;; d^3 x }" class="latex" /></p>
<p>Let&#8217;s just assume that this integral is finite for the moment. This is the first gadget we need.</p>
<p>The second gadget is called <b><a href="http://en.wikipedia.org/wiki/Enstrophy">enstrophy</a></b> <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon" class="latex" /> of the fluid flow. This is a measure of how much eddies there are. It is the integral</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cepsilon+%3D+%5Cint+%5C%7C+%5Cnabla+%5Ctimes+%5Cvec%7Bu%7D+%5C%7C%5E2+%5C%3B+d%5E3+x+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;epsilon = &#92;int &#92;| &#92;nabla &#92;times &#92;vec{u} &#92;|^2 &#92;; d^3 x }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+%5Ctimes&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla &#92;times" class="latex" /> denotes the <b><a href="http://en.wikipedia.org/wiki/Curl_%28mathematics%29">curl</a></b> of the fluid velocity. The faster the fluid rotates, the bigger the curl is. </p>
<p>(The math geeks will notice that the vector fields <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bu%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;vec{u}" class="latex" /> that have a finite kinetic energy and a finite enstrophy are precisely the elements of the Sobolev space <img src="https://s0.wp.com/latex.php?latex=H%5E1%28%5Cmathbb%7BR%7D%5E3%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H^1(&#92;mathbb{R}^3)" class="latex" />)</p>
<p>Here is the relationship of the decay of the kinetic energy and the enstrophy, which is a consequence of the Navier-Stokes equations (and suitable boundary conditions):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7Bd%7D%7Bd+t%7D+E+%3D+-+%5Cmu+%5Cepsilon%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{d}{d t} E = - &#92;mu &#92;epsilon} " class="latex" /></p>
<p>This equation says that the energy decays with time, and it decays faster if there is a higher viscosity, and if there are more and stronger eddies.</p>
<p>If you are interested in the mathematically precise derivation of this equation, you can look it up in the book I already mentioned:</p>
<p>&bull; C. Foias, R. Rosa, O. Manley and R. Temam, <i>Navier-Stokes Equations and Turbulence</i>, Cambridge U. Press, Cambridge, 2001.  </p>
<p>This connection of eddies and dissipation could indicate that there is also a connection of eddies and some maximum entropy principle. Since eddies maximize dissipation, natural fluid flows should somehow tend towards the production of eddies. It would be interesting to know more about this!</p>
<p>In this post we have seen eddies at different length scales. There are buzzwords in meteorology for this:</p>
<div align="center">
<img width="450" src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/scales_in_meteorolgy.png" alt="" />
</div>
<p>You have seen eddies at the &#8216;microscale&#8217; (left) and at the &#8216;mesoscale&#8217; (middle).  A blog post about eddies should of course mention the most famous eddy of the last decade, which formed at the &#8216;synoptic scale&#8217;:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/www.azimuthproject.org/azimuth/files/katrina.png" alt="" />
</div>
<p>Do you recognize it? That was Hurricane Katrina.</p>
<p>It is obviously important to understand disasters like this one on the synoptic scale. This is an active topic of ongoing research, both in meteorology and in climate science.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/08/25/eddy-who/#comments">46 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/08/25/eddy-who/" rel="bookmark" title="Permanent Link to Eddy Who?">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-4085 post type-post status-publish format-standard hentry category-biology category-economics category-sustainability category-this-weeks-finds" id="post-4085">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/06/27/this-weeks-finds-week-315/" rel="bookmark">This Week&#8217;s Finds (Week&nbsp;315)</a></h2>
				<small>27 June, 2011</small><br />


				<div class="entry">
					<p>
This is the second and final part of my interview with Thomas Fischbacher.  We&#8217;re talking about sustainable agriculture, and he was just about to discuss the role of paying attention to <i>flows</i>.</p>
<p>
<b>JB</b>: So, tell us about flows.</p>
<p>
<b>TF</b>: For natural systems, some of the most important flows are those of energy, water, mineral nutrients, and biomass.  Now, while they are physically real, and keep natural systems going, we should remind ourselves that nature by and large does not make high level decisions to orchestrate them. So, flows arise due to processes in nature, but nature &#8216;works&#8217; without being consciously aware of them. (Still, there are mechanisms such as evolutionary pressure that ensure that the flow networks of natural ecosystems work&mdash;those assemblies that were non-viable in the long term did not make it.)</p>
<p>
Hence, flows are above everything else a useful conceptual framework&mdash;a mental tool devised by us for us&mdash;that helps us to make sense of an otherwise extremely complex and confusing natural world. The nice thing about flows is that they reduce complexity by abstracting away details when we do not want to focus on them&mdash;such as which particular species are involved in the calcium ion economy, say. Still, they retain a lot of important information, quite unlike some models used by economists that actually guide&mdash;or misguide&mdash;our present decision-making.  They tell us a lot about key processes and longer term behaviour&mdash;in particular, if something needs to be corrected.</p>
<p>
Sustainability is a complex subject that links to many different aspects of human experience&mdash;and of course the non-human world around us. When confronted with such a subject, my approach is to start by asking: &#8216;what I am most certain about&#8217;, and use these key insights as &#8216;anchors&#8217; that set the scene. Everything else must respect these insights. Occasionally, some surprising new insight forces me to reevaluate some fundamental assumptions, and repaint part of the picture. But that&#8217;s life&mdash;that&#8217;s how we learn.</p>
<p>
Very often, I find that those aspects which are both useful to obtain deeper insights and at the same time accessible to us are related to <i>flows</i>.</p>
<p>
<b>JB</b>: Can you give an example?</p>
<p>
<b>TF</b>: Okay, here&#8217;s another puzzle.  What is the largest flow of solids induced by civilization?</p>
<p>
<b>JB</b>: Umm&#8230; maybe the burning of fossil fuels, passing carbon into the atmosphere?</p>
<p>
<b>TF</b>: I am by now fairly sure that the answer is: the unintentional export of topsoil from the land into the sea by wind and water erosion, due to agriculture. According to Brady &amp; Weil, around the year 2000, the U.S. annually &#8216;exported&#8217; about 4&times;10<sup>12</sup> kilograms of topsoil to the sea.  That&#8217;s roughly three cubic kilometers, taking a reasonable estimate for the density of humus.</p>
<p>
<b>JB</b>: Okay.  In <a href="http://www.azimuthproject.org/azimuth/show/Carbon+emissions">2007</a>, the U.S. burnt 1.6 &times; 10<sup>12</sup> kilograms of carbon.  So, that&#8217;s comparable.</p>
<p>
<b>TF</b>:  Yes.  When I cross check my number combining data from the NRCS on average erosion rates and from the CIA World Factbook on cultivated land area, I get a result that is within the same ballpark, so it seems to make sense. In comparison, total U.S. exports of economic goods in 2005 were 4.89&times;10<sup>11</sup> kilograms: about an order of magnitude less, according to statistics from the Federal Highway Administration.</p>
<p>
If we look at present soil degradation rates alone, it is patently clear that we see major changes ahead. In the long term, we just cannot hope to keep on feeding the population using methods that keep on rapidly destroying fertility. So, we pretty much know that something will happen there. (Sounds obvious, but alas, thinking of a number of discussions I had with some economists, I must say that, sadly, it is far from being so.)</p>
<p>
What actually will happen mostly depends on how wisely we act.  The possibilities range from nuclear war to a mostly smooth swift transition to fertility-building food production systems that also take large amounts of CO<sub>2</sub> out of the atmosphere and convert it to soil humus. I am, of course, much in favour of scenarios close to the latter one, but that won&#8217;t happen unless we put in some effort&mdash;first and foremost, to educate people about how it can be done.</p>
<p>
Flow analysis can be an extremely powerful tool for diagnosis, but its utility goes far beyond this. When we design systems, paying attention to how we design the flow networks of energy, water, materials, nutrients, etc., often makes a world of a difference.
</p>
<p>
Nature is a powerful teacher here: <b>in a forest, there is no &#8216;waste&#8217;, as one system&#8217;s output is another system&#8217;s input</b>. What else is &#8216;waste&#8217; but an accumulation of unused output? So, &#8216;waste&#8217; is an indication of an output mismatch problem. Likewise, if a system&#8217;s input is not in the right form, we have to pre-process it, hence do work, hence use energy. Therefore, if a process or system continually requires excessive amounts of energy (as many of our present designs do), this may well be an indication of a design problem&mdash;and could be related to an input mismatch.</p>
<p>
Also, the flow networks of natural systems usually show both extremely high recycling rates and a lot of multi-functionality, which provides resilience. Every species provides its own portfolio of services to the assembly, which may include pest population control, creating habitat for other species, food, accumulating important nutrients, &#8216;waste&#8217; transformation, and so on. No element has a single objective, in contrast to how we humans by and large like to engineer our systems.  Each important function is covered by more than one element. Quite unlike many of our past approaches, design along such principles can have long-term viability.  <i>Nature works</i>. So, we clearly can learn from studying nature&#8217;s networks and adopting some principles for our own designs.</p>
<p>
Designing for sustainability with, around, and inspired by natural systems is an interesting intellectual challenge, much like solving a jigsaw puzzle. We cannot simultaneously comprehend the totality of all interactions and relations between adjacent pieces as we build it, but we keep on discovering clues by closely studying different aspects: form, colour, pattern. If we are on the right track, and one clue tells us how something should fit, we will discover that other aspects will fit as well. If we made a mistake, we need to apply force to maintain it and hammer other pieces into place&mdash;and unless we correct that mistake, we will need ever more brutal interventions to artificially stabilize the problems which are mere consequences of the original mistake. Think using nuclear weapons to seal off spilling oil wells drilled in deep waters needed because we used up all the easily accessible high-quality fuels.  One mistake begets another.</p>
<p>There is a reason why jigsaw puzzles &#8216;work&#8217;: they were created that way. There is also a reason why the dance of natural systems &#8216;works&#8217;: <a href="http://en.wikipedia.org/wiki/Coevolution">coevolution</a>. What happens when we run out of steam to stabilize poor designs (i.e. in an energy crisis)? We, as a society, will be forced to confront our past arrogance and pay close attention to resolving the design mistakes we so far always tried to talk away. That&#8217;s something I&#8217;d call &#8216;true progress&#8217;.</p>
<p>
Actually, it&#8217;s quite evident now: many of our &#8216;problems&#8217; are rather just symptoms of more fundamental problems. But as we do not track these down to the actual root, we keep on expending ever more energy by stacking palliatives on top of one another. Growing corn as a biofuel in a process that both requires a lot of external energy input and keeps on degrading soil fertility is a nice example. Now, if we look closer, we find numerous further, superficially unrelated, problems that should make us ask the question: &quot;Did we assemble this part of the puzzle correctly? Is this approach really such a good idea? What else could we do instead? What other solutions would suggest themselves if we paid attention to the hints given by nature?&quot; But we don&#8217;t do that. It&#8217;s almost as if we were proud to be thick.</p>
<p>
<b>JB</b>: How would designing with flows in mind work?</p>
<p>
<b>TF</b>: First, we have to be clear about the boundaries of our domain of influence. Resources will at some point enter our domain of influence and at some point leave it again. This certainly holds for a piece of land on which we would like to implement sustainable food production where one of the most important flows is that of water. But it also holds for a household or village economy, where an important flow through the system is that of purchase power&mdash;i.e. money (but in the wider sense). As resources percolate through a system, their utility generally degrades&mdash;entropy at work. Water high up in the landscape has more potential uses than water further down. So, we can derive a guiding principle for design: capture resources as early as possible, release them as late as possible, and see that you guide them in such a way that their natural drive to go downhill makes them perform many useful duties in between. Considering water flowing over a piece of land, this would suggest setting up rainwater catchment systems high up in the landscape. This water then can serve many useful purposes: there certainly are agricultural/silvicultural and domestic uses, maybe even aquaculture, potentially small-scale hydropower (say, in the 10-100 watts range), and possibly fire control.</p>
<p>
<b>JB</b>: When I was a kid, I used to break lots of things.  I guess lots of kids do.  But then I started paying attention to <i>why</i> I broke things, and I discovered there were two main reasons.  First, I might be distracted: paying attention to one thing while doing another.  Second, I might be trying to overcome a problem by force instead of by slowing down and thinking about it.  If I was trying to untangle a complicated knot, I might get frustrated and just pull on it&#8230; and rip the string.</p>
<p>
I think that as a culture we make both these mistakes quite often.  It sounds like part of what you&#8217;re saying is: &quot;Pay more attention to what&#8217;s going on, and when you encounter problems, slow down and think about their origin a bit&mdash;don&#8217;t just try to bully your way through them.&quot;</p>
<p>
But the tool of measuring flows is a nice way to organize this thought process.   When you first told me about &#8216;input mismatch problems&#8217; and &#8216;output mismatch problems&#8217;, it came as a real revelation!  And I&#8217;ve been thinking about them a lot, and I want to keep doing that.</p>
<p>
One thing I noticed is that problems tend to come in pairs.  When the output of one system doesn&#8217;t fit nicely into the input of the next, we see two problems. First, &#8216;waste&#8217; on the output side.  Second, &#8216;deficiency&#8217; on the input side.  Sometimes it&#8217;s obvious that these are two aspects of the same problem.  But sometimes we fail to see it.</p>
<p>
For example, a while ago some ground squirrels chewed a hole in an irrigation pipe in our yard.  Of course that&#8217;s our punishment for using too much water in a naturally dry environment, but look at the two problems it created.  One: big gushers of water shooting out of the hole whenever that irrigation pipe was used, which caused all sort of further problems.  Two: not enough water to the plants that system was supposed to be irrigating.  Waste on one side, deficiency on the other.</p>
<p>
That&#8217;s obvious, easy to see, and easy to fix: first plug the hole, then think carefully about why we&#8217;re using so much water in the first place.  We&#8217;d already replaced our lawn with plants that use less water, but maybe we can do better.</p>
<p>
But here&#8217;s a bigger problem that&#8217;s harder to fix.  Huge amounts of fertilizer are being used on the cornfields of the midwestern United States.  With the agricultural techniques they&#8217;re using, there&#8217;s a constant deficiency of nitrogen and phosphorus, so it&#8217;s supplied artificially.  The figures I&#8217;ve seen show that about 30% of the energy used in US agriculture goes into making fertilizers.  So, it&#8217;s been said that we&#8217;re &#8216;eating oil&#8217;&mdash;though technically, a lot of nitrogen fertilizer is made using natural gas.  Anyway: a huge deficiency problem.</p>
<p>
On the other hand, where is all this fertilizer going?  In the midwestern United States, a lot of it winds up washing down the Mississipi River. And as a result, there are enormous &#8216;dead zones&#8217; in the Gulf of Mexico.  The fertilizer feeds algae, the algae dies and decays, and the decay process takes oxygen out of the water, killing off any life that needs oxygen.  These dead zones range from 15 and 18 thousand square kilometers, and they&#8217;re in a place that&#8217;s one of the prime fishing spots for the US.  So: a huge waste problem.</p>
<p>
But they&#8217;re the same problem!</p>
<p>
It reminds me of the old joke about a guy who was trying to button his shirt.   &quot;There are two things wrong with this shirt!  First, it has an extra button on top.  Second, it has an extra buttonhole on bottom!&quot;</p>
<p>
<b>TF</b>: Bill Mollison said it in a quite humorous-yet-sarcastic way in this episode of the <i>Global Gardener</i> movie:</p>
<p>&bull; Bill Mollison, <a href="http://www.youtube.com/watch?v=Qpyocn1Vc5U#t=1m05">Urban permaculture strategies &#8211; part 1</a>, YouTube.</p>
<p>
While the potential to grow a large amount of calories in cities may be limited, growing fruit and vegetables nevertheless does make sense for multiple reasons. One of them is that many things that previously went into the garbage bin now have a much more appropriate place to go&mdash;such as the compost heap. Many urbanites who take up gardening are quite amazed when they realize how much of their household waste actually always &#8216;wanted&#8217; to end up in a garden.</p>
<p>
<b>JB</b>: Indeed.  After I bought a compost bin, the amount of trash I threw out dropped dramatically.  And instead of feeling vaguely guilty as I threw orange peels into the trash where they&#8217;d be mummified in a plastic bag in a landfill,  I could feel vaguely virtuous as I watched them gradually turn into soil.  It doesn&#8217;t take as long as you might think.  And it comes as a bit of a revelation at first: &quot;Oh, so <i>that&#8217;s</i> how we get soil.&quot;</p>
<p>
<b>TF</b>: Perhaps the biggest problem I see with a mostly non-gardening society is that people without even the slightest own experience in growing food are expected to make up their mind about very important food-related questions and contribute to the democratic decision making process. Again, I must emphasize that whoever does not consciously invest some effort into getting at least some minimal first hand experience to improve their judgment capabilities will be easy prey for rat-catchers. And by and large, society is not aware of how badly they are lied to when it comes to food.</p>
<p>
But back to flows. Every few years or so, I stumble upon a jaw-dropping idea, or a principle, that makes me realize that it is so general and powerful that, really, the limits of what it can be used for are the limits of my imagination and creativity. I recently had such a revelation with the <a href="http://mathworld.wolfram.com/PSLQAlgorithm.html">PSLQ integer relation algorithm</a>. Using flows as a mental tool for analysis and design was another such case. All of a sudden, a lot made sense, and could be analyzed with ease.</p>
<p>
There always is, of course, the &#8216;man with a hammer problem&#8217;&mdash;if you are very fond of a new and shiny hammer, everything will look like a nail. I&#8217;ve also heard that expressed as &#8216;an idea is a very dangerous thing if it is the only one you have&#8217;.</p>
<p>
So, while keeping this in mind, now that we got an idea about flows in nature, let us ask: &quot;how can we abuse these concepts?&quot;  Mathematicians prefer the term &#8216;abstraction&#8217;, but it&#8217;s fun either way. So, let&#8217;s talk about the flow of money in economies. What is money? Essentially, it is just a book-keeping device invented to keep track of favours owed by society to individuals and vice versa. What function does it have? It works as &#8216;grease&#8217;, facilitating trade.</p>
<p>
So, suppose you are a mayor of a small village. One of your important objectives is of course prosperity for your villagers. Your village trades with and hence is linked to an external economy, and just as goods and services are exchanged, so is money. So, at some point, purchase power (in the form of money) enters your domain of influence, and at some point, it will leave it again. What you want it to do is to facilitate many different economic activities&mdash;so you want to ensure it circulates within the village as long as possible. You should pay some attention to situations where money accumulates&mdash;for everything that accumulates without being put to good use is a form of &#8216;waste&#8217;, hence pollution. So, this naturally leads us to two ideas: (a) What incentives can you find to keep money on circulating within the village? (There are many answers, limited only by creativity.) And (b) what can you do to constrain the outflow? If the outlet is made smaller, system outflow will match inflow at a higher internal pressure, hence a higher level of resource availability within the system.</p>
<p>
This leads us to an idea no school will ever tell you about&mdash;for pretty much the same reason why no state-run school will ever teach how to plan and successfully conduct a revolution. The road to prosperity is to systematically reduce your &#8216;Need To Earn&#8217;&mdash;i.e. the best way to spend money is to set up systems that allow you to keep more money in your pocket. An frequent misconception that keeps on arising when I mention this is that some think this idea would be about austerity. Quite to the contrary. You can make as much money as you want&mdash;but one thing you should keep in mind is that if you have that trump card up your sleeve that you could at any time just disconnect from most of the economy and get by with almost no money at all for extended periods of time, you are in a far better position to take risks and grasp exceptional opportunities as they arise as someone would be who committed himself to having to earn a couple of thousand pounds a month.
</p>
<p>
The problem is not with earning a lot of money. The problem is with being <i>forced</i> to continually make a lot of money.  We readily manage to identify this as a key problem of drug addicts, but fail to see the same mechanism at work in mainstream society. A key assumption in economic theory is that exchange is voluntary. But how well is that assumption satisfied in practice if such forces are in place?</p>
<p>
Now, what would happen if people started to get serious about investing the money they earn to systematically reduce their need to earn money in the future? Some decisions such as getting a photovoltaic array may have &#8216;payback times&#8217; in the range of one or two decades, but I consider this &#8216;payback time&#8217; concept as a self-propagating flawed idea. If something gives me an advantage in terms of depending on less external input now, this reduction of vulnerability also has to be taken into account&mdash;&#8217;payback times&#8217; do not do that. So&mdash;if most people did such things, i.e. made strategic decisions to set up systems so that their essential needs can be satisfied with minimal effort&mdash;especially money, this would put a lot of political power back into their hands. A number of self-proclaimed &#8216;leaders&#8217; certainly don&#8217;t like the idea of people being in a position to just ignore their orders. Also note that this would have a funny effect on the GDP&mdash;ever heard of &#8216;imputations&#8217;?</p>
<p>
<b>JB</b>: No, what are those?</p>
<p>
<b>TF</b>: It&#8217;s a funny thing, perhaps best explained by an example. If you fully own your own house, then you don&#8217;t pay rent. But for the purpose of determining the GDP, you are regarded as paying as much rent to yourself (!) as you would get if you rented out the house.   See:</p>
<p>&bull; <a href="http://en.wikipedia.org/wiki/Imputed_rent">Imputed rent</a>, Wikipedia.</p>
<p>
Evidently, if people make a dedicated effort at the household level to become less dependent on the economy by being able to provide most of their essential needs themselves (housing, food, water, energy, etc.) to a much larger extent, this amounts to investing money in order to need less money in the future. If many people did this systematically, it would superficially have a devastating effect on the GDP&mdash;but it would bring about a much more resilient (because less dependent) society.</p>
<p>
The problem is that the GDP really is not an appropriate measure for progress. But obviously, those who publish these figures know that as well, hence the need to fudge the result with imputations. So, a simple conclusion is: whenever there is an opportunity to invest money in a way that makes you less dependent on the economy in the future, that might be well worth a closer look. Especially if you get the idea that, if many people did this, the state would likely have to come up with other imputations to make the impact on the GDP disappear!</p>
<p>
<b>JB</b>: That&#8217;s a nice thought.  I tend to worry about how the GDP and other economic indicators warp our view of what&#8217;s right to do.  But you&#8217;re saying that if people can get up the nerve to do what&#8217;s right, regardless, the economic indicators may just take care of themselves.</p>
<p>
<b>TF</b>: We have to remember that sustainability is about systems that are viable in the long run. Environmental sustainability is just one important aspect. But you won&#8217;t go on for long doing what you do unless it also has economic long-term viability. Hence, we are dealing with multi-dimensional design constraints. And just as flow network analysis is useful to get an idea about the environmental context, the same holds for the economic context. It&#8217;s just that the resources are slightly different ones&mdash;money, labour, raw materials, etc. These thoughts can be carried much further, but I find it quite worthwhile to instead look at an example where someone did indeed design a successful system along such principles. In the UK, the first example that would come to my mind is <a href="http://hillholtwood.com/">Hill Holt Wood</a>, because the founding director, <a href="http://www.theecologist.org/how_to_make_a_difference/wildlife/361025/case_study_managing_woodlands_through_social_enterprise.html">Nigel Lowthrop</a>, did do so many things right. I have high admiration for his work.</p>
<p>
<b>JB</b>: When it comes to design of sustainable systems, you also seem to be a big fan of <a href="http://en.wikipedia.org/wiki/Bill_Mollison">Bill Mollison</a> and some of the &#8216;permaculture&#8217; movement that he started.  Could you say a bit about that?  Why is it important?</p>
<p>
<b>TF</b>: The primary reason why permaculture matters is that it has demonstrated some stunning successes with important issues such as land rehabilitation.</p>
<p>
&#8216;Permaculture&#8217; means a lot of different things to a lot of different people. Curiously, where I grew up, the term is somewhat known, but mostly associated with an Austrian farmer, not Bill Mollison. And I&#8217;ve seen some physicists who first had come into contact with it through <a href="http://en.wikipedia.org/wiki/David_Holmgren">David Holmgren</a>&#8216;s book revise their opinions when they later read Mollison. Occasionally, some early adopters did not really understand the scientific aspects of it and tried to link it with some strange personal beliefs of the sort Martin Gardner discussed in <i>Fads and Fallacies in the Name of Science</i>. And so on. So, before we discuss permaculture, I have to point out that one might sometimes have to take a close look to evaluate it. A number of things claiming to be &#8216;permaculture&#8217; actually are not.</p>
<p>
When I started&mdash;some time ago&mdash;to make a systematic effort to get a useful overview over the structure of our massive sustainability-related problems, a key question to me always was: &quot;what should <i>I</i> do?&quot;&mdash;and a key conviction was: &quot;someone must have had some good ideas about all this already.&quot; This led me to actually not read some well-known &quot;environmentalist&quot; books many people had read which are devoid of any discussion of our options and potential solutions, but to do a lot of detective work instead.</p>
<p>
In doing so, I travelled, talked to a number of people, read a lot of books and manuscripts, did a number of my own experiments, cross-checked things against order-of-magnitude guesstimates, against the research literature, and so on. At one point&mdash;I think it was when I took a closer look into the work of the laureates of the <a href="http://www.rightlivelihood.org/">&#8216;Right Livelihood award&#8217;</a> (sometimes called the &#8216;Alternative Nobel Prize&#8217;)&mdash;I came across Bill Mollison&#8217;s work. And it struck a chord.</p>
<p>
Back in the 90s, when mad cow disease was a big topic in Europe, I spent quite some time pondering questions such as: &quot;what&#8217;s wrong with the way farming works these days?&quot; I immediately recognized a number of insights I independently had arrived at back then when studying Bill Mollison&#8217;s work, and yet, he went so much further&mdash;talked about a whole universe of issues I still was mostly unaware of at that time. So, an inner voice said to me: &quot;if you take a close look at what that guy already did, that might save you a lot of time&quot;. Now, Mollison did get some things wrong, but I still think taking a close look at what he has to say is a very effective way to get a big picture overview over what we can achieve, and what needs urgent attention. I think it greatly helps (at least to me) that he comes from a scientific background. Before he decided to quit academia in 1978 and work full time on developing permaculture, he was a lecturer at the University of Hobart, Tasmania.</p>
<p>
<b>JB</b>: But what actually <i>is</i> &#8216;permaculture&#8217;?</p>
<p>
<b>TF</b>: That depends a lot on who you ask, but I like to think about permaculture as if it were an animal. The &#8216;skeleton&#8217; is a framework with cleverly designed &#8216;static properties&#8217; that holds the &#8216;flesh&#8217; together in a way so that it can achieve things. The actual &#8216;flesh&#8217; is provided by solutions to specific problems with long term viability being a key requirement. But it is more than just a mere semi-amorphous collage of solutions, due to its skeleton. The backbone of this animal is a very simple (deliberately so) yet functional (this is important) core ethics which one could regard as being the least common denominator of values considered as essential across pretty much all cultures. This gives it stability.  Other bones that make this animal walk and talk are related to key principles. And these principles are mostly just applied common sense.</p>
<p>
For example, it is pretty clear that as non-renewable resources keep on becoming more and more scarce, we will have to seriously ponder the question: what can we grow that can replace them? If our design constraints change, so does our engineering&mdash;should (for one reason or another) some particular resource such as steel become much more expensive than it is today, we would of course look into the question whether, say, bamboo may be a viable alternative for some applications. And that is not as exotic an idea as it may sound these days.</p>
<p>
So, unquestionably, the true solutions to our problems will be a lot about growing things.  But growing things in the way that our current-day agriculture mostly does it seems highly suspicious, as this keeps on destroying soil.  So, evidently, we will have to think less along the lines of farming and more along the lines of gardening. Also, we must not fool ourselves about a key issue: most people on this planet are poor, hence for an approach to have wide impact, it must be accessible to the poor. Techniques that revolve around gardening often are.</p>
<p>
Next, isn&#8217;t waiting for the big (hence, capital intensive) &#8216;technological miracle fix&#8217; conspicuously similar to the concept of a &#8216;pie in the sky&#8217;? If we had any sense, shouldn&#8217;t we consider solving today&#8217;s problems with today&#8217;s solutions?</p>
<p>
If one can distinguish between permaculture as it stands and attempts by some people who are interested in it to re-mold it so that it becomes &#8216;the permaculture part of permaculture plus Anthroposophy/Alchemy/Biodynamics/Dianetics/Emergy/Manifestation/New Age beliefs/whatever&#8217;, there is a lot of common sense in permaculture&mdash;the sort of &#8216;a practical gardener&#8217;s common sense&#8217;. In this framework, there is a place for both modern scientific methods and ancient tribal wisdom. I hence consider it a healthy antidote to both fanatical worship of &#8216;the almighty goddess of technological progress&#8217;&mdash;or any sort of fanatical worship for that matter&mdash;as well as to funny superstitious beliefs.</p>
<p>
There are some things in the permaculture world, however, where I would love to see some change. For example, it would be great if people who know how to get things done paid more attention to closely keeping records of what they do to solve particular problems and to making these widely accessible. Solutions of the &#8216;it worked great for a friend of a friend&#8217; sort do us a big disservice. Also, there are a number of ideas that easily get represented in overly simplistic form&mdash;such as &#8216;edge is good&#8217;&mdash;where one better should retain some healthy skepticism.</p>
<p>
<b>JB</b>: Well, I&#8217;m going to keep on pressing you: what is permaculture&#8230; according to you?  Can you list some of the key principles?</p>
<p>
<b>TF</b>: That question is much easier to answer. The way I see it, permaculture is a design-oriented approach towards systematically reducing the total effort that has to be expended (in particular, in the long run) in order to keep society going and allow people to live satisfying lives. Here, &#8216;effort&#8217; includes both work that is done by non-renewable resources (in particular fossil fuels), as well as human labour. So, permaculture is not about returning to pre-industrial agricultural drudgery with an extremely low degree of specialization, but rather about combining modern science with traditional wisdom to find low-effort solutions to essential problems. In that sense, it is quite generic and deals with issues ranging from food production to water supply to energy efficient housing and transport solutions.</p>
<p>
To give one specific example: Land management practices that reduce the organic matter content of soils and hence soil fertility are bound to increase the effort needed to produce food in the long run and hence considered a step in the wrong direction. So, a permaculture approach would focus on using strategies that manage to build soil fertility while producing food. There are a number of ways to do that, but a key element is a deep understanding of nature&#8217;s soil food web and nutrient cycling processes. For example, permaculture pays great attention to ensuring a healthy soil microflora.</p>
<p>
When the objective is to minimize the effort needed to sustain us, it is very important to closely observe those situations where we have to expend energy on a continual basis in order to fight natural processes. When this happens, there is a conflict between our views how things ought to look like and a system trying to demonstrate its own evolution. In some situations, we really want it that way and have to pay the corresponding price. But there are others&mdash;quite many of them&mdash;where we would be well advised to spend some thought on whether we could make our life easier by &#8216;going with the flow&#8217;. If thistles keep on being a nuisance on some piece of land, we might consider trying to fill this ecological niche by growing some closely related species, say some artichoke. If a meadow needs to be mowed regularly so that it does not turn into a shrub thicket, we would instead consider planting some useful shrubs in that place.</p>
<p>
Naturally, permaculture design favours perennial plants in climatic regions where the most stable vegetation would be a forest. But it does not have to be this way. There are high-yielding low-effort (in particular: no-till, no-pesticide) ways to grow grains as well, mostly going back to Masanobu Fukuoka. They have gained some popularity in India, where they are known as &#8216;Rishi Kheti&#8217;&mdash;&#8217;agriculture of the sages&#8217;. Here&#8217;s a photo gallery containing some fairly recent pictures:</p>
<p>&bull; <a href="http://picasaweb.google.com/rajuktitus">Raju Titus&#8217;s Public Gallery</a>, Picasa.</p>
<div align="center">
<a href="https://picasaweb.google.com/rajuktitus/WHEATJAN09"><img border="2" width="400" src="https://lh3.googleusercontent.com/-v030kHkGggg/SYQ1ZNNaQrI/AAAAAAAAGm8/yIoINh9UU_M/s512/101_0593.JPG" /></a><br />
<br />
Wheat growing amid fruit trees: no tillage, no pesticides &mdash; Hoghangabad, India</p>
</div>
<p>
An interesting perspective towards weeds which we usually do not take is: the reason this plant could establish itself here is that it&#8217;s filling an unfilled ecological niche.</p>
<p>
<b>JB</b>: Actually I&#8217;ve heard someone say: &quot;If you have weeds, it means you don&#8217;t have enough plants&quot;.</p>
<p>
<b>TF</b>:  Right.  So, when I take that weed out, I&#8217;d be well advised to take note of nature&#8217;s lesson and fill that particular niche with an ecological analog that is more useful. Otherwise, it will quite likely come back and need another intervention.</p>
<p>
I would consider this &quot;letting systems demonstrate their own evolution while closely watching what they want to tell us and providing some guidance&quot; the most important principle of permaculture.</p>
<p>
Another important principle is the &#8216;<a href="http://en.wikipedia.org/wiki/User_pays">user pays</a>&#8216; principle. A funny idea that comes up disturbingly often up in discussions of sustainability issues (even if it is not articulated explicitly) is that there are only a limited amount of resources which we keep on using up, and once we are done with that, this would be the end of mankind. Actually, that&#8217;s not how the world works.</p>
<p>
Take an apple tree, for example.  It starts out as a tiny seed, and has to accumulate a massive amount of (nutrient) resources to grow into a mature tree. Yet, once it completes its life cycle, dies down and is consumed by fungi, it leaves the world in a more fertile state than before.  Fertility tends to keep growing, because natural systems by and large work according to the principle that <b>any agent that takes something from the natural world will return something of equal or even greater ecosystemic value</b>.</p>
<p>Let me come back to an example I briefly mentioned earlier on.  At a very coarse level of detail, grazing cows eat grass and return cow dung. Now, in the intestines of the cow, quite a lot of interesting biochemistry has happened that converted nonprotein nitrogen (say, urea) into much more valuable protein:</p>
<p>&bull; W. D. Gallup, Ruminant nutrition, review of utilization of nonprotein nitrogen in the ruminant, <a href="http://pubs.acs.org/doi/abs/10.1021/jf60065a005"><i>Journal of Agricultural and Food Chemistry</i></a> <b>4</b> (1956), 625-627.</p>
<p>
A completely different example: nutrient accumulators such as <a href="http://en.wikipedia.org/wiki/Comfrey">comfrey</a> act as powerful pumps that draw up mineral nutrients from the subsoil, where they would be otherwise inaccessible, and make them available for ecosystemic cycling. </p>
<div align="center">
<a href="http://en.wikipedia.org/wiki/Comfrey"><img border="2" src="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Russian_comfrey_800.jpg/400px-Russian_comfrey_800.jpg" /></a><br />
<br />
Russian comfrey, <i>Symphytum x uplandicum</i></p>
</div>
<p>It is indeed possible to not only use this concept for garden management, but as a fundamental principle to run a sustainable economy. At the small scale (businesses), its viability has been demonstrated, but unfortunately this aspect of permaculture has not received as much attention yet as it should. Here, the key questions are along the lines of: do you need a washing machine, or is your actual need better matched by the description &#8216;access to some laundry service&#8217;?</p>
<p>
Concerning energy and material flows, an important principle is &quot;be aware of the boundaries of your domain of influence, capture them as early as you can, release them as late as you can, and extract as much beneficial use out of them as possible in between&quot;. We already talked about that. In the era of cheap labour from fossil fuels, it is often a very good idea to use big earthworking machinery to slightly adjust the topography of the landscape in order to capture and make better use of rainwater. Done right, such water harvesting earthworks can last many hundreds of years, and pay back the effort needed to create them many times over in terms of enhanced biological productivity. If this were implemented on a broad scale, not just by a small percentage of farmers, this could add significantly to flood protection as well. I am fairly confident that we will be doing this a lot in the 21st century, as the climate gets more erratic and we face both more extreme rainfall events (note that saturation water vapour pressure increases by about 7% for every Kelvin of temperature increase) as well as longer droughts. It would be smart to start with this now, rather than when high quality fuels are much more expensive. It would have been even smarter to start with this 20 years ago.</p>
<p>
A further important principle is to create stability through a high degree of network connectivity. We&#8217;ve also briefly talked about that already. In ecosystem design, this means to ensure that every important ecosystemic function is provided by more than one element (read: species), while every species provides multiple functions to the assembly. So, if something goes wrong with one element, there are other stabilizing forces in place. The mental picture which I like to use here is that of a stellar cluster: If we put a small number of stars next to one another, the system will undergo fairly complicated dynamics and eventually separate: in some three-star encounters, two stars will enter a very close orbit, while the third receives enough energy to go over escape velocity. If we lump together a large number of stars, their dynamics will thermalize and make it much more difficult for an individual star to obtain enough energy to leave the cluster&mdash;and keep it for a sufficiently long time to actually do so. Of course, individual stars do &#8216;boil off&#8217;, but the entire system does not fall apart as fast as just a few stars would.</p>
<p>
There are various philosophies how to best approach weaving an ecosystemic net, ranging from &#8216;<a href="http://www.cmsl.co.nz/assets/sm/2256/61/033-PEDERSENZARI.pdf">ecosystem mimicry</a>&#8216;;&mdash;i.e. taking wild nature and substituting some species with ecological analogs that are more useful to us&mdash;to &#8216;total synthesis of a species assembly&#8217;, i.e. combining species which in theory should grow well together due to their ecological characteristics, even though they might never have done so in nature. </p>
<p>
<b>JB</b>: Cool.  You&#8217;ve given me quite a lot to think about.  Finally, could you also leave me with a few good books to read on permaculture?</p>
<p>
<b>TF</b>: It depends on what you want to focus on. Concerning a practical hands-on introduction, this is probably the most evolved text:</p>
<p>&bull; Bill Mollison, <i>Introduction to Permaculture</i>, Tagari Publications, Tasmania, 1997.</p>
<p>
If you want more theory but are fine with a less refined piece of work, then this is quite useful:</p>
<p>&bull; Bill Mollison, <i>Permaculture &#8211; A Designer&#8217;s Manual</i>, Tagari Publications, Tasmania, 1988.</p>
<p>
Concerning temperate climates&mdash;in particular, Europe&mdash;this is a well researched piece of work that almost could be used as a college textbook:</p>
<p>&bull; Patrick Whitefield, <i>The Earth Care Manual: a Permaculture Handbook for Britain and Other Temperate Climates</i>, Permanent Publications, East Meon, 2004.</p>
<p>
For Europeans, this would probably be my first recommendation.</p>
<p>
<b>JB</b>: Thanks!  It&#8217;s been a very thought-provoking interview.</p>
<p></p>
<p><div align="center">
<a href="http://www.tagari.com/store/12"><img border="2" src="http://math.ucr.edu/home/baez/mollison_permaculture_a_designers'_manual.jpg" alt="" /></a>
</div>
</p>
<hr />
<p>
<i>Ecologists never apply good ecology to their gardens. Architects never understand the transmission of heat in buildings. And physicists live in houses with demented energy systems. It&#8217;s curious that we never apply what we know to how we actually live.</i> &#8211; <a href="http://www.scottlondon.com/interviews/mollison.html">Bill Mollison</a></p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/27/this-weeks-finds-week-315/#comments">80 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/economics/" rel="category tag">economics</a>, <a href="https://johncarlosbaez.wordpress.com/category/sustainability/" rel="category tag">sustainability</a>, <a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/" rel="category tag">this week's finds</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/27/this-weeks-finds-week-315/" rel="bookmark" title="Permanent Link to This Week&#8217;s Finds (Week&nbsp;315)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2735 post type-post status-publish format-standard hentry category-this-weeks-finds" id="post-2735">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/" rel="bookmark">This Week&#8217;s Finds (Week&nbsp;312)</a></h2>
				<small>14 March, 2011</small><br />


				<div class="entry">
					<p>This is the second part of my interview with <a href="http://yudkowsky.net/">Eliezer Yudkowsky</a>.  If you click on some technical terms here, you&#8217;ll go down to a section where I explain them.  </p>
<p>
<b>JB</b>: You&#8217;ve made a great case for working on artificial intelligence&mdash;and more generally, understanding how intelligence works, to figure out how we can improve it.  It&#8217;s especially hard to argue against studying rationality.  Even most people who doubt computers will ever get smarter will admit the possibility that people can improve.  And it seems clear that the almost every problem we face could benefit from better thinking.</p>
<p>
I&#8217;m intrigued by the title <i>The Art of Rationality</i> because it suggests that there&#8217;s a kind of art to it.  We don&#8217;t know how to teach someone to be a great artist, but maybe we can teach them to be a better artist.  So, what are some of the key principles when it comes to thinking better?</p>
<p>
<b>EY</b>: Stars above, what an open-ended question.  The idea behind the book is to explain all the drop-dead basic fundamentals that almost no one seems to know about, like what is evidence, what is simplicity, what is truth, the importance of actually changing your mind now and then, the major known <a href="#Cognitive Bias">cognitive biases</a> that stop people from changing their minds, what it means to live in a universe where things are made of parts, and so on.  This is going to be a book primarily aimed at people who are not completely frightened away by complex mathematical concepts such as addition, multiplication, and division (i.e., all you need to understand <a href="#Bayes' Theorem">Bayes&#8217; Theorem</a> if it&#8217;s explained properly), albeit with the whole middle of the book being just practical advice based on cognitive biases for the benefit of people who don&#8217;t want to deal with multiplication and division.   Each chapter is going to address a different aspect of rationality, not in full textbook detail, just enough to convey the sense of a concept, with each chapter being around 5-10,000 words broken into 4-10 bite-size sections of 500-2000 words each.  Which of the 27 currently planned book chapters did you want me to summarize?</p>
<p>
But if I had to pick just one thing, just one concept that&#8217;s most important, I think it would be the difference between rationality and rationalization.</p>
<p>
Suppose there&#8217;s two boxes, only one of which contains a diamond.  And on the two boxes there are various signs and portents which distinguish, imperfectly and probabilistically, between boxes which contain diamonds, and boxes which don&#8217;t.  I could take a sheet of paper, and I could write down all the signs and portents that I understand, and do my best to add up the evidence, and then on the bottom line I could write, &quot;And therefore, there is a 37% probability that Box A contains the diamond.&quot;  That&#8217;s rationality.  Alternatively, I could be the owner of Box A, and I could hire a clever salesman to sell Box A for the highest price he can get; and the clever salesman starts by writing on the bottom line of his sheet of paper, &quot;And therefore, Box A contains the diamond&quot;, and then he writes down all the arguments he can think of on the lines above.</p>
<p>
But consider:  At the moment the salesman wrote down the bottom line on that sheet of paper, the truth or falsity of the statement was fixed.  It&#8217;s already right or already wrong, and writing down arguments on the lines above isn&#8217;t going to change that.  Or if you imagine a spread of probable worlds, some of which have different boxes containing the diamond, the <i>correlation</i> between the ink on paper and the diamond&#8217;s location became fixed at the moment the ink was written down, and nothing which doesn&#8217;t change the ink or the box is going to change that correlation.</p>
<p>
That&#8217;s &quot;rationalization&quot;, which should really be given a name that better distinguishes it from rationality, like &quot;anti-rationality&quot; or something.  It&#8217;s like calling lying &quot;truthization&quot;.  You can&#8217;t make rational what isn&#8217;t rational to start with.</p>
<p>
Whatever process your brain uses, in reality, to decide what you&#8217;re going to argue for, that&#8217;s what determines your real-world effectiveness.  Rationality isn&#8217;t something you can use to argue for a side you already picked.  Your only chance to be rational is while you&#8217;re still choosing sides, before you write anything down on the bottom line.  If I had to pick one concept to convey, it would be that one.</p>
<p>
<b>JB</b>: Okay.  I wasn&#8217;t really trying to get you to summarize a whole book.  I&#8217;ve seen you explain a whole lot of heuristics designed to help us be more rational.  So I was secretly wondering if the &quot;art of rationality&quot; is mainly a long list of heuristics, or whether you&#8217;ve been able to find a few key principles that somehow spawn all those heuristics.</p>
<p>
Either way, it could be a tremendously useful book.  And even if you could distill the basic ideas down to something quite terse, in practice people are going to need all those heuristics&mdash;especially since many of them take the form &quot;here&#8217;s something you tend to do without noticing you&#8217;re doing it&mdash;so watch out!&quot;  If we&#8217;re saddled with dozens of <a href="#Cognitive Bias">cognitive biases</a> that we can only overcome through strenuous effort, then your book has to be long.  You can&#8217;t just say &quot;apply Bayes&#8217; rule and all will be well.&quot;</p>
<p>
I can see why you&#8217;d single out the principle that &quot;rationality only comes into play before you&#8217;ve made up your mind&quot;, because so much seemingly rational argument is really just a way of bolstering support for pre-existing positions.  But what is rationality? Is it something with a simple essential core, like &quot;updating probability estimates according to Bayes&#8217; rule&quot;, or is its very definition inherently long and complicated?</p>
<p>
<b>EY</b>:  I&#8217;d say that there are parts of rationality that we do understand very well in principle.  <a href="#Bayes' Theorem">Bayes&#8217; Theorem</a>, the <a href="#Expected Utility">expected utility formula</a>, and <a href="#Solomonoff Induction">Solomonoff induction</a> between them will get you quite a long way.  Bayes&#8217; Theorem says how to update based on the evidence, Solomonoff induction tells you how to assign your priors (in principle, it should go as the <a href="#Kolmogorov Complexity">Kolmogorov complexity</a> aka algorithmic complexity of the hypothesis), and then once you have a function which predicts what will probably happen as the result of different actions, the expected utility formula says how to choose between them.</p>
<p>
Marcus Hutter has a formalism called <a href="#AIXI">AIXI</a> which combines all three to write out an AI as a <a href="http://www.hutter1.net/ai/uaibook.htm#oneline">single equation</a> which requires infinite computing power plus a <a href="#Halting Oracle">halting oracle</a> to run.  And Hutter and I have been debating back and forth for quite a while on which AI problems are or aren&#8217;t solved by AIXI.  For example, I look at the equation as written and I see that AIXI will try the experiment of dropping an anvil on itself to resolve its uncertainty about what happens next, because the formalism as written invokes a sort of Cartesian dualism with AIXI on one side of an impermeable screen and the universe on the other; the equation for AIXI says how to predict sequences of percepts using Solomonoff induction, but it&#8217;s too simple to encompass anything as reflective as &quot;dropping an anvil on myself will destroy that which is processing these sequences of percepts&quot;.  At least that&#8217;s what I claim; I can&#8217;t actually remember whether Hutter was agreeing with me about that as of our last conversation.  Hutter sees AIXI as important because he thinks it&#8217;s a theoretical solution to almost all of the important problems; I see AIXI as important because it demarcates the line between things that we understand in a fundamental sense and a whole lot of other things we don&#8217;t.</p>
<p>
So there are parts of rationality&mdash;big, important parts too&mdash;which we know how to derive from simple, compact principles in the sense that we could write very simple pieces of code which would behave rationally along that dimension given unlimited computing power.</p>
<p>
But as soon as you start asking &quot;How can <i>human beings</i> be more rational?&quot; then things become hugely more complicated because human beings make much more complicated errors that need to be patched on an individual basis, and asking &quot;How can I be rational?&quot; is only one or two orders of magnitude simpler than asking &quot;How does the brain work?&quot;, i.e., you <i>can</i> hope to write a single book that will cover many of the major topics, but not quite answer it in an interview question&#8230;</p>
<p>
On the other hand, the question &quot;What is it that I am <i>trying to do</i>, when I try to be rational?&quot; is a question for which big, important chunks can be answered by saying &quot;Bayes&#8217; Theorem&quot;, &quot;expected utility formula&quot; and &quot;simplicity prior&quot; (where Solomonoff induction is the canonical if uncomputable simplicity prior).</p>
<p>
At least from a mathematical perspective.  From a human perspective, if you asked &quot;What am I trying to do, when I try to be rational?&quot; then the fundamental answers would run more along the lines of &quot;Find the truth without flinching from it and without flushing all the arguments you disagree with out the window&quot;, &quot;When you don&#8217;t know, try to avoid just making stuff up&quot;, &quot;Figure out whether the strength of evidence is great enough to support the weight of every individual detail&quot;, &quot;Do what should lead to the best consequences, but not just what looks on the immediate surface like it should lead to the best consequences, you may need to follow extra rules that compensate for known failure modes like shortsightedness and moral rationalizing&quot;&#8230;</p>
<p>
<b>JB</b>:  Fascinating stuff!</p>
<p>
Yes, I can see that trying to improve humans is vastly more complicated than designing a system from scratch&#8230; but also very exciting, because you can tell a human a high-level principle like &quot; &quot;When you don&#8217;t know, try to avoid just making stuff up&quot; and have some slight hope that they&#8217;ll understand it without it being explained in a mathematically precise way.</p>
<p>
I guess AIXI dropping an anvil on itself is a bit like some of the self-destructive experiments that parents fear their children will try, like sticking a pin into an electrical outlet.  And it seems impossible to avoid doing such experiments without having a base of knowledge that was either &quot;built in&quot; or acquired by means of previous experiments.</p>
<p>
In the latter case, it seems just a matter of luck that none of these previous experiments were fatal.  Luckily, people also have &quot;built in&quot; knowledge.  More precisely, we have access to our ancestor&#8217;s knowledge and habits, which get transmitted to us genetically and culturally.  But still, a fair amount of random blundering, suffering, and even death was required to build up that knowledge base.</p>
<p>
So when you imagine &quot;seed AIs&quot; that keep on improving themselves and eventually become smarter than us, how can you reasonably hope that they&#8217;ll avoid making truly spectacular mistakes?   How can they learn really new stuff without a lot of risk?</p>
<p>
<b>EY</b>:  The best answer I can offer is that they can be conservative externally and deterministic internally.</p>
<p>
Human minds are constantly operating on the ragged edge of error, because we have evolved to compete with other humans.  If you&#8217;re a bit more conservative, if you double-check your calculations, someone else will grab the banana and that conservative gene will not be passed on to descendants.  Now this does not mean we couldn&#8217;t end up in a bad situation with AI companies competing with each other, but there&#8217;s at least the <i>opportunity</i> to do better.  </p>
<p>
If I recall correctly, the Titanic sank from managerial hubris and cutthroat cost competition, not engineering hubris.  The original liners were designed far more conservatively, with triple-redundant compartmentalized modules and soon.  But that was before cost competition took off, when the engineers could just add on safety features whenever they wanted.  The part about the Titanic being extremely safe was pure marketing literature. </p>
<p>
There is also no good reason why any machine mind should be overconfident the way that humans are.  There are studies showing that, yes, managers prefer subordinates who make overconfident promises to subordinates who make accurate promises&mdash;sometimes I still wonder that people are this silly, but given that people are this silly, the social pressures and evolutionary pressures follow.  And we have <i>lots</i> of studies showing that, for whatever reason, humans are hugely overconfident; less than half of students finish their papers by the time they think it 99% probable they&#8217;ll get done, etcetera.</p>
<p>
And this is a form of stupidity an AI can simply do without.  Rationality is not omnipotent; a <a href="#Bounded rationality">bounded rationalist</a> cannot do all things.  But there is no reason why a bounded rationalist should ever have to <i>overpromise</i>, be systematically overconfident, systematically tend to claim it can do what it can&#8217;t.  It does not have to systematically underestimate the value of getting more information, or overlook the possibility of unspecified <a href="#Black Swan">Black Swans</a> and what sort of general behavior helps to compensate.  (A bounded rationalist <i>does</i> end up overlooking specific Black Swans because it doesn&#8217;t have enough computing power to think of all <i>specific</i> possible catastrophes.)</p>
<p>
And contrary to how it works in say Hollywood, even if an AI does manage to accidentally kill a human being, that doesn&#8217;t mean it&#8217;s going to go &#8220;I HAVE KILLED&#8221; and dress up in black and start shooting nuns from rooftops.  What it ought to do&mdash;what you&#8217;d want to see happen&mdash;would be for the utility function to go on undisturbed, and for the probability distribution to update based on whatever unexpected thing just happened and contradicted its old hypotheses about what does and does not kill humans.  In other words, keep the same goals and say &#8220;oops&#8221; on the world-model; keep the same terminal values and revise its instrumental policies.  These sorts of external-world errors are not catastrophic unless they can actually wipe out the planet in one shot, somehow.</p>
<p>
The catastrophic sort of error, the sort you can&#8217;t recover from, is an error in modifying your own source code.  If you accidentally change your utility function you will no longer <i>want</i> to change it back.  And in this case you might indeed ask, &quot;How will an AI make millions or billions of code changes to itself without making a mistake like that?&quot;  But there are in fact methods powerful enough to do a billion error-free operations.  A friend of mine once said something along the lines of &quot;a CPU does a mole of transistor operations, error-free, in a day&quot; though I haven&#8217;t checked the numbers.  When chip manufacturers are building a machine with hundreds of millions of interlocking pieces and they don&#8217;t want to have to change it after it leaves the factory, they may go so far as to prove the machine correct, using human engineers to navigate the proof space and suggest lemmas to prove (which AIs can&#8217;t do, they&#8217;re defeated by the exponential explosion) and complex theorem-provers to prove the lemmas (which humans would find boring) and simple verifiers to check the generated proof.  It takes a combination of human and machine abilities and it&#8217;s extremely expensive.  But I strongly suspect that an Artificial General Intelligence with a good design would be able to treat <i>all</i> its code that way&mdash;that it would combine all those abilities in a single mind, and find it easy and natural to prove theorems about its code changes.  It could not, of course, prove theorems about the external world (at least not without highly questionable assumptions).  It could not prove external actions correct.  The only thing it could write proofs about would be events inside the highly deterministic environment of a CPU&mdash;that is, its own thought processes.  But it could prove that it was processing probabilities about those actions in a Bayesian way, and prove that it was assessing the probable consequences using a particular utility function.  It could prove that it was sanely trying to achieve the same goals.</p>
<p>
A self-improving AI that&#8217;s unsure about whether to do something ought to just wait and do it later after self-improving some more.  It doesn&#8217;t have to be overconfident.  It doesn&#8217;t have to operate on the ragged edge of failure.  It doesn&#8217;t have to stop gathering information too early, if more information can be productively gathered before acting.  It doesn&#8217;t have to fail to understand the concept of a Black Swan.  It doesn&#8217;t have to do all this using a broken error-prone brain like a human one.  It doesn&#8217;t have to be stupid in the ways like overconfidence that humans seem to have specifically evolved to be stupid.  It doesn&#8217;t have to be poorly calibrated (assign 99% probabilities that come true less that 99 out of 100 times), because bounded rationalists can&#8217;t do everything but they don&#8217;t have to claim what they can&#8217;t do.  It can <i>prove</i> that its self-modifications aren&#8217;t making itself crazy or changing its goals, at least if the transistors work as specified, or make no more than any possible combination of 2 errors, etc.  And if the worst does happen, so long as there&#8217;s still a world left afterward, it will say &quot;Oops&quot; and not do it again.  This sounds to me like essentially the optimal scenario given any sort of bounded rationalist whatsoever.</p>
<p>
And finally, if I was building a self-improving AI, I wouldn&#8217;t ask it to operate heavy machinery until after it had grown up.  Why should it?</p>
<p>
<b>JB</b>: Indeed!  </p>
<p>Okay&mdash;I&#8217;d like to take a break here, explain some terms you used, and pick up next week with some less technical questions, like <i>what&#8217;s a better use of time: tackling environmental problems, or trying to prepare for a technological singularity?</i><br />
<br />&nbsp;<br />&nbsp;</p>
<h4>Some explanations</h4>
<p>Here are some quick explanations.  If you click on the links here you&#8217;ll get more details:</p>
<p><a name="Cognitive Bias"><br /></a></p>
<p>&bull; <b>Cognitive Bias</b>.  A <a href="http://en.wikipedia.org/wiki/Cognitive_bias">cognitive bias</a> is a way in which people&#8217;s judgements systematically deviate from some norm&mdash;for example, from ideal rational behavior.  You can see a <a href="http://en.wikipedia.org/wiki/List_of_cognitive_biases">long list of cognitive biases</a> on Wikipedia.  It&#8217;s good to know a lot of these and learn how to spot them in yourself and your friends.  </p>
<p>
For example, <a href="http://en.wikipedia.org/wiki/Confirmation_bias">confirmation bias</a> is the tendency to pay more attention to information that confirms our existing beliefs.  Another great example is the <a href="http://en.wikipedia.org/wiki/Bias_blind_spot">bias blind spot</a>: the tendency for people to think of themselves as less cognitively biased than average!  I&#8217;m sure glad I don&#8217;t suffer from <i>that</i>.
</p>
<p><a name="Bayes' Theorem"><br /></a> </p>
<p>&bull; <b>Bayes&#8217; Theorem</b>.  This is a rule for updating our opinions about probabilities when we get new information.  Suppose you start out thinking the probability of some event A is P(A), and the probability of some event B is P(B).  Suppose P(A|B) is the probability of event A <i>given</i> that B happens.  Likewise, suppose P(B|A) is the probability of B given that A happens.  Then the probability that both A and B happen is
</p>
<p>
P(A|B) P(B)
</p>
<p>
but by the same token it&#8217;s also
</p>
<p>
P(B|A) P(A)
</p>
<p>
so these are equal.  A little algebra gives <a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes&#8217; Theorem</a>:
</p>
<p>
P(A|B) = P(B|A) P(A) / P(B)
</p>
<p>
If for some reason we know everything on the right-hand side, we can this equation to work out P(A|B), and thus update our probability for event A when we see event B happen.
</p>
<p>
For a longer explanation with examples, see:
</p>
<p>&bull; Eliezer Yudkowsky, <a href="http://yudkowsky.net/rational/bayes">An intuitive explanation of Bayes&#8217; Theorem</a>.</p>
<p>Some handy jargon: we call P(A) the <b>prior</b> probability of A, and P(A|B) the <b>posterior</b> probability.   </p>
<p><a name="Solomonoff Induction"><br /></a> </p>
<p>&bull;<b>Solomonoff Induction</b>.  Bayes&#8217; Theorem helps us compute posterior probabilities, but where do we get the prior probabilities from?  How can we guess probabilities before we&#8217;ve observed anything? </p>
<p>This famous puzzle led Ray Solomonoff to invent <a href="http://en.wikipedia.org/wiki/Inductive_inference">Solomonoff induction</a>.  The key new idea is <a href="http://www.scholarpedia.org/article/Algorithmic_probability">algorithmic probability theory</a>.  This is a way to define a probability for any string of letters in some alphabet, where a string counts as more probable if it&#8217;s less complicated.  If we think of a string as a &quot;hypothesis&quot;&mdash;it could be a sentence in English, or an equation&mdash;this becomes a way to formalize <a href="http://en.wikipedia.org/wiki/Occam%27s_razor">Occam&#8217;s razor</a>: the idea that given two competing hypotheses, the simpler one is more likely to be true.</p>
<p>
So, algorithmic probability lets us define a prior probability distribution on hypotheses, the so-called &#8220;simplicity prior&#8221;, that implements Occam&#8217;s razor.</p>
<p>
More precisely, suppose we have a special programming language where:</p>
<ol>
<li> Computer programs are written as strings of bits.
</p>
</li>
<li>They contain a special bit string meaning &#8220;END&#8221; at the end, and nowhere else.
</p>
</li>
<li>
They don&#8217;t take an input: they just run and either halt and print out a string of letters, or never halt.</p>
</li>
</ol>
<p>Then to get the algorithmic probability of a string of letters, we take all programs that print out that string and add up</p>
<p><div align="center">
2<sup>-length of program</sup></p>
</div>
<p>So, you can see that a string counts as more probable if it has more short programs that print it out.</p>
<p><a name="Kolmogorov Complexity"><br /></a></p>
<p> &bull;<b>Kolmogorov complexity</b>.  The <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmologorov complexity</a> of a string of letters is the length of the shortest program that prints it out, where programs are written in a special language as described above.   This is a way of measuring how complicated a string is.  It&#8217;s closely related to the algorithmic entropy: the difference between the Kolmogorov complexity of a string and minus the logarithm of its algorithmic probability is bounded by a constant, if we take logarithms using base 2.  For more on all this stuff, see:</p>
<p>&bull; M. Li and P. Vit&aacute;nyi, <i>An Introduction to Kolmogorov Complexity Theory and its Applications</i>, Springer, Berlin, 2008.</p>
<p><a name="Halting Oracle"><br /></a> </p>
<p> &bull; <b>Halting Oracle</b>.  Alas, the algorithmic probability of a string is not <a href="http://en.wikipedia.org/wiki/Computability_theory">computable</a>.   Why?  Because to compute it, you&#8217;d need to go through all the programs in your special language that print out that string and add up a contribution from each one.   But to do that, you&#8217;d need to know which programs halt&mdash;and there&#8217;s no systematic way to answer that question, which is called the <a href="http://en.wikipedia.org/wiki/Halting_problem">halting problem</a>.
</p>
<p>
But, we can pretend!  We can pretend we have a magic box that will tell us whether any program in our special language halts.  Computer scientists call any sort of magic box that answers questions an <a href="http://en.wikipedia.org/wiki/Oracle_machine">oracle</a>.  So, our particular magic box called a <a href="http://www.xamuel.com/the-halting-problem/">halting oracle</a>.</p>
<p><a name="AIXI"><br /></a> </p>
<p> &bull; <b>AIXI</b>.  AIXI is <a href="http://www.hutter1.net/">Marcus Hutter&#8217;s</a> attempt to define an agent that &quot;behaves optimally in any computable environment&quot;.  Since AIXI relies on the idea of algorithmic probability, you can&#8217;t run AIXI on a computer unless it has infinite computer power and&mdash;the really hard part&mdash;access to a halting oracle.  However, Hutter has also defined computable approximations to AIXI.  For a quick intro, see this:</p>
<p>&bull; Marcus Hutter, <a href="http://www.hutter1.net/ai/aixigentle.pdf">Universal intelligence: a mathematical top-down approach</a>.</p>
<p>
For more, try this:
</p>
<p>&bull; Marcus Hutter, <i>Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability</i>, Springer, Berlin, 2005.</p>
<p><a name="Expected Utility"><br /></a> </p>
<p> &bull; <b>Utility</b>.  <a href="http://en.wikipedia.org/wiki/Utility">Utility</a> is a hypothetical numerical measure of satisfaction.  If you know the probabilities of various outcomes, and you know what your utility will be in each case, you can compute your &quot;expected utility&quot;  by taking the probabilities of the different outcomes, multiplying them by the corresponding utilities, and adding them up.  In simple terms, this is how happy you&#8217;ll be <i>on average</i>.  The <a href="http://en.wikipedia.org/wiki/Expected_utility_hypothesis">expected utility hypothesis</a> says that a rational decision-maker has a utility function and will try to maximize its expected utility.  </p>
<p><a name="Bounded Rationality"><br /></a> </p>
<p> &bull;<b>Bounded Rationality</b>.  In the real world, any decision-maker has limits on its computational power and the time it has to make a decision.  The idea that rational decision-makers &quot;maximize expected utility&quot; is oversimplified unless it takes this into account somehow.  Theories of <a href="http://en.wikipedia.org/wiki/Bounded_rationality">bounded rationality</a> try to take these limitations into account.  One approach is to think of decision-making as yet another activity whose costs and benefits must be taken into account when making decisions.  Roughly: you must decide how much time you want to spend deciding.  Of course, there&#8217;s an interesting circularity here.</p>
<p><a name="Black Swan"><br /></a> </p>
<p> &bull; <b>Black Swan</b>.  According to <a href="http://en.wikipedia.org/wiki/Nassim_Nicholas_Taleb">Nassim Taleb</a>, human history is dominated by <a href="http://en.wikipedia.org/wiki/Black_swan_theory">black swans</a>: important events that were unpredicted and indeed unpredictable, but rationalized by hindsight and thus made to seem as if they <i>could</i> have been predicted.  He believes that rather than trying to predict such events (which he considers largely futile), we should try to get good at adapting to them.  For more see:</p>
<p>&bull; Nassim Taleb, <i><a href="http://en.wikipedia.org/wiki/The_Black_Swan_%28Taleb_book%29">The Black Swan: The Impact of the Highly Improbable</a></i>, Random House, New York, 2007.</p>
<hr />
<p>
<em>The first principle is that you must not fool yourself&mdash;and you are the easiest person to fool.</em> &#8211; Richard Feynman</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/#comments">30 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/" rel="category tag">this week's finds</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/" rel="bookmark" title="Permanent Link to This Week&#8217;s Finds (Week&nbsp;312)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2142 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2142">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/" rel="bookmark">Algorithmic Thermodynamics (Part&nbsp;2)</a></h2>
				<small>6 January, 2011</small><br />


				<div class="entry">
					<p>Here are some notes for a talk tomorrow at the <a href="http://www.quantumlah.org/">Centre for Quantum Technologies</a>.  You can think of this as a kind of followup to <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">my first post on this subject</a>.  </p>
<h4>Introduction</h4>
<p>The idea of <i>entropy</i> arose in thermodynamics, and its connection to probability theory is fundamental to statistical mechanics.  Its connection to <i>information</i> was developed later by Shannon.  We now think of entropy and information as two sides of the same coin.  </p>
<p>But there&#8217;s another concept, called <i>algorithmic information</i>, which was developed in work on logic and computer science.  This concept is related to Shannon&#8217;s earlier notion of information, but it also seems rather different.  </p>
<p>For example, Shannon&#8217;s ideas let us compute the information of a <i>probability distribution</i> on bit strings.  So if we detect a radio signal from an extraterrestrial life form, that sends us a string of a dozen 0&#8217;s and 1&#8217;s each day, and it seems the string is chosen randomly according to some probability distribution, we can calculate the information per message.  But if I just show you a <i>single</i> bit string, like this:</p>
<div align="center">
011101100001
</div>
<p>it makes no sense to ask what its information is.  On the other hand, we <i>can</i> define its algorithmic information.</p>
<p>Nonetheless, my goal here is to show you that algorithmic information can really be seen as a <i>special case</i> of the old probabilistic concept of information.  This lets us take all the familiar tools from statistical mechanics and thermodynamics, and apply them to algorithmic information theory.  Mike Stay and I started to do this here:</p>
<p>&bull; John Baez and Mike Stay, <a href="http://arxiv.org/abs/1010.2067">Algorithmic thermodynamics</a>.</p>
<p>Unfortunately, I&#8217;ll only have time to sketch a bit of what we did!  </p>
<h4>Algorithmic information &#8211; first definition</h4>
<p>Here&#8217;s a definition of algorithmic information.  Later we&#8217;ll see a better one that&#8217;s almost equivalent.  </p>
<p>Fix a programming language where a program is a finite bit string and its output, if it halts, is again a finite bit string.  Then the <b>algorithmic information</b> of a finite bit string is the length of the shortest program that has that bit string as output.  </p>
<p>So, for example, the algorithmic information of a string of a trillion 0&#8217;s is low, because you can write a short program that prints this out.  On the other hand, the algorithmic information of a long &#8220;random&#8221; string of bits will be high, because the shortest program to print it out will be a program that says &#8220;print out this string: 01011100101100&#8230;&#8221; &mdash; so the program is approximately as long as the string itself: slightly longer, but only by a fixed amount.   </p>
<p>Of course you may ask what &#8220;random&#8221; means here.  I haven&#8217;t defined it!   But the point is, people have used these ideas to <i>give a definition</i> what it means for a string of bits to be random.  There are different ways to do it.  For example: a bit string is <b>&epsilon;-random</b> if the shortest program that prints it out is at least &epsilon; times as long as the string itself.  </p>
<p>You may also wonder: &#8220;Doesn&#8217;t the definition of algorithmic information depend on the details of our programming language?&#8221;  And the answer is, &#8220;Yes, but not very much.&#8221;  More precisely, for any two universal programming languages, there&#8217;s a constant <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> such that for any finite bit string, the algorithmic information of that string will differ by at most <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> depending on which language we use.  </p>
<h4>Ordinary information</h4>
<p>Let&#8217;s see how algorithmic information compares to the usual concept of information introduced by Shannon.  The usual concept works like this:</p>
<p>If an event of probability <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> occurs, we define the <b>information</b> gained by learning this event has occurred to be <img src="https://s0.wp.com/latex.php?latex=-+%5Clog+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;log p" class="latex" />.  We take the logarithm because probabilities of independent events multiply, and we want information to add.  The minus sign makes the information positive.   We can use any base we want for the logarithm: physicists like <img src="https://s0.wp.com/latex.php?latex=e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e" class="latex" />, while computer scientists favor 2.</p>
<p>If there&#8217;s a set <img src="https://s0.wp.com/latex.php?latex=X+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X " class="latex" /> of possible outcomes, where the outcome <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> occurs with probability <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" />, then the <i>average</i> or <i>expected</i> amount of information we gain by learning the outcome is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bx+%5Cin+X%7D+p_x+%5Clog+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{x &#92;in X} p_x &#92;log p_x " class="latex" /></p>
<p>We call this the <b>entropy</b> of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  </p>
<p>Now, ordinary information doesn&#8217;t look very much like algorithmic information!  But there&#8217;s a second definition of algorithmic information, almost equivalent to the first one, that makes the relation clearer.</p>
<h4>Algorithmic information &#8211; second definition</h4>
<p>First, a minor shift in viewpoint.  Before we defined algorithmic information for a finite bit string.  Now let&#8217;s define it for a natural number.  This change in viewpoint is no big deal, since we can set up a one-to-one correspondence between natural numbers and finite bit strings that&#8217;s easy to compute.  </p>
<p>We&#8217;ll still think of our programs as finite bit strings.  Not every such string gives a program that halts.   We also may demand that bit strings have special features to count as programs.  For example, we may demand that programs end in the string 11111, just like a Fortran program must end with the word END.  </p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be the set of programs that halt.  Without loss of generality, we can assume that <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is <b>prefix-free</b>.  This means that if <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />, no other bit string starting with the string <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is also in <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  For example, if the string 11111 marks the end of a program, and is only allowed to show up at the end of the program, <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> will be prefix-free.  </p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=V%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x)" class="latex" /> be the length of the program <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />.  It&#8217;s easy to see that if <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is prefix-free, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+X%7D+2%5E%7B-V%28x%29%7D+%5Cle+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in X} 2^{-V(x)} &#92;le 1 " class="latex" /></p>
<p>Making this sum finite is the reason we want the prefix-free condition &mdash; you&#8217;ll see exactly why in a minute.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=N%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N(x)" class="latex" /> be the output of the program <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />.  Remember, we now think of the output as a natural number.  So, we have functions</p>
<p><img src="https://s0.wp.com/latex.php?latex=V%2C+N+%3A+X+%5Cto+%5Cmathbb%7BN%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V, N : X &#92;to &#92;mathbb{N}" class="latex" /></p>
<p>We define the <b>algorithmic information</b> of a number <img src="https://s0.wp.com/latex.php?latex=n+%5Cin+%5Cmathbb%7BN%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;in &#92;mathbb{N}" class="latex" /> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+-+%5Clog+%5Csum_%7Bx+%5Cin+X%2C+N%28x%29+%3D+n%7D+2%5E%7B-V%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = - &#92;log &#92;sum_{x &#92;in X, N(x) = n} 2^{-V(x)} " class="latex" /></p>
<p>So: we do a sum over all programs <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> having the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> as output.  We sum up <img src="https://s0.wp.com/latex.php?latex=2%5E%7B-V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2^{-V(x)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=V%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x)" class="latex" /> is the length of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.   The sum converges because </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+X%7D+2%5E%7B-V%28x%29%7D+%5Cle+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in X} 2^{-V(x)} &#92;le 1 " class="latex" /></p>
<p>That&#8217;s where the prefix-free condition comes into play.<br />
Finally, we take minus the logarithm of this sum.</p>
<p>This seems a bit complicated!  But suppose there&#8217;s just <i>one</i> program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> having the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> as output.  Then the sum consists of one term, the logarithm cancels out the exponential, and the minus signs cancel too, so we get </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+V%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = V(x)" class="latex" /></p>
<p>This matches our first definition: the algorithmic information of a number is the length of the shortest program having that number as output. </p>
<p>Of course in this case the shortest program is the <i>only</i> program having that output.  If we have more than one program with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> as output, the second definition of algorithmic entropy gives a smaller answer than the first definition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S_%7B%5Cmathrm%7Bsecond%7D%7D%28n%29+%5Cle+S_%7B%5Cmathrm%7Bfirst%7D%7D%28n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_{&#92;mathrm{second}}(n) &#92;le S_{&#92;mathrm{first}}(n) " class="latex" /></p>
<p>However, there&#8217;s a famous theorem, proved by Leonid Levin in 1974, that says the new definition is not very different from the old one.  The difference is bounded by a constant.  More precisely: for any universal prefix-free programming language, there&#8217;s a constant <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> such that for every <img src="https://s0.wp.com/latex.php?latex=n+%5Cin+%5Cmathbb%7BN%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;in &#92;mathbb{N}" class="latex" />, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S_%7B%5Cmathrm%7Bsecond%7D%7D%28n%29+%5Cge+S_%7B%5Cmathrm%7Bfirst%7D%7D%28n%29+-+K+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_{&#92;mathrm{second}}(n) &#92;ge S_{&#92;mathrm{first}}(n) - K " class="latex" /></p>
<p>Since the algorithmic information depends on our programming language, it was only well-defined within an additive constant in the first place.  So, switching from the first definition to the second one doesn&#8217;t significantly change the concept.  But, it makes the relation to ordinary information a lot easier to see! </p>
<p>From now on we&#8217;ll use the second definition.</p>
<h4>Algorithmic information versus ordinary information</h4>
<p>To relate algorithmic information to ordinary information, we need to get logarithms and probabilities into the game.  We see a logarithm here:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+-+%5Clog+%5Csum_%7Bx+%5Cin+X%2C+N%28x%29+%3D+n%7D+2%5E%7B-V%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = - &#92;log &#92;sum_{x &#92;in X, N(x) = n} 2^{-V(x)} " class="latex" /></p>
<p>but it&#8217;s in a funny place, outside the sum &mdash; and where are the probabilities?  </p>
<p>To solve both these puzzles, let&#8217;s define a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on the set of natural numbers by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_n+%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Csum_%7Bx+%5Cin+X%2C+N%28x%29+%3D+n%7D+2%5E%7B-V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_n = &#92;frac{1}{Z} &#92;sum_{x &#92;in X, N(x) = n} 2^{-V(x)}" class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalizing constant, to make the probabilities sum to 1.   Taking</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+2%5E%7B-V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} 2^{-V(x)}" class="latex" /></p>
<p>ensures that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bn+%5Cin+%5Cmathbb%7BN%7D%7D+p_n+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{n &#92;in &#92;mathbb{N}} p_n = 1 " class="latex" /></p>
<p>Now, notice that</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+-+%5Clog+p_n+%5C%3B+%2B+%5C%3B+%5Clog+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = - &#92;log p_n &#92;; + &#92;; &#92;log Z " class="latex" /></p>
<p>So, up to an additive constant, the algorithmic entropy of the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=-%5Clog+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;log p_n" class="latex" />.  But <img src="https://s0.wp.com/latex.php?latex=-%5Clog+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;log p_n" class="latex" /> is just the information gained upon learning the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />, if we started out knowing only that <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> was chosen randomly according to the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.</p>
<p>What&#8217;s the meaning of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />?   Simple: <img src="https://s0.wp.com/latex.php?latex=p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_n" class="latex" /> is the probability that the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is the output of a randomly chosen program&#8230; where &#8216;randomly chosen&#8217; means chosen according to a certain specific rule.  The rule is that increasing the length of a program by 1 makes it half as probable.  In other words, the probability <img src="https://s0.wp.com/latex.php?latex=q_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x" class="latex" /> of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is </p>
<p><img src="https://s0.wp.com/latex.php?latex=q_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+2%5E%7B-V%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x = &#92;frac{1}{Z} 2^{-V(x)} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is the normalization factor chosen to make these probabilities sum to 1. </p>
<p>So, the relation between algorithmic information and ordinary information is this:</p>
<blockquote><p>The algorithmic information of a number is the information gained by learning that number, if beforehand we only knew that it was the output of a randomly chosen program.</p></blockquote>
<h4>Algorithmic thermodynamics</h4>
<p>Why should the probability of the program <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> be defined as</p>
<p><img src="https://s0.wp.com/latex.php?latex=q_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+2%5E%7B-V%28x%29%7D+%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x = &#92;frac{1}{Z} 2^{-V(x)} ?" class="latex" /></p>
<p>There is no reason we <i>need</i> to use this probability distribution.  However, there&#8217;s something good about it.  It&#8217;s an example of a <b>Gibbs state</b>, meaning a probability distribution that maximizes entropy subject to a constraint on the expected value of some observable.  Nature likes to maximize entropy, so Gibbs states are fundamental to statistical mechanics.  So is the quantity <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />: it&#8217;s called the <b>partition function</b>.</p>
<p>The idea works like this: suppose we want to maximize the entropy of a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on the set of programs, subject to a constraint on the expected value of the length of the program.  Then the answer is</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cgamma+V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = &#92;frac{1}{Z} e^{-&#92;gamma V(x)}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> is some number, and the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> ensures that the probabilities sum to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-+%5Cgamma+V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{- &#92;gamma V(x)}" class="latex" /></p>
<p>So, <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> equals our previous probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> when </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%5Cln+2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma = &#92;ln 2 " class="latex" /></p>
<p>However, it is also interesting to consider other values of <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" />, and in fact Tadaki has already done so:</p>
<p>&bull; K. Tadaki, <a href="http://arxiv.org/abs/nlin.CD/0212001">A generalization of Chaitin’s halting probability and halting self-similar sets</a>, <i>Hokkaido Math. J.</i> <b>31</b> (2002), 219–253. </p>
<p>The partition function converges for <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%5Cge+2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma &#92;ge 2" class="latex" /> but diverges otherwise.  </p>
<p>More generally, we can look for the probability distribution that maximizes entropy subject to constraints on the expected value of <i>several</i> observables.  For example, let:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(x) = " class="latex" /> the logarithm of the runtime of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V%28x%29+%3D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x) = " class="latex" /> the length of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=N%28x%29+%3D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N(x) = " class="latex" /> the output of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>Then the probability distribution that maximizes entropy subject to constraints on the expected values of these three quantities is</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta+E%28x%29+-+%5Cgamma+V%28x%29+-+%5Cdelta+N%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = &#92;frac{1}{Z} e^{-&#92;beta E(x) - &#92;gamma V(x) - &#92;delta N(x)} " class="latex" /></p>
<p>where now the partition function is</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-%5Cbeta+E%28x%29+-+%5Cgamma+V%28x%29+-+%5Cdelta+N%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{-&#92;beta E(x) - &#92;gamma V(x) - &#92;delta N(x)}" class="latex" /></p>
<p>We&#8217;ve chosen our notation to remind experts on statistical mechanics that they&#8217;ve seen formulas like this before. The exact same formulas describe a piston full of gas in thermal equilibrium!     From a formal, purely mathematical viewpoint:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> is analogous to the <b>internal energy</b> of the gas, and <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is analogous to <img src="https://s0.wp.com/latex.php?latex=1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/T" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the <b>temperature</b> in units where Boltzmann&#8217;s constant is 1.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> is analogous to the <b>volume</b> of the gas, and <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> is analogous to <img src="https://s0.wp.com/latex.php?latex=P%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P/T" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> is the <b>pressure</b>.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> is analogous to the <b>number of molecules</b> in the gas, and <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta" class="latex" /> is analogous to <img src="https://s0.wp.com/latex.php?latex=-%5Cmu%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;mu/T" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> is the <b>chemical potential</b>.</p>
<p>The analogy here is quite arbitrary.  I&#8217;m not saying that the length of a program is profoundly similar to the volume of a cylinder of gas; we could have chosen the letter <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> to stand for the length of a program and everything would still work.  </p>
<p>But the analogy works.  In other words: now we can take a lot of familiar facts about thermodynamics and instantly translate them into analogous facts about algorithmic information theory!  For example, define the entropy of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in the usual way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+%5Csum_%7Bx+%5Cin+X%7D+p_x+%5Cln+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = &#92;sum_{x &#92;in X} p_x &#92;ln p_x " class="latex" /></p>
<p>We can think of this as a function of either <img src="https://s0.wp.com/latex.php?latex=T%2C+P%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T, P," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> or the expected values of <img src="https://s0.wp.com/latex.php?latex=E%2C+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, V" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" />.  In thermodynamics we learn lots of equations like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+E%7D+%5Cright%7C_%7BV%2CN%7D+%3D+%5Cfrac%7B1%7D%7BT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;frac{&#92;partial S}{&#92;partial E} &#92;right|_{V,N} = &#92;frac{1}{T}" class="latex" /></p>
<p>where by standard abuse of notation we&#8217;re using <img src="https://s0.wp.com/latex.php?latex=E%2C+V%2C+N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, V, N" class="latex" /> to stand for their expected values.  And, <i>all these equations remain true in algorithmic thermodynamics!</i>  </p>
<p>The interesting part is figuring out what all these equations really <i>mean</i> in the context of algorithmic thermodynamics.  For a start on that, try <a href="http://arxiv.org/abs/1010.2067">our paper</a>.</p>
<p>For example, we call <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> <b>algorithmic temperature</b>.  If you allow programs to run longer, more of them will halt and give an answer. Thanks to the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+E%7D+%5Cright%7C_%7BV%2CN%7D+%3D+%5Cfrac%7B1%7D%7BT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;frac{&#92;partial S}{&#92;partial E} &#92;right|_{V,N} = &#92;frac{1}{T}" class="latex" /></p>
<p>the algorithmic temperature is roughly the number of times you have to double the runtime in order to double the number of programs that satisfy the constraints on length and output.</p>
<p>We also consider the algorithmic analogues of thermodynamic cycles, familiar from old work on steam engines.  Charles Babbage described a computer powered by a steam engine; we describe a heat engine powered by programs!  The significance of this line of thinking remains fairly mysterious. However, I&#8217;m hoping that it will help point the way toward a further synthesis of algorithmic information theory and thermodynamics.</p>
<div align="center">
<img src="https://i2.wp.com/math.ucr.edu/home/baez/piston_tiny.jpg" />
</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/#comments">43 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/" rel="bookmark" title="Permanent Link to Algorithmic Thermodynamics (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1080 post type-post status-publish format-standard hentry category-physics" id="post-1080">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/09/29/ashtekar-on-black-hole-evaporation/" rel="bookmark">Ashtekar on Black Hole&nbsp;Evaporation</a></h2>
				<small>29 September, 2010</small><br />


				<div class="entry">
					<p>This post is a bit different from the usual fare here.  The <a href="http://relativity.phys.lsu.edu/">relativity group at Louisiana State University</a> runs an innovative series of talks, the <a href="http://relativity.phys.lsu.edu/ilqgs">International Loop Quantum Gravity Seminar</a>, where participants worldwide listen and ask questions by telephone, and the talks are made available online.  Great idea!  Why fly the speaker&#8217;s body thousands of miles through the stratosphere from point A to point B when all you really want are their precious megabytes of wisdom?  </p>
<p>This seminar is now starting up a blog, to go along with the talks.  Jorge Pullin invited me to kick it off with a post.  Following his instructions, I won&#8217;t say anything very technical.   I&#8217;ll just provide an easy intro that anyone who likes physics can enjoy.</p>
<p>&bull; Abhay Ashtekar, Quantum evaporation of 2-d black holes, 21 September 2010.  <a href="http://relativity.phys.lsu.edu/ilqgs/ashtekar092110.pdf">PDF</a> of the slides, and audio in either <a href="http://relativity.phys.lsu.edu/ilqgs/ashtekar092110.wav">.wav</a> (45MB) or <a href="http://relativity.phys.lsu.edu/ilqgs/ashtekar092110.aif">.aif</a> format (4MB).</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/ashtekar/ashtekar3.jpg" alt="" />
</div>
<p><a href="http://gravity.psu.edu/people/Ashtekar/">Abhay Ashtekar</a> has long been one of the leaders of <a href="http://relativity.livingreviews.org/Articles/lrr-2008-5/">loop quantum gravity</a>.  Einstein described gravity using a revolutionary theory called general relativity.  In the mid-1980s, Ashtekar discovered a way to reformulate the equations of general relativity in a way that brings out their similarity to the equations describing the other forces of nature.  Gravity has always been the odd man out, so this was very intriguing.  </p>
<p>Shortly thereafter, <a href="http://www.cpt.univ-mrs.fr/~rovelli/">Carlo Rovelli</a> and <a href="http://www.perimeterinstitute.ca/index.php?option=com_content&amp;task=view&amp;id=30&amp;Itemid=72&amp;pi=1010">Lee Smolin</a> used this new formulation to tackle the problem of <i>quantizing</i> gravity: that is, combining general relativity with the insights from quantum mechanics.  The result is called &#8220;loop quantum gravity&#8221; because in an early version it suggested that at tiny distance scales, the geometry of space was not smooth, but made of little knotted or intersecting loops. </p>
<p>Later work suggested a <a href="http://www.einstein-online.info/spotlights/spin_networks">network-like structure</a>, and still later <i>time</i> was brought into the game.  The whole story is still very tentative and controversial, but it&#8217;s quite a fascinating business. Maybe this movie will give you a rough idea of the images that flicker through people&#8217;s minds when they think about this stuff:</p>
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="450" height="254" src="https://www.youtube.com/embed/mOSokCXeTbw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
<p>&#8230; though personally I hear much cooler music in my head.  Bach, or Eno &mdash; not these cheesy detective show guitar licks.</p>
<p>Now, one of the goals of any theory of quantum gravity must be to resolve certain puzzles that arise in naive attempts to blend general relativity and quantum mechanics.  And one of the most famous is the so-called <a href="http://en.wikipedia.org/wiki/Black_hole_information_paradox">black hole information paradox</a>.   (I don&#8217;t think it&#8217;s actually a &#8220;paradox&#8221;, but that&#8217;s what people usually call it.)</p>
<p>The problem began when Hawking showed, by a theoretical calculation, that <a href="http://en.wikipedia.org/wiki/Hawking_radiation">black holes aren&#8217;t exactly black</a>.   In fact he showed how to compute the temperature of a black hole, and found that it&#8217;s not zero.   Anything whose temperature is above absolute zero will radiate light: visible light if it&#8217;s hot enough, infrared if it&#8217;s cooler, microwaves if it&#8217;s even cooler, and so on.   So, black holes must &#8216;glow&#8217; slightly.  </p>
<p><i>Very</i> slightly.  The black holes that astronomers have actually detected, formed by collapsing stars, would have a ridiculously low temperature: for example, about 0.00000002 degrees Kelvin for a  black hole that&#8217;s 3 times the mass of our Sun.  So, nobody has actually seen the radiation from a black hole.  </p>
<p>But Hawking&#8217;s calculations say that the smaller a black hole is, the hotter it is!  Its temperature is inversely proportional to its mass.  So, in principle, if we wait long enough, and keep stuff from falling into our black hole, it will &#8216;evaporate&#8217;.  In other words: it will gradually radiate away energy, and thus lose mass (since E = mc<sup>2</sup>), and thus get hotter, and thus radiate more energy, and so on, in a vicious feedback loop.  In the end, it will disappear in a big blast of gamma rays!</p>
<p>At least that&#8217;s what Hawking&#8217;s calculations say.  These calculations were not based on a full-fledged theory of quantum gravity, so they&#8217;re probably just <i>approximately</i> correct. This may be the way out of the &#8220;black hole information paradox&#8221;.</p>
<p>But what&#8217;s the paradox?  Patience &mdash; I&#8217;m gradually leading up to it.  First, you need to know that in all the usual physical processes we see, information is conserved.  If you&#8217;ve studied physics you&#8217;ve probably heard that various important quantities don&#8217;t change with time: they&#8217;re &#8220;conserved&#8221;.  You&#8217;ve probably heard about conservation of energy, and momentum, and angular momentum and electric charge.  But conservation of information is equally fundamental, or perhaps even more so: it says that if you know everything about what&#8217;s going on now, you can figure out everything about what&#8217;s going on later &mdash; and vice versa, too! </p>
<p>Actually, if you&#8217;ve studied physics a little but not very much, you may find my remarks puzzling. If so, don&#8217;t feel bad!  Conservation of information is not usually mentioned in the courses that introduce the other conservation laws.  The concept of information is fundamental to thermodynamics, but it appears in disguised form: &#8220;entropy&#8221;. There&#8217;s a minus sign lurking around here: while <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">information</a> is a precise measure of how much you <i>do</i> know, <a href="http://en.wikipedia.org/wiki/Entropy">entropy</a> measures how much you <i>don&#8217;t</i> know.  And to add to the confusion, the first thing they tell you about entropy is that it&#8217;s <i>not</i> conserved.  Indeed, the <a href="http://en.wikipedia.org/wiki/Second_law_of_thermodynamics">Second Law of Thermodynamics</a> says that the entropy of a closed system tends to increase!</p>
<p>But after a few years of hard thinking and heated late-night arguments with your physics pals, it starts to make sense.  Entropy as considered in thermodynamics is a measure of how much information you <i>lack</i> about a system when you only know certain things about it &mdash; things that are easily measured.  For example, if you have a box of gas, you might measure its volume and energy.  You&#8217;d still be ignorant about the details of all the molecules inside.  The amount of information you lack is the entropy of the gas.  </p>
<p>And as time passes, information tends to pass from easily measured forms to less easily measured forms, so entropy increases.   But the information is still there in principle &mdash; it&#8217;s just hard to access.  So information is conserved.</p>
<p>There&#8217;s a lot more to say here.  For example: why does information tend to pass from easily measured forms to less easily measured forms, instead of the reverse?   Does thermodynamics require a fundamental difference between future and past &mdash; a so-called <a href="http://en.wikipedia.org/wiki/Entropy_%28arrow_of_time%29">&#8220;arrow of time&#8221;</a>?  Alas, I have to sidestep this question, because I&#8217;m supposed to be telling you about the black hole information paradox.</p>
<p>So: back to black holes!  </p>
<p>Suppose you drop an encyclopedia into a black hole.  The information in the encyclopedia seems to be gone.  At the very least, it&#8217;s extremely hard to access!  So, people say the entropy has increased.  But could the information still be there in hidden form?  </p>
<p>Hawking&#8217;s original calculations suggested the answer is <i>no</i>.  Why?  Because they said that as the black hole radiates and shrinks away, the radiation it emits contains no information about the encyclopedia you threw in &mdash; or at least, no information except a few basic things like its energy, momentum, angular momentum and electric charge.  So no matter how clever you are, you can&#8217;t examine this radiation and use it to reconstruct the encyclopedia article on, say, Aardvarks.  This information is <i>lost to the world forever!</i></p>
<p>So what&#8217;s the black hole information paradox?  Well, it&#8217;s not exactly a &#8220;paradox&#8221;.  The problem is just that in every other process known to physicists, information is conserved &mdash; so it seems very unpalatable to allow any exception to this rule.  But if you try to figure out a way to <i>save</i> information conservation in the case of black holes, it&#8217;s tough.  Tough enough, in fact, to have bothered many smart physicists for decades.</p>
<p>Indeed, Stephen Hawking and the physicist <a href="http://www.theory.caltech.edu/people/preskill/">John Preskill</a> made a <a href="http://en.wikipedia.org/wiki/Thorne%E2%80%93Hawking%E2%80%93Preskill_bet">famous bet</a> about this puzzle in 1997.  Hawking bet that information wasn&#8217;t conserved; Preskill bet it was.  In fact, they bet an encyclopedia!</p>
<div align="center">
<img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/dublin/preskill.jpg" />
</div>
<p>In 2004 Hawking conceded the bet to Preskill, as shown above.  It happened a conference in Dublin &mdash; I was there and <a href="http://math.ucr.edu/home/baez/week207.html">blogged about it</a>.  Hawking conceded because he did some new calculations suggesting that information <i>can</i> gradually leak out of the black hole, thanks to the radiation.   In other words: if you throw an encyclopedia in a black hole, a sufficiently clever physicist <i>can indeed</i> reconstruct the article on Aardvarks by carefully examining the radiation from the black hole.  It would be incredibly hard, since the information would be highly scrambled.  But it could be done in principle.</p>
<p>Unfortunately, Hawking&#8217;s calculation is very hand-wavy at certain crucial steps &mdash; in fact, more hand-wavy than certain calculations that had already been done with the help of string theory (or more precisely, the <a href="http://en.wikipedia.org/wiki/AdS/CFT_correspondence">AdS-CFT conjecture</a>).   And, neither approach makes it easy to see in detail <i>how</i> the information comes out in the radiation.  </p>
<p>This finally brings us to Ashtekar&#8217;s talk.  Despite what you might guess from my warmup, his talk was <i>not</i> about loop quantum gravity.  Certainly everyone working on loop quantum gravity would love to see this theory resolve the black hole information paradox.  I&#8217;m sure Ashtekar is aiming in that direction.  But his talk was about a warmup problem, a &#8220;toy model&#8221; involving black holes in 2d spacetime instead of our real-world 4-dimensional spacetime.  </p>
<p>The advantage of 2d spacetime is that the math becomes a lot easier there.  There&#8217;s been a lot of work on black holes in 2d spacetime, and Ashtekar is presenting some new work on an existing model, the Callen-Giddings-Harvey-Strominger black hole.  This new work is a mixture of analytical and numerical calculations done over the last 2 years by Ashtekar together with <a href="http://physics.princeton.edu/~fpretori/">Frans Pretorius</a>, <a href="http://arxiv.org/find/all/1/all:+AND+Fethi+Ramazanoglu/0/1/0/all/0/1">Fethi Ramazanoglu</a>, <a href="http://arxiv.org/find/all/1/all:+AND+victor+taveras/0/1/0/all/0/1">Victor Taveras</a> and <a href="http://arxiv.org/find/all/1/all:+AND+Madhavan+Varadarajan/0/1/0/all/0/1">Madhavan Varadarajan</a>.</p>
<p>I will not attempt to explain this work in detail!  The main point is this: <b>all the information that goes into the black hole leaks back out in the form of radiation as the black hole evaporates</b>.  </p>
<p>But the talks also covers many other interesting issues.  For example, the final stages of black hole evaporation display interesting properties that are <i>independent of the details of its initial state</i>.  Physicists call this sort of phenomenon &#8220;universality&#8221;.</p>
<p>Furthermore, when the black hole finally shrinks to nothing, it sends out a pulse of gravitational radiation, but <i>not enough to destroy the universe</i>.  It may seem very peculiar to imagine that the death spasms of a black hole <i>could</i> destroy the universe, but in fact some approximate &#8220;semiclassical&#8221; calculations of Hawking and Stewart suggested just that!  They found that in 2d spacetime, the dying black hole emitted a pulse of infinite spacetime curvature &mdash; dubbed a &#8220;thunderbolt&#8221; &mdash; which made it impossible to continue spacetime beyond that point.  But they suggested that a more precise calculation, taking quantum gravity fully into account, would eliminate this effect.  And this seems to be the case.</p>
<p>For more, listen to Ashtekar&#8217;s talk while looking at the PDF file of his slides!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/09/29/ashtekar-on-black-hole-evaporation/#comments">35 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/09/29/ashtekar-on-black-hole-evaporation/" rel="bookmark" title="Permanent Link to Ashtekar on Black Hole&nbsp;Evaporation">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/page/8/?s=information+and+entropy" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/page/9/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jksOwjAMRC9EarWLSl0gzpImVuXQfLAd2t6eVAIJsWA1nrGePbAV43JSTApBwOOTHJa9C3KBr1Wspqx1oSRgfaRkZssQrShym4yydXf5hdq98KjIx1u6rbgcTeG8H4axZaIfhpJbq0c5oWYxzui79uhPkY38giogdRbHVJRyOjvc4rUfh2nqh2Hswwtq4FN0'></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZVaU01aEU1L0xnSXBRYkdYP2syVFJfblNQL0pKUHJxVEdLflpYZlR8d0ZWL2dTd0lPZ0JNcy5QX1JhdW1ZdkNRQVRMPTdFOHBRZVFFTXZ3TnImLXpafFJ5XUZHS2MsQkh6PW9qdGdyRnljelFoYy1hM1BTP2k0MVc9d1tZaTl6NUJYcEk3fi9SZVVOTDZMdkQyLUU2WXx6Qjh6bzFDY1FZblB1W3FGcUFpUWg/bXE3ZWQ9LGMuckZXdDFGP05OYnVBeSs0'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>