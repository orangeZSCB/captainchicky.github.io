<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth | Page 4</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2013%2F02%2F21%2Fmaximum-entropy-and-ecology%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/page\/4\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fpage%2F4%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2013%2F02%2F21%2Fmaximum-entropy-and-ecology%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Page 4 &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results paged paged-4 search-paged-4 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-14821 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy" id="post-14821">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/02/21/maximum-entropy-and-ecology/" rel="bookmark">Maximum Entropy and&nbsp;Ecology</a></h2>
				<small>21 February, 2013</small><br />


				<div class="entry">
					<p>I already talked about <a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/">John Harte&#8217;s book on how to stop global warming</a>.   Since I&#8217;m trying to apply information theory and thermodynamics to ecology, I was also interested in this book of his:</p>
<p>&bull; <a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a>, <i>Maximum Entropy and Ecology</i>, Oxford U. Press, Oxford, 2011.</p>
<p>There&#8217;s a lot in this book, and I haven&#8217;t absorbed it all, but let me try to briefly summarize his <b>maximum entropy theory of ecology</b>. This aims to be &#8220;a comprehensive, parsimonious, and testable theory of the distribution, abundance, and energetics of species across spatial scales&#8221;.  One great thing is that he makes quantitative predictions using this theory and compares them to a lot of real-world data.  But let me just tell you about the theory.  </p>
<p>It&#8217;s heavily based on the <a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy">principle of maximum entropy</a> (MaxEnt for short), and there are two parts:</p>
<blockquote><p>
Two MaxEnt calculations are at the core of the theory: the first yields all the metrics that describe abundance and energy distributions, and the second describes the spatial scaling properties of species&#8217; distributions.
</p></blockquote>
<h3> Abundance and energy distributions </h3>
<p>The first part of Harte&#8217;s theory is all about a conditional probability distribution </p>
<p><img src="https://s0.wp.com/latex.php?latex=R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(n,&#92;epsilon | S_0, N_0, E_0) " class="latex" /></p>
<p>which he calls the <b>ecosystem structure function</b>.  Here:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=S_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_0" class="latex" />: the total number of species under consideration in some area.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=N_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N_0" class="latex" />: the total number of individuals under consideration in that area.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_0" class="latex" />: the total rate of metabolic energy consumption of all these individuals.</p>
<p>Given this, </p>
<p><img src="https://s0.wp.com/latex.php?latex=R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%5C%2C+d+%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(n,&#92;epsilon | S_0, N_0, E_0) &#92;, d &#92;epsilon " class="latex" /></p>
<p>is the probability that given <img src="https://s0.wp.com/latex.php?latex=S_0%2C+N_0%2C+E_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_0, N_0, E_0," class="latex" /> if a species is picked from the collection of species, then it has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> individuals, and if an individual is picked at random from that species, then its rate of metabolic energy consumption is in the interval <img src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C+%5Cepsilon+%2B+d+%5Cepsilon%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;epsilon, &#92;epsilon + d &#92;epsilon)." class="latex" /></p>
<p>Here of course <img src="https://s0.wp.com/latex.php?latex=d+%5Cepsilon&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;epsilon" class="latex" /> is &#8216;infinitesimal&#8217;, meaning that we take a limit where it goes to zero to make this idea precise (if we&#8217;re doing analytical work) or take it to be very small (if we&#8217;re estimating <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R" class="latex" /> from data).  </p>
<p>I believe that when we &#8216;pick a species&#8217; we&#8217;re treating them all as equally probable, not weighting them according to their number of individuals.  </p>
<p>Clearly <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R" class="latex" /> obeys some constraints.  First, since it&#8217;s a probability distribution, it obeys the normalization condition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n &#92;int d &#92;epsilon &#92;; R(n,&#92;epsilon | S_0, N_0, E_0) = 1 }" class="latex" /></p>
<p>Second, since the average number of individuals per species is <img src="https://s0.wp.com/latex.php?latex=N_0%2FS_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N_0/S_0," class="latex" /> we have:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+n+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%3D+N_0+%2F+S_0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n &#92;int d &#92;epsilon &#92;; n R(n,&#92;epsilon | S_0, N_0, E_0) = N_0 / S_0 }" class="latex" /></p>
<p>Third, since the average over species of the total rate of metabolic energy consumption of individuals within the species is <img src="https://s0.wp.com/latex.php?latex=E_0%2F+S_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_0/ S_0," class="latex" /> we have:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+n+%5Cepsilon+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%3D+E_0+%2F+S_0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n &#92;int d &#92;epsilon &#92;; n &#92;epsilon R(n,&#92;epsilon | S_0, N_0, E_0) = E_0 / S_0 }" class="latex" /></p>
<p>Harte&#8217;s theory is that <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R" class="latex" /> <i>maximizes entropy subject to these three constraints</i>.  Here entropy is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%5Cln%28R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;sum_n &#92;int d &#92;epsilon &#92;; R(n,&#92;epsilon | S_0, N_0, E_0) &#92;ln(R(n,&#92;epsilon | S_0, N_0, E_0)) } " class="latex" /></p>
<p>Harte uses this theory to calculate <img src="https://s0.wp.com/latex.php?latex=R%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R," class="latex" /> and tests the results against data from about 20 ecosystems.  For example, he predicts the abundance of species as a function of their rank, with rank 1 being the most abundant, rank 2 being the second most abundant, and so on.  And he gets results like this:</p>
<div align="center">
<a href="http://math.ucr.edu/home/baez/ecological/harte_species_abundance_distribution.jpg"><img src="https://i2.wp.com/math.ucr.edu/home/baez/ecological/harte_species_abundance_distribution.jpg" /></a></div>
<p>The data here are from:</p>
<p>&bull; Green, Harte, and Ostling&#8217;s work on a serpentine grassland, </p>
<p>&bull; Luquillo&#8217;s work on a 10.24-hectare tropical forest, and</p>
<p>&bull; Cocoli&#8217;s work on a 2-hectare wet tropical forest.</p>
<p>The fit looks good to me&#8230; but I should emphasize that I haven&#8217;t had time to study these matters in detail.  For more, you can read this paper, at least if your institution subscribes to this journal:</p>
<p>&bull; J. Harte, T. Zillio, E. Conlisk and A. Smith, Maximum entropy and the state-variable approach to macroecology, <a href="http://www.esajournals.org/doi/full/10.1890/07-1369.1"><i>Ecology</i></a> <b>89</b> (2008), 2700&ndash;2711.</p>
<h3> Spatial abundance distribution </h3>
<p>The second part of Harte&#8217;s theory is all about a conditional probability distribution </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Pi(n | A, n_0, A_0) " class="latex" /></p>
<p>This is the probability that <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> individuals of a species are found in a region of area <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> given that it has <img src="https://s0.wp.com/latex.php?latex=n_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_0" class="latex" /> individuals in a larger region of area <img src="https://s0.wp.com/latex.php?latex=A_0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_0." class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Pi" class="latex" /> obeys two constraints.  First, since it&#8217;s a probability distribution, it obeys the normalization condition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n++%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n  &#92;Pi(n | A, n_0, A_0) = 1 }" class="latex" /></p>
<p>Second, since the mean value of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> across regions of area <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> equals <img src="https://s0.wp.com/latex.php?latex=n_0+A%2FA_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_0 A/A_0," class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+n+%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29+%3D+n_0+A%2FA_0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n n &#92;Pi(n | A, n_0, A_0) = n_0 A/A_0 }" class="latex" /></p>
<p>Harte&#8217;s theory is that <img src="https://s0.wp.com/latex.php?latex=%5CPi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Pi" class="latex" /> <i>maximizes entropy subject to these two constraints</i>.   Here entropy is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B-+%5Csum_n++%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29%5Cln%28%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{- &#92;sum_n  &#92;Pi(n | A, n_0, A_0)&#92;ln(&#92;Pi(n | A, n_0, A_0)) } " class="latex" /></p>
<p>Harte explains two approaches to use this idea to derive &#8216;scaling laws&#8217; for how <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> varies with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />.   And again, he compares his predictions to real-world data, and get results that look good to my (amateur, hasty) eye!</p>
<p>I hope sometime I can dig deeper into this subject.  Do you have any ideas, or knowledge about this stuff?</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/02/21/maximum-entropy-and-ecology/#comments">27 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/02/21/maximum-entropy-and-ecology/" rel="bookmark" title="Permanent Link to Maximum Entropy and&nbsp;Ecology">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10254 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics" id="post-10254">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/" rel="bookmark">Information Geometry (Part&nbsp;12)</a></h2>
				<small>24 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/">Last time</a> we saw that if a population evolves toward an &#8216;evolutionarily stable state&#8217;, then the amount of information our population has &#8216;left to learn&#8217; can never increase!  It must always decrease or stay the same.</p>
<p>This result sounds wonderful: it&#8217;s a lot like the second law of thermodynamics, which says entropy must always increase.  Of course there are some conditions for this wonderful result to hold.  The main condition is that the population evolves according to the <a href="http://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a>.  But the other is the existence of an evolutionarily stable state.   Last time I wrote down the rather odd-looking definition of &#8216;evolutionary stable state&#8217; without justifying it.   I need to do that soon.  But if you&#8217;ve never thought about evolutionary game theory, I think giving you a little background will help.  So today let me try that.</p>
<h3> Evolutionary game theory </h3>
<p>We&#8217;ve been thinking of evolution as similar to <i>inference</i> or <i>learning</i>.  In this analogy, organisms are like &#8216;hypotheses&#8217;, and the population &#8216;does experiments&#8217; to see if these hypotheses make &#8216;correct predictions&#8217; (i.e., can reproduce) or not.  The successful ones are reinforced while the unsuccessful ones are weeded out.  As a result, the population &#8216;learns&#8217;.  And under the conditions of the theorem we discussed last time, the relative information&#8212;the amount &#8216;left to learn&#8217;&#8212;goes down!</p>
<p>While you might object to various points of this analogy, it&#8217;s  useful&#8212;and that&#8217;s really all you can ask of an analogy.  It&#8217;s useful because it lets us steal chunks of math from the subjects of <a href="http://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a> and <a href="http://en.wikipedia.org/wiki/Machine_learning">machine learning</a> and apply them to the study of biodiversity and evolution!  This is what Marc Harper has been doing:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.  </p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>But now let&#8217;s bring in another analogy, also contained in Harper&#8217;s work.  We can also think of evolution as similar to a <i>game</i>.  In this analogy, organisms are like &#8216;strategies&#8217;&#8212;or if you prefer, they have strategies.  The winners get to reproduce, while the losers don&#8217;t.  <a href="http://en.wikipedia.org/wiki/John_Maynard_Smith">John Maynard Smith</a> started developing this analogy in 1973, and eventually wrote a whole book on it:</p>
<p>&bull; John Maynard Smith, <i>Evolution and the Theory of Games</i>, Cambridge University Press, 1982.</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/John_Maynard_Smith"><img src="https://i0.wp.com/math.ucr.edu/home/baez/john_maynard_smith.jpg" alt="" /></a></div>
<p>As far as I can tell, evolutionary game theory has brought almost as many chunks of math <i>to</i> game theory as it has taken from it.  Maybe it&#8217;s just my ignorance showing, but it seems that game theory becomes considerably deeper when we think about games that many players play again and again, with the winners getting to reproduce, while the losers are eliminated.  </p>
<p>According to <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">William Sandholm</a>:</p>
<blockquote><p>
The birth of evolutionary game theory is marked by the publication of a series of papers by mathematical biologist John Maynard Smith. Maynard Smith adapted the methods of traditional game theory, which were created to model the behavior of rational economic agents, to the context of biological natural selection.  He proposed his notion of an evolutionarily stable strategy (ESS) as a way of explaining the existence of ritualized animal conflict.</p>
<p>Maynard Smithâ€™s equilibrium concept was provided with an explicit dynamic foundation through a differential equation model introduced by Taylor and Jonker. Schuster and Sigmund, following Dawkins, dubbed this model the replicator dynamic, and recognized the close links between this game-theoretic dynamic and dynamics studied much earlier in population ecology and population genetics. By the 1980s, evolutionary game theory was a well-developed and firmly established modeling framework in biology.</p>
<p>Towards the end of this period, economists realized the value of the evolutionary approach to game theory in social science contexts, both as a method of providing foundations for the equilibrium concepts of traditional game theory, and as a tool for selecting among equilibria in games that admit more than one. Especially in its early stages, work by economists in evolutionary game theory hewed closely to the interpretation set out by biologists, with the notion of ESS and the replicator dynamic understood as modeling natural selection in populations of agents genetically programmed to behave in specific ways. But it soon became clear that models of essentially the same form could be used to study the behavior of populations of active decision makers.  Indeed, the two approaches sometimes lead to identical models: the replicator dynamic itself can be understood not only as a model of natural selection, but also as one of imitation of successful opponents.  </p>
<p>While the majority of work in evolutionary game theory has been undertaken by biologists and economists, closely related models have been applied to questions in a variety of fields, including transportation science, computer science, and sociology. Some paradigms from evolutionary game theory are close relatives of certain models from physics, and so have attracted the attention of workers in this field. All told, evolutionary game theory provides a common ground for workers from a wide range of disciplines.
</p></blockquote>
<h3> The Prisoner&#8217;s Dilemma </h3>
<p>In game theory, the most famous example is the <a href="http://en.wikipedia.org/wiki/Prisoner%27s_dilemma">Prisoner&#8217;s Dilemma</a>.  In its original form, this &#8216;game&#8217; is played just once:</p>
<blockquote><p>
Two men are arrested, but the police don&#8217;t have enough information to convict them.  So they separate the two men, and offer both the same deal: if one testifies against his partner (or <b>defects</b>), and the other remains silent (and thus <b>cooperates</b> with his partner), the defector goes free and the cooperator goes to jail for 12 months. If both remain silent, both are sentenced to only 1 month in jail for a minor charge. If they both defect, they both receive a 3-month sentence. Each prisoner must choose either to defect or cooperate with his partner in crime; neither gets to hear what the other decides.  What will they do?
</p></blockquote>
<p>Traditional game theory emphasizes the so-called &#8216;Nash equilibrium&#8217; for this game, in which both prisoners defect.  Why don&#8217;t they both cooperate?   They&#8217;d both be better off if they both cooperated.  However, for them to both cooperate is &#8216;unstable&#8217;: either one could shorten their sentence by defecting!  By definition, a <a href="http://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a> has the property that neither player can improve his situation by unilaterally changing his strategy.  </p>
<p>In the Prisoner&#8217;s Dilemma, the Nash equilibrium is not very nice: both parties would be happier if they&#8217;d only cooperate.  That&#8217;s why it&#8217;s called a &#8216;dilemma&#8217;.  Perhaps the most tragic example today is global warming.  Even if all players would be better off if all cooperate to reduce carbon emissions, any <i>one</i> will be better off if everybody <i>except themselves</i> cooperates while they emit more carbon.</p>
<p>For this and many other reasons, people have been interested in &#8216;solving&#8217; the Prisoner&#8217;s Dilemma: that is, finding reasons why cooperation might be favored over defection.  </p>
<p>This book got people really excited in seeing what evolutionary game theory has to say about the Prisoner&#8217;s Dilemma:</p>
<p>&bull; Robert Axelrod, <i>The Evolution of Cooperation</i>, Basic Books, New York, 1984.  (A related article with the same title is <a href="www-personal.umich.edu/~axe/research/Axelrod%20and%20Hamilton%20EC%201981.pdf">available online</a>.)</p>
<p>The idea is that under certain circumstances, strategies that are &#8216;nicer&#8217; than defection will gradually take over.  The most famous of these strategies is &#8216;tit for tat&#8217;, meaning that you cooperate the first time and after that do whatever your opponent just did.  I won&#8217;t go into this further, because it&#8217;s a big digression and I&#8217;m already digressing too far.  I&#8217;ll just mention that from the outlook of evolutionary game theory, the Prisoner&#8217;s Dilemma is still full of surprises.  Just this week, some fascinating new work has been causing a stir:</p>
<p>&bull; William Press and Freeman Dyson, <a href="http://edge.org/conversation/on-iterated-prisoner-dilemma">Iterated Prisoner&#8217;s Dilemma contains strategies that dominate any evolutionary opponent</a>, <i>Edge</i>, 18 June 2012.</p>
<p>I hope I&#8217;ve succeeded in giving you a vague superficial sense of the history of evolutionary game theory and why it&#8217;s interesting.  Next time I&#8217;ll get serious about the task at hand, which is to understand &#8216;evolutionarily stable strategies&#8217;.   If you want to peek ahead, try this nice paper:</p>
<p>&bull; William H. Sandholm, <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">Evolutionary game theory</a>, 12 November 2007.</p>
<p>This is where I got the long quote by Sandholm on the history of evolutionary game theory.  The original quote contained lots of references; if you&#8217;re interested in those, go to page 3 of this paper.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/#comments">10 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;12)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10114 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-probability" id="post-10114">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/" rel="bookmark">Information Geometry (Part&nbsp;11)</a></h2>
				<small>7 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/">Last time</a> we saw that given a bunch of different species of self-replicating entities, the entropy of their population distribution can go either up or down as time passes.  This is true even in the pathetically simple case where all the replicators have constant fitness&mdash;so they don&#8217;t interact with each other, and don&#8217;t run into any &#8216;limits to growth&#8217;.</p>
<p>This is a bit of a bummer, since it would be nice to use entropy to explain how replicators are always extracting information from their environment, thanks to natural selection.</p>
<p>Luckily, a slight variant of entropy, called &#8216;relative entropy&#8217;, behaves better.  When our replicators have an &#8216;evolutionary stable state&#8217;, the relative entropy is <i>guaranteed to always change in the same direction</i> as time passes!</p>
<p>Thanks to Einstein, we&#8217;ve all heard that times and distances are relative.  But how is entropy relative?</p>
<p>It&#8217;s easy to understand if you think of entropy as lack of information.  Say I have a coin hidden under my hand.  I tell you it&#8217;s heads-up.  How much information did I just give you?  Maybe 1 bit?  That&#8217;s true if you know it&#8217;s a fair coin and I flipped it fairly before covering it up with my hand.  But what if you put the coin down there yourself a minute ago, heads up, and I just put my hand over it?  Then I&#8217;ve given you no information at all.  The difference is the choice of &#8216;prior&#8217;: that is, what probability distribution you attributed to the coin <i>before</i> I gave you my message.</p>
<p>My love affair with relative entropy began in college when my friend Bruce Smith and I read Hugh Everett&#8217;s thesis, <a href="http://www.pbs.org/wgbh/nova/manyworlds/pdf/dissertation.pdf"><i>The Relative State Formulation of Quantum Mechanics</i></a>.  This was the origin of what&#8217;s now often called the &#8216;many-worlds interpretation&#8217; of quantum mechanics.  But it also has a great introduction to relative entropy.  Instead of talking about &#8216;many worlds&#8217;, I wish people would say that Everett explained some of the mysteries of quantum mechanics using the fact that entropy is relative.</p>
<p>Anyway, it&#8217;s nice to see relative entropy showing up in biology.</p>
<h3> Relative Entropy </h3>
<div align="center">
<img src="http://math.ucr.edu/home/baez/mathematical/bertrand's_paradox.png" />
</div>
<p>Inscribe an equilateral triangle in a circle.  Randomly choose a line segment joining two points of this circle.  What is the probability that this segment is longer than a side of the triangle?</p>
<p>This puzzle is called <a href="http://en.wikipedia.org/wiki/Bertrand_paradox_%28probability%29">Bertrand&#8217;s paradox</a>, because different ways of solving it give different answers.   To crack the paradox, you need to realize that it&#8217;s meaningless to say you&#8217;ll &#8220;randomly&#8221; choose something until you say more about how you&#8217;re going to do it.</p>
<p>In other words, you can&#8217;t compute the probability of an event until you pick a recipe for computing probabilities.  Such a recipe is called a <a href="http://en.wikipedia.org/wiki/Probability_measure">probability measure</a>.</p>
<p>This applies to computing entropy, too!   The formula for entropy clearly involves a <a href="http://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a>, even when our set of events is finite:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Csum_i+p_i+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;sum_i p_i &#92;ln(p_i) " class="latex" /></p>
<p>But this formula conceals a fact that becomes obvious when our set of events is infinite.  Now the sum becomes an integral:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Cint_X+p%28x%29+%5Cln%28p%28x%29%29+%5C%2C+d+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;int_X p(x) &#92;ln(p(x)) &#92;, d x" class="latex" /></p>
<p>And now it&#8217;s clear that this formula makes no sense until we choose the <a href="http://en.wikipedia.org/wiki/Measure_%28mathematics%29">measure</a> <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />   On a finite set we have a god-given choice of measure, called <a href="http://en.wikipedia.org/wiki/Counting_measure">counting measure</a>.  Integrals with respect to this are just sums.   But in general we don&#8217;t have such a god-given choice.  And even for finite sets, working with counting measure is a <i>choice</i>: we are <i>choosing</i> to believe that in the absence of further evidence, all options are equally likely.</p>
<p>Taking this fact into account, it seems like we need two things to compute entropy: a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x)" class="latex" />, and a measure <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />   That&#8217;s on the right track.  But an even better way to think of it is this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Cint_X++%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D+%5Cln+%5Cleft%28%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D%5Cright%29+%5C%2C+dx+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;int_X  &#92;frac{p(x) dx}{dx} &#92;ln &#92;left(&#92;frac{p(x) dx}{dx}&#92;right) &#92;, dx }" class="latex" /></p>
<p>Now we see the entropy depends <i>two</i> measures: the probability measure <img src="https://s0.wp.com/latex.php?latex=p%28x%29++dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x)  dx " class="latex" /> we care about, but also the measure <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />  Their ratio is important, but that&#8217;s not enough: we also need one of these measures to do the integral.  Above I used the measure <img src="https://s0.wp.com/latex.php?latex=dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dx" class="latex" /> to do the integral, but we can also use <img src="https://s0.wp.com/latex.php?latex=p%28x%29+dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x) dx" class="latex" /> if we write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Cint_X+%5Cln+%5Cleft%28%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D%5Cright%29+p%28x%29+dx+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;int_X &#92;ln &#92;left(&#92;frac{p(x) dx}{dx}&#92;right) p(x) dx } " class="latex" /></p>
<p>Either way, we are computing the entropy of one measure <i>relative to another</i>.  So we might as well admit it, and talk about <b>relative entropy</b>.</p>
<p>The entropy of the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu" class="latex" /> <b>relative to</b> the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;nu" class="latex" /> is defined by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+S%28d+%5Cmu%2C+d+%5Cnu%29+%26%3D%26+%5Cdisplaystyle%7B+-+%5Cint_X+%5Cfrac%7Bd+%5Cmu%28x%29+%7D%7Bd+%5Cnu%28x%29%7D+%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29++d%5Cnu%28x%29+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cint_X++%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29+d%5Cmu%28x%29+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} S(d &#92;mu, d &#92;nu) &amp;=&amp; &#92;displaystyle{ - &#92;int_X &#92;frac{d &#92;mu(x) }{d &#92;nu(x)} &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right)  d&#92;nu(x) } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;int_X  &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right) d&#92;mu(x) } &#92;end{array} " class="latex" /></p>
<p>The second formula is simpler, but the first looks more like summing <img src="https://s0.wp.com/latex.php?latex=-p+%5Cln%28p%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-p &#92;ln(p)," class="latex" /> so they&#8217;re both useful.</p>
<p>Since we&#8217;re taking entropy to be lack of information, we can also get rid of the minus sign and define <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"><b>relative information</b></a> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+I%28d+%5Cmu%2C+d+%5Cnu%29+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_X+%5Cfrac%7Bd+%5Cmu%28x%29+%7D%7Bd+%5Cnu%28x%29%7D+%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29++d%5Cnu%28x%29+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B++%5Cint_X++%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29+d%5Cmu%28x%29+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} I(d &#92;mu, d &#92;nu) &amp;=&amp; &#92;displaystyle{ &#92;int_X &#92;frac{d &#92;mu(x) }{d &#92;nu(x)} &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right)  d&#92;nu(x) } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{  &#92;int_X  &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right) d&#92;mu(x) } &#92;end{array} " class="latex" /></p>
<p>If you thought something was randomly distributed according to the probability measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cnu%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;nu," class="latex" /> but then you you discover it&#8217;s randomly distributed according to the probability measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu," class="latex" /> how much information have you gained?  The answer is <img src="https://s0.wp.com/latex.php?latex=I%28d%5Cmu%2Cd%5Cnu%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(d&#92;mu,d&#92;nu)." class="latex" /></p>
<p>For more on relative entropy, read <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">Part 6</a> of this series.  I gave some examples illustrating how it works.  Those should convince you that it&#8217;s a useful concept.</p>
<p>Okay: now let&#8217;s switch back to a more lowbrow approach.  In the case of a finite set, we can revert to thinking of our two measures as probability distributions, and write the information gain as</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Cdisplaystyle%7B++%5Csum_i++%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i+%7D%5Cright%29+q_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;displaystyle{  &#92;sum_i  &#92;ln &#92;left(&#92;frac{q_i}{p_i }&#92;right) q_i} " class="latex" /></p>
<p>If you want to sound like a Bayesian, call <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Prior_probability">prior probability distribution</a> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution">posterior probability distribution</a>.  Whatever you call them, <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> is the amount of information you get if you thought <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and someone tells you &#8220;no, <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />!â€</p>
<p>We&#8217;ll use this idea to think about how a population gains information about its environment as time goes by, thanks to natural selection.  The rest of this post will be an exposition of Theorem 1 in this paper:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>Harper says versions of this theorem ave previously appeared in work by Ethan Akin, and independently in work by Josef Hofbauer and Karl Sigmund.  He also credits others <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15724">here</a>.  An idea this good is rarely noticed by just one person.</p>
<h3> The change in relative information </h3>
<p>So: consider <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different species of replicators.   Let <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> be the population of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, and assume these populations change according to the <b><a href="http://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) &#92;, P_i } " class="latex" /></p>
<p>where each function <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> depends smoothly on all the populations.  And as usual, we let</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_j+P_j%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_j P_j} } " class="latex" /></p>
<p>be the fraction of replicators in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.</p>
<p>Let&#8217;s study the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is some fixed probability distribution.   We&#8217;ll see something great happens when <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a stable equilibrium solution of the replicator equation.  In this case, the relative information can never increase!  It can only decrease or stay constant.</p>
<p>We&#8217;ll think about what all this <i>means</i> later.  First, let&#8217;s see that it&#8217;s true!  Remember,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+I%28q%2Cp%29+%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i++%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7B+p_i+%7D%5Cright%29+q_i+%7D++%5C%5C+%5C%5C+%26%3D%26++%5Cdisplaystyle%7B+%5Csum_i++%5CBig%28%5Cln%28q_i%29+-+%5Cln%28p_i%29+%5CBig%29+q_i+%7D+%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} I(q,p) &amp;=&amp; &#92;displaystyle{ &#92;sum_i  &#92;ln &#92;left(&#92;frac{q_i}{ p_i }&#92;right) q_i }  &#92;&#92; &#92;&#92; &amp;=&amp;  &#92;displaystyle{ &#92;sum_i  &#92;Big(&#92;ln(q_i) - &#92;ln(p_i) &#92;Big) q_i } &#92;end{array}" class="latex" /></p>
<p>and only <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> depends on time, not <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" />, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29%7D++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cfrac%7Bd%7D%7Bdt%7D+%5Csum_i+%5Cln%28p_i%29++q_i+%7D%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Csum_i+%5Cfrac%7B%5Cdot%7Bp%7D_i%7D%7Bp_i%7D+%5C%2C+q_i+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;frac{d}{dt} I(q,p)}  &amp;=&amp; &#92;displaystyle{ - &#92;frac{d}{dt} &#92;sum_i &#92;ln(p_i)  q_i }&#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;sum_i &#92;frac{&#92;dot{p}_i}{p_i} &#92;, q_i } &#92;end{array} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i" class="latex" /> is the rate of change of the probability <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /> We saw a nice formula for this in <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/">Part 9</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, p_i }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_i%28P%29+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P) = f_i(P_1, &#92;dots, P_n) " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_i+f_i%28P%29+p_i++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_i f_i(P) p_i  } " class="latex" /></p>
<p>is the <b>mean fitness</b> of the species.  So, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%7D+%3D+%5Cdisplaystyle%7B+-+%5Csum_i+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+q_i+%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) } = &#92;displaystyle{ - &#92;sum_i &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, q_i }  " class="latex" /></p>
<p>Nice, but we can fiddle with this expression to get something more enlightening.  Remember, the numbers <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> sum to one.  So:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%7D+%26%3D%26++%5Cdisplaystyle%7B++%5Clangle+f%28P%29+%5Crangle+-+%5Csum_i+f_i%28P%29+q_i++%7D+%5C%5C++%5C%5C+%26%3D%26+%5Cdisplaystyle%7B++%5Csum_i+f_i%28P%29+%28p_i+-+q_i%29++%7D++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;frac{d}{dt} I(q,p) } &amp;=&amp;  &#92;displaystyle{  &#92;langle f(P) &#92;rangle - &#92;sum_i f_i(P) q_i  } &#92;&#92;  &#92;&#92; &amp;=&amp; &#92;displaystyle{  &#92;sum_i f_i(P) (p_i - q_i)  }  &#92;end{array} " class="latex" /></p>
<p>where in the last step I used the definition of the mean fitness.  This result looks even cuter if we treat the numbers <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)" class="latex" /> as the components of a vector <img src="https://s0.wp.com/latex.php?latex=f%28P%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)," class="latex" /> and similarly for the numbers <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />  Then we can use the dot product of vectors to say</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%3D+f%28P%29+%5Ccdot+%28p+-+q%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) = f(P) &#92;cdot (p - q) }" class="latex" /></p>
<p>So, the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> will always decrease if</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+%5Ccdot+%28p+-+q%29+%5Cle+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) &#92;cdot (p - q) &#92;le 0" class="latex" /></p>
<p>for all choices of the population <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" /></p>
<p>And now something really nice happens: this is also the condition for <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to be an <a href="http://en.wikipedia.org/wiki/Evolutionarily_stable_state">evolutionarily stable state</a>.  This concept goes back to <a href="http://en.wikipedia.org/wiki/John_Maynard_Smith">John Maynard Smith</a>, the founder of evolutionary game theory.  In 1982 he wrote:</p>
<blockquote><p>
A population is said to be in an evolutionarily stable state if its genetic composition is restored by selection after a disturbance, provided the disturbance is not too large.
</p></blockquote>
<p>I will explain the math next time&#8212;I need to straighten out some things in my mind first.  But the basic idea is compelling: an evolutionarily stable state is like a situation where our replicators &#8216;know all there is to know&#8217; about the environment and each other.  In any other state, the population has &#8216;something left to learn&#8217;&#8212;and the amount left to learn is the relative information we&#8217;ve been talking about!  But as time goes on, the information still left to learn <i>decreases!</i></p>
<p>Note: in the real world, nature has never found an evolutionarily stable state&#8230; except sometimes approximately, on sufficiently short time scales, in sufficiently small regions.   So we are still talking about an idealization of reality!   But that&#8217;s okay, as long as we know it.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/#comments">58 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;11)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-9984 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-probability" id="post-9984">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/" rel="bookmark">Information Geometry (Part&nbsp;10)</a></h2>
				<small>4 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/">Last time</a> I began explaining the tight relation between three concepts:</p>
<p>&bull; entropy, </p>
<p>&bull; information&mdash;or more precisely, lack of information, </p>
<p>and</p>
<p>&bull; biodiversity.</p>
<p>The idea is to consider <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different species of &#8216;replicators&#8217;.  A replicator is any entity that can reproduce itself, like an organism, a gene, or a meme.  A replicator can come in different kinds, and a &#8216;species&#8217; is just our name for one of these kinds.  If <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> is the population of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, we can interpret the fraction</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_j+P_j%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_j P_j} } " class="latex" /></p>
<p>as a probability: the probability that a randomly chosen replicator belongs to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  This suggests that we define <i><a href="http://en.wikipedia.org/wiki/Entropy_%28statistical_thermodynamics%29">entropy</a></i> just as we do in statistical mechanics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Csum_i+p_i+%5Cln%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;sum_i p_i &#92;ln(p_i) } " class="latex" /></p>
<p>In the study of statistical inference, entropy is a measure of uncertainty, or lack of <i><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">information</a></i>.  But now we can interpret it as a measure of <i><a href="http://www.loujost.com/Statistics%20and%20Physics/Diversity%20and%20Similarity/JostEntropy%20AndDiversity.pdf">biodiversity</a></i>: it&#8217;s zero when just one species is present, and small when a few species have much larger populations than all the rest, but gets big otherwise.  </p>
<p>Our goal here is play these viewpoints off against each other.  In short, we want to think of natural selection, and even biological evolution, as a process of statistical inference&mdash;or in simple terms, <i>learning</i>.  </p>
<p>To do this, let&#8217;s think about how entropy changes with time.  Last time we introduced a simple model called the <b><a href="http://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) &#92;, P_i } " class="latex" /></p>
<p>where each population grows at a rate proportional to some &#8216;fitness functions&#8217; <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" />.  We can get some intuition by looking at the pathetically simple case where these functions are actually <i>constants</i>, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i &#92;, P_i } " class="latex" /></p>
<p>The equation then becomes trivial to solve:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+P_i%28t%29+%3D+e%5E%7Bt+f_i+%7D+P_i%280%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ P_i(t) = e^{t f_i } P_i(0)} " class="latex" /></p>
<p>Last time I showed that in this case, the entropy will eventually decrease.  It will go to zero as <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;to +&#92;infty" class="latex" /> whenever one species is fitter than all the rest and starts out with a nonzero population&#8212;since then this species will eventually take over.  </p>
<p>But remember, the entropy of a probability distribution is its <i>lack</i> of information.  So the decrease in entropy signals an increase in information.  And last time I argued that this makes perfect sense.   As the fittest species takes over and biodiversity drops, <i>the population is acquiring information about its environment</i>.  </p>
<p>However, I never said the entropy is <i>always</i> decreasing, because that&#8217;s false!  Even in this pathetically simple case, entropy can increase.</p>
<p>Suppose we start with many replicators belonging to one very unfit species, and a few belonging to various more fit species.  The probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> will start out sharply peaked, so the entropy will start out low:</p>
<div align="center">
<img width="250" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/biodiversity_0.png" />
</div>
<p>Now think about what happens when time passes.  At first the unfit species will rapidly die off, while the population of the other species slowly grows:</p>
<div align="center">
<img width="250" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/biodiversity_1.png" />
</div>
<p>&nbsp;</p>
<div align="center">
<img width="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/biodiversity_2.png" />
</div>
<p>So the probability distribution will, for a while, become less sharply peaked.  Thus, <i>for a while</i>, the entropy will increase!  </p>
<p>This seems to conflict with our idea that the population&#8217;s entropy should decrease as it acquires information about its environment.  But in fact this phenomenon is familiar in the study of statistical inference.  If you start out with strongly held <i>false</i> beliefs about a situation, the first effect of learning more is to become <i>less</i> certain about what&#8217;s going on! </p>
<p>Get it?  Say you start out by assigning a high probability to some wrong guess about a situation.   The entropy of your probability distribution is low: you&#8217;re quite certain about what&#8217;s going on.  But you&#8217;re wrong.  When you first start suspecting you&#8217;re wrong, you become more uncertain about what&#8217;s going on.   Your probability distribution flattens out, and the entropy goes up. </p>
<p>So, sometimes learning involves a decrease in information&#8212;<i>false</i> information.  There&#8217;s nothing about the mathematical concept of information that says this information is <i>true</i>.</p>
<p>Given this, it&#8217;s good to work out a formula for the rate of change of entropy, which will let us see more clearly when it goes down and when it goes up.  To do this, first let&#8217;s derive a completely general formula for the time derivative of the entropy of a probability distribution.  Following Sir Isaac Newton, we&#8217;ll use a dot to stand for a time derivative:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B++%5Cdot%7BS%7D%7D+%26%3D%26+%5Cdisplaystyle%7B+-++%5Cfrac%7Bd%7D%7Bdt%7D+%5Csum_i+p_i+%5Cln+%28p_i%29%7D+%5C%5C+++%5C%5C++%26%3D%26+-+%5Cdisplaystyle%7B+%5Csum_i+%5Cdot%7Bp%7D_i+%5Cln+%28p_i%29+%2B+%5Cdot%7Bp%7D_i+%7D++%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{  &#92;dot{S}} &amp;=&amp; &#92;displaystyle{ -  &#92;frac{d}{dt} &#92;sum_i p_i &#92;ln (p_i)} &#92;&#92;   &#92;&#92;  &amp;=&amp; - &#92;displaystyle{ &#92;sum_i &#92;dot{p}_i &#92;ln (p_i) + &#92;dot{p}_i }  &#92;end{array}" class="latex" /></p>
<p>In the last term we took the derivative of the logarithm and got a factor of <img src="https://s0.wp.com/latex.php?latex=1%2Fp_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/p_i" class="latex" /> which cancelled the factor of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />.  But since </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Csum_i+p_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;sum_i p_i = 1 } " class="latex" /></p>
<p>we know</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+%5Cdot%7Bp%7D_i+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i &#92;dot{p}_i = 0 } " class="latex" /></p>
<p>so this last term vanishes:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7BS%7D%3D+-%5Csum_i+%5Cdot%7Bp%7D_i+%5Cln+%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{S}= -&#92;sum_i &#92;dot{p}_i &#92;ln (p_i) } " class="latex" /></p>
<p>Nice!   To go further, we need a formula for <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i" class="latex" />.  For this we might as well return to the general replicator equation, dropping the pathetically special assumption that the fitness functions are actually constants.   Then we saw last time that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, p_i }" class="latex" /></p>
<p>where we used the abbreviation</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_i%28P%29+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P) = f_i(P_1, &#92;dots, P_n) " class="latex" /></p>
<p>for the fitness of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, and defined the <b>mean fitness</b> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_i+f_i%28P%29+p_i++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_i f_i(P) p_i  } " class="latex" /></p>
<p>Using this cute formula for <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i" class="latex" />, we get the final result:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7BS%7D+%3D+-+%5Csum_i+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle+%5CBig%29+%5C%2C+p_i+%5Cln+%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{S} = - &#92;sum_i &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle &#92;Big) &#92;, p_i &#92;ln (p_i) } " class="latex" /></p>
<p>This is strikingly similar to the formula for entropy itself.  But now each term in the sum includes a factor saying how much more fit than average, or less fit, that species is.  The quantity <img src="https://s0.wp.com/latex.php?latex=-+p_i+%5Cln%28p_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- p_i &#92;ln(p_i)" class="latex" /> is always nonnegative, since the graph of <img src="https://s0.wp.com/latex.php?latex=-x+%5Cln%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-x &#92;ln(x)" class="latex" /> looks like this:</p>
<div align="center">
<img src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/-xlnx.png" />
</div>
<p>So, the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th term contributes positively to the change in entropy if the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is fitter than average, but negatively if it&#8217;s less fit than average.</p>
<p>This may seem counterintuitive!</p>
<p><b>Puzzle 1.</b> How can we reconcile this fact with our earlier observations about the case when the fitness of each species is population-independent?  Namely: a) if initially most of the replicators belong to one very unfit species, the entropy will rise at first, but b) in the long run, when the fittest species present take over, the entropy drops?  </p>
<p>If this seems too tricky, look at some examples!  The first illustrates observation a); the second illustrates observation b):</p>
<p><b>Puzzle 2.</b>  Suppose we have two species, one with fitness equal to 1 initially constituting 90% of the population, the other with fitness equal to 10 initially constituting just 10% of the population:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccc%7D+f_1+%3D+1%2C+%26+%26++p_1%280%29+%3D+0.9+%5C%5C+%5C%5C++++++++++++++++++++++++++++f_2+%3D+10+%2C+%26+%26+p_2%280%29+%3D+0.1+++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccc} f_1 = 1, &amp; &amp;  p_1(0) = 0.9 &#92;&#92; &#92;&#92;                            f_2 = 10 , &amp; &amp; p_2(0) = 0.1   &#92;end{array} " class="latex" /></p>
<p>At what rate does the entropy change at <img src="https://s0.wp.com/latex.php?latex=t+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = 0" class="latex" />?  Which species is responsible for most of this change?</p>
<p><b>Puzzle 3.</b>  Suppose we have two species, one with fitness equal to 10 initially constituting 90% of the population, and the other with fitness equal to 1 initially constituting just 10% of the population:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccc%7D+f_1+%3D+10%2C+%26+%26++p_1%280%29+%3D+0.9+%5C%5C+%5C%5C++++++++++++++++++++++++++++f_2+%3D+1+%2C+%26+%26+p_2%280%29+%3D+0.1+++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccc} f_1 = 10, &amp; &amp;  p_1(0) = 0.9 &#92;&#92; &#92;&#92;                            f_2 = 1 , &amp; &amp; p_2(0) = 0.1   &#92;end{array} " class="latex" /></p>
<p>At what rate does the entropy change at <img src="https://s0.wp.com/latex.php?latex=t+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = 0" class="latex" />?  Which species is responsible for most of this change?</p>
<p>I had to work through these examples to understand what&#8217;s going on.  Now I do, and it all makes sense.</p>
<h3> Next time </h3>
<p>Still, it would be nice if there were some quantity that <i>always goes down</i> with the passage of time, reflecting our naive idea that the population gains information from its environment, and thus loses entropy, as time goes by.  </p>
<p>Often there <i>is</i> such a quantity. But it&#8217;s not the naive entropy: it&#8217;s the <i>relative</i> entropy.  I&#8217;ll talk about that next time.  In the meantime, if you want to prepare, please reread <a href="http://math.ucr.edu/home/baez/information/information_geometry_6.html">Part 6</a> of this series, where I explained this concept.  Back then, I argued that <i>whenever you&#8217;re tempted to talk about entropy, you should talk about relative entropy</i>.  So, we should try that here.</p>
<p>There&#8217;s a big idea lurking here: <i>information is relative</i>.  How much information a signal gives you depends on your prior assumptions about what that signal is likely to be.  If this is true, perhaps biodiversity is relative too.  </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/#comments">28 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;10)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-9904 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-probability" id="post-9904">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/" rel="bookmark">Information Geometry (Part&nbsp;9)</a></h2>
				<small>1 June, 2012</small><br />


				<div class="entry">
					<div align="center">
<a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/barcelona_biodiversity_poster.jpg" /><br />
</a>
</div>
<p>It&#8217;s time to continue this <a href="http://math.ucr.edu/home/baez/information/">information geometry</a> series, because I&#8217;ve promised to give the following talk at a  <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx">conference on the mathematics of biodiversity</a> in early July&#8230; and I still need to do some of the research!  <img src="https://i2.wp.com/math.ucr.edu/home/baez/emoticons/uhh.gif" alt="" /></p>
<blockquote>
<h4>Diversity, information geometry and learning</h4>
<p>As is well known, some measures of biodiversity are formally identical to measures of information developed by Shannon and others.  Furthermore, Marc Harper has shown that the replicator equation in evolutionary game theory is formally identical to a process of Bayesian inference, which is studied in the field of machine learning using ideas from information geometry. Thus, in this simple model, a population of organisms can be thought of as a &#8216;hypothesis&#8217; about how to survive, and natural selection acts to update this hypothesis according to Bayes&#8217; rule.  The question thus arises to what extent natural changes in biodiversity can be usefully seen as analogous to a form of learning. However, some of the same mathematical structures arise in the study of chemical reaction networks, where the increase of entropy, or more precisely decrease of free energy, is not usually considered a form of &#8216;learning&#8217;. We report on some preliminary work on these issues.
</p></blockquote>
<p>So, let&#8217;s dive in!  To some extent I&#8217;ll be explaining these two papers:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>However, I hope to bring in some more ideas from physics, the study of biodiversity, and the theory of stochastic Petri nets, also known as chemical reaction networks.  So, this series may start to overlap with my <a href="http://math.ucr.edu/home/baez/networks/">network theory</a> posts.  We&#8217;ll see.  We won&#8217;t get far today: for now, I just want to review and expand on what we did <a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/">last time</a>.</p>
<h3> The replicator equation </h3>
<p>The <b><a href="https://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a></b> is a simplified model of how populations change.  Suppose we have <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> types of self-replicating entity.  I&#8217;ll call these entities <b>replicators</b>.  I&#8217;ll call the types of replicators <b>species</b>, but they don&#8217;t need to be species in the biological sense.  For example, the replicators could be genes, and the types could be <a href="http://en.wikipedia.org/wiki/Allele">alleles</a>.  Or the replicators could be restaurants, and the types could be restaurant chains.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=P_i%28t%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t)," class="latex" /> or just <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> for short, be the population of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species at time <img src="https://s0.wp.com/latex.php?latex=t.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t." class="latex" />  Then the replicator equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) &#92;, P_i } " class="latex" /></p>
<p>So, the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> changes at a rate proportional to <img src="https://s0.wp.com/latex.php?latex=P_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i," class="latex" /> but the &#8216;constant of proportionality&#8217; need not be constant: it can be any smooth function <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> of the populations of all the species.  We call <img src="https://s0.wp.com/latex.php?latex=f_i%28P_1%2C+%5Cdots%2C+P_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P_1, &#92;dots, P_n)" class="latex" /> the <b><a href="http://en.wikipedia.org/wiki/Fitness_%28biology%29">fitness</a></b> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.</p>
<p>Of course this model is absurdly general, while still leaving out lots of important effects, like the spatial variation of populations, or the ability for the population of some species to start at zero and become nonzero&#8212;which happens thanks to mutation.  Nonetheless this model is worth taking a good look at.</p>
<p>Using the magic of vectors we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=P+%3D+%28P_1%2C+%5Cdots+%2C+P_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P = (P_1, &#92;dots , P_n)" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+%3D+%28f_1%28P%29%2C+%5Cdots%2C+f_n%28P%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) = (f_1(P), &#92;dots, f_n(P))" class="latex" /></p>
<p>This lets us write the replicator equation a wee bit more tersely as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P%7D%7Bd+t%7D+%3D+f%28P%29+P%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P}{d t} = f(P) P} " class="latex" /></p>
<p>where on the right I&#8217;m multiplying vectors componentwise, the way your teachers tried to brainwash you into never doing:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+P+%3D+%28f%28P%29_1+P_1%2C+%5Cdots%2C+f%28P%29_n+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) P = (f(P)_1 P_1, &#92;dots, f(P)_n P_n) " class="latex" /></p>
<p>In other words, I&#8217;m thinking of <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" /> as functions on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}" class="latex" /> and multiplying them pointwise.  This will be a nice way of thinking if we want to replace this finite set by some more general space.</p>
<p>Why would we want to do that?  Well, we might be studying lizards with different length tails, and we might find it convenient to think of the set of possible tail lengths as the half-line <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> instead of a finite set.</p>
<p>Or, just to get started, we might want to study the pathetically simple case where <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" /> doesn&#8217;t depend on <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" />  Then we just have a fixed function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and a time-dependent function <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P%7D%7Bd+t%7D+%3D+f+P%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P}{d t} = f P} " class="latex" /></p>
<p>If we&#8217;re physicists, we might write <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> more suggestively as <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> and write the operator multiplying by <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=-+H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- H." class="latex" />  Then our equation becomes</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+%5Cpsi%7D%7Bd+t%7D+%3D+-+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d &#92;psi}{d t} = - H &#92;psi } " class="latex" /></p>
<p>This looks a lot like Schr&ouml;dinger&#8217;s equation, but since there&#8217;s no factor of <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B-1%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sqrt{-1}," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is real-valued, it&#8217;s more like the heat equation or the &#8216;master equation&#8217;, the basic equation of stochastic mechanics.</p>
<p>For an explanation of Schr&ouml;dinger&#8217;s equation and the master equation, try <a href="http://math.ucr.edu/home/baez/networks/networks_12.html">Part 12</a> of the network theory series.  In that post I didn&#8217;t include a minus sign in front of the <img src="https://s0.wp.com/latex.php?latex=H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H." class="latex" />  That&#8217;s no big deal: it&#8217;s just a different convention than the one I want today.  A more serious issue is that in stochastic mechanics, <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> stands for a <i>probability distribution</i>.  This suggests that we should get probabilities into the game somehow.</p>
<h3> The replicator equation in terms of probabilities </h3>
<p>Luckily, that&#8217;s exactly what people usually do!   Instead of talking about the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, they talk about the <i>probability</i> <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> that one of our organisms will belong to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  This amounts to normalizing our populations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_j+P_j%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  p_i = &#92;frac{P_i}{&#92;sum_j P_j} } " class="latex" /></p>
<p>Don&#8217;t you love it when notations work out well?  Our big <b>P</b>opulation <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> has gotten normalized to give little <b>p</b>robability <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /></p>
<p>How do these probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> change with time?  Now is the moment for that least loved rule of elementary calculus to come out and take a bow: the quotient rule for derivatives!</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%5Csum_j+P_j+%5Cquad+-+%5Cquad+P_i+%5Csum_j+%5Cfrac%7Bd+P_j%7D%7Bd+t%7D%5Cright%29+%5Cbig%7B%2F%7D+%5Cleft%28++%5Csum_j+P_j+%5Cright%29%5E2+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left(&#92;frac{d P_i}{d t} &#92;sum_j P_j &#92;quad - &#92;quad P_i &#92;sum_j &#92;frac{d P_j}{d t}&#92;right) &#92;big{/} &#92;left(  &#92;sum_j P_j &#92;right)^2 }" class="latex" /></p>
<p>Using our earlier version of the replicator equation, this gives:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D++%5Cleft%28f_i%28P%29+P_i+%5Csum_j+P_j+%5Cquad+-+%5Cquad+P_i+%5Csum_j+f_j%28P%29+P_j+%5Cright%29+%5Cbig%7B%2F%7D+%5Cleft%28++%5Csum_j+P_j+%5Cright%29%5E2+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} =  &#92;left(f_i(P) P_i &#92;sum_j P_j &#92;quad - &#92;quad P_i &#92;sum_j f_j(P) P_j &#92;right) &#92;big{/} &#92;left(  &#92;sum_j P_j &#92;right)^2 }" class="latex" /></p>
<p>Using the definition of <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> this simplifies to:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D++f_i%28P%29+p_i+%5Cquad+-+%5Cquad+%5Cleft%28+%5Csum_j+f_j%28P%29+p_j+%5Cright%29+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} =  f_i(P) p_i &#92;quad - &#92;quad &#92;left( &#92;sum_j f_j(P) p_j &#92;right) p_i }" class="latex" /></p>
<p>The stuff in parentheses actually has a nice meaning: it&#8217;s just the <b>mean fitness</b>.  In other words, it&#8217;s the average, or expected, fitness of an organism chosen at random from the whole population.  Let&#8217;s write it like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_j+f_j%28P%29+p_j++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_j f_j(P) p_j  } " class="latex" /></p>
<p>So, we get the <b><a>replicator equation</a></b> in its classic form:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle+%5CBig%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle &#92;Big) &#92;, p_i }" class="latex" /></p>
<p>This has a nice meaning: for the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type to increase, their fitness must exceed the mean fitness.  If you&#8217;re trying to increase <a href="http://en.wikipedia.org/wiki/Market_share">market share</a>, what matters is not how good you are, but how much <i>better than average</i> you are.  If everyone else is lousy, you&#8217;re in luck.</p>
<h3> Entropy </h3>
<p>Now for something a bit new.  Once we&#8217;ve gotten a probability distribution into the game, its entropy is sure to follow:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28p%29+%3D+-+%5Csum_i+p_i+%5C%2C+%5Cln%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(p) = - &#92;sum_i p_i &#92;, &#92;ln(p_i) } " class="latex" /></p>
<p>This says how &#8216;smeared-out&#8217; the overall population is among the various different species.  Alternatively, it says how much <i>information</i> it takes, on average, to say which species a randomly chosen organism belongs to.   For example, if there are <img src="https://s0.wp.com/latex.php?latex=2%5EN&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2^N" class="latex" /> species, all with equal populations, the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> works out to <img src="https://s0.wp.com/latex.php?latex=N+%5Cln+2.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N &#92;ln 2." class="latex" />  So in this case, it takes <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> bits of information to say which species a randomly chosen organism belongs to.</p>
<p>In biology, entropy is one of many ways people measure biodiversity.  For a quick intro to some of the issues involved, try:</p>
<p>&bull; Tom Leinster, <a href="https://johncarlosbaez.wordpress.com/2011/11/07/measuring-biodiversity/">Measuring biodiversity</a>, <i>Azimuth</i>, 7 November 2011.</p>
<p>&bull; Lou Jost, <a href="http://www.loujost.com/Statistics%20and%20Physics/Diversity%20and%20Similarity/JostEntropy%20AndDiversity.pdf">Entropy and diversity</a>, <i>Oikos</i> <b>113</b> (2006), 363&#8211;375.</p>
<p>But we don&#8217;t need to understand this stuff to see how entropy is connected to the replicator equation.  Marc Harper&#8217;s paper explains this in detail:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>and I hope to go through quite a bit of it here.  But not today!  Today I just want to look at a pathetically simple, yet still interesting, example.</p>
<h3> Exponential growth </h3>
<p>Suppose the fitness of each species is independent of the populations of all the species.   In other words, suppose each fitness <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)" class="latex" /> is actually a constant, say <img src="https://s0.wp.com/latex.php?latex=f_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i." class="latex" />  Then the replicator equation reduces to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i &#92;, P_i } " class="latex" /></p>
<p>so it&#8217;s easy to solve:</p>
<p><img src="https://s0.wp.com/latex.php?latex=P_i%28t%29+%3D+e%5E%7Bt+f_i%7D+P_i%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t) = e^{t f_i} P_i(0)" class="latex" /></p>
<p>You don&#8217;t need a detailed calculation to see what&#8217;s going to happen to the probabilities</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Csum_j+P_j%28t%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i(t) = &#92;frac{P_i(t)}{&#92;sum_j P_j(t)}} " class="latex" /></p>
<p>The most fit species present will eventually take over!   If one species, say the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th one, has a fitness greater than the rest, then the population of this species will eventually grow faster than all the rest, at least if its population starts out greater than zero.  So as <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%2B%5Cinfty%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;to +&#92;infty," class="latex" /> we&#8217;ll have</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) &#92;to 1" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_j%28t%29+%5Cto+0+%5Cquad+%5Cmathrm%7Bfor%7D+%5Cquad+j+%5Cne+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_j(t) &#92;to 0 &#92;quad &#92;mathrm{for} &#92;quad j &#92;ne i" class="latex" /></p>
<p>Thus the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> will become more sharply peaked, and <i>its entropy will eventually approach zero</i>.</p>
<p>With a bit more thought you can see that even if more than one species shares the maximum possible fitness, the entropy will eventually decrease, though not approach zero.</p>
<p>In other words, <i>the biodiversity will eventually drop</i> as all but the most fit species are overwhelmed.  Of course, this is only true in our simple idealization.  In reality, biodiversity behaves in more complex ways&amp;mdash;in part because species interact, and in part because mutation tends to smear out the probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />  We&#8217;re not looking at these effects yet.  They&#8217;re extremely important&#8230; in ways we can only fully understand if we start by looking at what happens when they&#8217;re not present.</p>
<p>In still other words, <i>the population will absorb information from its environment</i>.  This should make intuitive sense: the process of natural selection resembles &#8216;learning&#8217;.  As fitter organisms become more common and less fit ones die out, the environment puts its stamp on the probability distribution <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />  So, this probability distribution should gain information.</p>
<p>While intuitively clear, this last claim also follows more rigorously from thinking of entropy as negative information.  Admittedly, it&#8217;s always easy to get confused by minus signs when relating entropy and information.   A while back I said the entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28p%29+%3D+-+%5Csum_i+p_i+%5C%2C+%5Cln%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(p) = - &#92;sum_i p_i &#92;, &#92;ln(p_i) } " class="latex" /></p>
<p>was the average information required to say which species a randomly chosen organism belongs to.  If this entropy is going down, isn&#8217;t the population <i>losing</i> information?</p>
<p>No, this is a classic sign error.  It&#8217;s like the concept of &#8216;work&#8217; in physics.  We can talk about the work some system does on its environment, or the work done by the environment on the system, and these are almost the same&#8230; <i>except one is minus the other!</i></p>
<p>When you are very ignorant about some system&#8212;say, some rolled dice&mdash;your estimated probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> for its various possible states are very smeared-out, so the entropy <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> is large.  As you gain information, you revise your probabilities and they typically become more sharply peaked, so <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> goes down.   When you know as much as you possibly can, <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> equals zero.</p>
<p>So, the entropy <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> is the amount of information you have left to learn: the amount of information you <i>lack</i>, not the amount you <i>have</i>.  As you gain information, this goes down.  There&#8217;s no paradox here.</p>
<p>It works the same way with our population of replicators&#8212;at least in the special case where the fitness of each species is independent of its population.  The probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is like a &#8216;hypothesis&#8217; assigning to each species <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> the probability <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> that it&#8217;s the best at self-replicating.   As some replicators die off while others prosper, they gather information their environment, and this hypothesis gets refined.  So, the entropy <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> drops.</p>
<h3> Next time </h3>
<p>Of course, to make closer contact to reality, we need to go beyond the special case where the fitness of each species is a constant!   Marc Harper does this, and I want to talk about his work someday, but first I have a few more remarks to make about the pathetically simple special case I&#8217;ve been focusing on. I&#8217;ll save these for next time, since I&#8217;ve probably strained your patience already.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comments">17 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;9)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3832 post type-post status-publish format-standard hentry category-information-and-entropy category-probability" id="post-3832">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/" rel="bookmark">A Characterization of&nbsp;Entropy</a></h2>
				<small>2 June, 2011</small><br />


				<div class="entry">
					<p><a href="http://golem.ph.utexas.edu/category/2011/05/an_operadic_introduction_to_en.html">Over at the <i>n</i>-Category CafÃ©</a> some of us have been trying an experiment: writing a math paper in full public view, both on that blog and on its associated wiki: the <a href="http://nlab.mathforge.org/nlab/show/HomePage"><i>n</i>Lab</a>.  One great thing about doing things this way is that  people can easily chip in with helpful suggestions.  It&#8217;s also more fun!  Both these tend to speed the process.</p>
<p>Like Frankenstein&#8217;s monster, our paper&#8217;s main result was initially jolted into life by huge blasts of power: in this case, not lightning but category theory.  It was awesome to behold, but too scary for <i>this</i> blog.</p>
<div align="center">
<img width="440" src="https://bosquechica.files.wordpress.com/2009/02/frankenstein11.jpg?w=440" />
</div>
<p>First <a href="http://www.maths.gla.ac.uk/~tl/">Tom Leinster</a> realized that the concept of entropy fell out &#8212; unexpectedly, but very naturally &#8212; from considerations involving <a href="http://en.wikipedia.org/wiki/Operad_theory">&#8216;operads&#8217;</a>, which are collections of abstract operations.  He was looking at a particular operad where the operations are &#8216;convex linear combinations&#8217;, and he discovered that this operad has entropy lurking in its heart.  Then <a href="http://users.icfo.es/Tobias.Fritz/">Tobias Fritz</a> figured out a nice way to state Tom&#8217;s result without mentioning operads.  By now we&#8217;ve taught the monster table manners, found it shoes that fit, and it&#8217;s ready for polite society:</p>
<p>â€¢ John Baez, Tobias Fritz and Tom Leinster, <a href="http://arxiv.org/abs/1106.1791">A characterization of entropy in terms of information loss</a>,  <a href="http://www.mdpi.com/1099-4300/13/11/1945"><i>Entropy</i></a> <b>13</b> (2011), 1945&ndash;1957.</p>
<p>The idea goes like this.   Say you&#8217;ve got a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with a <b><a href="http://en.wikipedia.org/wiki/Probability_measure">probability measure</a></b> <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on it, meaning a number <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+p_i+%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le p_i &#92;le 1" class="latex" /> for each point <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" />, obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi+%5Cin+X%7D+p_i+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i &#92;in X} p_i = 1" class="latex" /></p>
<p>Then the <b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a></b> of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%2C+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{i &#92;in X} p_i &#92;, &#92;ln(p_i) " class="latex" /></p>
<p>This funny-looking formula can be justified in many ways.  Our new way involves focusing not on entropy itself, but on <i>changes</i> in entropy.  This makes sense for lots of reasons.  For example, in physics we don&#8217;t usually measure entropy directly.  Instead, we measure changes in entropy, using the fact that a system at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> absorbing a tiny amount of heat <img src="https://s0.wp.com/latex.php?latex=%5CDelta+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta Q" class="latex" /> in a reversible way will experience an entropy change of <img src="https://s0.wp.com/latex.php?latex=%5CDelta+Q+%2F+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta Q / T" class="latex" />.  But our real reason for focusing on changes in entropy is that it gives a really slick theorem.</p>
<p>Suppose we have two finite sets with probability measures, say <img src="https://s0.wp.com/latex.php?latex=%28X%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,p)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(Y,q)" class="latex" />.  Then we define a <b>morphism</b> <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" /> to be a measure-preserving function: in other words, one for which the probability <img src="https://s0.wp.com/latex.php?latex=q_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_j" class="latex" /> of any point in <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> is the sum of the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> of the points in <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=f%28i%29+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(i) = j" class="latex" />.</p>
<p>A morphism of this sort is a deterministic process that carries one random situation to another.  For example, if I have a random integer between -10 and 10, chosen according to some probability distribution, and I square it, I get a random integer between 0 and 100.   A process of this sort always <i>decreases</i> the entropy: given any morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28q%29+%5Cle+S%28p%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q) &#92;le S(p) " class="latex" /></p>
<p>Since the second law of thermodynamics says that entropy always <i>increases</i>, this may seem counterintuitive or even paradoxical!   But there&#8217;s no paradox here.  It makes more intuitive sense if you think of entropy as information, and the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> as some kind of data processing that doesn&#8217;t introduce any additional randomness. Such a process can only decrease the amount of information.  For example, squaring the number -5 gives the same answer as squaring 5, so if I tell you &#8220;this number squared is 25&#8221;, I&#8217;m giving you less information than if I said &#8220;this number is -5&#8221;.</p>
<p>For this reason, we call the difference <img src="https://s0.wp.com/latex.php?latex=S%28p%29+-+S%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) - S(q)" class="latex" /> the <b>information loss</b> of the morphism <img src="https://s0.wp.com/latex.php?latex=f+%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : (X,p) &#92;to (Y,q)" class="latex" />.  And here&#8217;s our characterization of Shannon entropy in terms of information loss:</p>
<p>First, let&#8217;s write a morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=f+%3A+p+%5Cto+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : p &#92;to q" class="latex" /> for short.  Suppose <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> is a function that assigns to any such morphism a number <img src="https://s0.wp.com/latex.php?latex=F%28f%29+%5Cin+%5B0%2C%5Cinfty%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f) &#92;in [0,&#92;infty)," class="latex" /> which we think of as its information loss.  And suppose that <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> obeys three axioms:</p>
<ol>
<li>Functoriality.  Whenever we can compose morphisms <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />, we demand that </li>
</ol>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f+%5Ccirc+g%29+%3D+F%28f%29+%2B+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f &#92;circ g) = F(f) + F(g)" class="latex" /></p>
<p>In other words: <b>when we do a process consisting of two stages, the amount of information lost in the whole process is the sum of the amounts lost in each stage!</b></p>
<ol>
<li>Convex linearity.   Suppose we have two finite sets equipped with probability measures, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, and a real number <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5B0%2C+1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda &#92;in [0, 1]" class="latex" />.  Then there is a probability measure <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p+%5Coplus+%281+-+%5Clambda%29+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p &#92;oplus (1 - &#92;lambda) q" class="latex" /> on the disjoint union of the two sets, obtained by weighting the two measures by <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=1+-+%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 - &#92;lambda" class="latex" />, respectively.  Similarly, given morphisms <img src="https://s0.wp.com/latex.php?latex=f%3A+p+%5Cto+p%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: p &#92;to p&#039;" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g%3A+q+%5Cto+q%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g: q &#92;to q&#039;" class="latex" /> there is an obvious morphism from <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p+%5Coplus+%281+-+%5Clambda%29+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p &#92;oplus (1 - &#92;lambda) q" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p%27+%5Coplus+%281+-+%5Clambda%29+q%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p&#039; &#92;oplus (1 - &#92;lambda) q&#039;" class="latex" />.   Let&#8217;s call this morphism <img src="https://s0.wp.com/latex.php?latex=%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda f &#92;oplus (1 - &#92;lambda) g" class="latex" />.  We demand that </li>
</ol>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g%29+%3D+%5Clambda+F%28f%29+%2B+%281+-+%5Clambda%29+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(&#92;lambda f &#92;oplus (1 - &#92;lambda) g) = &#92;lambda F(f) + (1 - &#92;lambda) F(g)" class="latex" /></p>
<p>In other words: <b>if we flip a probability-Î» coin to decide whether to do one process or another, the information lost is Î» times the information lost by the first process plus (1 &#8211; Î») times the information lost by the second!</b></p>
<ol>
<li>Continuity.  The same function between finite sets can be thought of as a measure-preserving map in different ways, by changing the measures on these sets.   In this situation the quantity <img src="https://s0.wp.com/latex.php?latex=F%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f)" class="latex" /> should depend continuously on the measures in question.</li>
</ol>
<p>In other words: <b>if we slightly change what we do a process to, the information it loses changes only slightly</b>.</p>
<p>Then we conclude that there exists a constant <img src="https://s0.wp.com/latex.php?latex=c+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;ge 0" class="latex" /> such that for any morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f%29+%3D+c%28S%28p%29+-+S%28q%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f) = c(S(p) - S(q)) " class="latex" /></p>
<p>In other words: <b>the information loss is some multiple of the change in Shannon entropy!</b></p>
<p>What&#8217;s pleasing about this theorem is that the three axioms are pretty natural, and it&#8217;s hard to see the formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%2C+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{i &#92;in X} p_i &#92;, &#92;ln(p_i) " class="latex" /></p>
<p>hiding in them&#8230; but it&#8217;s actually there.</p>
<p>(We also prove a version of this theorem for <a href="http://en.wikipedia.org/wiki/Tsallis_entropy">Tsallis entropy</a>, in case you care.   This obeys a mutant version of axiom 2, namely:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g%29+%3D+%5Clambda%5E%5Calpha+F%28f%29+%2B+%281+-+%5Clambda%29%5E%5Calpha+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(&#92;lambda f &#92;oplus (1 - &#92;lambda) g) = &#92;lambda^&#92;alpha F(f) + (1 - &#92;lambda)^&#92;alpha F(g)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> is a parameter with <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &lt; &#92;alpha&lt; &#92;infty" class="latex" />.  Tsallis entropy is a close relative of RÃ©nyi entropy, which I discussed here <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">earlier</a>.  Just as  RÃ©nyi entropy is a kind of <i>q</i>-derivative of the free energy, the Tsallis entropy is a <i>q</i>-derivative of the partition function.  I&#8217;m not sure either of them are really important, but when you&#8217;re trying to uniquely characterize Shannon entropy, it&#8217;s nice for it to have some competitors to fight against, and these are certainly the main two.  Both of them depend on a parameter and reduce to the Shannon entropy at a certain value of that parameter.)</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comments">32 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/" rel="bookmark" title="Permanent Link to A Characterization of&nbsp;Entropy">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3688 post type-post status-publish format-standard hentry category-biology category-information-and-entropy" id="post-3688">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/" rel="bookmark">Information Geometry (Part&nbsp;8)</a></h2>
				<small>26 May, 2011</small><br />


				<div class="entry">
					<p>Now this series on information geometry will take an unexpected turn toward &#8216;green mathematics&#8217;.  Lately I&#8217;ve been talking about relative entropy.  Now I&#8217;ll say how this concept shows up in the study of evolution!</p>
<p>That&#8217;s an unexpected turn to me, at least.  I learned of this connection just two days ago in a conversation with <a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a>, a mathematician who is a postdoc in bioinformatics at UCLA, working with my friend <a href="http://bioinfo.mbi.ucla.edu/leelab/">Chris Lee</a>.  I was visiting Chris for a couple of days after attending the thesis defenses of some grad students of mine who just finished up at U.C. Riverside.   Marc came by and told me about this paper:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>and now I can&#8217;t resist telling you.</p>
<p>First of all: what does information theory have to do with biology?  Let me start with a very general answer: biology is different from physics because biological systems are packed with information you can&#8217;t afford to ignore.  </p>
<p>Physicists love to think about systems that take only a little information to describe.  So when they get a system that takes a <i>lot</i> of information to describe, they use a trick called &#8216;statistical mechanics&#8217;, where you try to ignore most of this information and focus on a few especially important variables.  For example, if you hand a physicist a box of gas, they&#8217;ll try to avoid thinking about the state of each atom, and instead focus on a few macroscopic quantities like the volume and total energy. Ironically, the mathematical concept of information arose first here&mdash;although they didn&#8217;t call it information back then; they called it &#8216;entropy&#8217;.  The entropy of a box of gas is precisely the amount of information you&#8217;ve decided to forget when you play this trick of focusing on the macroscopic variables.  Amazingly, remembering just this&mdash;the sheer <i>amount</i> of information you&#8217;ve forgotten&mdash;can be extremely useful&#8230; at least for the systems physicists like best.</p>
<p>But biological systems are different.  They store lots of information (for example in DNA), transmit lots of information (for example in the form of biochemical signals), and collect a lot of information from their environment.  And this information isn&#8217;t uninteresting &#8216;noise&#8217;, like the positions of atoms in a gas.  The details really matter.   Thus, we need to keep track of lots of information to have a chance of understanding any particular biological system.   </p>
<p>So, part of doing biology is developing new ways to think about physical systems that contain lots of <i>relevant</i> information.  This is why physicists consider biology &#8216;messy&#8217;.  It&#8217;s also why biology and computers go hand in hand in the subject called &#8216;bioinformatics&#8217;.  There&#8217;s no avoiding this: in fact, it will probably force us to <i>automate the scientific method!</i>  That&#8217;s what Chris Lee and Marc Harper are really working on:</p>
<p>&bull; Chris Lee, <a href="http://vimeo.com/23235162">General information metrics for automated experiment planning</a>, presentation in the UCLA Chemistry &amp; Biochemistry Department faculty luncheon series, 2 May 2011.</p>
<p>But more about that some other day.  Let me instead give <i>another</i> answer to the question of what information theory has to do with biology.  </p>
<p>There&#8217;s an analogy between evolution and the scientific method.  Simply put, life is an experiment to see what works; natural selection weeds out the bad guesses, and over time the better guesses predominate.  This process transfers information from the world to the &#8216;experimenter&#8217;: the species that&#8217;s doing the evolving, or the scientist.   Indeed, the only way the experimenter can get information is by making guesses that can be wrong.  </p>
<p>All this is simple enough, but the nice thing is that we can make it more precise.</p>
<p>On the one hand, there&#8217;s a simple model of the scientific method called <a href="http://en.wikipedia.org/wiki/Bayesian_inference">&#8216;Bayesian inference&#8217;</a>.  Assume there&#8217;s a set of mutually exclusive alternatives: possible ways the world can be.   And suppose we start with a <a href="http://en.wikipedia.org/wiki/Prior_probability">&#8216;prior probability distribution&#8217;</a>: a preconceived notion of how probable each alternative is.   Say we do an experiment and get a result that depends on which alternative is true.  We can work out how likely this result was given our prior, and&mdash;using a marvelously simple formula called <a href="http://en.wikipedia.org/wiki/Bayes'_theorem">Bayes&#8217; rule</a>&mdash;we can use this to update our prior and obtain a new improved probability distribution, called the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution">&#8216;posterior probability distribution&#8217;</a>.  </p>
<p>On the other hand, suppose we have a species with several different possible genotypes.  A population of this species will start with some number of organisms with each genotype.  So, we get a probability distribution saying how likely it is that an organism has any given genotype.  These genotypes are our &#8216;mutually exclusive alternatives&#8217;, and this probability distribution is our &#8216;prior&#8217;.  Suppose each generation the organisms have some expected number of offspring that depends on their genotype.   Mathematically, it turns out this is just like updating our prior using Bayes&#8217; rule!   The result is a new probability distribution of genotypes: the &#8216;posterior&#8217;.</p>
<p>I learned about this from Chris Lee on the 19th of December, 2006.  In my <a href="http://math.ucr.edu/home/baez/diary/december_2006.html#december19.06">diary</a> that day, I wrote:</p>
<blockquote><p>
The analogy is mathematically precise, and fascinating.  In rough terms, it says that <i>the process of natural selection resembles the process of Bayesian inference</i>.  A population of organisms can be thought of as having various &#8216;hypotheses&#8217; about how to survive&mdash;each hypothesis corresponding to a different <a href="http://en.wikipedia.org/wiki/Allele"> allele</a>.  (Roughly, an allele is one of several alternative versions of a gene.)  In each successive generation, the process of natural selection modifies the proportion of organisms having each hypothesis, according to Bayes&#8217; rule!</p>
<p>Now let&#8217;s be more precise:</p>
<p><a href="http://en.wikipedia.org/wiki/Bayes'_theorem#Alternative_forms_of_Bayes.27_theorem">Bayes&#8217; rule</a> says if we start with a &#8216;prior probability&#8217; for some hypothesis to be true, divide it by the probability that some observation is made, then multiply by the &#8216;conditional probability&#8217; that this observation will be made given that the hypothesis is true, we&#8217;ll get the &#8216;posterior probability&#8217; that the hypothesis is true <i>given that the observation is made</i>.</p>
<p>Formally, the exact same equation shows up in population genetics!  In fact, Chris showed it to me&mdash;it&#8217;s equation 9.2 on page 30 of this<br />
book:</p>
<p>&bull; R. B&uuml;rger, <em>The Mathematical Theory of Selection, Recombination and Mutation</em>, section I.9: Selection at a single locus, Wiley, 2000.  </p>
<p>But, now all the terms in the equation have different meanings!  </p>
<p>Now, instead of a &#8216;prior probability&#8217; for a hypothesis to be true, we have the frequency of occurrence of some <a href="http://en.wikipedia.org/wiki/Allele">allele</a> in some generation of a population.  Instead of the probability that we make some observation, we have the expected number of offspring of an organism. Instead of the &#8216;conditional probability&#8217; of making the observation, we have the expected number of offspring of an organism <i>given</i> that it has this allele. And, instead of the &#8216;posterior probability&#8217; of our hypothesis, we have the frequency of occurrence of that allele in the next generation.</p>
<p>(Here we are assuming, for simplicity, an asexually reproducing &#8216;haploid&#8217; population &#8211; that is, one with just a single set of chromosomes.)</p>
<p>This is a great idea&mdash;Chris felt sure someone must have already had it. A natural context would be research on <a href="http://en.wikipedia.org/wiki/Genetic_programming">genetic programming</a>, a machine learning technique that uses an evolutionary algorithm to optimize a population of computer programs according to a fitness landscape determined by their ability to perform a given task.  Since there has also been a lot of work on Bayesian approaches to machine learning, surely someone has noticed their mathematical relationship?
</p></blockquote>
<p>I see at least <a href="http://www.iq.harvard.edu/blog/sss/archives/2007/01/bayesian_infere.shtml">one person</a> found these ideas as new and exciting as I did.  But I still can&#8217;t believe Chris was the first to clearly formulate them, so I&#8217;d still like to know who did.</p>
<p>Marc Harper actually went to work with Chris after reading that diary entry of mine.  By now he&#8217;s gone a lot further with this analogy by focusing on the role of <i>information</i>.    As we keep updating our prior using Bayes&#8217; rule, we should be gaining information about the real world.  This idea has been made very precise in the theory of <a href="http://en.wikipedia.org/wiki/Machine_learning">&#8216;machine learning&#8217;</a>. Similarly, as a population evolves through natural selection, it should be gaining information about its environment.  </p>
<p>I&#8217;ve been talking about Bayesian updating as a discrete-time process: something that happens once each generation for our population.  That&#8217;s fine and dandy, definitely worth studying, but Marc&#8217;s paper focuses on a continuous-time version called the <a href="http://en.wikipedia.org/wiki/Replicator_equation">&#8216;replicator equation&#8217;</a>.  It goes like this.  Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be the set of alternative genotypes.  For each <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> be the number of organisms that have the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.   Say that </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i P_i } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is the <b>fitness</b> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype.   Let <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> be the probability that at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />, a randomly chosen organism will have the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_%7Bi+%5Cin+X%7D+P_i+%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_{i &#92;in X} P_i } }" class="latex" /></p>
<p>Then a little calculus gives the <b>replicator equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i++-+%5Clangle+f+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{d p_i}{d t} = &#92;left( f_i  - &#92;langle f &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+f+%5Crangle+%3D+%5Csum_%7Bi+%5Cin+X%7D++f_i++p_i++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle f &#92;rangle = &#92;sum_{i &#92;in X}  f_i  p_i  " class="latex" /></p>
<p>is the <b>mean fitness</b> of the organisms.  So, the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type grows at a rate proportional to the fitness of that type <i>minus the mean fitness</i>.   It ain&#8217;t enough to be good: you gotta be better than average.</p>
<p>Note that all this works not just when each fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is a mere number, but also when it&#8217;s a function of the whole list of probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />.  That&#8217;s good, because in the real world, the fitness of one kind of bug may depend on the fraction of bugs of various kinds.  </p>
<p>But what does all this have to do with <i>information?</i>  </p>
<p>Marc&#8217;s paper has a lot to say about this!   But just to give you a taste, here&#8217;s a simple fact involving relative entropy, which was first discovered by Ethan Atkin.  Suppose evolution as described by the replicator equation brings the whole list of probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />&mdash;let&#8217;s call this list <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />&mdash;closer and closer to some stable equilibrium, say <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  Then if a couple of technical conditions hold, the entropy of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> keeps decreasing, and approaches zero.  </p>
<p>Remember what I told you about <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  In Bayesian inference, the entropy <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is how much information we gain if we start with <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as our prior and then do an experiment that pushes us to the posterior <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  </p>
<p>So, in simple rough terms: <i><b>as it approaches a stable equilibrium, the amount of information a species has left to learn keeps dropping, and goes to zero!</b></i></p>
<p>I won&#8217;t fill in the precise details, because I bet you&#8217;re tired already.  You can find them in Section 3.5, which is called &#8220;Kullback-Leibler Divergence is a Lyapunov function for the Replicator Dynamic&#8221;.  If you know all the buzzwords here, you&#8217;ll be in buzzword heaven now.  &#8216;Kullback-Leibler divergence&#8217; is just another term for relative entropy.  &#8216;Lyapunov function&#8217; means that it keeps dropping and goes to zero.  And the &#8216;replicator dynamic&#8217; is the replicator equation I described above.</p>
<p>Perhaps next time I&#8217;ll say more about this stuff.  For now, I just hope you see why it makes me so happy.  </p>
<p>First, it uses information geometry to make precise the sense in which evolution is a process of acquiring information.  That&#8217;s very cool.  We&#8217;re looking at a simplified model&mdash;the replicator equation&mdash;but doubtless this is just the beginning of a very long story that keeps getting deeper as we move to less simplified models.  </p>
<p>Second, if you read my summary of <a href="https://johncarlosbaez.wordpress.com/2011/05/02/networks-and-population-biology/">Chris Canning&#8217;s talks on evolutionary game theory</a>, you&#8217;ll see everything I just said meshes nicely with that.  He was taking the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_i+%3D+%5Csum_%7Bj+%5Cin+X%7D+A_%7Bi+j%7D+p_j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i = &#92;sum_{j &#92;in X} A_{i j} p_j " class="latex" /></p>
<p>where the <b>payoff matrix</b> <img src="https://s0.wp.com/latex.php?latex=A_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{i j}" class="latex" /> describes the &#8216;winnings&#8217; of an organism with the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype when it meets an organism with the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th genotype.  This gives a particularly nice special case of the replicator equation.</p>
<p>Third, this particularly nice special case happens to be the <a href="https://johncarlosbaez.wordpress.com/2011/04/03/network-theory-part-3/">rate equation</a> for a certain stochastic Petri net.  So, we&#8217;ve succeeded in connecting the &#8216;diagram theory&#8217; discussion to the &#8216;information geometry&#8217; discussion!  This has all sort of implications, which will take quite a while to explore.   </p>
<p>As the saying goes, in mathematics:</p>
<blockquote><p>
Everything sufficiently beautiful is connected to all other beautiful things.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/#comments">105 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;8)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3546 post type-post status-publish format-standard hentry category-azimuth category-biology category-networks" id="post-3546">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/05/06/networks-and-population-biology-part-4/" rel="bookmark">Networks and Population Biology (Part&nbsp;4)</a></h2>
				<small>6 May, 2011</small><br />


				<div class="entry">
					<p>Today was the last day of the <a href="http://www2.ims.nus.edu.sg/Programs/011mathbio/tut2.php">tutorials on discrete mathematics and probability in networks and population biology</a>.  Persi Diaconis gave two talks, one on &#8216;exponential families&#8217; of random graphs and one on &#8216;exchangeability&#8217;.  Since there&#8217;s way too much to summarize, I&#8217;ll focus on explaining ideas from the first talk, leaving you to read about the second here:</p>
<p>â€¢ Persi Diaconis and Svante Janson, <a href="http://www-stat.stanford.edu/~cgates/PERSI/papers/limit08.pdf">Graph limits and exchangeable random graphs</a>.</p>
<p>Susan Holmes also gave two talks.  The first was on <a href="http://en.wikipedia.org/wiki/Metagenomics">metagenomics</a> and the <a href="http://en.wikipedia.org/wiki/Human_Microbiome_Project">human microbiome</a>â€”very cool stuff.  Did you know that your body contains 100 trillion bacteria, and only 10 trillion human cells?  And you&#8217;ve got 1000 species of bacteria in your gut?  Statistical ecologists are getting very interested in this.</p>
<p>Her second talk was about doing statistics when you&#8217;ve got lots of data of <i>different kinds</i> that need to be integrated: numbers, graphs and trees, images, spatial information, and so on.  This is clearly the wave of the future.  You can see the slides for this talk here:</p>
<p>â€¢ Susan Holmes, <a href="http://www.stanford.edu/group/mmds/slides2010/Holmes.pdf">Heterogeneous data challenge: combining complex data</a>.</p>
<p>The basic idea of Persi Diaconis&#8217; talk was simple and shocking.  Suppose you choose a random graph in the most obvious way designed to heighten the chance that it contains a triangle, or some other figure.  Then in fact all you&#8217;ve done is change the chance that there&#8217;s an edge between any given pair of vertices!</p>
<p>But to make this preciseâ€”to make it even <i>true</i>â€”we need to say what the rules are.</p>
<p>For starters, let me point you back to <a href="https://johncarlosbaez.wordpress.com/2011/05/04/networks-and-population-biology-part-2/">Part 2</a> for Persi&#8217;s definitions of &#8216;graph&#8217; and &#8216;graph homomorphism&#8217;.  If we fix a finite set  <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cdots%2C+n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1,&#92;dots, n&#92;}" class="latex" />, there will be a big set <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{G}_n" class="latex" /> of graphs with exactly these vertices.  To define a kind of &#8216;random graph&#8217;, we first pick a probability measure on each set <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{G}_n" class="latex" />.  Then, we demand that these probability measures converge in a certain sense as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty" class="latex" />.</p>
<p>However, we can often describe random graphs in a more intuitive way!  For example, the simplest random graphs are the <a href="http://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">ErdÅ‘s&#8211;RÃ©nyi random graphs</a>.   These depend on a parameter <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in [0,1]" class="latex" />.  The idea here is that we take our set of vertices and for each pair we flip a coin that lands heads up with probability <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  If it lands heads up, we stick in an edge between those vertices; otherwise not.  So, the presence or absence of each edge is an independent random variable.</p>
<p>Here&#8217;s a picture of an ErdÅ‘s&#8211;RÃ©nyi random graph drawn by von Frisch, with a 1% chance of an edge between any two vertices.  But it&#8217;s been drawn in a way so that the best-connected vertices are near the middle, so it doesn&#8217;t look as random as it is:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/File:Erdos_generated_network-p0.01.jpg"><img src="https://i0.wp.com/math.ucr.edu/home/baez/networks/Erdos-Renyi_random_graph_p=0.01.jpg" /></a></div>
<p>People have studied the ErdÅ‘s&#8211;RÃ©nyi random graphs very intensively, so now people are eager to study random graphs with more interesting correlations.  For example, consider the graph where we draw an edge between any two people who are friends.  If you&#8217;re my friend and I&#8217;m friends with someone else, that improves the chances that you&#8217;re friends with them!  In other words, friends tend to form &#8216;triangles&#8217;.  But in an ErdÅ‘s&#8211;RÃ©nyi random graph there&#8217;s no effect like that.</p>
<p>&#8216;Exponential families&#8217; of random graphs seem like a way around this problem.  The idea here is to pick a specific collection of graphs <img src="https://s0.wp.com/latex.php?latex=H_1%2C+%5Cdots%2C+H_k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1, &#92;dots, H_k" class="latex" /> and say how commonly we want these to appear in our random graph.  If we only use one graph <img src="https://s0.wp.com/latex.php?latex=H_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1" class="latex" />, and we take this to be two vertices connected by an edge, we&#8217;ll get an ErdÅ‘s&#8211;RÃ©nyi random graph.  But, if we also want our graph to contain a lot of triangles, we can pick <img src="https://s0.wp.com/latex.php?latex=H_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_2" class="latex" /> to be a triangle.</p>
<p>More precisely, remember from <a href="https://johncarlosbaez.wordpress.com/2011/05/04/networks-and-population-biology-part-2/">Part 2</a> that <img src="https://s0.wp.com/latex.php?latex=t%28H_i%2CG%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t(H_i,G)" class="latex" /> is the fraction of functions mapping <img src="https://s0.wp.com/latex.php?latex=H_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_i" class="latex" /> to vertices of <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" /> that are actually graph homomorphisms.  This is the smart way to keep track of how often <img src="https://s0.wp.com/latex.php?latex=H_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_i" class="latex" /> shows up inside <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" />.  So, we pick some numbers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_1%2C+%5Cdots+%2C+%5Cbeta_k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_1, &#92;dots , &#92;beta_k" class="latex" /> and define a probability measure on <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{G}_n" class="latex" /> as follows: the probability of any particular graph <img src="https://s0.wp.com/latex.php?latex=G+%5Cin+%5Cmathcal%7BG%7D_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G &#92;in &#92;mathcal{G}_n" class="latex" /> should be proportional to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cexp+%5Cleft%28+%5Cbeta_1+%5C%2C+t%28H_1%2C+G%29+%2B+%5Ccdots+%2B+%5Cbeta_k+%5C%2C+t%28H_k%2C+G%29+%5Cright%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;exp &#92;left( &#92;beta_1 &#92;, t(H_1, G) + &#92;cdots + &#92;beta_k &#92;, t(H_k, G) &#92;right)} " class="latex" /></p>
<p>If you&#8217;re a physicist you&#8217;ll call this a &#8216;Gibbs state&#8217;, and you&#8217;ll know this is the way to get a probability distribution that maximizes entropy while holding the expected values of <img src="https://s0.wp.com/latex.php?latex=t%28H_i%2C+G%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t(H_i, G)" class="latex" /> constant.  Statisticians like to call the whole family of Gibbs states as we vary the number <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> an &#8216;exponential family&#8217;.  But the cool part, for me, is that we can apply ideas from physicsâ€”namely, statistical mechanicsâ€”to graph theory.</p>
<p>So far we&#8217;ve got a probability measure on our set <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathcal{G}_n" class="latex" /> of graphs with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> vertices.  These probability measures converge in a certain sense as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty" class="latex" />.   But Diaconis and Chatterjee proved a shocking theorem: <i>for almost all choices of the graphs <img src="https://s0.wp.com/latex.php?latex=H_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_i" class="latex" /> and numbers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i &gt; 0" class="latex" />, these probability measures converge to an ErdÅ‘s&#8211;RÃ©nyi random graph!</i>  And in the other cases, they converge to a probabilistic mixture of ErdÅ‘s&#8211;RÃ©nyi random graphs.</p>
<p>In short, as long as the numbers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> are positive, exponential families don&#8217;t buy us much.  We could just work with ErdÅ‘s&#8211;RÃ©nyi random graphs, or probabilistic mixtures of these.  The exponential families are still very interesting to study, but they don&#8217;t take us into truly new territory.</p>
<p>The theorem is here:</p>
<p>â€¢ Sourav Chatterjee and Persi Diaconis, <a href="http://www-stat.stanford.edu/~cgates/PERSI/papers/exponentialRGM3.pdf">Estimating and understanding random graph models</a>.</p>
<p>To reach new territory, we can try letting some <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> be negative.  The paper talks about this too.  Here many questions remain open!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/05/06/networks-and-population-biology-part-4/#comments">18 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/azimuth/" rel="category tag">azimuth</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/05/06/networks-and-population-biology-part-4/" rel="bookmark" title="Permanent Link to Networks and Population Biology (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2576 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2576">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/" rel="bookmark">Information Geometry (Part&nbsp;7)</a></h2>
				<small>2 March, 2011</small><br />


				<div class="entry">
					<p>Today, I want to describe how the <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Fisher information metric</a> is related to <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  I&#8217;ve explained both these concepts separately (click the links for details); now I want to put them together. </p>
<p>But first, let me explain what this whole series of blog posts is about.  Information geometry, obviously!  But what&#8217;s that?</p>
<p>Information geometry is the geometry of &#8216;statistical manifolds&#8217;.   Let me explain that concept twice: first vaguely, and then precisely.  </p>
<p>Vaguely speaking, a statistical manifold is a <a href="http://en.wikipedia.org/wiki/Manifold">manifold</a> whose points are hypotheses about some situation.   For example, suppose you have a coin.  You could have various hypotheses about what happens when you flip it.  For example: you could hypothesize that the coin will land heads up with probability <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is any number between 0 and 1.  This makes the interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,1]" class="latex" /> into a statistical manifold.  Technically this is a manifold <i>with boundary</i>, but that&#8217;s okay.</p>
<p>Or, you could have various hypotheses about the IQ&#8217;s of American politicians.  For example: you could hypothesize that they&#8217;re distributed according to a Gaussian probability distribution with mean <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and standard deviation <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />.  This makes the space of pairs <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x,y)" class="latex" /> into a statistical manifold.  Of course we require <img src="https://s0.wp.com/latex.php?latex=y+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;ge 0" class="latex" />, which gives us a manifold with boundary. We might also want to assume <img src="https://s0.wp.com/latex.php?latex=x+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;ge 0" class="latex" />, which would give us a manifold <i>with corners</i>, but that&#8217;s okay too.  We&#8217;re going to be pretty relaxed about what counts as a &#8216;manifold&#8217; here.</p>
<p>If we have a manifold whose points are hypotheses about some situation, we say the manifold &#8216;parametrizes&#8217; these hypotheses.  So, the concept of statistical manifold is fundamental to the subject known as <a href="http://en.wikipedia.org/wiki/Parametric_statistics">parametric statistics</a>.  </p>
<p>Parametric statistics is a huge subject!   You could say that information geometry is the application of geometry to this subject.</p>
<p>But now let me go ahead and make the idea of &#8216;statistical manifold&#8217; more precise.  There&#8217;s a classical and a quantum version of this idea.  I&#8217;m working at the <a href="http://www.quantumlah.org/">Centre of Quantum Technologies</a>, so I&#8217;m being paid to be quantum&mdash;but today I&#8217;m in a classical mood, so I&#8217;ll only describe the classical version.  Let&#8217;s say a <b>classical statistical manifold</b> is a smooth function <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> from a manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the space of probability distributions on some measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  </p>
<p>We should think of <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> as a space of <b>events</b>.  In our first example, it&#8217;s just <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%2C+T%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{H, T&#92;}" class="latex" />: we flip a coin and it lands either heads up or tails up.   In our second it&#8217;s <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" />: we measure the IQ of an American politician and get some real number.</p>
<p>We should think of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> as a space of <b>hypotheses</b>.  For each point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />, we have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  This is hypothesis about the events in question: for example &#8220;when I flip the coin, there&#8217;s 55% chance that it will land heads up&#8221;, or &#8220;when I measure the IQ of an American politician, the answer will be distributed according to a Gaussian with mean 0 and standard deviation 100.&#8221;</p>
<p>Now, suppose someone hands you a classical statistical manifold <img src="https://s0.wp.com/latex.php?latex=%28M%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(M,p)" class="latex" />.  Each point in <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> is a hypothesis.  Apparently some hypotheses are more similar than others.  It would be nice to make this precise.  So, you might like to define a metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> that says how &#8216;far apart&#8217; two hypotheses are.  People know lots of ways to do this; the challenge is to find ways that have clear meanings.</p>
<p>Last time I explained the concept of <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  Suppose we have two probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />. Then the <b>entropy of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /></b> is the amount of information you gain when you start with the hypothesis <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> but then discover that you should switch to the new improved hypothesis <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  It equals:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp%7D%7Bq%7D+%5C%3B+%5Cln%28%5Cfrac%7Bp%7D%7Bq%7D%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; &#92;frac{p}{q} &#92;; &#92;ln(&#92;frac{p}{q}) &#92;; q d &#92;omega " class="latex" /></p>
<p>You could try to use this to define a distance between points <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> in our statistical manifold, like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29+%3D++%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp_x%7D%7Bp_y%7D+%5C%3B+%5Cln%28%5Cfrac%7Bp_x%7D%7Bp_y%7D%29+%5C%3B+p_y+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y) =  &#92;int_&#92;Omega &#92;; &#92;frac{p_x}{p_y} &#92;; &#92;ln(&#92;frac{p_x}{p_y}) &#92;; p_y d &#92;omega " class="latex" /></p>
<p>This is definitely an important function.  Unfortunately, as I explained <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">last time</a>, it doesn&#8217;t obey the axioms that a distance function should!   Worst of all, it doesn&#8217;t obey the triangle inequality.</p>
<p>Can we &#8216;fix&#8217; it?  Yes, we can!   And when we do, we get the Fisher information metric, which is actually a <i>Riemannian</i> metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  Suppose we put local coordinates on some patch of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> containing the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  Then the <b>Fisher information metric</b> is given by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D%28x%29+%3D+%5Cint_%5COmega++%5Cpartial_i+%28%5Cln+p_x%29+%5C%3B+%5Cpartial_j+%28%5Cln+p_x%29+%5C%3B+p_x+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}(x) = &#92;int_&#92;Omega  &#92;partial_i (&#92;ln p_x) &#92;; &#92;partial_j (&#92;ln p_x) &#92;; p_x d &#92;omega" class="latex" /></p>
<p>You can think of my whole series of articles so far as an attempt to understand this funny-looking formula.   I&#8217;ve shown how to get it from a few different starting-points, most recently back in <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Part 3</a>.  But now let&#8217;s get it starting from relative entropy!</p>
<p>Fix any point in our statistical manifold and choose local coordinates for which this point is the origin, <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" />.  The amount of information we gain if move to some other point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is the relative entropy <img src="https://s0.wp.com/latex.php?latex=S%28x%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,0)" class="latex" />.   But what&#8217;s this like when <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is really close to <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" />?  We can imagine doing a Taylor series expansion of <img src="https://s0.wp.com/latex.php?latex=S%28x%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,0)" class="latex" /> to answer this question.</p>
<p>Surprisingly, to first order the answer is always zero!  Mathematically:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>In plain English: if you change your mind slightly, you  learn a negligible amount &mdash; <i>not</i> an amount proportional to how much you changed your mind.</p>
<p>This must have some profound significance.  I wish I knew what.  Could it mean that people are reluctant to change their minds except in big jumps?</p>
<p>Anyway, if you think about it, this fact makes it obvious that <img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y)" class="latex" /> can&#8217;t obey the triangle inequality.  <img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y)" class="latex" /> could be pretty big, but if we draw a curve from <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />, and mark <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> closely spaced points <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> on this curve, then <img src="https://s0.wp.com/latex.php?latex=S%28x_%7Bi%2B1%7D%2C+x_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x_{i+1}, x_i)" class="latex" /> is zero to first order, so it must be of order <img src="https://s0.wp.com/latex.php?latex=1%2Fn%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/n^2" class="latex" />, so if the triangle inequality were true we&#8217;d have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29+%5Cle+%5Csum_i+S%28x_%7Bi%2Bi%7D%2Cx_i%29+%5Cle+%5Cmathrm%7Bconst%7D+%5C%2C+n+%5Ccdot+%5Cfrac%7B1%7D%7Bn%5E2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y) &#92;le &#92;sum_i S(x_{i+i},x_i) &#92;le &#92;mathrm{const} &#92;, n &#92;cdot &#92;frac{1}{n^2}" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />, which is a contradiction.</p>
<p>In plain English: if you change your mind in one big jump, the amount of information you gain is more than the sum of the amounts you&#8217;d gain if you change your mind in lots of little steps!  This seems pretty darn strange, but the paper I mentioned in <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">part 1</a> helps:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>.</p>
<p>You&#8217;ll see he takes a curve and chops it into lots of little pieces as I just did, and explains what&#8217;s going on.</p>
<p>Okay, so what about second order?  What&#8217;s</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} ?" class="latex" /></p>
<p>Well, this is the punchline of this blog post: <i>it&#8217;s the Fisher information metric:</i></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>And since the Fisher information metric is a <a href="http://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian metric</a>, we can then apply <a href="http://en.wikipedia.org/wiki/Riemannian_manifold#Riemannian_manifolds_as_metric_spaces">the usual recipe</a> and define distances in a way that obeys the triangle inequality.  Crooks calls this distance <b>thermodynamic length</b> in the special case that he considers, and he explains its physical meaning.</p>
<p>Now let me prove that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>This can be somewhat tedious if you do it by straighforwardly grinding it out&mdash;I know, I did it.  So let me show you a better way, which requires more conceptual acrobatics but less brute force.</p>
<p>The trick is to work with the <b>universal</b> statistical manifold for the measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  Namely, we take <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to be the space of <i>all</i> probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />!  This is typically an <i>infinite-dimensional</i> manifold, but that&#8217;s okay: we&#8217;re being relaxed about what counts as a manifold here.  In this case, we don&#8217;t need to write <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> for the probability distribution corresponding to the point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />.  In this case, a point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> just <i>is</i> a probability distribution on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, so we&#8217;ll just call it <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  </p>
<p>If we can prove the formulas for this universal example, they&#8217;ll automatically follow for every other example, by abstract nonsense.  Why?  Because <i>any</i> statistical manifold with measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> is the same as a manifold with a smooth map to the <i>universal</i> statistical manifold!  So, geometrical structures on the universal one <a href="http://en.wikipedia.org/wiki/Pullback_%28differential_geometry%29">&#8216;pull back&#8217;</a> to give structures on all the rest.  The Fisher information metric and the function <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> can be defined as pullbacks in this way!   So, to study them, we can just study the universal example.  </p>
<p>(If you&#8217;re familiar with &#8216;classifying spaces for bundles&#8217; or other sorts of &#8216;classifying spaces&#8217;, all this should seem awfully familiar.  It&#8217;s a standard math trick.)</p>
<p>So, let&#8217;s prove that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>by proving it in the universal example. Given any probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, and taking a nearby probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bp%7D%7Bq%7D+%3D+1+%2B+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{p}{q} = 1 + f " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is some small function.   We only need to show that <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> is zero to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  And this is pretty easy.  By definition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp%7D%7Bq%7D+%5C%2C+%5Cln%28%5Cfrac%7Bp%7D%7Bq%7D%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; &#92;frac{p}{q} &#92;, &#92;ln(&#92;frac{p}{q}) &#92;; q d &#92;omega " class="latex" /></p>
<p>or in other words,</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+%281+%2B+f%29+%5C%2C+%5Cln%281+%2B+f%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; (1 + f) &#92;, &#92;ln(1 + f) &#92;; q d &#92;omega " class="latex" /></p>
<p>We can calculate this to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and show we get zero.   But let&#8217;s actually work it out to second order, since we&#8217;ll need that later:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cln+%281+%2B+f%29+%3D+f+-+%5Cfrac%7B1%7D%7B2%7D+f%5E2+%2B+%5Ccdots+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (1 + f) = f - &#92;frac{1}{2} f^2 + &#92;cdots " class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%281+%2B+f%29+%5C%2C+%5Cln+%281%2B+f%29+%3D+f+%2B+%5Cfrac%7B1%7D%7B2%7D+f%5E2+%2B+%5Ccdots+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1 + f) &#92;, &#92;ln (1+ f) = f + &#92;frac{1}{2} f^2 + &#92;cdots " class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+S%28p%2Cq%29+%26%3D%26+%5Cint_%5COmega+%5C%3B+%281+%2B+f%29+%5C%3B+%5Cln%281+%2B+f%29+%5C%3B+q+d+%5Comega+%5C%5C+%26%3D%26+%5Cint_%5COmega+f+%5C%2C+q+d+%5Comega+%2B+%5Cfrac%7B1%7D%7B2%7D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+%2B+%5Ccdots+%5Cend%7Baligned%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{aligned} S(p,q) &amp;=&amp; &#92;int_&#92;Omega &#92;; (1 + f) &#92;; &#92;ln(1 + f) &#92;; q d &#92;omega &#92;&#92; &amp;=&amp; &#92;int_&#92;Omega f &#92;, q d &#92;omega + &#92;frac{1}{2} &#92;int_&#92;Omega f^2&#92;, q d &#92;omega + &#92;cdots &#92;end{aligned} " class="latex" /></p>
<p>Why does this vanish to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />?  It&#8217;s because <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> are both probability distributions and  <img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" />, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%281+%2B+f%29+%5C%2C+q+d%5Comega+%3D+%5Cint_%5COmega+p+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega (1 + f) &#92;, q d&#92;omega = &#92;int_&#92;Omega p d&#92;omega = 1" class="latex" /></p>
<p>but also</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+q+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega q d&#92;omega = 1" class="latex" /></p>
<p>so subtracting we see</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+f+%5C%2C+q+d%5Comega+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega f &#92;, q d&#92;omega = 0" class="latex" /></p>
<p>So, <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> vanishes to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  <i>Voil&agrave;!</i></p>
<p>Next let&#8217;s prove the more interesting formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>which relates relative entropy to the Fisher information metric. Since both sides are symmetric matrices, it suffices to show their diagonal entries agree in any coordinate system:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bii%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = g_{ii}" class="latex" /></p>
<p>Devoted followers of this series of posts will note that I keep using this trick, which takes advantage of the <a href="http://en.wikipedia.org/wiki/Polarization_identity">polarization identity</a>.  </p>
<p>To prove </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bii%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = g_{ii}" class="latex" /></p>
<p>it&#8217;s enough to consider the universal example.  We take the origin to be some probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and take <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> to be a nearby probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> which is pushed a tiny bit in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate direction.  As before we write <img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" />.  We look at the second-order term in our formula for <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2} &#92;int_&#92;Omega f^2&#92;, q d &#92;omega " class="latex" /></p>
<p>Using the usual second-order Taylor&#8217;s formula, which has a <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> built into it, we can say</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = &#92;int_&#92;Omega f^2&#92;, q d &#92;omega " class="latex" /> </p>
<p>On the other hand, our formula for the Fisher information metric gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D+%5Cleft.+%5Cint_%5COmega++%5Cpartial_i+%5Cln+p+%5C%3B+%5Cpartial_i+%5Cln+p+%5C%3B+q+d+%5Comega+%5Cright%7C_%7Bp%3Dq%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} = &#92;left. &#92;int_&#92;Omega  &#92;partial_i &#92;ln p &#92;; &#92;partial_i &#92;ln p &#92;; q d &#92;omega &#92;right|_{p=q} " class="latex" /></p>
<p>The right hand sides of the last two formulas look awfully similar!  And indeed they agree, because we can show that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cpartial_i+%5Cln+p+%5Cright%7C_%7Bp+%3D+q%7D+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;partial_i &#92;ln p &#92;right|_{p = q} = f" class="latex" /></p>
<p>How?  Well, we assumed that <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is what we get by taking <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and pushing it a little bit in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate direction; we have also written that little change as</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" /></p>
<p>for some small function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  So, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%28p%2Fq%29+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i (p/q) = f" class="latex" /></p>
<p>and thus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+p+%3D+f+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i p = f q" class="latex" /></p>
<p>and thus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cln+p+%3D+%5Cfrac%7B%5Cpartial_i+p%7D%7Bp%7D+%3D+%5Cfrac%7Bfq%7D%7Bp%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;ln p = &#92;frac{&#92;partial_i p}{p} = &#92;frac{fq}{p}" class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cpartial_i+%5Cln+p+%5Cright%7C_%7Bp%3Dq%7D+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;partial_i &#92;ln p &#92;right|_{p=q} = f" class="latex" /></p>
<p>as desired.</p>
<p>This argument may seem a little hand-wavy and nonrigorous, with words like &#8216;a little bit&#8217;.  If you&#8217;re used to taking arguments involving infinitesimal changes and translating them into calculus (or differential geometry), it should make sense.  If it doesn&#8217;t, I apologize.  It&#8217;s easy to make it more rigorous, but only at the cost of more annoying notation, which doesn&#8217;t seem good in a blog post.</p>
<h4>Boring technicalities</h4>
<p>If you&#8217;re actually the kind of person who reads a section called &#8216;boring technicalities&#8217;, I&#8217;ll admit to you that my calculations don&#8217;t make sense if the integrals diverge, or we&#8217;re dividing by zero in the ratio <img src="https://s0.wp.com/latex.php?latex=p%2Fq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q" class="latex" />.  To avoid these problems, here&#8217;s what we should do.  Fix a <a href="http://en.wikipedia.org/wiki/%CE%A3-finite_measure"><img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma" class="latex" />-finite</a> measure space <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega, d&#92;omega)" class="latex" />.  Then, define the <b>universal statistical manifold</b> to be the space <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> consisting of all probability measures that are <a href="http://en.wikipedia.org/wiki/Equivalence_%28measure_theory%29">equivalent</a> to <img src="https://s0.wp.com/latex.php?latex=d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;omega" class="latex" />, in the usual sense of measure theory.  By <a href="http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem">Radon-Nikodym</a>, we can write any such measure as <img src="https://s0.wp.com/latex.php?latex=q+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q d &#92;omega" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in L^1(&#92;Omega, d&#92;omega)" class="latex" />.  Moreover, given two of these guys, say <img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q d&#92;omega" class="latex" />, they are <a href="http://en.wikipedia.org/wiki/Absolute_continuity#Absolute_continuity_of_measures">absolutely continuous</a> with respect to each other, so we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega+%3D+%5Cfrac%7Bp%7D%7Bq%7D+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega = &#92;frac{p}{q} &#92;; q d &#92;omega " class="latex" /></p>
<p>where the ratio <img src="https://s0.wp.com/latex.php?latex=p%2Fq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q" class="latex" /> is well-defined almost everywhere and lies in <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+q+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, q d&#92;omega)" class="latex" />.  This is enough to guarantee that we&#8217;re never dividing by zero, and I think it&#8217;s enough to make sure all my integrals converge.</p>
<p>We do still need to make <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> into some sort of infinite-dimensional manifold, to justify all the derivatives.   There are various ways to approach this issue, all of which start from the fact that <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, d&#92;omega)" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Banach_space">Banach space</a>, which is about the nicest sort of infinite-dimensional manifold one could imagine.  Sitting in <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, d&#92;omega)" class="latex" /> is the hyperplane consisting of functions <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> with </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+q+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega q d&#92;omega = 1" class="latex" /></p>
<p>and this is a <a href="http://en.wikipedia.org/wiki/Banach_manifold">Banach manifold</a>.  To get <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> we need to take a subspace of that hyperplane.  If this subspace were open then <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> would be a Banach manifold in its own right.  I haven&#8217;t checked this yet, for various reasons.  </p>
<p>For one thing, there&#8217;s a nice theory of <a href="http://en.wikipedia.org/wiki/Diffeology">&#8216;diffeological spaces&#8217;</a>, which generalize manifolds.  Every Banach manifold is a diffeological space, and every subset of a diffeological space is again a diffeological space.  For many purposes we don&#8217;t need our &#8216;statistical manifolds&#8217; to be manifolds: diffeological spaces will do just fine.  This is one reason why I&#8217;m being pretty relaxed here about what counts as a &#8216;manifold&#8217;.</p>
<p>For another, I know that people have worked out a lot of this stuff, so I can just look things up when I need to.  And so can you!  This book is a good place to start:</p>
<p>&bull; Paolo Gibilisco, Eva Riccomagno, Maria Piera Rogantin and Henry P. Wynn, <i>Algebraic and Geometric Methods in Statistics</i>, Cambridge U. Press, Cambridge, 2009.</p>
<p>I find the chapters by Raymond Streater especially congenial.  For the technical issue I&#8217;m talking about now it&#8217;s worth reading section 14.2, &#8220;Manifolds modelled by Orlicz spaces&#8221;, which tackles the problem of constructing a universal statistical manifold in a more sophisticated way than I&#8217;ve just done.  And in chapter 15, &#8220;The Banach manifold of quantum states&#8221;, he tackles the quantum version!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comments">25 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;7)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2467 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-2467">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" rel="bookmark">R&eacute;nyi Entropy and Free&nbsp;Energy</a></h2>
				<small>10 February, 2011</small><br />


				<div class="entry">
					<p>I want to keep telling you about information geometry&#8230; but I got sidetracked into thinking about something slightly different, thanks to some fascinating discussions here at the CQT.  </p>
<p>There are a lot of people interested in entropy here, so some of us &mdash; <a href="http://arxiv.org/find/all/1/all:+AND+oscar+dahlsten/0/1/0/all/0/1">Oscar Dahlsten</a>, <a href="http://arxiv.org/find/all/1/all:+AND+mile+gu/0/1/0/all/0/1">Mile Gu</a>, <a href="http://arxiv.org/find/all/1/all:+AND+elisabeth+rieper/0/1/0/all/0/1">Elisabeth Rieper</a>, <a href="http://arxiv.org/find/all/1/all:+AND+wonmin+son/0/1/0/all/0/1">Wonmin Son</a> and me &mdash; decided to start meeting more or less regularly.  I call it the Entropy Club.  I&#8217;m learning a lot of wonderful things, and I hope to tell you about them someday.  But for now, here&#8217;s a little idea I came up with, triggered by our conversations:</p>
<p>&bull; John Baez, <a href="http://arxiv.org/abs/1102.2098">R&eacute;nyi entropy and free energy</a>.</p>
<p>In 1960, Alfred R&eacute;nyi defined a generalization of the usual Shannon entropy that depends on a parameter.  If <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a probability distribution on a finite set, its <b><a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a></b> of order <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is defined to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Cbeta+%3D+%5Cfrac%7B1%7D%7B1+-+%5Cbeta%7D+%5Cln+%5Csum_i+p_i%5E%5Cbeta+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;beta = &#92;frac{1}{1 - &#92;beta} &#92;ln &#92;sum_i p_i^&#92;beta } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+%5Cbeta+%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le &#92;beta &lt; &#92;infty" class="latex" />.   This looks pretty weird at first, and we need <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cne+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;ne 1" class="latex" /> to avoid dividing by zero, but you can show that the R&eacute;nyi entropy approaches the <b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a></b> as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> approaches<br />
1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7B%5Cbeta+%5Cto+1%7D+H_%5Cbeta+%3D+-%5Csum_%7Bi%7D+p_i+%5Cln+p_i+.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lim_{&#92;beta &#92;to 1} H_&#92;beta = -&#92;sum_{i} p_i &#92;ln p_i . " class="latex" /></p>
<p>(A fun puzzle, which I leave to you.)  So, it&#8217;s customary to define <img src="https://s0.wp.com/latex.php?latex=H_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1" class="latex" /> to be the Shannon entropy&#8230; and then the R&eacute;nyi entropy generalizes the Shannon entropy by allowing an adjustable parameter <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />. </p>
<p>But what does it <i>mean?</i></p>
<p>If you ask people what&#8217;s good about the R&eacute;nyi entropy, they&#8217;ll usually say: it&#8217;s additive!  In other words, when you combine two independent probability distributions into a single one, their R&eacute;nyi entropies add.  And that&#8217;s true &mdash; but there are other quantities that have the same property.  So I wanted a better way to think about R&eacute;nyi entropy, and here&#8217;s what I&#8217;ve come up with so far.</p>
<p>Any probability distribution can be seen as the state of thermal equilibrium for some Hamiltonian at some fixed temperature, say <img src="https://s0.wp.com/latex.php?latex=T+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = 1" class="latex" />.  And that Hamiltonian is unique.  Starting with that Hamiltonian, we can then compute the free energy <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> at any temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />, and up to a certain factor this free energy turns out to be the R&eacute;nyi entropy <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" />.  More precisely:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%281+-+T%29+H_%5Cbeta.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = (1 - T) H_&#92;beta. " class="latex" /></p>
<p>So, up to the fudge factor <img src="https://s0.wp.com/latex.php?latex=1+-+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 - T" class="latex" />, R&eacute;nyi entropy is the same as free energy. It seems like a good thing to know &mdash; but I haven&#039;t seen anyone say it anywhere!  Have you?</p>
<p>Let me show you why it&#8217;s true &mdash; the proof is pathetically simple.  We start with our probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />.  We can always write</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{- E_i} " class="latex" /></p>
<p>for some real numbers <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" />.   Let&#8217;s think of these numbers <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> as energies.  Then the state of thermal equilibrium, also known as the <b><a href="http://en.wikipedia.org/wiki/Canonical_ensemble">canonical ensemble</a></b> or <b><a href="http://en.wikipedia.org/wiki/Gibbs_state">Gibbs state</a></b> at inverse temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Be%5E%7B-+%5Cbeta+E_i%7D%7D%7BZ%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{e^{- &#92;beta E_i}}{Z} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is the <b><a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29#Definition">partition function</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_i+e%5E%7B-%5Cbeta+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_i e^{-&#92;beta E_i} " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=Z+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = 1" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />, the Gibbs state reduces to our original probability distribution at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />.</p>
<p>Now in thermodynamics, the quantity</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-+%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = - &#92;frac{1}{&#92;beta} &#92;ln Z " class="latex" /></p>
<p>is called the <b><a href="http://en.wikipedia.org/wiki/Helmholtz_free_energy#Relation_to_the_partition_function">free energy</a></b>.  It&#8217;s important, because it equals the total expected energy of our system, minus the energy in the form of heat.   Roughly speaking, it&#8217;s the energy that you can use.</p>
<p>Let&#8217;s see how the R&eacute;nyi entropy is related to the free energy.  The proof is a trivial calculation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Cbeta+F+%3D+%5Cln+Z+%3D+%5Cln+%5Csum_%7Bi+%5Cin+X%7D+e%5E%7B-%5Cbeta+E_i%7D+%3D+%5Cln+%5Csum_%7Bi+%5Cin+X%7D+p_i%5E%5Cbeta+%3D+%281+-+%5Cbeta%29+H_%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;beta F = &#92;ln Z = &#92;ln &#92;sum_{i &#92;in X} e^{-&#92;beta E_i} = &#92;ln &#92;sum_{i &#92;in X} p_i^&#92;beta = (1 - &#92;beta) H_&#92;beta " class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta+%3D+-++%5Cfrac%7B%5Cbeta%7D%7B1+-+%5Cbeta%7D+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta = -  &#92;frac{&#92;beta}{1 - &#92;beta} F" class="latex" /></p>
<p>at least for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cne+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;ne 1" class="latex" />.  But you can also check that both sides of this equation have well-defined limits as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to 1" class="latex" />.  </p>
<p>The relation between free energy and R&eacute;nyi entropy looks even neater if we solve for <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> and write the answer using <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%281+-+T%29H_%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = (1 - T)H_&#92;beta " class="latex" /></p>
<p>So, what&#8217;s this fact good for?  I&#8217;m not sure yet!  In my paper, I combine it with this equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+E+%5Crangle+-+T+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle E &#92;rangle - T S " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle" class="latex" /> is the expected energy in the Gibbs state at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle+%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Csum_i+E_i+%5C%2C+e%5E%7B-%5Cbeta+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle = &#92;frac{1}{Z} &#92;sum_i E_i &#92;, e^{-&#92;beta E_i} " class="latex" /></p>
<p>while <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the usual Shannon entropy of this Gibbs state.  I also show that all this stuff works quantum-mechanically as well as classically.  But so far, it seems the main benefit is that R&eacute;nyi entropy has become a lot less mysterious.  It&#8217;s not a mutant version of Shannon entropy: it&#8217;s just a familiar friend in disguise.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comments">138 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" rel="bookmark" title="Permanent Link to R&eacute;nyi Entropy and Free&nbsp;Energy">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/5/?s=information+and+entropy" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/page/3/?s=information+and+entropy" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see whatâ€™s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkinâ€™s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/page/4/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jksOwjAMRC9EarWLSl0gzpImVuXQfLAd2t6eVAIJsWA1nrGePbAV43JSTApBwOOTHJa9C3KBr1Wspqx1oSRgfaRkZssQrShym4yydXf5hdq98KjIx1u6rbgcTeG8H4axZaIfhpJbq0c5oWYxzui79uhPkY38giogdRbHVJRyOjvc4rUfh2nqh2Hswwtq4FN0'></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTY5VzhXTFdQUTZueWpTWF9xeWZyOV9iRS9FUUV1SmZSaV9VNDlkOHNVLEpvUVd2TGRqamlTXXVXJl9ZYVNIL3xNaFEya1pQfkhCblR0WlQ2bHdDQk5pRyZ6TWp5Ykh4bU95NlNhYUUlRUNKZHYlRWlZaGFNcURMTnUzQUxqK0Riai9fdHxEOFZ5Lnl5aH5yWmlyL2drcHF5WkxLUUNkZTlqdG5KfmI1QyxWVnJiWl9CZ1V4JXRFPWVINVVncV9mX2o0OVFa'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>