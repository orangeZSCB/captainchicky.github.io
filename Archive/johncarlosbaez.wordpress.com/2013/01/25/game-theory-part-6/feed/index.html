<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Game Theory (Part 6)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/</link>
	<description></description>
	<lastBuildDate>Wed, 30 Jan 2013 16:31:02 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: romain		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24964</link>

		<dc:creator><![CDATA[romain]]></dc:creator>
		<pubDate>Wed, 30 Jan 2013 16:31:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=14385#comment-24964</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24934&quot;&gt;Todd Trimble&lt;/a&gt;.

That&#039;s where the concept of Pareto-optimality shows its power. In Yudkowsky&#039;s game, both (-10000, 1, 1, 1,...., 1) and (0, 0, 0, 0,...., 0) are Pareto-optimal, and there&#039;s no way to say that any case is better without adding extra structure to the problem.

The concept of Pareto-optimality is the furthest one can go in the direction of &quot;maximizing total happiness&quot;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24934">Todd Trimble</a>.</p>
<p>That&#8217;s where the concept of Pareto-optimality shows its power. In Yudkowsky&#8217;s game, both (-10000, 1, 1, 1,&#8230;., 1) and (0, 0, 0, 0,&#8230;., 0) are Pareto-optimal, and there&#8217;s no way to say that any case is better without adding extra structure to the problem.</p>
<p>The concept of Pareto-optimality is the furthest one can go in the direction of &#8220;maximizing total happiness&#8221;.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24938</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 30 Jan 2013 02:17:25 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=14385#comment-24938</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24934&quot;&gt;Todd Trimble&lt;/a&gt;.

I&#039;m scared of anyone who believes in &#039;maximizing total utility&#039; to the point of willingness to torture one person to prevent a very large number of people from getting dust specks in their eyes.  I&#039;m even more scared of them if they believe in &#039;maximizing expected total utility&#039;, so that they&#039;d be willing to torture someone to prevent a very very large number of people from having a &lt;i&gt;tiny chance&lt;/i&gt; of getting dust specks in their eyes.  

But &#039;total utility&#039; is such an ill-defined concept that we can also reject the principle of &#039;maximizing total utility&#039; on general theoretical grounds, not just on the grounds that it feels wrong.  ]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24934">Todd Trimble</a>.</p>
<p>I&#8217;m scared of anyone who believes in &#8216;maximizing total utility&#8217; to the point of willingness to torture one person to prevent a very large number of people from getting dust specks in their eyes.  I&#8217;m even more scared of them if they believe in &#8216;maximizing expected total utility&#8217;, so that they&#8217;d be willing to torture someone to prevent a very very large number of people from having a <i>tiny chance</i> of getting dust specks in their eyes.  </p>
<p>But &#8216;total utility&#8217; is such an ill-defined concept that we can also reject the principle of &#8216;maximizing total utility&#8217; on general theoretical grounds, not just on the grounds that it feels wrong.  </p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Todd Trimble		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24934</link>

		<dc:creator><![CDATA[Todd Trimble]]></dc:creator>
		<pubDate>Tue, 29 Jan 2013 22:02:50 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=14385#comment-24934</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24875&quot;&gt;John Baez&lt;/a&gt;.

I&#039;m pretty sure I&#039;m not a utilitarian either, if that means we sum &quot;total happiness&quot; (whatever the hell that would mean!) over all people and over all time, and we reach the same conclusion as Yudkowsky does &lt;a href=&quot;http://lesswrong.com/lw/kn/torture_vs_dust_specks/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (scroll down a bit in the comments to see his conclusion). That this &quot;summing of happiness&quot; is even a meaningful operation is very far from clear (and yet it seems to be accepted by E.Y. without, er, &quot;a blink of the eye&quot;).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24875">John Baez</a>.</p>
<p>I&#8217;m pretty sure I&#8217;m not a utilitarian either, if that means we sum &#8220;total happiness&#8221; (whatever the hell that would mean!) over all people and over all time, and we reach the same conclusion as Yudkowsky does <a href="http://lesswrong.com/lw/kn/torture_vs_dust_specks/" rel="nofollow">here</a> (scroll down a bit in the comments to see his conclusion). That this &#8220;summing of happiness&#8221; is even a meaningful operation is very far from clear (and yet it seems to be accepted by E.Y. without, er, &#8220;a blink of the eye&#8221;).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: romain		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24920</link>

		<dc:creator><![CDATA[romain]]></dc:creator>
		<pubDate>Tue, 29 Jan 2013 16:35:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=14385#comment-24920</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24875&quot;&gt;John Baez&lt;/a&gt;.

Thid idea of maximizing happiness over time reminds me of a talk by Yonatan Loewenstein about intertemporal game. It&#039;s about games where players are &quot;you&quot; but at different times. 

For example, if you have a dental cavity that is painful (daily payoff=-2) but not as much as going to the dentist to get it cured (daily payoff=-10), then the &quot;present you&quot; does not want to go the dentist. The &quot;next moment you&quot; may wish you had done it before but won&#039;t go to the dentist either. 

And you will end up always postponing the appointment at the dentist.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24875">John Baez</a>.</p>
<p>Thid idea of maximizing happiness over time reminds me of a talk by Yonatan Loewenstein about intertemporal game. It&#8217;s about games where players are &#8220;you&#8221; but at different times. </p>
<p>For example, if you have a dental cavity that is painful (daily payoff=-2) but not as much as going to the dentist to get it cured (daily payoff=-10), then the &#8220;present you&#8221; does not want to go the dentist. The &#8220;next moment you&#8221; may wish you had done it before but won&#8217;t go to the dentist either. </p>
<p>And you will end up always postponing the appointment at the dentist.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24875</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 28 Jan 2013 20:58:30 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=14385#comment-24875</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24790&quot;&gt;Arrow&lt;/a&gt;.

I&#039;m not a utilitarian, but people who are face some challenges trying to make this philosophy precise.  When we say &quot;maximum total happiness&quot;, do we mean &lt;i&gt;summed over people and integrated over time&lt;/i&gt;?   If so, being very happy for a while and then starving to death might not be our goal.  What about beings that aren&#039;t people?  Etcetera, etcetera---I think there are papers in philosophy journals carefully debating all these questions.   

It&#039;s very hard to even define happiness in a way that justifies summing it over people.  What if I claim I&#039;m always twice as happy as you under any conditions?  If true, this means my happiness contributes twice as much to the total than yours, so it&#039;s more important to please me than to please you, if we&#039;re trying to maximize this total!  If false, how do we show it&#039;s false?

The &lt;a href=&quot;http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem&quot; rel=&quot;nofollow&quot;&gt;von Neumann--Morgenstern utility theorem&lt;/a&gt; gives conditions under which we can numerically compute someone&#039;s happiness (or strictly speaking, utility).  However, the result is only well-defined up to an additive constant and a multiplicative constant.  So, it doesn&#039;t let us determine whether I am really twice as happy as you!

Furthermore, the conditions of this theorem are rarely true in ordinary life... much to the secret shame of economists.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24790">Arrow</a>.</p>
<p>I&#8217;m not a utilitarian, but people who are face some challenges trying to make this philosophy precise.  When we say &#8220;maximum total happiness&#8221;, do we mean <i>summed over people and integrated over time</i>?   If so, being very happy for a while and then starving to death might not be our goal.  What about beings that aren&#8217;t people?  Etcetera, etcetera&#8212;I think there are papers in philosophy journals carefully debating all these questions.   </p>
<p>It&#8217;s very hard to even define happiness in a way that justifies summing it over people.  What if I claim I&#8217;m always twice as happy as you under any conditions?  If true, this means my happiness contributes twice as much to the total than yours, so it&#8217;s more important to please me than to please you, if we&#8217;re trying to maximize this total!  If false, how do we show it&#8217;s false?</p>
<p>The <a href="http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem" rel="nofollow">von Neumann&#8211;Morgenstern utility theorem</a> gives conditions under which we can numerically compute someone&#8217;s happiness (or strictly speaking, utility).  However, the result is only well-defined up to an additive constant and a multiplicative constant.  So, it doesn&#8217;t let us determine whether I am really twice as happy as you!</p>
<p>Furthermore, the conditions of this theorem are rarely true in ordinary life&#8230; much to the secret shame of economists.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Arrow		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24790</link>

		<dc:creator><![CDATA[Arrow]]></dc:creator>
		<pubDate>Sat, 26 Jan 2013 22:03:22 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=14385#comment-24790</guid>

					<description><![CDATA[So to maximize utility/happiness in real life every rational agent should get a brain implant in their reward center and proceed to self stimulate:

http://en.wikipedia.org/wiki/Brain_stimulation_reward

There might be a problem since according to that article an overabundance of happiness can lead to starvation and death. I&#039;m not sure if it was ever tested in humans though. If true and the &quot;maximum total happiness&quot; is the goal the implant should probably be limited somehow, for example it could require certain minimal level of blood sugar to work.

That makes me wonder if and when such technology becomes popular. It seems to be within our technological capabilities now but I have yet to see an internet ad for a brain implant. Of course if widespread such practice would have far reaching consequences.]]></description>
			<content:encoded><![CDATA[<p>So to maximize utility/happiness in real life every rational agent should get a brain implant in their reward center and proceed to self stimulate:</p>
<p><a href="http://en.wikipedia.org/wiki/Brain_stimulation_reward" rel="nofollow ugc">http://en.wikipedia.org/wiki/Brain_stimulation_reward</a></p>
<p>There might be a problem since according to that article an overabundance of happiness can lead to starvation and death. I&#8217;m not sure if it was ever tested in humans though. If true and the &#8220;maximum total happiness&#8221; is the goal the implant should probably be limited somehow, for example it could require certain minimal level of blood sugar to work.</p>
<p>That makes me wonder if and when such technology becomes popular. It seems to be within our technological capabilities now but I have yet to see an internet ad for a brain implant. Of course if widespread such practice would have far reaching consequences.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Frederik De Roo		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/01/25/game-theory-part-6/#comment-24780</link>

		<dc:creator><![CDATA[Frederik De Roo]]></dc:creator>
		<pubDate>Sat, 26 Jan 2013 17:04:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=14385#comment-24780</guid>

					<description><![CDATA[&lt;blockquote&gt;
So: we say a rational agent does the best possible job of maximizing their payoff given the information they have. But in economics, this payoff is often called utility. 
&lt;/blockquote&gt;

For the amusement of the class, this reminds me of a line in &quot;Gut Feelings&quot; by G. Gigerenzer (also recommended by Tim van Beek in our &lt;a href=&quot;http://www.azimuthproject.org/azimuth/show/Recommended+reading&quot; rel=&quot;nofollow&quot;&gt;Recommended reading&lt;/a&gt;)

&lt;blockquote&gt;
A professor from Columbia University was struggling over whether to accept an offer from a rival university or to stay. His colleague took him aside and said, &quot;Just maximize your expected utility -- you always write about doing this.&quot; Exasperated, the professor responded, &quot;Come on, this is serious.&quot;
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<blockquote><p>
So: we say a rational agent does the best possible job of maximizing their payoff given the information they have. But in economics, this payoff is often called utility.
</p></blockquote>
<p>For the amusement of the class, this reminds me of a line in &#8220;Gut Feelings&#8221; by G. Gigerenzer (also recommended by Tim van Beek in our <a href="http://www.azimuthproject.org/azimuth/show/Recommended+reading" rel="nofollow">Recommended reading</a>)</p>
<blockquote><p>
A professor from Columbia University was struggling over whether to accept an offer from a rival university or to stay. His colleague took him aside and said, &#8220;Just maximize your expected utility &#8212; you always write about doing this.&#8221; Exasperated, the professor responded, &#8220;Come on, this is serious.&#8221;
</p></blockquote>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
