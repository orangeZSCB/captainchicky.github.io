<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Why Most Published Research Findings Are False	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/</link>
	<description></description>
	<lastBuildDate>Sun, 13 Oct 2013 18:53:21 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33559</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 13 Oct 2013 18:53:21 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33559</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33553&quot;&gt;Roger Witte&lt;/a&gt;.

Yes, but note that this study in &lt;i&gt;Science&lt;/i&gt; did not compare open access journals to other journals---like, umm, &lt;i&gt;Science&lt;/i&gt;.  

This is bad, because if there are problems with &lt;i&gt;all&lt;/i&gt; journals, or most journals, the study will make these seem like problems with open access journals!  By analogy, consider a study of black American men who beat their wives, that does not bother to compare the behavior of white men.  Would that be wise?

There&#039;s a more detailed critique here:

&#8226; Mike Taylor, &lt;a href=&quot;http://svpow.com/2013/10/07/anti-tutorial-how-to-design-and-execute-a-really-bad-study/&quot; rel=&quot;nofollow&quot;&gt;Anti-tutorial: how to design and execute a really bad study&lt;/a&gt;, &lt;i&gt;Sauropod Vertebra Picture of the Week&lt;/i&gt;, 7 October 2013.

Quoting:

&lt;blockquote&gt;
Suppose, hypothetically, that you worked for an organisation whose nominal goal is the advancement of science, but which has mutated into a highly profitable subscription-based publisher. And suppose you wanted to construct a study that showed the alternative — open-access publishing — is inferior.

What would you do?

You might decide that a good way to test publishers is by sending them an obviously flawed paper and seeing whether their peer-review weeds it out.

But you wouldn’t want to risk showing up subscription publishers. So the first thing you’d do is decide up front not to send your flawed paper to any subscription journals. You might justify this by saying something like “the turnaround time for traditional journals is usually months and sometimes more than a year. How could I ever pull off a representative sample?&quot;.

Next, you’d need to choose a set of open-access journals to send it to. At this point, you would carefully avoid consulting the membership list of the Open Access Scholarly Publishers Association, since that list has specific criteria and members have to adhere to a code of conduct. You don’t want the good open-access journals — they won’t give you the result you want.

Instead, you would draw your list of publishers from the much broader Directory of Open Access Journals, since that started out as a catalogue rather than a whitelist. (That’s changing, and journals are now being cut from the list faster than they’re being added, but lots of old entries are still in place.)

Then, to help remove many of the publishers that are in the game only to advance research, you’d trim out all the journals that don’t levy an article processing charge.

But the resulting list might still have an inconveniently high proportion of quality journals. So you would bring down the quality by adding in known-bad publishers from Beall’s list of predatory open-access publishers.

To make sure you get a good, impressive result that will have a lot of “impact”, you might find it necessary to &lt;a href=&quot;http://gunther-eysenbach.blogspot.ca/2013/10/unscientific-spoof-paper-accepted-by.html&quot; rel=&quot;nofollow&quot;&gt;discard some inconvenient data points&lt;/a&gt;, omitting from the results some open-access journals that rejected the paper.

Now you have your results, it’s time to spin them. Use sweeping, unsupported generalisations like “Most of the players are murky. The identity and location of the journals’ editors, as well as the financial workings of their publishers, are often purposefully obscured.”
&lt;/blockquote&gt;

The link is to another critique:

&#8226; Gunther Eysenbach, &lt;a href=&quot;http://gunther-eysenbach.blogspot.ca/2013/10/unscientific-spoof-paper-accepted-by.html&quot; rel=&quot;nofollow&quot;&gt;Unscientific spoof paper accepted by 157 &quot;black sheep&quot; open access journals - but the Bohannon study has severe flaws itself&lt;/a&gt;, 5 October 2013.

Clearly we need bloggers to keep an on the supposedly serious researchers who publish in &lt;i&gt;Science&lt;/i&gt;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33553">Roger Witte</a>.</p>
<p>Yes, but note that this study in <i>Science</i> did not compare open access journals to other journals&#8212;like, umm, <i>Science</i>.  </p>
<p>This is bad, because if there are problems with <i>all</i> journals, or most journals, the study will make these seem like problems with open access journals!  By analogy, consider a study of black American men who beat their wives, that does not bother to compare the behavior of white men.  Would that be wise?</p>
<p>There&#8217;s a more detailed critique here:</p>
<p>&bull; Mike Taylor, <a href="http://svpow.com/2013/10/07/anti-tutorial-how-to-design-and-execute-a-really-bad-study/" rel="nofollow">Anti-tutorial: how to design and execute a really bad study</a>, <i>Sauropod Vertebra Picture of the Week</i>, 7 October 2013.</p>
<p>Quoting:</p>
<blockquote><p>
Suppose, hypothetically, that you worked for an organisation whose nominal goal is the advancement of science, but which has mutated into a highly profitable subscription-based publisher. And suppose you wanted to construct a study that showed the alternative — open-access publishing — is inferior.</p>
<p>What would you do?</p>
<p>You might decide that a good way to test publishers is by sending them an obviously flawed paper and seeing whether their peer-review weeds it out.</p>
<p>But you wouldn’t want to risk showing up subscription publishers. So the first thing you’d do is decide up front not to send your flawed paper to any subscription journals. You might justify this by saying something like “the turnaround time for traditional journals is usually months and sometimes more than a year. How could I ever pull off a representative sample?&#8221;.</p>
<p>Next, you’d need to choose a set of open-access journals to send it to. At this point, you would carefully avoid consulting the membership list of the Open Access Scholarly Publishers Association, since that list has specific criteria and members have to adhere to a code of conduct. You don’t want the good open-access journals — they won’t give you the result you want.</p>
<p>Instead, you would draw your list of publishers from the much broader Directory of Open Access Journals, since that started out as a catalogue rather than a whitelist. (That’s changing, and journals are now being cut from the list faster than they’re being added, but lots of old entries are still in place.)</p>
<p>Then, to help remove many of the publishers that are in the game only to advance research, you’d trim out all the journals that don’t levy an article processing charge.</p>
<p>But the resulting list might still have an inconveniently high proportion of quality journals. So you would bring down the quality by adding in known-bad publishers from Beall’s list of predatory open-access publishers.</p>
<p>To make sure you get a good, impressive result that will have a lot of “impact”, you might find it necessary to <a href="http://gunther-eysenbach.blogspot.ca/2013/10/unscientific-spoof-paper-accepted-by.html" rel="nofollow">discard some inconvenient data points</a>, omitting from the results some open-access journals that rejected the paper.</p>
<p>Now you have your results, it’s time to spin them. Use sweeping, unsupported generalisations like “Most of the players are murky. The identity and location of the journals’ editors, as well as the financial workings of their publishers, are often purposefully obscured.”
</p></blockquote>
<p>The link is to another critique:</p>
<p>&bull; Gunther Eysenbach, <a href="http://gunther-eysenbach.blogspot.ca/2013/10/unscientific-spoof-paper-accepted-by.html" rel="nofollow">Unscientific spoof paper accepted by 157 &#8220;black sheep&#8221; open access journals &#8211; but the Bohannon study has severe flaws itself</a>, 5 October 2013.</p>
<p>Clearly we need bloggers to keep an on the supposedly serious researchers who publish in <i>Science</i>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Roger Witte		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33553</link>

		<dc:creator><![CDATA[Roger Witte]]></dc:creator>
		<pubDate>Sun, 13 Oct 2013 08:19:59 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33553</guid>

					<description><![CDATA[This article showing lack of rigour in open access journals gives further cause for alarm http://www.sciencemag.org/content/342/6154/60.full]]></description>
			<content:encoded><![CDATA[<p>This article showing lack of rigour in open access journals gives further cause for alarm <a href="http://www.sciencemag.org/content/342/6154/60.full" rel="nofollow ugc">http://www.sciencemag.org/content/342/6154/60.full</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Are Most Published Research Findings False? &#124; the endo spot		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33445</link>

		<dc:creator><![CDATA[Are Most Published Research Findings False? &#124; the endo spot]]></dc:creator>
		<pubDate>Tue, 01 Oct 2013 19:22:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33445</guid>

					<description><![CDATA[[&#8230;] John Baez does a nice job of simplifying the corollaries, and there is some good discussion to follow it on his blog. [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] John Baez does a nice job of simplifying the corollaries, and there is some good discussion to follow it on his blog. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Trurl		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33365</link>

		<dc:creator><![CDATA[Trurl]]></dc:creator>
		<pubDate>Thu, 26 Sep 2013 17:22:53 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33365</guid>

					<description><![CDATA[related:
http://www.jstor.org/stable/2138833]]></description>
			<content:encoded><![CDATA[<p>related:<br />
<a href="http://www.jstor.org/stable/2138833" rel="nofollow ugc">http://www.jstor.org/stable/2138833</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Graham Jones		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33336</link>

		<dc:creator><![CDATA[Graham Jones]]></dc:creator>
		<pubDate>Wed, 25 Sep 2013 10:18:06 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33336</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33308&quot;&gt;David Ketcheson&lt;/a&gt;.

Yes this does look good, but people have been saying things like this for a long time without much changing. 

This article Lost Branches on the Tree of Life 
http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001636
documents the sad state of affairs in my area. For example, BEAST is a very popular program for phylogenetic analysis. The user must supply an xml file as input, and that file can be included in supplementary material when publishing. But

&quot;Our survey of publications that implemented BEAST revealed that only 11 out of 100 (11%) examined studies provided access to the underlying xml input file, which is critical for reproducing BEAST results.&quot;

and

&quot;Failure on a Massive Scale

&quot;Our findings indicate that while some journals (e.g., Evolution, Nature, PLOS Biology, Systematic Biology) currently require nucleotide sequence alignments, associated tree files, and other relevant data to be deposited in public repositories, most journals do not have these requirements; resultantly, the systematics community is doing a poor job of making the actual datasets available. More troublesome perhaps is that the situation has barely improved over the 12 years covered in this study (Figures 1 and 2). In addition, when data are deposited, they often do not include critical information such as what was actually included in data alignments (e.g., what characters were excluded, full taxon names; see Table S1 and Figure S1). Without accurate details describing how alignments were implemented, it is difficult or perhaps impossible to faithfully reproduce the study results. Additionally, parameters for the program BEAST are rarely made available for scrutiny. Lastly, in many cases when data were not deposited to TreeBASE, the authors indicated that the data could be obtained directly from them; however, our survey indicates this is typically not the case (only ~40% of authors even respond, and of these only a small percent actually provide the requested data)—hence, many alignments and analysis parameters seem to be lost forever.&quot;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33308">David Ketcheson</a>.</p>
<p>Yes this does look good, but people have been saying things like this for a long time without much changing. </p>
<p>This article Lost Branches on the Tree of Life<br />
<a href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001636" rel="nofollow ugc">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001636</a><br />
documents the sad state of affairs in my area. For example, BEAST is a very popular program for phylogenetic analysis. The user must supply an xml file as input, and that file can be included in supplementary material when publishing. But</p>
<p>&#8220;Our survey of publications that implemented BEAST revealed that only 11 out of 100 (11%) examined studies provided access to the underlying xml input file, which is critical for reproducing BEAST results.&#8221;</p>
<p>and</p>
<p>&#8220;Failure on a Massive Scale</p>
<p>&#8220;Our findings indicate that while some journals (e.g., Evolution, Nature, PLOS Biology, Systematic Biology) currently require nucleotide sequence alignments, associated tree files, and other relevant data to be deposited in public repositories, most journals do not have these requirements; resultantly, the systematics community is doing a poor job of making the actual datasets available. More troublesome perhaps is that the situation has barely improved over the 12 years covered in this study (Figures 1 and 2). In addition, when data are deposited, they often do not include critical information such as what was actually included in data alignments (e.g., what characters were excluded, full taxon names; see Table S1 and Figure S1). Without accurate details describing how alignments were implemented, it is difficult or perhaps impossible to faithfully reproduce the study results. Additionally, parameters for the program BEAST are rarely made available for scrutiny. Lastly, in many cases when data were not deposited to TreeBASE, the authors indicated that the data could be obtained directly from them; however, our survey indicates this is typically not the case (only ~40% of authors even respond, and of these only a small percent actually provide the requested data)—hence, many alignments and analysis parameters seem to be lost forever.&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33324</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 24 Sep 2013 19:13:23 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33324</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33308&quot;&gt;David Ketcheson&lt;/a&gt;.

That looks good!  Let me quote some, since most people will be too busy to click the link:

&lt;blockquote&gt;

For reproducibility to be fostered and maintained, workshop participants agreed that cultural changes need to take place within the field of computationally based research that instill the open and transparent communication of results as a default. Such a mode will increase productivity---less time wasted in trying to recover output that was lost or misplaced, less time wasted trying to double-check results in the manuscript with computational output, and less time wasted trying to determine whether other published results (or even their own) are truly reliable. Open access to any data used in the research and to both primary and auxiliary source code also provides the basis for research to be communicated transparently creating the opportunity to build upon previous work, in a similar spirit as open software provided the basis for Linux. Code and data should be made available under open licensing terms as discussed in Appendix F. This practice enables researchers both to benefit more deeply from the creative energies of the global community and to participate more fully in it. Most great science is built upon the discoveries of preceding generations and open access to the data and code associated with published computational science allows this tradition to continue. Researchers should be encouraged to recognize the potential benefits of openness and reproducibility.

It is also important to recognize that there are costs and barriers to shifting to a practice of reproducible research, particularly when the culture does not recognize the value of developing this new paradigm or the effort that can be required to develop or learn to use suitable tools. This is of particular concern to young people who need to earn tenure or secure a permanent position. To encourage more movement towards openness and reproducibility, it is crucial that such work be acknowledged and rewarded. The current
system, which places a great deal of emphasis on the number of journal publications and virtually none on reproducibility (and often too little on related computational issues such as
verification and validation), penalizes authors who spend extra time on a publication rather than doing the minimum required to meet current community standards. Appropriate credit should given for code and data contributions including an expectation of citation. Another suggestion is to instantiate yearly award from journals and/or professional societies, to be awarded to investigators for excellent reproducible practice. Such awards are highly motivating to young researchers in particular, and potentially could result in a sea change in attitudes. These awards could also be cross-conference and journal awards; the collected list of award recipients would both increase the visibility of researchers following good practices and provide examples for others.  More generally, it is unfortunate that software development and data curation are often discounted in the scientific community, and programming is treated as something to spend as little time on as possible. Serious scientists are not expected to carefully test code, let alone document it, in the same way they are trained to properly use other tools or document their experiments. It has been said in some quarters that writing a large piece of software is akin to building infrastructure such as a telescope rather than a creditable scientific contribution, and not worthy of tenure or comparable status at a research laboratory. This attitude must change if we are to encourage young researchers to specialize in computing skills that are essential for the future of mathematical and scientific research. We believe the more proper analog to a large scale scientific instrument is a supercomputer, whereas software reflects the intellectual engine that makes the supercomputers useful, and has scientific value beyond the hardware itself. Important computational results, accompanied
by verification, validation, and reproducibility, should be accorded with honors similar to a strong publication record.

Several tools were presented at the workshop that enable users to write and publish documents that integrate the text and figures seen in reports with code and data used to generate both text and graphical results, such as IPython, Sage notebooks, Lepton, knitr,
and Vistrails. Slides for these talks are available on the wiki [1] and Appendix E discusses these and other tools in detail.

&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33308">David Ketcheson</a>.</p>
<p>That looks good!  Let me quote some, since most people will be too busy to click the link:</p>
<blockquote>
<p>For reproducibility to be fostered and maintained, workshop participants agreed that cultural changes need to take place within the field of computationally based research that instill the open and transparent communication of results as a default. Such a mode will increase productivity&#8212;less time wasted in trying to recover output that was lost or misplaced, less time wasted trying to double-check results in the manuscript with computational output, and less time wasted trying to determine whether other published results (or even their own) are truly reliable. Open access to any data used in the research and to both primary and auxiliary source code also provides the basis for research to be communicated transparently creating the opportunity to build upon previous work, in a similar spirit as open software provided the basis for Linux. Code and data should be made available under open licensing terms as discussed in Appendix F. This practice enables researchers both to benefit more deeply from the creative energies of the global community and to participate more fully in it. Most great science is built upon the discoveries of preceding generations and open access to the data and code associated with published computational science allows this tradition to continue. Researchers should be encouraged to recognize the potential benefits of openness and reproducibility.</p>
<p>It is also important to recognize that there are costs and barriers to shifting to a practice of reproducible research, particularly when the culture does not recognize the value of developing this new paradigm or the effort that can be required to develop or learn to use suitable tools. This is of particular concern to young people who need to earn tenure or secure a permanent position. To encourage more movement towards openness and reproducibility, it is crucial that such work be acknowledged and rewarded. The current<br />
system, which places a great deal of emphasis on the number of journal publications and virtually none on reproducibility (and often too little on related computational issues such as<br />
verification and validation), penalizes authors who spend extra time on a publication rather than doing the minimum required to meet current community standards. Appropriate credit should given for code and data contributions including an expectation of citation. Another suggestion is to instantiate yearly award from journals and/or professional societies, to be awarded to investigators for excellent reproducible practice. Such awards are highly motivating to young researchers in particular, and potentially could result in a sea change in attitudes. These awards could also be cross-conference and journal awards; the collected list of award recipients would both increase the visibility of researchers following good practices and provide examples for others.  More generally, it is unfortunate that software development and data curation are often discounted in the scientific community, and programming is treated as something to spend as little time on as possible. Serious scientists are not expected to carefully test code, let alone document it, in the same way they are trained to properly use other tools or document their experiments. It has been said in some quarters that writing a large piece of software is akin to building infrastructure such as a telescope rather than a creditable scientific contribution, and not worthy of tenure or comparable status at a research laboratory. This attitude must change if we are to encourage young researchers to specialize in computing skills that are essential for the future of mathematical and scientific research. We believe the more proper analog to a large scale scientific instrument is a supercomputer, whereas software reflects the intellectual engine that makes the supercomputers useful, and has scientific value beyond the hardware itself. Important computational results, accompanied<br />
by verification, validation, and reproducibility, should be accorded with honors similar to a strong publication record.</p>
<p>Several tools were presented at the workshop that enable users to write and publish documents that integrate the text and figures seen in reports with code and data used to generate both text and graphical results, such as IPython, Sage notebooks, Lepton, knitr,<br />
and Vistrails. Slides for these talks are available on the wiki [1] and Appendix E discusses these and other tools in detail.</p>
</blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: David Ketcheson		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33308</link>

		<dc:creator><![CDATA[David Ketcheson]]></dc:creator>
		<pubDate>Mon, 23 Sep 2013 14:20:12 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33308</guid>

					<description><![CDATA[Reproducibility is a big deal in computational mathematics, both pure and applied.  Last December I participated in a workshop at ICERM on this topic.  You can read the final report, which is certainly an &quot;effort to improve the quality of scientific research&quot;, here: http://icerm.brown.edu/html/programs/topical/tw12_5_rcem/icerm_report.pdf]]></description>
			<content:encoded><![CDATA[<p>Reproducibility is a big deal in computational mathematics, both pure and applied.  Last December I participated in a workshop at ICERM on this topic.  You can read the final report, which is certainly an &#8220;effort to improve the quality of scientific research&#8221;, here: <a href="http://icerm.brown.edu/html/programs/topical/tw12_5_rcem/icerm_report.pdf" rel="nofollow ugc">http://icerm.brown.edu/html/programs/topical/tw12_5_rcem/icerm_report.pdf</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: arch1		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33042</link>

		<dc:creator><![CDATA[arch1]]></dc:creator>
		<pubDate>Fri, 13 Sep 2013 13:32:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33042</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33025&quot;&gt;John Baez&lt;/a&gt;.

Thank you John (I&#039;d assumed this term too generic for Wikipedia, but I was mistaken).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33025">John Baez</a>.</p>
<p>Thank you John (I&#8217;d assumed this term too generic for Wikipedia, but I was mistaken).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33026</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 13 Sep 2013 02:32:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33026</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33014&quot;&gt;Michael Brazier&lt;/a&gt;.

That&#039;s a huge question, since the IPCC reports are thousands of pages long and rely on vast numbers of different studies, and often a meta-analysis of these studies.  I bet you&#039;ll only get ideologically motivated answers if you pose the question at such a broad level: some people are motivated to attack the IPCC, and others to defend it.  To get more interesting replies it&#039;s probably better to focus on a more specific aspect of the IPCC reports.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33014">Michael Brazier</a>.</p>
<p>That&#8217;s a huge question, since the IPCC reports are thousands of pages long and rely on vast numbers of different studies, and often a meta-analysis of these studies.  I bet you&#8217;ll only get ideologically motivated answers if you pose the question at such a broad level: some people are motivated to attack the IPCC, and others to defend it.  To get more interesting replies it&#8217;s probably better to focus on a more specific aspect of the IPCC reports.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33025</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 13 Sep 2013 02:13:07 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=16682#comment-33025</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33016&quot;&gt;arch1&lt;/a&gt;.

I don&#039;t have much to say about question 2); maybe a statistician around here could tackle that one!

As for question 1), here&#039;s what &lt;a href=&quot;http://en.wikipedia.org/wiki/Odds_ratio&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; says about the &#039;odds ratio&#039;:

&lt;blockquote&gt;
In statistics, imagine each individual in a population either does or does not have a property ″A,″ and also either does or does not have a property ″B.″ For example, ″A″ might be &quot;has high blood pressure,″ and ″B″ might be ″drinks more than one alcoholic drink a day,″ where both properties need to be appropriately defined and quantified (the properties need not be medical, though, and they need not be ″good″ or ″bad″). The &lt;b&gt;odds ratio&lt;/b&gt; (usually abbreviated ″OR″) is one of three main ways to quantify how strongly the having or not having of the property A is associated with having or not having the property B in that population. As the name implies, to compute the OR, one follows these steps: 1) computes the odds that an individual in the population has ″A″ given that he or she has ″B″ (probability of ″A″ given ″B″ divided by the probability of not-″A″ given ″B″); 2) Computes the odds that an individual in the population has ″A″ given that he or she does not have ″B″; and 3) Divides the first odds by the second odds to obtain the odds ratio, the OR. If the OR is greater than 1, then having ″A″ is ″associated″ with having ″B″ in the sense that the having of ″B″ raises (relative to not-having ″B″) the odds of having ″A.″ Note that this is not enough to establish that B is a contributing cause of ″A″: it could be that the association is due to a third property, ″C,″ which is a contributing cause of both ″A″ and ″B.″
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/09/11/why-most-published-research-findings-are-false/#comment-33016">arch1</a>.</p>
<p>I don&#8217;t have much to say about question 2); maybe a statistician around here could tackle that one!</p>
<p>As for question 1), here&#8217;s what <a href="http://en.wikipedia.org/wiki/Odds_ratio" rel="nofollow">Wikipedia</a> says about the &#8216;odds ratio&#8217;:</p>
<blockquote><p>
In statistics, imagine each individual in a population either does or does not have a property ″A,″ and also either does or does not have a property ″B.″ For example, ″A″ might be &#8220;has high blood pressure,″ and ″B″ might be ″drinks more than one alcoholic drink a day,″ where both properties need to be appropriately defined and quantified (the properties need not be medical, though, and they need not be ″good″ or ″bad″). The <b>odds ratio</b> (usually abbreviated ″OR″) is one of three main ways to quantify how strongly the having or not having of the property A is associated with having or not having the property B in that population. As the name implies, to compute the OR, one follows these steps: 1) computes the odds that an individual in the population has ″A″ given that he or she has ″B″ (probability of ″A″ given ″B″ divided by the probability of not-″A″ given ″B″); 2) Computes the odds that an individual in the population has ″A″ given that he or she does not have ″B″; and 3) Divides the first odds by the second odds to obtain the odds ratio, the OR. If the OR is greater than 1, then having ″A″ is ″associated″ with having ″B″ in the sense that the having of ″B″ raises (relative to not-having ″B″) the odds of having ″A.″ Note that this is not enough to establish that B is a contributing cause of ″A″: it could be that the association is due to a third property, ″C,″ which is a contributing cause of both ″A″ and ″B.″
</p></blockquote>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
