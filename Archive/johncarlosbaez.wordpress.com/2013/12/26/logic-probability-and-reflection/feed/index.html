<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Logic, Probability and Reflection	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/</link>
	<description></description>
	<lastBuildDate>Sat, 08 Feb 2014 02:48:04 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Jeffrey Ketland		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-36149</link>

		<dc:creator><![CDATA[Jeffrey Ketland]]></dc:creator>
		<pubDate>Sat, 08 Feb 2014 02:48:04 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-36149</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35401&quot;&gt;Bruce Smith&lt;/a&gt;.

Bruce, thanks for all that and apologies for the delay! I think I see better now the whole issue; in a nutshell, I think the underlying problem is this:

(i) Design aim: parent needs to *certify* the child is safe before switching on child.
(ii) Design aim: preferably child is smarter.
(iii) Godel.

But these three are incompatible given the maths. (iii) is more or less fixed, as it&#039;s a standard result in math logic. So, one has to give up, or get round (i) or (ii). I think your hope is to get round (ii) by making the child in a sense both smarter but weaker, but I think it&#039;s hopeless ... I think you should give up design aim (i).

There is a good way to make the child smarter: give the child the truth theory for the parent. (This would be the &quot;reflection approach&quot;, in line with Feferman&#039;s work on transfinite progressions, reflective closure, etc.)

One oddity of all this is that the parent (well, PA) can talk about their own truth theory (which is merely talking about syntax), but the parent can&#039;t *believe* this theory ... For example, I (JK) know that the truth theory for my language $latex L_{JK} $ contains axioms &quot;JK says &quot;p&quot; if and only if p&quot;. I can easily describe this. However, I cannot believe it. The addition of this axiom to my &quot;belief box&quot; makes me inconsistent (indeed I also know this...!).

For example, adding the simplest truth theory to PA gives a theory called DT (disquotational truth), which has axioms &quot;A is true iff A&quot;, for each sentence $latex A \in L_{PA} $. In fact, DT is a conservative extension of PA, and in fact PA proves this. So,

$latex PA \vdash Con(PA) \to Con(DT) $

Jeff]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35401">Bruce Smith</a>.</p>
<p>Bruce, thanks for all that and apologies for the delay! I think I see better now the whole issue; in a nutshell, I think the underlying problem is this:</p>
<p>(i) Design aim: parent needs to *certify* the child is safe before switching on child.<br />
(ii) Design aim: preferably child is smarter.<br />
(iii) Godel.</p>
<p>But these three are incompatible given the maths. (iii) is more or less fixed, as it&#8217;s a standard result in math logic. So, one has to give up, or get round (i) or (ii). I think your hope is to get round (ii) by making the child in a sense both smarter but weaker, but I think it&#8217;s hopeless &#8230; I think you should give up design aim (i).</p>
<p>There is a good way to make the child smarter: give the child the truth theory for the parent. (This would be the &#8220;reflection approach&#8221;, in line with Feferman&#8217;s work on transfinite progressions, reflective closure, etc.)</p>
<p>One oddity of all this is that the parent (well, PA) can talk about their own truth theory (which is merely talking about syntax), but the parent can&#8217;t *believe* this theory &#8230; For example, I (JK) know that the truth theory for my language <img src="https://s0.wp.com/latex.php?latex=L_%7BJK%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L_{JK} " class="latex" /> contains axioms &#8220;JK says &#8220;p&#8221; if and only if p&#8221;. I can easily describe this. However, I cannot believe it. The addition of this axiom to my &#8220;belief box&#8221; makes me inconsistent (indeed I also know this&#8230;!).</p>
<p>For example, adding the simplest truth theory to PA gives a theory called DT (disquotational truth), which has axioms &#8220;A is true iff A&#8221;, for each sentence <img src="https://s0.wp.com/latex.php?latex=A+%5Cin+L_%7BPA%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;in L_{PA} " class="latex" />. In fact, DT is a conservative extension of PA, and in fact PA proves this. So,</p>
<p><img src="https://s0.wp.com/latex.php?latex=PA+%5Cvdash+Con%28PA%29+%5Cto+Con%28DT%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="PA &#92;vdash Con(PA) &#92;to Con(DT) " class="latex" /></p>
<p>Jeff</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: lee bloomquist		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35493</link>

		<dc:creator><![CDATA[lee bloomquist]]></dc:creator>
		<pubDate>Wed, 22 Jan 2014 12:34:32 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35493</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35492&quot;&gt;lee bloomquist&lt;/a&gt;.

&lt;a href=&quot;https://docs.google.com/file/d/0B9LMgeIAqlIEUWNGTF9EQlZ0LW8/edit?usp=docslist_api&quot; rel=&quot;nofollow&quot;&gt;Here are the details of how it works.&lt;/a&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35492">lee bloomquist</a>.</p>
<p><a href="https://docs.google.com/file/d/0B9LMgeIAqlIEUWNGTF9EQlZ0LW8/edit?usp=docslist_api" rel="nofollow">Here are the details of how it works.</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: lee bloomquist		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35492</link>

		<dc:creator><![CDATA[lee bloomquist]]></dc:creator>
		<pubDate>Wed, 22 Jan 2014 12:25:59 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35492</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35470&quot;&gt;John Baez&lt;/a&gt;.

Yes, just wondering if there might be a classic game here, one that triggers discussion like the iterated prisoner&#039;s dilemma, the ultimatum game, and the tragedy of the commons. It seems like the axioms or models for making decisions in this kind of game could be different, if even a little, for every choice transition in a Petri net of the game. &lt;a href=&quot;https://docs.google.com/file/d/0B9LMgeIAqlIETEdkYU5acm41X3c/edit?usp=docslist_api&quot; rel=&quot;nofollow&quot;&gt;Here&#039;s an example of what I&#039;m calling a choice transition.&lt;/a&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35470">John Baez</a>.</p>
<p>Yes, just wondering if there might be a classic game here, one that triggers discussion like the iterated prisoner&#8217;s dilemma, the ultimatum game, and the tragedy of the commons. It seems like the axioms or models for making decisions in this kind of game could be different, if even a little, for every choice transition in a Petri net of the game. <a href="https://docs.google.com/file/d/0B9LMgeIAqlIETEdkYU5acm41X3c/edit?usp=docslist_api" rel="nofollow">Here&#8217;s an example of what I&#8217;m calling a choice transition.</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35470</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 21 Jan 2014 10:40:17 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35470</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35427&quot;&gt;lee bloomquist&lt;/a&gt;.

That&#039;s &lt;i&gt;not h&lt;/i&gt;ow people usually think of it, but I supposed you can think of a &quot;rational&quot; agent as one who only takes some action after it proves a theorem saying it should.  Of course this theorem must be derived from some set of &#039;axioms&#039;: assumptions about the state of affairs and the goals of the agent.  So, personally, I think rationality is useful but rather limited in its powers, especially when it comes to choosing ones basic goals.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35427">lee bloomquist</a>.</p>
<p>That&#8217;s <i>not h</i>ow people usually think of it, but I supposed you can think of a &#8220;rational&#8221; agent as one who only takes some action after it proves a theorem saying it should.  Of course this theorem must be derived from some set of &#8216;axioms&#8217;: assumptions about the state of affairs and the goals of the agent.  So, personally, I think rationality is useful but rather limited in its powers, especially when it comes to choosing ones basic goals.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: lee bloomquist		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35427</link>

		<dc:creator><![CDATA[lee bloomquist]]></dc:creator>
		<pubDate>Mon, 20 Jan 2014 05:29:25 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35427</guid>

					<description><![CDATA[Gentlemen, forgive an intrusion, but to focus you on a common question, in Theory of Games, is the idea that &quot;rational&quot; means here that there are networks of theorems from the start of the game to the end?]]></description>
			<content:encoded><![CDATA[<p>Gentlemen, forgive an intrusion, but to focus you on a common question, in Theory of Games, is the idea that &#8220;rational&#8221; means here that there are networks of theorems from the start of the game to the end?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bruce Smith		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35401</link>

		<dc:creator><![CDATA[Bruce Smith]]></dc:creator>
		<pubDate>Sat, 18 Jan 2014 22:57:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35401</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35398&quot;&gt;Jeffrey Ketland&lt;/a&gt;.

(my reply to &quot;Thanks, Bruce, I get the constraint now&quot; got accidentally posted a bit higher up. Maybe I clicked the wrong &quot;reply&quot; link.)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35398">Jeffrey Ketland</a>.</p>
<p>(my reply to &#8220;Thanks, Bruce, I get the constraint now&#8221; got accidentally posted a bit higher up. Maybe I clicked the wrong &#8220;reply&#8221; link.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bruce Smith		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35400</link>

		<dc:creator><![CDATA[Bruce Smith]]></dc:creator>
		<pubDate>Sat, 18 Jan 2014 22:56:07 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35400</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35399&quot;&gt;Bruce Smith&lt;/a&gt;.

(This comment &quot;Of course self-certification of safeness is both impossible and useless&quot; was supposed to go lower down, as a reply to Jeffrey saying &quot;Thanks, Bruce, I get the constraint now&quot;. I don&#039;t know why it ended up earlier than the comment it replies to.)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35399">Bruce Smith</a>.</p>
<p>(This comment &#8220;Of course self-certification of safeness is both impossible and useless&#8221; was supposed to go lower down, as a reply to Jeffrey saying &#8220;Thanks, Bruce, I get the constraint now&#8221;. I don&#8217;t know why it ended up earlier than the comment it replies to.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bruce Smith		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35399</link>

		<dc:creator><![CDATA[Bruce Smith]]></dc:creator>
		<pubDate>Sat, 18 Jan 2014 22:54:24 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35399</guid>

					<description><![CDATA[Of course self-certification of safeness is both impossible and useless. Again I was unclear (in hindsight), in two ways:

- I meant the parent has to prove that *the act of turning on the child* is safe, not that *the parent is safe*;

- By &quot;child&quot; I didn&#039;t mean &quot;copy of parent&quot; (which some of your comments seem to assume, at least implicitly), but &quot;anything the parent wants to make, and delegate some authority and responsibility to, to help it do something&quot; -- so &quot;agent&quot; is probably a better word, and indeed the authors of TilingAgents.pdf used &quot;agent&quot; rather than (or at least more prominently than) &quot;child&quot;. So the thing I was calling &quot;the child&quot; might be arbitrarily different than the &quot;parent&quot; -- those terms only make sense because of the &quot;child&quot; having been made by the &quot;parent&quot;. (Maybe this terminology makes more sense to programmers (like me), who use &quot;parent&quot; and &quot;child&quot; in all kinds of ways, e.g. for nodes in directed graphs, for computer programs when one initiates the running of another, etc, than it does to other people.)

So to clarify again, and answer your points: the parent can&#039;t prove it&#039;s safe, but we hope it *is* safe, and we try to prove that ourselves. But to prove that, we have to prove it only takes safe actions (including when it makes agents or &quot;children&quot;). If we specify a limited set of actions it can take (perhaps including &quot;produce an agent of exact design X&quot;), and prove all these actions are safe, we&#039;re ok, but we don&#039;t want to be so limited. So we want to let the parent take any action *it* can prove is safe, using a theory whose proofs *we* already trust. (Of course there is no ultimate justification for our trust in a specific proof theory -- it&#039;s really a matter of faith. But as long as we have that faith, we want to take advantage of it and are willing to trust it.)

And to make sure the parent is not overly limited in what kind of agents it can produce to delegate some powers to, we want to include actions to do that (i.e. to produce agents of any design -- or for that matter, recognize already existing ones -- and delegate powers to them) in the set of actions it can do, in any form it can prove is safe.

And this is where we run into the logical problem, since at least in the way this has been formalized in TilingAgents.pdf, it doesn&#039;t seem possible to achieve all that unless the parent produces an agent whose own proof theory (for its own internal use in limiting its actions) is weaker than that of the parent.

I suspect that problem can be solved somehow, perhaps only by revising the way it&#039;s formalized, but it&#039;s still an open question.

(It&#039;s also conceivable it can be proved impossible to solve, much like various goals people had for formal systems were proved impossible.)

(By the way, in case there is any danger of some other reader taking some of these comments out of context, let me make it clear that I&#039;m not advocating that *human beings* should have to prove it&#039;s safe to have a child before doing so! (Or prove it&#039;s safe to take any other action.) This is only about a technical model of one hypothetical way to make certain kinds of machines safe for humans to use in general ways.)]]></description>
			<content:encoded><![CDATA[<p>Of course self-certification of safeness is both impossible and useless. Again I was unclear (in hindsight), in two ways:</p>
<p>&#8211; I meant the parent has to prove that *the act of turning on the child* is safe, not that *the parent is safe*;</p>
<p>&#8211; By &#8220;child&#8221; I didn&#8217;t mean &#8220;copy of parent&#8221; (which some of your comments seem to assume, at least implicitly), but &#8220;anything the parent wants to make, and delegate some authority and responsibility to, to help it do something&#8221; &#8212; so &#8220;agent&#8221; is probably a better word, and indeed the authors of TilingAgents.pdf used &#8220;agent&#8221; rather than (or at least more prominently than) &#8220;child&#8221;. So the thing I was calling &#8220;the child&#8221; might be arbitrarily different than the &#8220;parent&#8221; &#8212; those terms only make sense because of the &#8220;child&#8221; having been made by the &#8220;parent&#8221;. (Maybe this terminology makes more sense to programmers (like me), who use &#8220;parent&#8221; and &#8220;child&#8221; in all kinds of ways, e.g. for nodes in directed graphs, for computer programs when one initiates the running of another, etc, than it does to other people.)</p>
<p>So to clarify again, and answer your points: the parent can&#8217;t prove it&#8217;s safe, but we hope it *is* safe, and we try to prove that ourselves. But to prove that, we have to prove it only takes safe actions (including when it makes agents or &#8220;children&#8221;). If we specify a limited set of actions it can take (perhaps including &#8220;produce an agent of exact design X&#8221;), and prove all these actions are safe, we&#8217;re ok, but we don&#8217;t want to be so limited. So we want to let the parent take any action *it* can prove is safe, using a theory whose proofs *we* already trust. (Of course there is no ultimate justification for our trust in a specific proof theory &#8212; it&#8217;s really a matter of faith. But as long as we have that faith, we want to take advantage of it and are willing to trust it.)</p>
<p>And to make sure the parent is not overly limited in what kind of agents it can produce to delegate some powers to, we want to include actions to do that (i.e. to produce agents of any design &#8212; or for that matter, recognize already existing ones &#8212; and delegate powers to them) in the set of actions it can do, in any form it can prove is safe.</p>
<p>And this is where we run into the logical problem, since at least in the way this has been formalized in TilingAgents.pdf, it doesn&#8217;t seem possible to achieve all that unless the parent produces an agent whose own proof theory (for its own internal use in limiting its actions) is weaker than that of the parent.</p>
<p>I suspect that problem can be solved somehow, perhaps only by revising the way it&#8217;s formalized, but it&#8217;s still an open question.</p>
<p>(It&#8217;s also conceivable it can be proved impossible to solve, much like various goals people had for formal systems were proved impossible.)</p>
<p>(By the way, in case there is any danger of some other reader taking some of these comments out of context, let me make it clear that I&#8217;m not advocating that *human beings* should have to prove it&#8217;s safe to have a child before doing so! (Or prove it&#8217;s safe to take any other action.) This is only about a technical model of one hypothetical way to make certain kinds of machines safe for humans to use in general ways.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Jeffrey Ketland		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35398</link>

		<dc:creator><![CDATA[Jeffrey Ketland]]></dc:creator>
		<pubDate>Sat, 18 Jan 2014 20:11:19 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35398</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35396&quot;&gt;Bruce Smith&lt;/a&gt;.

Thanks, Bruce, I get the constraint now, I was missing it - it&#039;s the &quot;if&quot; clause below:

&quot;If the parent has to prove it’s safe to turn on the child before doing so, I don’t think anyone believes it could be possible for the child to have a more powerful proof theory than the parent. (Only unsafe operations, like evolution, can do that.)&quot;

So mu question would be, why does the parent have to *prove* this? Is it not ok for the parent simply to *be* safe?

Maybe I want to have a child. Maybe I am safe, maybe I&#039;m not. God knows! But in order to have a safe child, I merely need to *be* safe. I don&#039;t have to *prove* I&#039;m safe before I have a child. 

(It could be irresponsible, I suppose, for a person to reproduce if they already know, or have some evidence, they&#039;ll pass on some horrible condition. But I think that situation is a bit different - this would be an agent that knows that one of its own &quot;modules&quot; is broken; so the agent could request that module to be fixed, before generating a child.)

Also, it&#039;s a bit unclear what use there would be in *me* proving *I* am safe. After all, unsafe theories (e.g., inconsistent ones) *can* prove that they are themselves safe. And there are consistent theories which, wrongly, prove themselves inconsistent! (E.g., PA + ~Con(PA).) So, I&#039;m inclined to say that self-certification of safeness seems impossible, given Godel&#039;s incompleteness result.

Jeff]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35396">Bruce Smith</a>.</p>
<p>Thanks, Bruce, I get the constraint now, I was missing it &#8211; it&#8217;s the &#8220;if&#8221; clause below:</p>
<p>&#8220;If the parent has to prove it’s safe to turn on the child before doing so, I don’t think anyone believes it could be possible for the child to have a more powerful proof theory than the parent. (Only unsafe operations, like evolution, can do that.)&#8221;</p>
<p>So mu question would be, why does the parent have to *prove* this? Is it not ok for the parent simply to *be* safe?</p>
<p>Maybe I want to have a child. Maybe I am safe, maybe I&#8217;m not. God knows! But in order to have a safe child, I merely need to *be* safe. I don&#8217;t have to *prove* I&#8217;m safe before I have a child. </p>
<p>(It could be irresponsible, I suppose, for a person to reproduce if they already know, or have some evidence, they&#8217;ll pass on some horrible condition. But I think that situation is a bit different &#8211; this would be an agent that knows that one of its own &#8220;modules&#8221; is broken; so the agent could request that module to be fixed, before generating a child.)</p>
<p>Also, it&#8217;s a bit unclear what use there would be in *me* proving *I* am safe. After all, unsafe theories (e.g., inconsistent ones) *can* prove that they are themselves safe. And there are consistent theories which, wrongly, prove themselves inconsistent! (E.g., PA + ~Con(PA).) So, I&#8217;m inclined to say that self-certification of safeness seems impossible, given Godel&#8217;s incompleteness result.</p>
<p>Jeff</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bruce Smith		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35396</link>

		<dc:creator><![CDATA[Bruce Smith]]></dc:creator>
		<pubDate>Sat, 18 Jan 2014 17:53:05 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17111#comment-35396</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35386&quot;&gt;Bruce Smith&lt;/a&gt;.

I realized that some of my earlier comments are unclear because I am using &quot;smart&quot; in two different ways. So, to clarify:

If the parent has to prove it&#039;s safe to turn on the child before doing so, I don&#039;t think anyone believes it could be possible for the child to have a more powerful proof theory than the parent. (Only unsafe operations, like evolution, can do that.)

But what ought to be possible would be for the child to have the *same* proof theory as the parent, and be smarter or better in practical ways (like implementation shortcuts, speed, capacity, etc).

But it&#039;s not yet known how to do that in a general way; the most that&#039;s known (in principle only, not yet done in practice) is how the child can be *slightly weaker* in proof theory (though it could at the same time be smarter or better in other ways).

It is known how to do the equal-proof-theory child in a *non-general* way. For example, the parent could have a special operation &quot;replace me with an identical copy&quot;. It would be easy to prove that&#039;s safe. But what&#039;s wanted is for the same safety rules used for general operations on the physical world to also work for making and activating the child. That way it&#039;s clear that there is no artificial limit on the child&#039;s structure or implementation. (And there are other benefits, described in TilingAgents.pdf.) That&#039;s what is not yet known how to do in principle (to those authors or me, anyway).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/#comment-35386">Bruce Smith</a>.</p>
<p>I realized that some of my earlier comments are unclear because I am using &#8220;smart&#8221; in two different ways. So, to clarify:</p>
<p>If the parent has to prove it&#8217;s safe to turn on the child before doing so, I don&#8217;t think anyone believes it could be possible for the child to have a more powerful proof theory than the parent. (Only unsafe operations, like evolution, can do that.)</p>
<p>But what ought to be possible would be for the child to have the *same* proof theory as the parent, and be smarter or better in practical ways (like implementation shortcuts, speed, capacity, etc).</p>
<p>But it&#8217;s not yet known how to do that in a general way; the most that&#8217;s known (in principle only, not yet done in practice) is how the child can be *slightly weaker* in proof theory (though it could at the same time be smarter or better in other ways).</p>
<p>It is known how to do the equal-proof-theory child in a *non-general* way. For example, the parent could have a special operation &#8220;replace me with an identical copy&#8221;. It would be easy to prove that&#8217;s safe. But what&#8217;s wanted is for the same safety rules used for general operations on the physical world to also work for making and activating the child. That way it&#8217;s clear that there is no artificial limit on the child&#8217;s structure or implementation. (And there are other benefits, described in TilingAgents.pdf.) That&#8217;s what is not yet known how to do in principle (to those authors or me, anyway).</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
