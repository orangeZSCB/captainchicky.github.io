<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Relative Entropy (Part 3)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/</link>
	<description></description>
	<lastBuildDate>Mon, 26 Apr 2021 19:58:56 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-77813</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 02 Mar 2016 21:31:42 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-77813</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-77811&quot;&gt;Qiaochu Yuan&lt;/a&gt;.

Qiaochu wrote:

&lt;blockquote&gt;
  Second, it doesn’t follow from the definition that relative entropy is nonnegative. The problem is that you can’t a priori control the signs of the logarithms. It’s a nontrivial fact that this is true.
&lt;/blockquote&gt;

I&#039;m not sure what you&#039;re asserting is true.

However, when $latex p$ and $latex q$ are probability distributions on a finite set, I believe that

$latex \displaystyle{ \sum_{i \in X} p_i \ln\left(\frac{p_i}{q_i}\right) \ge 0 } $

though it may be infinite: we define $latex 0 \ln 0 = 0$ but $latex p \ln 0 = +\infty$ when $latex p &#062; 0$, to handle cases involving the logarithm of zero.

I believe the same sort of inequality holds whenever $latex p$ and $latex q$ are probability measures and $latex p$ is absolutely continuous with respect to $latex q.$  If you have a counterexample, I&#039;d like to see it!

You might like the argument on page 16 &lt;a href=&quot;http://arxiv.org/pdf/1512.02742v3&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;: it gives non-negativity for a generalization of relative information to &#039;non-normalized probability distributions&#039; on finite sets.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-77811">Qiaochu Yuan</a>.</p>
<p>Qiaochu wrote:</p>
<blockquote><p>
  Second, it doesn’t follow from the definition that relative entropy is nonnegative. The problem is that you can’t a priori control the signs of the logarithms. It’s a nontrivial fact that this is true.
</p></blockquote>
<p>I&#8217;m not sure what you&#8217;re asserting is true.</p>
<p>However, when <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> are probability distributions on a finite set, I believe that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5Cln%5Cleft%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%5Cright%29+%5Cge+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in X} p_i &#92;ln&#92;left(&#92;frac{p_i}{q_i}&#92;right) &#92;ge 0 } " class="latex" /></p>
<p>though it may be infinite: we define <img src="https://s0.wp.com/latex.php?latex=0+%5Cln+0+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;ln 0 = 0" class="latex" /> but <img src="https://s0.wp.com/latex.php?latex=p+%5Cln+0+%3D+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ln 0 = +&#92;infty" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=p+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &gt; 0" class="latex" />, to handle cases involving the logarithm of zero.</p>
<p>I believe the same sort of inequality holds whenever <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> are probability measures and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is absolutely continuous with respect to <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  If you have a counterexample, I&#8217;d like to see it!</p>
<p>You might like the argument on page 16 <a href="http://arxiv.org/pdf/1512.02742v3" rel="nofollow">here</a>: it gives non-negativity for a generalization of relative information to &#8216;non-normalized probability distributions&#8217; on finite sets.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-77812</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 02 Mar 2016 21:14:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-77812</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-77811&quot;&gt;Qiaochu Yuan&lt;/a&gt;.

Qiaochu wrote:

&lt;blockquote&gt;
  [...] relative entropy does not reduce to ordinary entropy with a uniform prior, even up to a constant. It reduces to the negative of ordinary entropy up to a constant. (This is why I’m tempted to call “relative entropy” the negative of what you wrote.)
&lt;/blockquote&gt;

Yeah, the missing minus sign annoys me too.  Lots of people call this quantity relative entropy, but in a &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/&quot; rel=&quot;nofollow&quot;&gt;later paper&lt;/a&gt; I wrote:

Most of our results involve a quantity that is variously known as &#039;relative information&#039;, &#039;relative entropy&#039;, &#039;information gain&#039; or the &#039;Kullback--Leibler divergence&#039;.  We shall use the first term.   Given two probability distributions $latex p$ and $latex q$ on a finite set $latex X,$ their &lt;strong&gt;relative information&lt;/strong&gt;, or more precisely the &lt;strong&gt;information of $latex p$ relative to $latex q,$&lt;/strong&gt; is

$latex         \displaystyle{     I(p,q) = \sum_{i \in X} p_i \ln\left(\frac{p_i}{q_i}\right) . } $

We use the word &#039;information&#039; instead of &#039;entropy&#039; because one expects entropy to increase with time, and the theorems we present will say that $latex I(p,q)$ decreases with time under various conditions.   The reason is that the Shannon entropy

$latex    \displaystyle{   S(p) = -\sum_{i \in X} p_i \ln p_i } $

contains a minus sign that is missing from the definition of relative information.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-77811">Qiaochu Yuan</a>.</p>
<p>Qiaochu wrote:</p>
<blockquote><p>
  [&#8230;] relative entropy does not reduce to ordinary entropy with a uniform prior, even up to a constant. It reduces to the negative of ordinary entropy up to a constant. (This is why I’m tempted to call “relative entropy” the negative of what you wrote.)
</p></blockquote>
<p>Yeah, the missing minus sign annoys me too.  Lots of people call this quantity relative entropy, but in a <a href="https://johncarlosbaez.wordpress.com/2015/11/27/relative-entropy-in-biological-systems/" rel="nofollow">later paper</a> I wrote:</p>
<p>Most of our results involve a quantity that is variously known as &#8216;relative information&#8217;, &#8216;relative entropy&#8217;, &#8216;information gain&#8217; or the &#8216;Kullback&#8211;Leibler divergence&#8217;.  We shall use the first term.   Given two probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on a finite set <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> their <strong>relative information</strong>, or more precisely the <strong>information of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /></strong> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++++I%28p%2Cq%29+%3D+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5Cln%5Cleft%28%5Cfrac%7Bp_i%7D%7Bq_i%7D%5Cright%29+.+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{     I(p,q) = &#92;sum_{i &#92;in X} p_i &#92;ln&#92;left(&#92;frac{p_i}{q_i}&#92;right) . } " class="latex" /></p>
<p>We use the word &#8216;information&#8217; instead of &#8216;entropy&#8217; because one expects entropy to increase with time, and the theorems we present will say that <img src="https://s0.wp.com/latex.php?latex=I%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(p,q)" class="latex" /> decreases with time under various conditions.   The reason is that the Shannon entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++S%28p%29+%3D+-%5Csum_%7Bi+%5Cin+X%7D+p_i+%5Cln+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   S(p) = -&#92;sum_{i &#92;in X} p_i &#92;ln p_i } " class="latex" /></p>
<p>contains a minus sign that is missing from the definition of relative information.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Qiaochu Yuan		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-77811</link>

		<dc:creator><![CDATA[Qiaochu Yuan]]></dc:creator>
		<pubDate>Wed, 02 Mar 2016 21:05:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-77811</guid>

					<description><![CDATA[Two comments: first, relative entropy does not reduce to ordinary entropy with a uniform prior, even up to a constant. It reduces to the negative of ordinary entropy up to a constant. (This is why I&#039;m tempted to call &quot;relative entropy&quot; the negative of what you wrote.)

Second, it doesn&#039;t follow from the definition that relative entropy is nonnegative. The problem is that you can&#039;t a priori control the signs of the logarithms. It&#039;s a nontrivial fact that this is true.]]></description>
			<content:encoded><![CDATA[<p>Two comments: first, relative entropy does not reduce to ordinary entropy with a uniform prior, even up to a constant. It reduces to the negative of ordinary entropy up to a constant. (This is why I&#8217;m tempted to call &#8220;relative entropy&#8221; the negative of what you wrote.)</p>
<p>Second, it doesn&#8217;t follow from the definition that relative entropy is nonnegative. The problem is that you can&#8217;t a priori control the signs of the logarithms. It&#8217;s a nontrivial fact that this is true.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Relative Entropy (Part 4) &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-36575</link>

		<dc:creator><![CDATA[Relative Entropy (Part 4) &#124; Azimuth]]></dc:creator>
		<pubDate>Sun, 16 Feb 2014 11:19:04 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-36575</guid>

					<description><![CDATA[Now Tobias Fritz and I have finally finished our paper on this subject:

&#8226; &lt;a href=&quot;http://arxiv.org/abs/1402.3067&quot; rel=&quot;nofollow&quot;&gt;A Bayesian characterization of relative entropy&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>Now Tobias Fritz and I have finally finished our paper on this subject:</p>
<p>&bull; <a href="http://arxiv.org/abs/1402.3067" rel="nofollow">A Bayesian characterization of relative entropy</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Categories in Control &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-36004</link>

		<dc:creator><![CDATA[Categories in Control &#124; Azimuth]]></dc:creator>
		<pubDate>Thu, 06 Feb 2014 07:38:28 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-36004</guid>

					<description><![CDATA[Tobias Fritz, a postdoc at the Perimeter Institute, is working with me on category-theoretic aspects of information theory. We published a paper on entropy with Tom Leinster, and we&#8217;ve got a followup on relative entropy that&#8217;s almost done. I should be working on it right this instant!  But for now, read the series of posts here on Azimuth: Relative Entropy Part 1, Part 2 and Part 3. [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>Tobias Fritz, a postdoc at the Perimeter Institute, is working with me on category-theoretic aspects of information theory. We published a paper on entropy with Tom Leinster, and we&#8217;ve got a followup on relative entropy that&#8217;s almost done. I should be working on it right this instant!  But for now, read the series of posts here on Azimuth: Relative Entropy Part 1, Part 2 and Part 3. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34940</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 30 Dec 2013 17:06:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-34940</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34932&quot;&gt;John Baez&lt;/a&gt;.

It&#039;s the &lt;a href=&quot;https://en.wikipedia.org/wiki/Upper_topology&quot; rel=&quot;nofollow&quot;&gt;upper topology&lt;/a&gt;, where the open sets are of the form $latex (a,\infty]$.  

I said &#039;real line&#039;, but that wasn&#039;t quite right: relative entropy can be infinite, so we should really think of it as taking values in $latex [0,\infty]$, and a function from a topological space taking values in $latex [0,\infty]$ is lower semicontinuous iff it&#039;s continuous when $latex [0,\infty]$ is given the upper topology.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34932">John Baez</a>.</p>
<p>It&#8217;s the <a href="https://en.wikipedia.org/wiki/Upper_topology" rel="nofollow">upper topology</a>, where the open sets are of the form <img src="https://s0.wp.com/latex.php?latex=%28a%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(a,&#92;infty]" class="latex" />.  </p>
<p>I said &#8216;real line&#8217;, but that wasn&#8217;t quite right: relative entropy can be infinite, so we should really think of it as taking values in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" />, and a function from a topological space taking values in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> is lower semicontinuous iff it&#8217;s continuous when <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> is given the upper topology.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: coreyyanofsky		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34937</link>

		<dc:creator><![CDATA[coreyyanofsky]]></dc:creator>
		<pubDate>Mon, 30 Dec 2013 13:24:10 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-34937</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34932&quot;&gt;John Baez&lt;/a&gt;.

Thanks! (Is the weird topology you&#039;re referring to Scott topology or upper topology or...?)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34932">John Baez</a>.</p>
<p>Thanks! (Is the weird topology you&#8217;re referring to Scott topology or upper topology or&#8230;?)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34932</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 30 Dec 2013 06:57:42 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-34932</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34931&quot;&gt;coreyyanofsky&lt;/a&gt;.

There are lots of characterizations of entropy and relative entropy. The original &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Characterization&quot; rel=&quot;nofollow&quot;&gt;Shannon--Khinchine characterization of Shannon entropy&lt;/a&gt; includes a condition that entropy is maximized when all outcomes are equally likely.  This seems awkward to formulate in category-theoretic terms.  It&#039;s easier to use characterizations that involve only &lt;i&gt;equations&lt;/i&gt; together with a &lt;i&gt;continuity&lt;/i&gt; requirement. 

A guy named Faddeev characterized entropy using only equations and a continuity requirement.  In a previous paper written with a mutual friend, Tobias and I reformulated Faddeev&#039;s characterization in category-theoretic terms:

&#8226; John Baez, Tobias Fritz and Tom Leinster, &lt;a href=&quot;http://arxiv.org/abs/1106.1791&quot; rel=&quot;nofollow&quot;&gt;A characterization of entropy in terms of information loss&lt;/a&gt;.

In our new paper we started by trying to phrase &lt;a href=&quot;http://www.renyi.hu/~petz/pdf/52.pdf&quot; rel=&quot;nofollow&quot;&gt;Petz&#039;s characterization of relative entropy&lt;/a&gt; in category-theoretic terms.  Petz&#039;s theorem turned out to be flawed, so Tobias had to fix it.  But we still owe our inspiration to him, and
we got a characterization involving only equations and a lower semicontinuity requirement.  (For what it&#039;s worth, lower semicontinuity can be thought of as continuity with respect to a funny topology on the real line.)

For a comparison of Faddeev&#039;s characterization of entropy and the Shannon--Khinchine characterization, try this:

&#8226; Shigeru Furuichi, &lt;a href=&quot;http://arxiv.org/abs/cond-mat/0410270&quot; rel=&quot;nofollow&quot;&gt;On uniqueness theorems for Tsallis entropy and Tsallis relative entropy&lt;/a&gt;.

This covers something more general called &#039;Tsallis entropy&#039;, but if you take the $latex q \to 1$ limit you get back to Shannon entropy.

Our paper with Tom Leinster covered the more general Tsallis case, but now that we&#039;re doing relative entropy we&#039;re having enough trouble just handling the Shannon case!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34931">coreyyanofsky</a>.</p>
<p>There are lots of characterizations of entropy and relative entropy. The original <a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Characterization" rel="nofollow">Shannon&#8211;Khinchine characterization of Shannon entropy</a> includes a condition that entropy is maximized when all outcomes are equally likely.  This seems awkward to formulate in category-theoretic terms.  It&#8217;s easier to use characterizations that involve only <i>equations</i> together with a <i>continuity</i> requirement. </p>
<p>A guy named Faddeev characterized entropy using only equations and a continuity requirement.  In a previous paper written with a mutual friend, Tobias and I reformulated Faddeev&#8217;s characterization in category-theoretic terms:</p>
<p>&bull; John Baez, Tobias Fritz and Tom Leinster, <a href="http://arxiv.org/abs/1106.1791" rel="nofollow">A characterization of entropy in terms of information loss</a>.</p>
<p>In our new paper we started by trying to phrase <a href="http://www.renyi.hu/~petz/pdf/52.pdf" rel="nofollow">Petz&#8217;s characterization of relative entropy</a> in category-theoretic terms.  Petz&#8217;s theorem turned out to be flawed, so Tobias had to fix it.  But we still owe our inspiration to him, and<br />
we got a characterization involving only equations and a lower semicontinuity requirement.  (For what it&#8217;s worth, lower semicontinuity can be thought of as continuity with respect to a funny topology on the real line.)</p>
<p>For a comparison of Faddeev&#8217;s characterization of entropy and the Shannon&#8211;Khinchine characterization, try this:</p>
<p>&bull; Shigeru Furuichi, <a href="http://arxiv.org/abs/cond-mat/0410270" rel="nofollow">On uniqueness theorems for Tsallis entropy and Tsallis relative entropy</a>.</p>
<p>This covers something more general called &#8216;Tsallis entropy&#8217;, but if you take the <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to 1" class="latex" /> limit you get back to Shannon entropy.</p>
<p>Our paper with Tom Leinster covered the more general Tsallis case, but now that we&#8217;re doing relative entropy we&#8217;re having enough trouble just handling the Shannon case!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: coreyyanofsky		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34931</link>

		<dc:creator><![CDATA[coreyyanofsky]]></dc:creator>
		<pubDate>Mon, 30 Dec 2013 06:00:56 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-34931</guid>

					<description><![CDATA[It seems to me that you doing something like rephrasing Shannon&#039;s original characterization of entropy in category theory terms. Is that fair, or am I totally off base?]]></description>
			<content:encoded><![CDATA[<p>It seems to me that you doing something like rephrasing Shannon&#8217;s original characterization of entropy in category theory terms. Is that fair, or am I totally off base?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34884</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 28 Dec 2013 07:50:38 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17070#comment-34884</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34834&quot;&gt;lee bloomquist&lt;/a&gt;.

I don&#039;t know!  Not much unless someone understands our results, knows about those experiments, and works hard to make a connection.  No such person exists yet.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comment-34834">lee bloomquist</a>.</p>
<p>I don&#8217;t know!  Not much unless someone understands our results, knows about those experiments, and works hard to make a connection.  No such person exists yet.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
