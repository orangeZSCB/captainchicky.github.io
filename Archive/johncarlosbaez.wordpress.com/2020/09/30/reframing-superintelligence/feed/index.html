<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Reframing Superintelligence	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/</link>
	<description></description>
	<lastBuildDate>Fri, 02 Oct 2020 19:18:04 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166780</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 02 Oct 2020 19:18:04 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166780</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166767&quot;&gt;Toby Bartels&lt;/a&gt;.

Yes.  China introduced the first &lt;em&gt;female&lt;/em&gt; artificial news anchor in 2019, and one can imagine a &#039;likeability arms race&#039; in this field:

[youtube https://www.youtube.com/watch?v=5iZuffHPDAw&#038;w=560&#038;h=315]

Also in video games, pornography, etc.

I know someone who used to work for Amazon studying people who direct abuse at Alexa.  While Alexa doesn&#039;t have any consciousness yet and can&#039;t be offended by abuse, she was concerned that Alexa could serve as a &#039;training ground&#039; for abusers, and was interested in studying this.   As artificial systems become ever more person-like this issue will grow, especially when pornographers and the sex trade get involved.  Pretty much any hair-raising scenario you can imagine will occur as soon as it becomes technically possible and cheap enough.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166767">Toby Bartels</a>.</p>
<p>Yes.  China introduced the first <em>female</em> artificial news anchor in 2019, and one can imagine a &#8216;likeability arms race&#8217; in this field:</p>
<p><iframe class="youtube-player" width="560" height="315" src="https://www.youtube.com/embed/5iZuffHPDAw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></p>
<p>Also in video games, pornography, etc.</p>
<p>I know someone who used to work for Amazon studying people who direct abuse at Alexa.  While Alexa doesn&#8217;t have any consciousness yet and can&#8217;t be offended by abuse, she was concerned that Alexa could serve as a &#8216;training ground&#8217; for abusers, and was interested in studying this.   As artificial systems become ever more person-like this issue will grow, especially when pornographers and the sex trade get involved.  Pretty much any hair-raising scenario you can imagine will occur as soon as it becomes technically possible and cheap enough.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phillip Helbig		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166768</link>

		<dc:creator><![CDATA[Phillip Helbig]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 21:50:58 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166768</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762&quot;&gt;John Baez&lt;/a&gt;.

&lt;i&gt;&quot;Natural life had “agency” long before intelligence, in the sense that even the simplest bacteria act in ways that help them find food, avoid threats and reproduce: it’s a Darwinian imperative. A human-designed system that reproduces because people like it and build more doesn’t have the same sort of pressure to acquire agency. Unless, of course, we are trying to build systems that have, or act like they have, agency! Some people are.&quot;&lt;/i&gt;

There is a novel by Rudy Rucker (who, in an interesting way, gives me an Einstein number of 4---not in terms of papers, but in the sense of 4 degrees of separation) where, in order to get over Penrose-type objections involving Gödel&#039;s incompleteness theorem, AI is allowed to &lt;i&gt;evolve&lt;/i&gt; from much more basic stuff, rather than be constructed &lt;i&gt;per se&lt;/i&gt;.

Another interesting take on that is &lt;i&gt;Code of the Lifemaker&lt;/i&gt; by James P. Hogan.  (Some of his earlier science fiction is good, whatever one thinks of his politics; this book and &lt;i&gt;Voyage from Yesteryear&lt;/i&gt; are probably my favourites).

Rucker is a jack of all trades---publishing as a mathematician as Rudolf v. B. Rucker, founding the cyberpunk literary genre, teaching computer graphics (which is part of my Einstein intersection, to borrow a title from someone else), painting, blogging, photographing, publishing, and so on.  He also wrote one of the very few books I&#039;ve read more than once: &lt;i&gt;Infinity and the Mind&lt;/i&gt;, which is a somewhat technical popular-mathematics book.  Highly recommended!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762">John Baez</a>.</p>
<p><i>&#8220;Natural life had “agency” long before intelligence, in the sense that even the simplest bacteria act in ways that help them find food, avoid threats and reproduce: it’s a Darwinian imperative. A human-designed system that reproduces because people like it and build more doesn’t have the same sort of pressure to acquire agency. Unless, of course, we are trying to build systems that have, or act like they have, agency! Some people are.&#8221;</i></p>
<p>There is a novel by Rudy Rucker (who, in an interesting way, gives me an Einstein number of 4&#8212;not in terms of papers, but in the sense of 4 degrees of separation) where, in order to get over Penrose-type objections involving Gödel&#8217;s incompleteness theorem, AI is allowed to <i>evolve</i> from much more basic stuff, rather than be constructed <i>per se</i>.</p>
<p>Another interesting take on that is <i>Code of the Lifemaker</i> by James P. Hogan.  (Some of his earlier science fiction is good, whatever one thinks of his politics; this book and <i>Voyage from Yesteryear</i> are probably my favourites).</p>
<p>Rucker is a jack of all trades&#8212;publishing as a mathematician as Rudolf v. B. Rucker, founding the cyberpunk literary genre, teaching computer graphics (which is part of my Einstein intersection, to borrow a title from someone else), painting, blogging, photographing, publishing, and so on.  He also wrote one of the very few books I&#8217;ve read more than once: <i>Infinity and the Mind</i>, which is a somewhat technical popular-mathematics book.  Highly recommended!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Toby Bartels		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166767</link>

		<dc:creator><![CDATA[Toby Bartels]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 21:49:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166767</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762&quot;&gt;John Baez&lt;/a&gt;.

Or if the systems are subject to variation and selection, which produces evolution.  The agency wouldn&#039;t be directed towards finding food, of course, but towards getting more people to like it.  But there&amp;apos;s still potential danger there, since being liked enough to get reproduced is not the same as doing what people want in the long term.  Indeed, researchers are already consciously trying to design systems that make people like them, only these researchers work in marketing rather than in computer science.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762">John Baez</a>.</p>
<p>Or if the systems are subject to variation and selection, which produces evolution.  The agency wouldn&#8217;t be directed towards finding food, of course, but towards getting more people to like it.  But there&apos;s still potential danger there, since being liked enough to get reproduced is not the same as doing what people want in the long term.  Indeed, researchers are already consciously trying to design systems that make people like them, only these researchers work in marketing rather than in computer science.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phillip Helbig		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166766</link>

		<dc:creator><![CDATA[Phillip Helbig]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 21:44:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166766</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762&quot;&gt;John Baez&lt;/a&gt;.

&lt;i&gt;&quot;I didn’t know Tegmark switched to AI research. Where is he working now? Let’s see… apparently still the MIT physics department.&quot;&lt;/i&gt;

Where can you go from there?  :-)  OK, some have moved somewhere else, for various reasons, but not many.

I guess that&#039;s the advantage of tenure: work on something else if it takes your fancy.  (I&#039;m sure that he still teaches physics.)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762">John Baez</a>.</p>
<p><i>&#8220;I didn’t know Tegmark switched to AI research. Where is he working now? Let’s see… apparently still the MIT physics department.&#8221;</i></p>
<p>Where can you go from there?  :-)  OK, some have moved somewhere else, for various reasons, but not many.</p>
<p>I guess that&#8217;s the advantage of tenure: work on something else if it takes your fancy.  (I&#8217;m sure that he still teaches physics.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phillip Helbig		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166765</link>

		<dc:creator><![CDATA[Phillip Helbig]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 21:41:28 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166765</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762&quot;&gt;John Baez&lt;/a&gt;.

One of Max&#039;s emphases are AI risks.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762">John Baez</a>.</p>
<p>One of Max&#8217;s emphases are AI risks.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166762</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 20:12:12 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166762</guid>

					<description><![CDATA[I didn&#039;t know Tegmark switched to AI research.  Where is he working now?  Let&#039;s see... apparently still the MIT physics department.

&lt;blockquote&gt; At some point agency might arise of itself so to speak. 
&lt;/blockquote&gt;

I guess a lot of worries about AI risk revolve around the probability of agency arising &quot;on its own&quot;.   Natural life had &quot;agency&quot; long before intelligence, in the sense that even the simplest bacteria act in ways that help them find food, avoid threats and reproduce: it&#039;s a Darwinian imperative.  A human-designed system that reproduces because people like it and build more doesn&#039;t have the same sort of pressure to acquire agency.   Unless, of course, we are trying to build systems that have, or act like they have, agency!    Some people are.]]></description>
			<content:encoded><![CDATA[<p>I didn&#8217;t know Tegmark switched to AI research.  Where is he working now?  Let&#8217;s see&#8230; apparently still the MIT physics department.</p>
<blockquote><p> At some point agency might arise of itself so to speak.
</p></blockquote>
<p>I guess a lot of worries about AI risk revolve around the probability of agency arising &#8220;on its own&#8221;.   Natural life had &#8220;agency&#8221; long before intelligence, in the sense that even the simplest bacteria act in ways that help them find food, avoid threats and reproduce: it&#8217;s a Darwinian imperative.  A human-designed system that reproduces because people like it and build more doesn&#8217;t have the same sort of pressure to acquire agency.   Unless, of course, we are trying to build systems that have, or act like they have, agency!    Some people are.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phillip Helbig		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166756</link>

		<dc:creator><![CDATA[Phillip Helbig]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 16:29:15 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166756</guid>

					<description><![CDATA[As Max Tegmark (and perhaps others) have pointed out, AI is already better then humans at things like chess and go.  While the man-beating chess computer was programmed by chess experts, the man-beating go computer essentially programmed itself.  At some point, AI will learn how to program AI, which could lead to Kurzweil&#039;s singularity.  At some point agency might arise of itself so to speak.  And while it might be improbable for the robots to turn on their creators and destroy them like in some B movie, perhaps their goals are not aligned with our goals.  How many species have become extinct because of humans?  Practically none were intentionally killed off; rather, our goals (or, rather, the goals of a large enough group of people to make a difference) and their goals were different.

Max&#039;s book &lt;a HREF=&quot;https://space.mit.edu/home/tegmark/ai.html&quot; rel=&quot;nofollow ugc&quot;&gt;&lt;i&gt;Life 3.0&lt;/i&gt;&lt;/a&gt; is an interesting read.

Max used to do mainly physics (he was a leading cosmologist) but is now doing mostly AI research.

It&#039;s been the good part of a century since then, but in my reading of modern AI literature, I note that most of the moral and other dilemmas were explored in Asimov&#039;s science fiction.]]></description>
			<content:encoded><![CDATA[<p>As Max Tegmark (and perhaps others) have pointed out, AI is already better then humans at things like chess and go.  While the man-beating chess computer was programmed by chess experts, the man-beating go computer essentially programmed itself.  At some point, AI will learn how to program AI, which could lead to Kurzweil&#8217;s singularity.  At some point agency might arise of itself so to speak.  And while it might be improbable for the robots to turn on their creators and destroy them like in some B movie, perhaps their goals are not aligned with our goals.  How many species have become extinct because of humans?  Practically none were intentionally killed off; rather, our goals (or, rather, the goals of a large enough group of people to make a difference) and their goals were different.</p>
<p>Max&#8217;s book <a HREF="https://space.mit.edu/home/tegmark/ai.html" rel="nofollow ugc"><i>Life 3.0</i></a> is an interesting read.</p>
<p>Max used to do mainly physics (he was a leading cosmologist) but is now doing mostly AI research.</p>
<p>It&#8217;s been the good part of a century since then, but in my reading of modern AI literature, I note that most of the moral and other dilemmas were explored in Asimov&#8217;s science fiction.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Wolfgang		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166755</link>

		<dc:creator><![CDATA[Wolfgang]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 14:38:54 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166755</guid>

					<description><![CDATA[Humans are in one way intrinsically curious, and in another way intrinsically lazy. Suppose we create a learning machine that potentially could improve itself indefinitely, is it naive to ask, what would be the rationale for the machine, to actually do so? The &#039;thought&#039; of ruling or even destroying mankind might be such a rationale, but a rather high level of self-awareness would be required for this to happen, and we don&#039;t even know, if any machine will obtain something like consciousness just by endless cycles of learning and self-improvement. Unless, maybe, there is something like a new  thermodynamic law, that would inevitably force this infinite improvement? It cannot be entropy, in my opinion, since entropy counteracts any form of learning.]]></description>
			<content:encoded><![CDATA[<p>Humans are in one way intrinsically curious, and in another way intrinsically lazy. Suppose we create a learning machine that potentially could improve itself indefinitely, is it naive to ask, what would be the rationale for the machine, to actually do so? The &#8216;thought&#8217; of ruling or even destroying mankind might be such a rationale, but a rather high level of self-awareness would be required for this to happen, and we don&#8217;t even know, if any machine will obtain something like consciousness just by endless cycles of learning and self-improvement. Unless, maybe, there is something like a new  thermodynamic law, that would inevitably force this infinite improvement? It cannot be entropy, in my opinion, since entropy counteracts any form of learning.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davidwlocke		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166737</link>

		<dc:creator><![CDATA[davidwlocke]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 02:39:05 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166737</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166719&quot;&gt;Toby Bartels&lt;/a&gt;.

A 1996 paper about requirement elicitation finds that we failed at eliciting requirements. For many reasons. In Agile, we don&#039;t even try. It is the unsolved problem of computer science. Agile has resulted in amateur-level performance. We are not even trying to explain things to another human. And, now AI wants to do what it does without explaining any of it to anyone.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166719">Toby Bartels</a>.</p>
<p>A 1996 paper about requirement elicitation finds that we failed at eliciting requirements. For many reasons. In Agile, we don&#8217;t even try. It is the unsolved problem of computer science. Agile has resulted in amateur-level performance. We are not even trying to explain things to another human. And, now AI wants to do what it does without explaining any of it to anyone.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davidwlocke		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166736</link>

		<dc:creator><![CDATA[davidwlocke]]></dc:creator>
		<pubDate>Thu, 01 Oct 2020 02:33:05 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28774#comment-166736</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166723&quot;&gt;John Baez&lt;/a&gt;.

Sorry about that. &quot;Innovation and Design in the Age of Artificial Intelligence&quot; by Roberto Veganti, Lica Veramindelli, and Marco Iansiti.Journal of Product Innovation Management 2020:37(3):212-227. c 2020 PDMA.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/09/30/reframing-superintelligence/#comment-166723">John Baez</a>.</p>
<p>Sorry about that. &#8220;Innovation and Design in the Age of Artificial Intelligence&#8221; by Roberto Veganti, Lica Veramindelli, and Marco Iansiti.Journal of Product Innovation Management 2020:37(3):212-227. c 2020 PDMA.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
