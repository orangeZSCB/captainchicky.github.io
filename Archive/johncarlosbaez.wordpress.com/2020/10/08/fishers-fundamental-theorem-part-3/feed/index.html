<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Fisher&#8217;s Fundamental Theorem (Part 3)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/</link>
	<description></description>
	<lastBuildDate>Tue, 13 Jul 2021 15:36:06 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Marc Harper		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166927</link>

		<dc:creator><![CDATA[Marc Harper]]></dc:creator>
		<pubDate>Fri, 09 Oct 2020 08:04:29 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166927</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166876&quot;&gt;Toby Bartels&lt;/a&gt;.

Similarly, any dynamic confined to the interior of the simplex is actually a replicator equation. This is called &quot;forward-invariance&quot;.

If a dynamic $latex \dot{p}_i = F_i(p)$ is forward-invariant on interior of the simplex, then we must have that $latex \sum_j{F_j(p)} = 0$ since $latex \sum_i{\dot{p}_i} = 0$. So we can rewrite the dynamic as follows:

$latex \dot{p}_i = F_i(p) = p_i \frac{F_i(p)}{p_i} =  p_i\frac{F_i(p)}{p_i} - p_i \sum_j{p_j \frac{F_j(p)}{p_j}}$

which is a replicator equation with fitness functions $latex N_i(p) = F_i(p) / p_i$.

AFAIK this was first noticed by Dashiell Fryer and myself [1] and [2]. It was also reported essentially as written above recently in [3], which notes a similar observation due to Smale in 1976 (before the common form of replicator equation was defined, typically attributed to Talyor and Jonker in 1978).

[1] Fryer (2012) On the existence of general equilibrium in finite games and general game dynamics. arXiv:1201.2384
[2] Harper, Fryer (2014). Lyapunov Functions for Time-Scale Dynamics on Riemannian Geometries of the Simplex, DGAA
[3] Raju, Krishnaprasad &quot;Lie algebra structure of fitness and replicator control&quot; (2020) arXiv:2005.09792]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166876">Toby Bartels</a>.</p>
<p>Similarly, any dynamic confined to the interior of the simplex is actually a replicator equation. This is called &#8220;forward-invariance&#8221;.</p>
<p>If a dynamic <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i+%3D+F_i%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i = F_i(p)" class="latex" /> is forward-invariant on interior of the simplex, then we must have that <img src="https://s0.wp.com/latex.php?latex=%5Csum_j%7BF_j%28p%29%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_j{F_j(p)} = 0" class="latex" /> since <img src="https://s0.wp.com/latex.php?latex=%5Csum_i%7B%5Cdot%7Bp%7D_i%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_i{&#92;dot{p}_i} = 0" class="latex" />. So we can rewrite the dynamic as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i+%3D+F_i%28p%29+%3D+p_i+%5Cfrac%7BF_i%28p%29%7D%7Bp_i%7D+%3D++p_i%5Cfrac%7BF_i%28p%29%7D%7Bp_i%7D+-+p_i+%5Csum_j%7Bp_j+%5Cfrac%7BF_j%28p%29%7D%7Bp_j%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i = F_i(p) = p_i &#92;frac{F_i(p)}{p_i} =  p_i&#92;frac{F_i(p)}{p_i} - p_i &#92;sum_j{p_j &#92;frac{F_j(p)}{p_j}}" class="latex" /></p>
<p>which is a replicator equation with fitness functions <img src="https://s0.wp.com/latex.php?latex=N_i%28p%29+%3D+F_i%28p%29+%2F+p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N_i(p) = F_i(p) / p_i" class="latex" />.</p>
<p>AFAIK this was first noticed by Dashiell Fryer and myself [1] and [2]. It was also reported essentially as written above recently in [3], which notes a similar observation due to Smale in 1976 (before the common form of replicator equation was defined, typically attributed to Talyor and Jonker in 1978).</p>
<p>[1] Fryer (2012) On the existence of general equilibrium in finite games and general game dynamics. arXiv:1201.2384<br />
[2] Harper, Fryer (2014). Lyapunov Functions for Time-Scale Dynamics on Riemannian Geometries of the Simplex, DGAA<br />
[3] Raju, Krishnaprasad &#8220;Lie algebra structure of fitness and replicator control&#8221; (2020) arXiv:2005.09792</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166917</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 09 Oct 2020 05:59:51 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166917</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166890&quot;&gt;Mike Stay&lt;/a&gt;.

&lt;blockquote&gt;
  It seems strange that the acceleration of the information is equal to a squared speed. Since information is measured in bits, d²/dt² I is bits per second squared, which means the Fisher speed has units of “square root of bits” per second.
&lt;/blockquote&gt;

Since bits are normally regarded as dimensionless this is tolerable, but I agree it&#039;s strange.  Fundamentally what&#039;s strange is that as you start with $latex p = q$ and start moving $latex q$ away from $latex p,$ the relative information $latex I(q,p)$ doesn&#039;t change to first order---only to second order!  &quot;To first order, you&#039;re never learning anything&quot;.   So relative information is not like distance.  It&#039;s more like the square of distance.

But of course this follows from the fact that $latex I(q,p)$ depends smoothly on $latex q,$ $latex I(q,p) \ge 0$ and $latex I(q,p) = 0$ when $latex q = p.$  A smooth nonnegative function that vanishes at some point must have vanishing derivative at that point.   It can have nonvanishing second derivative, though!

And this is actually connected in a nice way to how distance $latex &#124;x-y&#124;$ is the square root of a fundamentally simpler quantity $latex (x-y)^2.$]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166890">Mike Stay</a>.</p>
<blockquote><p>
  It seems strange that the acceleration of the information is equal to a squared speed. Since information is measured in bits, d²/dt² I is bits per second squared, which means the Fisher speed has units of “square root of bits” per second.
</p></blockquote>
<p>Since bits are normally regarded as dimensionless this is tolerable, but I agree it&#8217;s strange.  Fundamentally what&#8217;s strange is that as you start with <img src="https://s0.wp.com/latex.php?latex=p+%3D+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = q" class="latex" /> and start moving <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> away from <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> doesn&#8217;t change to first order&#8212;only to second order!  &#8220;To first order, you&#8217;re never learning anything&#8221;.   So relative information is not like distance.  It&#8217;s more like the square of distance.</p>
<p>But of course this follows from the fact that <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> depends smoothly on <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) &#92;ge 0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = 0" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=q+%3D+p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = p." class="latex" />  A smooth nonnegative function that vanishes at some point must have vanishing derivative at that point.   It can have nonvanishing second derivative, though!</p>
<p>And this is actually connected in a nice way to how distance <img src="https://s0.wp.com/latex.php?latex=%7Cx-y%7C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|x-y|" class="latex" /> is the square root of a fundamentally simpler quantity <img src="https://s0.wp.com/latex.php?latex=%28x-y%29%5E2.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x-y)^2." class="latex" /></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: leebloomquist		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166896</link>

		<dc:creator><![CDATA[leebloomquist]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 19:46:06 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166896</guid>

					<description><![CDATA[John (if I may),

&#039;Fitness&#039; and &#039;natural selection&#039; lead to the question, &quot;What exactly is it, which has the fitness to be selected by nature?&quot;

In the SEP article on fitness (https://plato.stanford.edu/entries/fitness/) there is some math and a section on &quot;How the Problems of Defining Biological Individuality Affect the Notion of Fitness.&quot; Here, the question of &quot;What is it that&#039;s fit?&quot; raises a key issue in immunology. And then, a book is cited: &quot;The Limits of the Self: Immunology and Biological Identity&quot; by Thomas Pradeu.

The issue is how the immune systems knows to attack an &#039;other,&#039; and when it overreacts (as sometimes in COVID) to attack its &#039;self.&#039;  Here is a review of The Limits of Self--

https://ndpr.nd.edu/news/the-limits-of-the-self-immunology-and-biological-identity/

This question is outside those addressed by Shannon-theoretic information. A different mathematics of information seems to be required. For example, say there is uncertainty among three possible entities within the detection process of the immune system: other1, other2, other3.

In Shannon&#039;s theory of information, if any one of these three is ultimately detected, the amount of information (or &#039;surprisal&#039;) is the same. In this situation, Shannon&#039;s theory makes no difference between other1, other2, or other3. The best text on this characteristic of Shannon information that I&#039;ve read is by Fred Dretske-- Chapter 1 of his book &#039;Knowledge and the Flow of Information.&#039;

Put another way, in this case, the problem for a mathematical theory of information is to detect the immunological &#039;self.&#039; As well as others.

Now consider situation theory, channel theory, or &#039;informationalism&#039; as introduced by Jon Barwise (the last, shortly before he passed away). My apologies for not formatting the math using latex. But here is some text on how that mathematical theory of information works to identify self and others:

First the self is a constituent of a situation. For example, say that along the lines of The Limits of Self, the self is a continuous process, symbolized in ordinary Petri nets by a transition (the continuous process), with one of its arrows pointed to a place labelled &#039;self&#039; (a possibility), and then an arrow from that place back into the transition (the continuous process). This Petri net could symbolize the self inside its situation.

Now add an element of information (an &#039;infon&#039;) to this situation-- that the self knows that it is in this situation in which it exists, or in which it occurs.

And then-- if there are any others in this situation, by knowing the situation in which they, as well as its self, exist, it therefore knows them as well.

This machinery is detailed using mathematical symbols in Barwise&#039;s Situation in Logic-- in his chapter on common knowledge.

https://web.stanford.edu/group/cslipublications/cslipublications/site/0937073326.shtml

https://projecteuclid.org/euclid.ndjfl/1039540766]]></description>
			<content:encoded><![CDATA[<p>John (if I may),</p>
<p>&#8216;Fitness&#8217; and &#8216;natural selection&#8217; lead to the question, &#8220;What exactly is it, which has the fitness to be selected by nature?&#8221;</p>
<p>In the SEP article on fitness (<a href="https://plato.stanford.edu/entries/fitness/" rel="nofollow ugc">https://plato.stanford.edu/entries/fitness/</a>) there is some math and a section on &#8220;How the Problems of Defining Biological Individuality Affect the Notion of Fitness.&#8221; Here, the question of &#8220;What is it that&#8217;s fit?&#8221; raises a key issue in immunology. And then, a book is cited: &#8220;The Limits of the Self: Immunology and Biological Identity&#8221; by Thomas Pradeu.</p>
<p>The issue is how the immune systems knows to attack an &#8216;other,&#8217; and when it overreacts (as sometimes in COVID) to attack its &#8216;self.&#8217;  Here is a review of The Limits of Self&#8211;</p>
<p><a href="https://ndpr.nd.edu/news/the-limits-of-the-self-immunology-and-biological-identity/" rel="nofollow ugc">https://ndpr.nd.edu/news/the-limits-of-the-self-immunology-and-biological-identity/</a></p>
<p>This question is outside those addressed by Shannon-theoretic information. A different mathematics of information seems to be required. For example, say there is uncertainty among three possible entities within the detection process of the immune system: other1, other2, other3.</p>
<p>In Shannon&#8217;s theory of information, if any one of these three is ultimately detected, the amount of information (or &#8216;surprisal&#8217;) is the same. In this situation, Shannon&#8217;s theory makes no difference between other1, other2, or other3. The best text on this characteristic of Shannon information that I&#8217;ve read is by Fred Dretske&#8211; Chapter 1 of his book &#8216;Knowledge and the Flow of Information.&#8217;</p>
<p>Put another way, in this case, the problem for a mathematical theory of information is to detect the immunological &#8216;self.&#8217; As well as others.</p>
<p>Now consider situation theory, channel theory, or &#8216;informationalism&#8217; as introduced by Jon Barwise (the last, shortly before he passed away). My apologies for not formatting the math using latex. But here is some text on how that mathematical theory of information works to identify self and others:</p>
<p>First the self is a constituent of a situation. For example, say that along the lines of The Limits of Self, the self is a continuous process, symbolized in ordinary Petri nets by a transition (the continuous process), with one of its arrows pointed to a place labelled &#8216;self&#8217; (a possibility), and then an arrow from that place back into the transition (the continuous process). This Petri net could symbolize the self inside its situation.</p>
<p>Now add an element of information (an &#8216;infon&#8217;) to this situation&#8211; that the self knows that it is in this situation in which it exists, or in which it occurs.</p>
<p>And then&#8211; if there are any others in this situation, by knowing the situation in which they, as well as its self, exist, it therefore knows them as well.</p>
<p>This machinery is detailed using mathematical symbols in Barwise&#8217;s Situation in Logic&#8211; in his chapter on common knowledge.</p>
<p><a href="https://web.stanford.edu/group/cslipublications/cslipublications/site/0937073326.shtml" rel="nofollow ugc">https://web.stanford.edu/group/cslipublications/cslipublications/site/0937073326.shtml</a></p>
<p><a href="https://projecteuclid.org/euclid.ndjfl/1039540766" rel="nofollow ugc">https://projecteuclid.org/euclid.ndjfl/1039540766</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Toby Bartels		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166894</link>

		<dc:creator><![CDATA[Toby Bartels]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 18:41:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166894</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166893&quot;&gt;Phillip Helbig&lt;/a&gt;.

There must be a sign error somewhere; Trump is massively in debt.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166893">Phillip Helbig</a>.</p>
<p>There must be a sign error somewhere; Trump is massively in debt.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phillip Helbig		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166893</link>

		<dc:creator><![CDATA[Phillip Helbig]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 18:36:36 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166893</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166892&quot;&gt;Phillip Helbig&lt;/a&gt;.

On a related note, power is energy per time and of course time is money and knowledge is power.  Solving for money, we see that it diverges as knowledge goes to zero, regardless of the energy (e.g. tweeting) expended, which explains why Donald Trump earns more than I do.  :-)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166892">Phillip Helbig</a>.</p>
<p>On a related note, power is energy per time and of course time is money and knowledge is power.  Solving for money, we see that it diverges as knowledge goes to zero, regardless of the energy (e.g. tweeting) expended, which explains why Donald Trump earns more than I do.  :-)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phillip Helbig		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166892</link>

		<dc:creator><![CDATA[Phillip Helbig]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 18:31:56 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166892</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166890&quot;&gt;Mike Stay&lt;/a&gt;.

But is that meaningful?  Think of something like flux density which is power per area per hertz.  But since power is energy per time this is energy per area per time per hertz, but hertz has units of 1 over time so they cancel and we are left with energy per area which is formally correct in a sense but intuitively removed from the concept of flux density.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166890">Mike Stay</a>.</p>
<p>But is that meaningful?  Think of something like flux density which is power per area per hertz.  But since power is energy per time this is energy per area per time per hertz, but hertz has units of 1 over time so they cancel and we are left with energy per area which is formally correct in a sense but intuitively removed from the concept of flux density.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Mike Stay		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166890</link>

		<dc:creator><![CDATA[Mike Stay]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 16:27:05 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166890</guid>

					<description><![CDATA[It seems strange that the acceleration of the information is equal to a squared speed.  Since information is measured in bits, d²/dt² I is bits per second squared, which means the Fisher speed has units of &quot;square root of bits&quot; per second.]]></description>
			<content:encoded><![CDATA[<p>It seems strange that the acceleration of the information is equal to a squared speed.  Since information is measured in bits, d²/dt² I is bits per second squared, which means the Fisher speed has units of &#8220;square root of bits&#8221; per second.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Phillip Helbig		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166880</link>

		<dc:creator><![CDATA[Phillip Helbig]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 09:31:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166880</guid>

					<description><![CDATA[Off topic, but what is your take on the fact that a mathematical physicist has won the Nobel Prize for physics?  Is that the first time that has happened?  Actually, Penrose is a mathematician by training.  (Fair enough I suppose since physicist Edward Witten, who also has a degree with a major in history and minor in linguistics and worked as a journalist and studied economics and maths for a while before coming to physics---not that it took long since he was a full professor at the IAS at Princeton at just barely 26---has won the Fields Medal.)]]></description>
			<content:encoded><![CDATA[<p>Off topic, but what is your take on the fact that a mathematical physicist has won the Nobel Prize for physics?  Is that the first time that has happened?  Actually, Penrose is a mathematician by training.  (Fair enough I suppose since physicist Edward Witten, who also has a degree with a major in history and minor in linguistics and worked as a journalist and studied economics and maths for a while before coming to physics&#8212;not that it took long since he was a full professor at the IAS at Princeton at just barely 26&#8212;has won the Fields Medal.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166879</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 05:03:47 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166879</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166876&quot;&gt;Toby Bartels&lt;/a&gt;.

Yes, the Lotka--Volterra equations are scarcely more than the general system of first-order time-independent ODE!  I should have said this.  It&#039;s interesting you can get anything out of them at all.

The main reason for writing

$latex \dot{P}_i = f_i(P_1, \dots, P_n) P_i $

instead of

$latex \dot{P}_i = g_i(P_1, \dots, P_n) $

is that the functions $latex f_i$ do useful things for us when we bring in probabilities: their means, variances and such are interesting.

But it&#039;s also important that the Lotka--Volterra equations imply that if $latex P_i(t)$ vanishes at some time it vanishes at all later times (since I&#039;m assuming the $latex f_i$ are differentiable---Lipschitz would be enough for this result).   This means there&#039;s &quot;no true novelty&quot;: no species can come into existence if it&#039;s not there already.   This means we&#039;re not doing a full-fledged study of &lt;i&gt;mutation&lt;/i&gt;.   So we&#039;re studying &quot;natural selection&quot; but not this other important aspect of evolution.   (There are also lots of other aspects of evolution that we&#039;re not getting into, of course.)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166876">Toby Bartels</a>.</p>
<p>Yes, the Lotka&#8211;Volterra equations are scarcely more than the general system of first-order time-independent ODE!  I should have said this.  It&#8217;s interesting you can get anything out of them at all.</p>
<p>The main reason for writing</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdot%7BP%7D_i+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+P_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{P}_i = f_i(P_1, &#92;dots, P_n) P_i " class="latex" /></p>
<p>instead of</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdot%7BP%7D_i+%3D+g_i%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{P}_i = g_i(P_1, &#92;dots, P_n) " class="latex" /></p>
<p>is that the functions <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> do useful things for us when we bring in probabilities: their means, variances and such are interesting.</p>
<p>But it&#8217;s also important that the Lotka&#8211;Volterra equations imply that if <img src="https://s0.wp.com/latex.php?latex=P_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t)" class="latex" /> vanishes at some time it vanishes at all later times (since I&#8217;m assuming the <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> are differentiable&#8212;Lipschitz would be enough for this result).   This means there&#8217;s &#8220;no true novelty&#8221;: no species can come into existence if it&#8217;s not there already.   This means we&#8217;re not doing a full-fledged study of <i>mutation</i>.   So we&#8217;re studying &#8220;natural selection&#8221; but not this other important aspect of evolution.   (There are also lots of other aspects of evolution that we&#8217;re not getting into, of course.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Toby Bartels		</title>
		<link>https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comment-166876</link>

		<dc:creator><![CDATA[Toby Bartels]]></dc:creator>
		<pubDate>Thu, 08 Oct 2020 01:33:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=28874#comment-166876</guid>

					<description><![CDATA[Typo:  You have a stray comma in the Lotka–Volterra equation (the first time).

This reminds me to remark that these generalized Lotka–Volterra equations hardly say anything more than that the quantities are governed by a system of time-independent differential equations at all.  What they do say beyond that is that $latex \dot{P}_i = 0$ when $latex P_i = 0$ (and that any implicit continuity assumptions you make about the differential equations hold even more strongly at zero).  This is quite reasonable, because if the population is zero at any time, then it can hardly change thereafter.  But you only look in the interior where none of the $latex P_i$ are zero anyway, so now it says nothing extra again.]]></description>
			<content:encoded><![CDATA[<p>Typo:  You have a stray comma in the Lotka–Volterra equation (the first time).</p>
<p>This reminds me to remark that these generalized Lotka–Volterra equations hardly say anything more than that the quantities are governed by a system of time-independent differential equations at all.  What they do say beyond that is that <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7BP%7D_i+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{P}_i = 0" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=P_i+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i = 0" class="latex" /> (and that any implicit continuity assumptions you make about the differential equations hold even more strongly at zero).  This is quite reasonable, because if the population is zero at any time, then it can hardly change thereafter.  But you only look in the interior where none of the <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> are zero anyway, so now it says nothing extra again.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
