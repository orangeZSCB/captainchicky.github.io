<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>risks | Azimuth | Page 5</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; risks Category Feed" href="https://johncarlosbaez.wordpress.com/category/risks/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F09%2F01%2Fmelting-permafrost%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/risks\/page\/5\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Frisks%2Fpage%2F5%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F09%2F01%2Fmelting-permafrost%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="risks &#8211; Page 5 &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/risks/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about risks written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive paged category category-risks category-93974 paged-5 category-paged-5 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-4708 post type-post status-publish format-standard hentry category-climate category-risks" id="post-4708">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/09/01/melting-permafrost/" rel="bookmark">Melting Permafrost (Part&nbsp;1)</a></h2>
				<small>1 September, 2011</small><br />


				<div class="entry">
					<p>Some people worry about rising sea levels due to global warming.  But that will happen slowly.  I worry about tipping points.</p>
<p>The word &#8220;tipping point&#8221; should remind you of pushing on a glass of water.  If you push it a little, and then stop, it&#8217;ll right itself: no harm done.  But if you push it past a certain point, it starts tipping over.  Then it&#8217;s hard to stop.</p>
<p>So, we need to study possible tipping points in the Earth&#8217;s climate system.  Here&#8217;s a list of them:</p>
<p>&bull; <a href="http://www.azimuthproject.org/azimuth/show/Tipping+point">Tipping point</a>, Azimuth Library.</p>
<p>Today I want to talk about one: melting <a href="http://en.wikipedia.org/wiki/Permafrost">permafrost</a>.  When melting permafrost in the Arctic starts releasing lots of carbon dioxide and <a href="http://en.wikipedia.org/wiki/Atmospheric_methane">methane</a>&#8212;a vastly more potent greenhouse gas&#8212;the Earth will get even hotter.  That, in turn, will melt even more permafrost.  In theory, this feedback loop could tip the Earth over to a much hotter state.  But how much should we worry about this?</p>
<p>Climate activist <a href="http://www.azimuthproject.org/azimuth/show/Joseph+Romm">Joe Romm</a> takes it very seriously:</p>
<p>&bull; Joe Romm, <a href="http://thinkprogress.org/romm/2011/02/17/207552/nsidc-thawing-permafrost-will-turn-from-carbon-sink-to-source-in-mid-2020s-releasing-100-billion-tons-of-carbon-by-2100/">NSIDC bombshell: Thawing permafrost feedback will turn Arctic from carbon sink to source in the 2020s, releasing 100 billion tons of carbon by 2100</a>, <i>Climate Progress</i>, 17 February 2011.</p>
<p>If you click on just one link of mine today, let it be this!  He writes in a clear, snappy way.  But let me take you through some of the details in my own more pedestrian fashion.</p>
<p>For starters, the Arctic is melting.  Here&#8217;s a graph of Arctic sea ice volume created by the <a href="http://psc.apl.washington.edu/wordpress/research/projects/arctic-sea-ice-volume-anomaly/">Pan-Arctic Ice Ocean Modeling and Assimilation System</a>&mdash;click to enlarge:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/temperature/arctic_sea_ice_volume_PIOMAS_2011.jpg"><img src="https://i1.wp.com/math.ucr.edu/home/baez/temperature/arctic_sea_ice_volume_PIOMAS_2011_small.jpg" /></a></div>
<p>The blue line is the linear best fit, but you can see it&#8217;s been melting faster lately.  Is this a glitch or a new trend?  Time will tell.  </p>
<p>2011 is considerably worse than 2007, the previous record-holder.  Here you can clearly see the estimated total volume in thousands of cubic kilometers, and how it changes with the seasons:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/temperature/arctic_sea_ice_volume_PIOMAS_annual_2011.jpg"><img src="https://i0.wp.com/math.ucr.edu/home/baez/temperature/arctic_sea_ice_volume_PIOMAS_annual_2011_small.jpg" /><br />
</a></div>
<p>As the Arctic melts, many things are changing.  The fabled Northwest Passage is becoming a practical waterway, so battles are starting to heat up over who controls it. The U.S. and other nations see it as an international waterway. But Canada <i>says</i> they own it, and have the right to regulate and protect it:</p>
<p>&bull; Jackie Northam, <a href="http://www.npr.org/2011/08/15/139556207/arctic-warming-unlocking-a-fabled-waterway">Arctic warming unlocking a fabled waterway</a>, <i>Morning Edition</i>, National Public Radio, 15 August 2011.</p>
<p>But the 800-pound gorilla in the room is the melting permafrost.   A lot of the Arctic is covered by permafrost, and it stores a lot of carbon, both as peat and as methane.  After all, peat is rotten plant material, and rotting plants make methane.   Recent work estimates that between 1400 and 1700 gigatonnes of carbon is stored in permafrost soils worldwide:</p>
<p>&bull; C. Tarnocai, J. G. Canadell, E. A. G. Schuur, P. Kuhry, G. Mazhitova, and S. Zimov, <a href="http://citeseerx.ist.psu.edu/search;jsessionid=54ECC51F1C0E23292094E566C265B629?q=Soil+organic+carbon+pools+in+the+northern+circumpolar+permafrost+region%2C&amp;submit=Search&amp;sort=rel">Soil organic carbon pools in the northern circumpolar permafrost region</a>, <i><a href="http://www.agu.org/pubs/crossref/2009/2008GB003327.shtml">Global Biogeochemical Cycles</a></i> <b>23</b> (2009), GB2023.</p>
<p>That&#8217;s more carbon than currently resides in all living things, and twice as much carbon as held by the atmosphere!</p>
<p>How much of this carbon will be released as the Arctic melts&#8212;and how fast?  There&#8217;s a new paper about that:</p>
<p>&bull; Kevin Schaefer, Tingjun Zhang, Lori Bruhwiler, Andrew Barrett, Amount and timing of permafrost carbon release in response to climate warming, <i><a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0889.2011.00527.x/full">Tellus B</a></i> <b>63</b> (2011), 165–180.</p>
<p>It&#8217;s not free, but you can read <a href="http://thinkprogress.org/romm/2011/02/17/207552/nsidc-thawing-permafrost-will-turn-from-carbon-sink-to-source-in-mid-2020s-releasing-100-billion-tons-of-carbon-by-2100/">Joe Romm&#8217;s summary</a>.  Here&#8217;s their estimate on how carbon will be released by melting permafrost:</p>
<div align="center"><a href="http://thinkprogress.org/romm/2011/02/17/207552/nsidc-thawing-permafrost-will-turn-from-carbon-sink-to-source-in-mid-2020s-releasing-100-billion-tons-of-carbon-by-2100/"><img src="https://i1.wp.com/math.ucr.edu/home/baez/temperature/permafrost_carbon_flux_Schaefer_et_al.gif" /></a></div>
<p>So, they&#8217;re guessing that melting permafrost will release a gigatonne of carbon per year by the mid-2030s.  Moreover, they say:</p>
<blockquote><p>
We predict that the PCF [permafrost carbon feedback] will change the Arctic from a carbon sink to a source after the mid-2020s and is strong enough to cancel 42-88% of the total global land sink. The thaw and decay of permafrost carbon is irreversible and accounting for the PCF will require larger reductions in fossil fuel emissions to reach a target atmospheric CO<sub>2</sub> concentration.
</p></blockquote>
<p>One of the authors explains more details <a href="http://nsidc.org/news/press/20110216_permafrost.html">here</a>:</p>
<blockquote><p>
&#8220;The amount of carbon released [by 2200] is equivalent to half the amount of carbon that has been released into the atmosphere since the dawn of the industrial age,” said NSIDC scientist Kevin Schaefer.  &#8220;That is a lot of carbon.&#8221;</p>
<p>The carbon from permanently frozen ground known as permafrost &#8220;will make its impact, not only on the climate, but also on international strategies to reduce climate change Schaefer said.  &#8220;If we want to hit a target carbon concentration, then we have to reduce fossil fuel emissions that much lower than previously calculated to account for this additional carbon from the permafrost,&#8221; Schaefer said. &#8220;Otherwise we will end up with a warmer Earth than we want.&#8221;</p>
<p>The carbon comes from plant material frozen in soil during the ice age of the Pleistocene: the icy soil trapped and preserved the biomass for thousands of years.  Schaefer equates the mechanism to storing broccoli in the home freezer: &#8220;As long as it stays frozen, it stays stable for many years,&#8221; he said. “But you take it out of the freezer and it will thaw out and decay.&#8221;</p>
<p>Now, permafrost is thawing in a warming climate and &#8220;just like the broccoli&#8221; the biomass will thaw and decay, releasing carbon into the atmosphere like any other decomposing plant material, Schaefer said.  To predict how much carbon will enter the atmosphere and when, Schaefer and coauthors modeled the thaw and decay of organic matter currently frozen in permafrost under potential future warming conditions as predicted by the Intergovernmental Panel on Climate Change.</p>
<p>They found that between 29-59 percent of the permafrost will disappear by 2200. That permafrost took tens of thousands of years to form, but will melt in less than 200, Schaefer said.
</p></blockquote>
<p>Sound alarmist?  In fact, there are three unrealistically conservative assumptions built into this paper:</p>
<p>1) The authors assume the &#8216;moderate warming&#8217; scenario called A1B, which has atmospheric concentrations of CO<sub>2</sub> reaching 520 ppm by 2050 and stabilizing at 700 ppm in 2100.   But so far we seem to be living out the <a href="http://www.ipcc-data.org/sres/ddc_sres_emissions.html">A1F1 scenario</a>, which reaches 1000 ppm by century’s end.</p>
<p>2) Their estimate of future temperatures neglects the effect of greenhouse gases released by melting permafrost.</p>
<p>3) They assume all carbon emitted by permafrost will be in the form of CO<sub>2</sub>, not methane.</p>
<p>Point 2) means that <i>the whole question of a feedback loop is not explored in this paper</i>.  I understand why.  To do that, you can&#8217;t use someone else&#8217;s climate model: you need to build your own!  But it&#8217;s something we need to study.  Do you know anyone who is?  Joe Romm says:</p>
<blockquote><p>
Countless studies make clear that global warming will release vast quantities of greenhouse gases into the atmosphere this decade.  Yet, <b>no climate model currently incorporates the amplifying feedback from methane released by a defrosting tundra.</b>
</p></blockquote>
<p>If we try to understand this feedback, point 3) becomes important.  After all, while methane goes away faster than CO<sub>2</sub>, its greenhouse effect is much stronger while it lasts.   For the first 20 years, methane has about 72 times the <a href="http://en.wikipedia.org/wiki/Global-warming_potential">global warming potential</a> of carbon dioxide.  Over the first 100 years, it&#8217;s about 25 times as powerful.</p>
<p>Let&#8217;s think about that a minute.  In 2008, we burnt about 8 gigatonnes of carbon.  If Schaefer <i>et al</i> are right, we can expect 1 extra gigatonne of carbon to be released from Arctic permafrost by around 2035.  If that&#8217;s almost all in the form of carbon dioxide, it makes our situation slightly worse.  But if a lot of it is methane, which is&#8212;let&#8217;s roughly say&#8212;72 times as bad&#8212;then our situation will be <i>dramatically</i> worse.  </p>
<p>But I don&#8217;t know how much of the carbon released will be in the form of methane.  I also don&#8217;t know how much of the methane will turn into other organic compounds before it gets into the atmosphere.  I&#8217;d really like to know!</p>
<p>I hope you learn more about this stuff and help me out.  Here are a few good references available for free online, to get started:</p>
<p>&bull; Edward A. G. Schuur <i>et al</i>, <a href="http://www.aibs.org/bioscience-press-releases/resources/Schuur.pdf">Vulnerability of permafrost carbon to climate change: implications for the global carbon cycle</a>, <i>Bioscience</i> <b>58</b> (2008), 701-714.</p>
<p>&bull; David M. Lawrence, Andrew G. Slater, Robert A. Tomas, Marika M. Holland and Clara Deser, <a href="http://www.colorado.edu/geography/class_homepages/geog_4271_f10/readings/week_10_lawrence_et_al_2008.pdf">Accelerated Arctic land warming and permafrost degradation during rapid sea ice loss</a>, <i>Geophysical Research Letters</i> <b>35</b> (2008), L11506.</p>
<p>&bull; Amanda Leigh Mascarelli, <a href="http://www.nature.com/climate/2009/0904/full/climate.2009.24.html">A sleeping giant?</a>, <i>Nature Reports Climate Change</i>, 5 March 2009.</p>
<p>The last one discusses the rise in atmospheric methane that was observed in 2007:</p>
<div align="center">
<a href="http://www.nature.com/climate/2009/0904/fig_tab/climate.2009.24_F1.html"><img src="https://i0.wp.com/www.nature.com/climate/2009/0904/images/climate.2009.24-f1.jpg" /></a></div>
<p>It also discusses the dangers of methane being released from ice-methane crystals called <a href="http://en.wikipedia.org/wiki/Methane_clathrate">methane clathrates</a> at the bottom of the ocean&#8212;something I&#8217;m deliberately <i>not</i> talking about here, because it deserves its own big discussion.  However, there are also clathrates in the permafrost.  Here&#8217;s a picture by W. F. Kuhs, showing what methane clathrate looks like at the atomic scale:</p>
<div align="center"><a href="http://peggy.uni-mki.gwdg.de/docs/kuhs/clathrate_hydrates.html"><img src="https://i2.wp.com/peggy.uni-mki.gwdg.de/images/clath_hydr/ch4cage.gif" /></a></div>
<p>The green guy in the middle is methane, trapped in a cage of water molecules.  Click for more details.  </p>
<p>If you know more good references, please tell me about them here or add them to:</p>
<p>&bull; <a href="http://www.azimuthproject.org/azimuth/show/Permafrost">Permafrost</a>, Azimuth Library.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/09/01/melting-permafrost/#comments">40 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/09/01/melting-permafrost/" rel="bookmark" title="Permanent Link to Melting Permafrost (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-4572 post type-post status-publish format-standard hentry category-probability category-risks" id="post-4572">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/08/19/bayesian-computations-of-expected-utility/" rel="bookmark">Bayesian Computations of Expected&nbsp;Utility</a></h2>
				<small>19 August, 2011</small><br />


				<div class="entry">
					<p><a href="http://www.givewell.org/">GiveWell</a> is an organization that rates charities.  They&#8217;ve met people who argue that</p>
<blockquote>
<p>charities working on reducing the risk of sudden human extinction must be the best ones to support, since the value of saving the human race is so high that “any imaginable probability of success” would lead to a higher expected value for these charities than for others.</p>
</blockquote>
<p>For example, say I have a dollar to spend on charity.  One charity says that with this dollar they can save the life of one child in Somalia.   Another says that with this dollar they can increase by .000001% our chance of saving 1 billion people from the effects of a massive asteroid colliding with the Earth.</p>
<p>Naively, in terms of the expected number of lives saved, the latter course of action seems 10 times better, since</p>
<div align="center">
.000001% &times; 1 billion = 10
</div>
<p>But is it really better?</p>
<p>It&#8217;s a subtle question, with all sorts of complicating factors, like <i>why should I trust these guys?</i>  </p>
<p>I&#8217;m not ready to present a thorough analysis of this sort of question today.   But I would like to hear what you think about it.  And I&#8217;d like you to read what the founder of Givewell has to say about it:</p>
<p>&bull; Holden Karnofsky, <a href="http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/">Why we can’t take expected value estimates literally (even when they’re unbiased)</a>, 18 August 2011.</p>
<p>He argues against what he calls an &#8216;explicit expected value&#8217; or &#8216;EEV&#8217; approach:</p>
<blockquote><p>
The mistake (we believe) is estimating the “expected value” of a donation (or other action) based solely on a fully explicit, quantified formula, many of whose inputs are guesses or very rough estimates. We believe that any estimate along these lines needs to be adjusted using a “Bayesian prior”; that this adjustment can rarely be made (reasonably) using an explicit, formal calculation; and that most attempts to do the latter, even when they seem to be making very conservative downward adjustments to the expected value of an opportunity, are not making nearly large enough downward adjustments to be consistent with the proper Bayesian approach.
</p></blockquote>
<p>His focus, in short, is on the fact that anyone saying &#8220;this money can increase by .000001% our chance of saving 1 billion people from an asteroid impact&#8221; is likely to be <i>pulling those numbers from thin air</i>.  If they can&#8217;t really back up their numbers with a lot of hard evidence, then our lack of confidence in their estimate should be taken into account somehow. </p>
<p>His article spends a lot of time analyzing a less complex but still very interesting example:</p>
<blockquote><p>
It seems fairly clear that a restaurant with 200 Yelp reviews, averaging 4.75 stars, ought to outrank a restaurant with 3 Yelp reviews, averaging 5 stars. Yet this ranking can’t be justified in an explicit expected utility framework, in which options are ranked by their estimated average/expected value.
</p></blockquote>
<p>This is the only question I really want to talk about today.  Actually I&#8217;ll focus on a similar question that Tim van Beek posed <a href="https://johncarlosbaez.wordpress.com/2011/07/15/rationality-in-humans-and-monkeys/#comment-6751">on this blog</a>:</p>
<blockquote><p>
You have two kinds of fertilizer, A and B. You know that of 4 trees who got A, three thrived and one died. Of 36 trees that got B, 24 thrived and 12 died. Which fertilizer would you buy?
</p></blockquote>
<p>So, 3/4 of the trees getting fertilizer A thrived, while only 2/3 of those getting fertilizer B thrived.  That makes fertilizer A seem better.  However, the sample size is considerably larger for fertilizer B, so we may feel <i>more confident</i> about the results in this case.  Which should we choose?</p>
<p>Nathan Urban <a href="https://johncarlosbaez.wordpress.com/2011/07/15/rationality-in-humans-and-monkeys/#comment-6790">tackled the problem in an interesting way</a>.  Let me sketch what he did before showing you his detailed work.  </p>
<p>Suppose that before doing any experiments at all, we assume the probability <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> that a fertilizer will make a tree thrive is a number <i>uniformly distributed between 0 and 1</i>.  This assumption is our &#8220;Bayesian prior&#8221;. </p>
<p>Note: I&#8217;m not saying this prior is &#8220;correct&#8221;.  You are allowed to choose a different prior!  Choosing a different prior will change your answer to this puzzle.  That can&#8217;t be helped.  We need to make some assumption to answer this kind of puzzle; we are simply making it explicit here.</p>
<p>Starting from this prior, Nathan works out the probability that <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> has some value <i>given that when we apply the fertilizer to 4 trees, 3 thrive</i>.   That&#8217;s the black curve below.   He also works out the probability that <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> has some value  <i>given that when we apply the fertilizer to 36 trees, 24 thrive</i>.  That&#8217;s the red curve:</p>
<div align="center"><a href="http://img30.imageshack.us/img30/1774/binomial.png" rel="nofollow"><img width="450" src="https://i2.wp.com/img30.imageshack.us/img30/1774/binomial.png" /></a></div>
<p>The red curve, corresponding to the experiment with 36 trees, is much more sharply peaked.  That makes sense.   It means that <i>when we do more experiments, we become more confident that we know what&#8217;s going on</i>.</p>
<p>We still have to choose a criterion to decide which fertilizer is best!  This is where &#8216;decision theory&#8217; comes in.  For example, suppose we want to maximize the expected number of the trees that thrive.  Then Nathan shows that fertilizer A is slightly better, despite the smaller sample size.</p>
<p>However, he also shows that if fertilizer A succeeded 4 out of 5 times, while fertilizer B succeeded 7 out of 9 times, the same evaluation procedure would declare fertilizer B better!  Its percentage success rate is less: about 78% instead of 80%.  However, the sample size is larger.  And in this particular case, <i>given our particular Bayesian prior</i> and <i>given what we are trying to maximize</i>, that&#8217;s enough to make fertilizer B win.</p>
<p>So if someone is trying to get you to contribute to a charity, there are many interesting issues involved in deciding if their arguments are valid or just a bunch of&#8230; fertilizer.</p>
<p>Here is Nathan&#8217;s detailed calculation:</p>
<blockquote>
<p>It&#8217;s fun to work out an official &#8216;correct&#8217; answer mathematically, as John suggested.  Of course, this ends up being a long way of confirming the obvious&#8212;and the answer is only as good as the assumptions&#8212;but I think it&#8217;s interesting anyway.  In this case, I&#8217;ll work it out by maximizing expected utility in Bayesian decision theory, for one choice of utility function.  This dodges the whole risk aversion point, but it opens discussion for how the assumptions might be modified to account for more real-world considerations.  Hopefully others can spot whether I&#8217;ve made mistakes in the derivations.</p>
<p>In Bayesian decision theory, the first thing you do is write down the data-generating process and then compute a posterior distribution for what is unknown.</p>
<p>In this case, we may assume the data-generating process (likelihood function) is a binomial distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BBin%7D%28s%2Cn%7C%5Cpi%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Bin}(s,n|&#92;pi)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> successes in <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> trials, given a probability of success <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" />.  Fertilizer A corresponds to <img src="https://s0.wp.com/latex.php?latex=s%3D3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s=3" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=n%3D4&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n=4" class="latex" /> and fertilizer B corresponds to <img src="https://s0.wp.com/latex.php?latex=s%3D24&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s=24" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=n%3D36&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n=36" class="latex" />.</p>
<p>The probability of success <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> is unknown, and we want to infer its posterior conditional on the data, <img src="https://s0.wp.com/latex.php?latex=p%28%5Cpi%7Cs%2Cn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;pi|s,n)" class="latex" />.  To compute a posterior we need to assume a prior on <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" />.</p>
<p>It turns out that the <a href="http://en.wikipedia.org/wiki/Beta_distribution" rel="nofollow">Beta</a> distribution is <a href="http://en.wikipedia.org/wiki/Conjugate_prior" rel="nofollow">conjugate</a> to a binomial likelihood, meaning that if we assume a Beta distributed prior, the then the posterior is also Beta distributed.  If the prior is <img src="https://s0.wp.com/latex.php?latex=%5Cpi+%5Csim+%5Cmathrm%7BBeta%7D%28%5Calpha_0%2C%5Cbeta_0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi &#92;sim &#92;mathrm{Beta}(&#92;alpha_0,&#92;beta_0)" class="latex" /> then the posterior is </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi+%5Csim+%5Cmathrm%7BBeta%7D%28%5Calpha%3D%5Calpha_0%2Bs%2C%5Cbeta%3D%5Cbeta_0%2Bn-s%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi &#92;sim &#92;mathrm{Beta}(&#92;alpha=&#92;alpha_0+s,&#92;beta=&#92;beta_0+n-s)." class="latex" /></p>
<p>One choice for a prior is a uniform prior on <img src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,1]" class="latex" />, which corresponds to a <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BBeta%7D%281%2C1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Beta}(1,1)" class="latex" /> distribution.  There are of course other prior choices which will lead to different conclusions.  For this prior, the posterior is <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BBeta%7D%28%5Cpi%3B+s%2B1%2C+n-s%2B1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Beta}(&#92;pi; s+1, n-s+1)" class="latex" />.  The posterior mode is </p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5Calpha-1%29%2F%28%5Calpha%2B%5Cbeta-2%29+%3D+s%2Fn&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;alpha-1)/(&#92;alpha+&#92;beta-2) = s/n" class="latex" /> </p>
<p>and the posterior mean is </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha%2F%28%5Calpha%2B%5Cbeta%29+%3D+%28s%2B1%29%2F%28n%2B2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha/(&#92;alpha+&#92;beta) = (s+1)/(n+2)." class="latex" /></p>
<p>So, what is the inference for fertilizers A and B?  I made a graph of the posterior distributions.  You can see that the inference for fertilizer B is sharper, as expected, since there is more data.  But the inference for fertilizer A tends towards higher success rates, which can be quantified.</p>
<div align="center"><a href="http://img30.imageshack.us/img30/1774/binomial.png" rel="nofollow"><img width="450" src="https://i2.wp.com/img30.imageshack.us/img30/1774/binomial.png" /></a></div>
<p>Fertilizer A has a posterior mode of 3/4 = 0.75 and B has a mode of 2/3 = 0.667, corresponding to the sample proportions.  The mode isn&#8217;t the only measure of central tendency we could use.  The means are 0.667 for A and 0.658 for B; the medians are 0.686 for A and 0.661 for B.  No matter which of the three statistics we choose, fertilizer A looks better than fertilizer B.</p>
<p>But we haven&#8217;t really done &#8220;decision theory&#8221; yet.  We&#8217;ve just compared point estimators.  Actually, we have done a little decision theory, implicitly.  It turns out that picking the mean corresponds to the estimator which minimizes the expected squared error in <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" />, where &#8220;squared error&#8221; can be thought of formally as a loss function in decision theory.  Picking the median corresponds to minimizing the expected absolute loss, and picking the mode corresponds to minimizing the minimizing the 0-1 loss (where you lose nothing if you guess <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> exactly and lose 1 otherwise).</p>
<p>Still, these don&#8217;t really correspond to a decision theoretic view of the problem.  We don&#8217;t care about the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> at all, let alone some point estimator of it.  We only care about <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> indirectly, insofar as it helps us predict something about what the fertilizer will do to new trees.  For that, we have to move from the posterior distribution <img src="https://s0.wp.com/latex.php?latex=p%28%5Cpi%7Cs%2Cn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;pi|s,n)" class="latex" /> to the predictive distribution </p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28y%7Cs%2Cn%29+%3D+%5Cint+p%28y%7C%5Cpi%2Cn%29%5C%2Cp%28%5Cpi%7Cs%2Cn%29%5C%2Cd%5Cpi+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(y|s,n) = &#92;int p(y|&#92;pi,n)&#92;,p(&#92;pi|s,n)&#92;,d&#92;pi ," class="latex" /> </p>
<p>where <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> is a random variable indicating whether a new tree will thrive under treatment.  Here I assume that the success of new trees follows the same binomial distribution as in the experimental group.</p>
<p>For a Beta posterior, the predictive distribution is beta-binomial, and the expected number of successes for a new tree is equal to the mean of the Beta distribution for <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> &#8211; i.e. the posterior mean we computed before, <img src="https://s0.wp.com/latex.php?latex=%28s%2B1%29%2F%28n%2B2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(s+1)/(n+2)" class="latex" />.  If we introduce a utility function such that we are rewarded 1 util for a thriving tree and 0 utils for non-thriving tree, then the expected utility is equal to the expected success rate.  Therefore, under these assumptions, we should choose the fertilizer that maximizes the quantity <img src="https://s0.wp.com/latex.php?latex=%28s%2B1%29%2F%28n%2B2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(s+1)/(n+2)" class="latex" />, which, as we&#8217;ve seen, favors fertilizer A (0.667) over fertilizer B (0.658).</p>
<p>An interesting mathematical question is, does this ever work out to a &#8220;non-obvious&#8221; conclusion?  That is, if fertilizer A has a sample success rate which is greater than fertilizer B&#8217;s sample success rate, but expected utility maximization prefers fertilizer B?  Mathematically, we&#8217;re looking for a set <img src="https://s0.wp.com/latex.php?latex=%7Bs%2Cs%27%2Cn%2Cn%27%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{s,s&#039;,n,n&#039;}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=s%2Fn%3Es%27%2Fn%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s/n&gt;s&#039;/n&#039;" class="latex" /> but <img src="https://s0.wp.com/latex.php?latex=%28s%2B1%29%2F%28n%2B2%29+%3C+%28s%27%2B1%29%2F%28n%27%2B2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(s+1)/(n+2) &lt; (s&#039;+1)/(n&#039;+2)" class="latex" />.  (Also there are obvious constraints on <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=s%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s&#039;" class="latex" />.)  The answer is yes.  For example, if fertilizer A has 4 of 5 successes while fertilizer B has 7 of 9 successes.</p>
</blockquote>
<p>By the way, on a quite different note: <a href="http://neo.jpl.nasa.gov/risk/a99942.html">NASA</a> currently rates the chances of the asteroid <a href="http://en.wikipedia.org/wiki/99942_Apophis">Apophis</a> colliding with the Earth in 2036 at 4.3 &times; 10<sup></sup><sup>-6</sup>.  It estimates that the energy of such a collision would be comparable with a 510-megatonne thermonuclear bomb.  This is ten times larger than the largest bomb actually exploded, the <a href="http://en.wikipedia.org/wiki/Tsar_Bomba">Tsar Bomba</a>.  The Tsar Bomba, in turn, was ten times larger than all the explosives used in World War II.</p>
<div align="center">
<a href="http://en.wikipedia.org/wiki/File:Tsar_photo11.jpg"><br />
<img src="http://upload.wikimedia.org/wikipedia/en/c/c9/Tsar_photo11.jpg" /><br />
</a>
</div>
<p>There&#8217;s an interesting <a href="http://www.technologyreview.com/blog/arxiv/27088/?ref=rss">Chinese plan</a> to deflect Apophis if that should prove necessary.  It is, however, quite a sketchy plan.  I expect people will make more detailed plans shortly before Apophis comes close to the Earth in 2029.  </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/08/19/bayesian-computations-of-expected-utility/#comments">25 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>, <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/08/19/bayesian-computations-of-expected-utility/" rel="bookmark" title="Permanent Link to Bayesian Computations of Expected&nbsp;Utility">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-4031 post type-post status-publish format-standard hentry category-risks" id="post-4031">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/06/14/calculating-catastrophe/" rel="bookmark">Calculating Catastrophe</a></h2>
				<small>14 June, 2011</small><br />


				<div class="entry">
					<p>This book could be interesting.  If you read it, could you tell us what you think?  </p>
<p>&bull; Gordon Woo, <i>Calculating Catastrophe</i>, World Scientific Press, Singapore, 2011.</p>
<p>Apparently Dr. Gordon Woo was trained in mathematical physics at Cambridge, MIT and Harvard, and has made his career as a &#8216;calculator of catastrophes&#8217;. He has consulted for the IAEA on the seismic safety of nuclear plants and for BP on offshore oil well drilling&mdash;it&#8217;ll be fun to see what he has to say about his triumphant success in preventing disasters in <i>both</i> those areas.  He now works at a company called <a href="http://www.rms.com/">Risk Management Solutions</a>, where he works on modelling catastrophes for insurance purposes, and has designed a model for terrorism risk. </p>
<p>According to the blurb I got:</p>
<blockquote><p>
This book has been written to explain, to a general readership, the underlying philosophical ideas and scientific principles that govern catastrophic events, both natural and man-made. Knowledge of the broad range of catastrophes deepens understanding of individual modes of disaster. This book will be of interest to anyone aspiring to understand catastrophes better, but will be of particular value to those engaged in public and corporate policy, and the financial markets.
</p></blockquote>
<p>The table of contents lists: Natural Hazards; Societal Hazards; A Sense of Scale; A Measure of Uncertainty; A Matter of Time; Catastrophe Complexity; Terrorism; Forecasting; Disaster Warning; Disaster Scenarios; Catastrophe Cover; Catastrophe Risk Securitization; Risk Horizons.</p>
<p>Maybe you know other good books on the same subject?</p>
<p>For a taste of his thinking, you can try this:</p>
<p>&bull; Gordon Woo, <a href="http://www.rms.com/publications/dhshandbook_terrorismrisk_woo.pdf">Terrorism risk</a>.  </p>
<p>Terrorism sounds like a particularly difficult risk to model, since it involves intelligent agents who try to do unexpected things.  But maybe there are still some guiding principles.  Woo writes:</p>
<blockquote>
<p>It turns out that the number of operatives involved in planning and preparing attacks has a tipping point in respect of the ease with which the dots might be joined by counter-terrorism forces. The opportunity for surveillance experts to spot a community of terrorists, and gather sufficient evidence for courtroom convictions, increases nonlinearly with the number of operatives &#8211; above a critical number, the opportunity improves dramatically. This nonlinearity emerges from analytical studies of networks, using modern graph theory methods (Derenyi et al. [21]). Below the tipping point, the pattern of terrorist links may not necessarily betray much of a signature to the counter-terrorism services. However, above the tipping point, a far more obvious signature may become apparent in the guise of a large connected network cluster of dots, which reveals the presence of a form of community. The most ambitious terrorist plans, involving numerous operatives, are thus liable to be thwarted. As exemplified by the audacious attempted replay in 2006 of the Bojinka spectacular, too many terrorists spoil the plot (Woo, [22]).</p>
<p>Intelligence surveillance and eavesdropping of terrorist networks thus constrain the pipeline of planned attacks that logistically might otherwise seem almost boundless. Indeed, such is the capability of the Western forces of counterterrorism, that most planned attacks, as many as 80% to 90%, are interdicted. For example, in the three years before the 7/7/05 London attack, eight plots were interdicted. Yet any non-interdicted planned attack is construed as a significant intelligence failure. The public expectation of flawless security is termed the ‘90-10 paradox.’ Even if 90% of plots are foiled, it is by the 10% which succeed that the security services are ultimately remembered.</p>
</blockquote>
<p>Of course the reference to &#8220;modern graph theoretical methods&#8221; will be less intimidating or impressive to many readers here than to the average, quite possibly innumerate reader of this document.  But here&#8217;s the actual reference, in case you&#8217;re curious:</p>
<p>&bull; I. Derenyi, G. Palla and T. Vicsek, <a href="http://angel.elte.hu/~derenyi/publ/clique_percolation.pdf">Clique percolation in random networks</a>, <i>Phys. Rev. Lett.</i> <b>94</b> (2005), 160202.</p>
<p>Just for fun, let me summarize the main result, so you can think about how relevant it might be to terrorist networks.</p>
<p>A graph is roughly a bunch of dots connected by edges.  A <b><a href="http://en.wikipedia.org/wiki/Clique_%28graph_theory%29">clique</a></b> in a graph is some subset of dots each of which is connected to every other.  So, if dots are people and we draw an edge when two people are friends, a clique is a bunch of people who are all friends with each other&mdash;hence the name &#8216;clique&#8217;.  But we might also use a clique to represent a bunch of people who are all engaged in the same activity, like a terrorist plot.</p>
<p>We&#8217;ve talked here before about <a href="https://johncarlosbaez.wordpress.com/2011/05/06/networks-and-population-biology-part-4/">Erdős–Rényi random graphs</a>.   These are graphs formed by taking a bunch of dots and randomly connecting each pair by an edge with some fixed probability <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  In the paper above, the authors argue that for an Erdős–Rényi random graph with <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> vertices, the chance that most of the cliques with <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> elements all touch each other and form one big fat &#8216;giant component&#8217; shoots up suddenly when</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%5Cge+%5B%28k-1%29+N%5D%5E%7B-1%2Fk-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ge [(k-1) N]^{-1/k-1}" class="latex" /></p>
<p>This sort of effect is familiar in many different contexts: it&#8217;s called a <a href="http://en.wikipedia.org/wiki/Percolation_threshold">&#8216;percolation threshold&#8217;</a>.  I can guess the implications for terrorist networks that Gordon Woo is alluding to.  However, doubt the details of the math are very important here, since social networks are <i>not</i> well modeled by Erdős–Rényi random graphs.</p>
<p>In the real world, if you and I have a mutual friend, that will increase the chance that we&#8217;ll be friends.  Similarly, if we share a conspirator, that increases the chance that we&#8217;re in the same conspiracy.  But in a world where friendship was described by an Erdős–Rényi random graph, that would not be the case!  </p>
<p>So, while I agree that large terrorist networks are easier to catch than small ones, I don&#8217;t think the math of Erdős–Rényi random graphs give any <i>quantitative</i> insight into <i>how much</i> easier it is.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/14/calculating-catastrophe/#comments">6 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/14/calculating-catastrophe/" rel="bookmark" title="Permanent Link to Calculating Catastrophe">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3970 post type-post status-publish format-standard hentry category-climate category-questions category-risks" id="post-3970">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/06/09/how-sea-level-rise-will-affect-new-york/" rel="bookmark">How Sea Level Rise Will Affect New&nbsp;York</a></h2>
				<small>9 June, 2011</small><br />


				<div class="entry">
					<p>Let&#8217;s try answering this question on Quora: </p>
<p><b><a href="http://www.quora.com/How-will-global-warming-and-particularly-sea-level-rises-affect-New-York-City">How will global warming, and particularly sea level rises, affect New York City?</a></b></p>
<p>I doubt sea level rise will be the first way we&#8217;ll get badly hurt by global warming.  I think it&#8217;ll be crop losses caused by floods, droughts and heat waves, and property damage caused by storms.  But the question focuses on sea level rise, so perhaps we should think about that&#8230; along with any other ways that New York City is <i>particularly</i> susceptible to the effects of global warming.</p>
<p>Suppose you know a lot about New York, but you need an estimate of sea level rise to get started.  In the Azimuth Project page on <a href="http://www.azimuthproject.org/azimuth/show/Sea+level+rise">sea level rise</a>, you&#8217;ll see a lot of discussion of this subject.  Naturally, it&#8217;s complicated.  But say you just want some numbers.  Okay: very roughly, by the end of the century we can expect a sea level of <b>at least 0.6 meters</b>, not counting any melting from Greenland and Antarctica and <b>at most 2 meters</b>, including Greenland and Antarctica.   That&#8217;s roughly between 2 and 6 feet.  </p>
<p>On the other hand, there&#8217;s at least one report saying <a href="http://www.sciencedaily.com/releases/2009/03/090315155112.htm">sea levels may rise in the Northeast US at twice the average global rate</a>.  What&#8217;s the latest word on that?</p>
<p>Now, here&#8217;s a website that claims to show what various amounts of sea level rise would do to different areas:</p>
<p>&bull; Firetree.net, <a href="http://flood.firetree.net/?ll=43.3251,-101.6015&amp;z=13&amp;m=1">Flood maps</a>, including <a href="http://geology.com/sea-level-rise/new-york.shtml">New York City</a>.</p>
<p>Details on how these maps were made are <a href="http://blog.firetree.net/2006/05/18/more-about-flood-maps/">here</a>.  One problem is that they focus too much on really big sea level rises: the smallest rise shown is 1 meter, then 2 meters&#8230; and it goes up to 60 meters! </p>
<p>Anyway, here&#8217;s part of New York City now:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/new_york_city_map.jpg"><img width="450" src="https://i1.wp.com/math.ucr.edu/home/baez/new_york_city_map.jpg" alt="" /></a></div>
<p>Here it is after a 1-meter (3-foot) sea level rise:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/new_york_city_map_1_meter_sea_level_rise.jpg"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/new_york_city_map_1_meter_sea_level_rise.jpg" alt="" /></a></div>
<p>(Click to enlarge any of these.)   And here&#8217;s 2 meters, or 6 feet:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/new_york_city_map_2_meter_sea_level_rise.jpg"><br />
<img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/new_york_city_map_2_meter_sea_level_rise.jpg" alt="" /></a></div>
<p>It&#8217;s a bit hard to spot the effects in Manhattan.  They&#8217;re much more noticeable in the low-lying areas between Jersey City and Secaucus.   What are those: parks, industrial areas, or suburbs?  I&#8217;ve heard New Yorkers crack jokes about the &#8216;swamps of Jersey&#8217;&#8230;</p>
<p>But of course, a lot of the city is underground.  What will happen to subways and other infrastructure, like sewage systems?  And what about water supplies?  On coastlines, saltwater can infiltrate into surface waters and aquifers.  Where does freshwater meet saltwater near New York City?  How will the effect of floods and storms change?</p>
<p>And of course, there are other parts of New York City these little maps don&#8217;t show: for those, go <a href="http://geology.com/sea-level-rise/new-york.shtml">here</a>.  But watch out: at first you&#8217;ll see the effect of a <i>7-meter</i> sea level rise&#8230; you&#8217;ll need to change the settings to see the effects of a more realistic rise.</p>
<p>If you live in a place that will be flooded, let me know!</p>
<p>Luckily, we don&#8217;t have to figure everything out ourselves: the state of New York has a task force devoted to this.  And as task forces do, they&#8217;ve written a report:</p>
<p>&bull; New York Department of Environmental Conservation, <a href="http://www.dec.ny.gov/energy/45202.html">Sea Level Rise Task Force</a>, <a href="http://www.dec.ny.gov/energy/67778.html">Final Report</a>.</p>
<p>New York City also has an ambitious environmental plan:</p>
<p>&bull; New York City, <a href="http://www.nyc.gov/html/planyc2030/html/theplan/the-plan.shtml">PlaNYC 2030</a>.</p>
<p>Finally, let me quote part of this:</p>
<p>&bull; Jim O&#8217;Grady, <a href="http://www.wnyc.org/articles/wnyc-news/2011/feb/09/climate-change/">Sea level rise could turn New York into Venice, experts warn</a>, <i>WNYC News</i>, 9 February 2011.</p>
<p>Because it looks ahead 200 years, this article paints a more dire picture than my remarks above:</p>
<blockquote>
<p><a href="http://www.gothamgazette.com/blogs/wonkster/2010/08/11/new-sustainability-director-selected/" target="_blank">David Bragdon</a>, Director of the <a href="http://www.nyc.gov/html/ops/html/long_term/long_term.shtml" target="_blank">Mayor&#8217;s Office of Long-Term Planning &amp; Sustainability</a>, is charged with preparing for the dangers of climate change. He said the city is taking precautions like raising the pumps at a wastewater treatment plant in the Rockaways and building the Willets Point development in Queens on six feet of landfill. The goal is to manage the risk from 100-year storms&mdash;one of the most severe. The mayor’s report says by the end of this century, 100-year storms could start arriving every 15 to 35 years.</p>
<p><a href="http://www.sipa.columbia.edu/academics/directory/khj1-fac.html" target="_blank">Klaus Jacob</a>, a Columbia  University research scientist who specializes in disaster risk management, said that estimate may be too conservative. “What is now the impact of a 100-year storm will be, by the end of this century, roughly a 10-year storm,”  he warned.</p>
<p>Back on the waterfront, oceanographer <a href="http://stormy.msrc.sunysb.edu/mjb/biography.html">Malcolm Bowman</a> offered what he said is a suitably outsized solution to this existential threat: <a href="http://dvice.com/archives/2007/11/maeslant-storm-surge-barrier-s.php" target="_blank">storm surge barriers</a>.</p>
<p>They would rise from the waters at Throgs Neck, where Long Island Sound and the East River meet, and at the opening to the lower harbor between the Rockaways and Sandy Hook, New Jersey. Like the barriers on the Thames River that protect London, they would stay open most of the time to let ships pass but close to protect the city during hurricanes and severe storms. </p>
<p>The structures at their highest points would be 30 feet above the harbor surface. Preliminary engineering studies put the cost at around $11 billion.</p>
<div align="center"><a><img src="https://i0.wp.com/parmenides.wnyc.org/media/photologue/photos/lower-bay.jpg" alt="" width="300" height="301" /></a></div>
<p>Jacob suggested a different but equally drastic approach. He said sea level rise may force New Yorkers to pull back from vulnerable neighborhoods. “We will have to densify the high-lying areas and use the low-lying areas as parks and buffer zones,” he said.</p>
<p>In this scenario, New York in 200 years looks like <a href="http://www.veniceonline.it/Maps/Map2_VeniceOnLine.jpg" target="_blank">Venice</a>. Concentrations of greenhouse gases in the atmosphere have melted ice sheets in Greenland and Antarctica and raised our local sea level by six to eight feet. Inundating storms at certain times of year swell the harbor until it spills into the streets. Dozens of skyscrapers in Lower Manhattan have been sealed at the base and entrances added to higher floors. The streets of the financial district have become canals.</p>
<p>“You may have to build bridges or get Venice gondolas or your little speed boats ferrying yourself up to those buildings,” Jacob said.</p>
<p>David Bragdon is not comfortable with such scenarios. He’d rather talk about the concrete steps he’s taking now, like updating the city’s flood evacuation plan to show more neighborhoods at risk. That would help the people living in them be better prepared to evacuate.</p>
<p>He said it&#8217;s too soon to contemplate the &#8220;extreme&#8221; step of moving &#8220;two, three, four hundred thousand people out of areas they’ve occupied for generations,&#8221; and disinvesting &#8220;literally billions of dollars of infrastructure in those areas.&#8221; On the other hand: &#8220;Another extreme would be to hide our heads in the sand and say, ‘Nothing’s going to happen.’”</p>
<p>Bragdon said he doesn&#8217;t think New Yorkers of the future will have to retreat very far from shore, if at all, but he’s not sure. And he would neither commit to storm surge barriers nor eliminate them as an option. He said what’s needed is more study—and that he’ll have further details in April, when the city updates <a href="http://www.nyc.gov/html/planyc2030/html/home/home.shtml" target="_blank">PlaNYC</a>.</p>
<p>Jacob warned that in preparing for disaster, no matter how far off, there&#8217;s a gulf between study and action. &#8220;There’s a good intent,&#8221; he said of New York&#8217;s climate change planning to date. &#8220;But, you know, mother nature doesn’t care about intent. Mother nature wants to see resiliency. And that is questionable, whether we have that.”
</p>
</blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/09/how-sea-level-rise-will-affect-new-york/#comments">29 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/questions/" rel="category tag">questions</a>, <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/09/how-sea-level-rise-will-affect-new-york/" rel="bookmark" title="Permanent Link to How Sea Level Rise Will Affect New&nbsp;York">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3259 post type-post status-publish format-standard hentry category-azimuth category-risks" id="post-3259">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/" rel="bookmark">What To Do? (Part&nbsp;1)</a></h2>
				<small>24 April, 2011</small><br />


				<div class="entry">
					<p>In a <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comment-5403">comment</a> on my last interview with Yudkowsky, Eric Jordan wrote:</p>
<blockquote><p>
John, it would be great if you could follow up at some point with your thoughts and responses to what Eliezer said <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comments">here</a>. He’s got a pretty firm view that environmentalism would be a waste of your talents, and it’s obvious where he’d like to see you turn your thoughts instead. I’m especially curious to hear what you think of his argument that there are already millions of bright people working for the environment, so your personal contribution wouldn’t be as important as it would be in a less crowded field.
</p></blockquote>
<p>I&#8217;ve been thinking about this a lot.  </p>
<p>Indeed, the reason I quit work on my previous area of interest&mdash;categorification and higher gauge theory&mdash;was the feeling that more and more people were moving into it.  When I started, it seemed like a lonely but exciting quest.  By now there are plenty of <a href="http://golem.ph.utexas.edu/category/2011/04/workshop_on_higher_gauge_theor.html">conferences</a> on it, attended by <a href="https://sites.google.com/site/hgtqgr/photo">plenty of people</a>.  It would be a full-time job just keeping up, much less doing something truly new.  That made me feel inadequate&mdash;and worse, unnecessary. Helping start a snowball roll downhill is fun&#8230; but what&#8217;s the point in chasing one that&#8217;s already rolling?  </p>
<p>The people working in this field include former grad students of mine and other youngsters I helped turn on to the subject.  At first this made me a bit frustrated.  It&#8217;s as if I engineered my own obsolescence.  If only I&#8217;d spent less time explaining things, and more time proving theorems, maybe I could have stayed at the forefront! </p>
<p>But by now I&#8217;ve learned to see the bright side: it means I&#8217;m <i>free to do other things</i>.  As I get older, I&#8217;m becoming ever more conscious of my limited lifespan and the vast number of things I&#8217;d like to try.</p>
<p>But <i>what to do?</i></p>
<p>This a big question.  It&#8217;s a bit self-indulgent to discuss it publicly&#8230; or maybe not.  It is, after all, a question we <i>all</i> face.  I&#8217;ll talk about me, because I&#8217;m not up to tackling this question in its universal abstract form.  But it could be you asking this, too.</p>
<p>For me this question was brought into sharp focus when I got a research position where I was allowed&mdash;nay, downright encouraged!&mdash;to follow my heart and work on what I consider truly important.  In the ordinary course of life we often feel too caught up in the flow of things to do more than make small course corrections.  Suddenly I was given a burst of freedom.  What to do with it?</p>
<p>In my earlier work, I&#8217;d always taken the attitude that I should tackle whatever questions seemed most beautiful and profound&#8230; subject to the constraint that I had a good chance of making some progress on them.  I realized that this attitude assumes other people will do most of the &#8216;dirty work&#8217;, whatever that may be.  But I figured I could get away with it.  I figured that if I were ever called to account&mdash;by my own conscience, say&mdash;I could point to the fact that I&#8217;d worked hard to understand the universe and also spent a lot of time teaching people, both in my job and in my spare time.  Surely that counts for something?</p>
<p>I had, however, for decades been observing the slow-motion train wreck that our civilization seems to be engaged in.   Global warming, ocean acidification and habitat loss may be combining to cause a mass extinction event, and perhaps&mdash;in conjunction with resource depletion&mdash;a serious setback to human civilization.  Now is not the time to go over all the evidence: suffice it to say that I think we may be heading for serious trouble.   </p>
<p>It&#8217;s hard to know just <i>how much</i> trouble.  If it were just routine &#8216;misery as usual&#8217;, I&#8217;ll admit I&#8217;d be happy to sit back and let everyone else deal with these problems.  But the more I study them, the more that seems untenable&#8230; especially since so many people are doing just that: sitting back and letting everyone else deal with them.</p>
<p>I&#8217;m not sure this complex of problems rises to the level of an <a href="http://en.wikipedia.org/wiki/Existential_risk">&#8216;existential risk&#8217;</a>&mdash;which <a href="http://www.nickbostrom.com/existential/risks.html">Nick Bostrom</a> defines as one where an adverse outcome would either annihilate intelligent life originating on Earth or permanently and drastically curtail its potential.   But I see scenarios where we clobber ourselves quite seriously.  They don&#8217;t even seem unlikely, and they don&#8217;t seem very far-off, and I don&#8217;t see people effectively rising to the occasion.  So, just as I&#8217;d move to put out a fire if I saw smoke coming out of the kitchen and everyone else was too busy watching TV to notice, I feel I have to do something.</p>
<p>But the question remains: <i>what to do?</i></p>
<p>Eliezer Yudkowsky had some unabashed advice:</p>
<blockquote><p>
I honestly don&#8217;t see how a rationalist can avoid this conclusion:  At this absolutely critical hinge in the history of the universe&mdash;Earth in the 21st century&mdash;rational altruists should devote their marginal attentions to risks that threaten to terminate intelligent life or permanently destroy a part of its potential.  Those problems, which <a href="http://www.nickbostrom.com/">Nick Bostrom</a> named <a href="http://www.nickbostrom.com/existential/risks.html">&#8216;existential risks&#8217;</a>, have got <i>all</i> the scope.  And when it comes to marginal impact, there are major risks outstanding that practically no one is working on.  Once you get the stakes on a gut level it&#8217;s hard to see how doing anything else could be <i>sane</i>.</p>
<p>So how do you go about protecting the future of intelligent life?  Environmentalism?  After all, there are environmental catastrophes that could knock over our civilization&#8230; but then if you want to put the whole universe at stake, it&#8217;s not enough for one civilization to topple, you have to argue that our civilization is above average in its chances of building a positive galactic future compared to whatever civilization would rise again a century or two later.  Maybe if there were ten people working on environmentalism and millions of people working on Friendly AI, I could see sending the next marginal dollar to environmentalism.  But with millions of people working on environmentalism, and major existential risks that are completely ignored&#8230; if you add a marginal resource that can, rarely, be steered by expected utilities instead of warm glows, devoting that resource to environmentalism <i>does not make sense</i>.</p>
<p>Similarly with other short-term problems.  Unless they&#8217;re little-known and unpopular problems, the marginal impact is not going to make sense, because millions of other people will already be working on them.  And even if you argue that some short-term problem leverages existential risk, it&#8217;s not going to be perfect leverage and some quantitative discount will apply, probably a large one.  I would be suspicious that the decision to work on a short-term problem was driven by warm glow, status drives, or simple conventionalism.</p>
<p>With that said, there&#8217;s also such a thing as <a href="http://en.wikipedia.org/wiki/Comparative_advantage">comparative advantage</a>&mdash;the old puzzle of the lawyer who works an hour in the soup clinic instead of working an extra hour as a lawyer and donating the money. Personally I&#8217;d say you can work an hour in the soup clinic to keep yourself going if you like, but you should <i>also</i> be working extra lawyer-hours and donating the money to the soup clinic, or better yet, to something with more scope.  (See <a href="http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/">&#8220;Purchase Fuzzies and Utilons Separately&#8221;</a> on <i>Less Wrong</i>.)  Most people can&#8217;t work effectively on Artificial Intelligence (some would question if <i>anyone</i> can, but at the very least it&#8217;s not an easy problem).  But there&#8217;s a variety of existential risks to choose from, plus a general background job of spreading sufficiently high-grade rationality and existential risk awareness.  One really should look over those before going into something short-term and conventional.  Unless your master plan is just to work the extra hours and donate them to the cause with the highest marginal expected utility per dollar, which is perfectly respectable.</p>
<p>Where should you go in life?  I don&#8217;t know exactly, but I think I&#8217;ll go ahead and say &#8220;not environmentalism&#8221;.  There&#8217;s just no way that the product of scope, marginal impact, and John Baez&#8217;s comparative advantage is going to end up being maximal at that point.
</p></blockquote>
<p>When I heard this, one of my first reactions was: &#8220;Of course I don&#8217;t want to do anything <i>&#8216;conventional&#8217;</i>, something that &#8216;millions of people&#8217; are already doing&#8221;.   After all, my sense of being just another guy in the crowd was a big factor in leaving work on categorification and higher gauge theory&mdash;and most people have never even <i>heard</i> of those subjects!   </p>
<p>I think so far the Azimuth Project is proceeding in a sufficiently unconventional way that while it may fall flat on its face, it&#8217;s at least trying something new.  Though I always want more people to join in, we&#8217;ve already got some good projects going that take advantage of my &#8216;comparative advantage&#8217;: the ability to do math and explain stuff.  </p>
<p>The most visible here is the <a href="http://math.ucr.edu/home/baez/networks/networks.html">network theory</a> project, which is a step towards the kind of math I think we need to understand a wide variety of complex systems.  I&#8217;ve been putting most of my energy into that lately, and coming up with ideas faster than I can explain them.  On top of that, Eric Forgy, Tim van Beek, Staffan Liljgeren, Matt Reece, David Tweed and others have other interesting projects cooking behind the scenes on the <a href="http://www.math.ntnu.no/~stacey/Mathforge/Azimuth/">Azimuth Forum</a>.  I&#8217;ll be talking about those soon, too.</p>
<p>I don&#8217;t feel satisfied, though.  I&#8217;m happy enough&mdash;that&#8217;s never a problem these days&mdash;but once you start trying to do things to help the world, instead of just have fun, it&#8217;s very tricky to determine the best way to proceed.</p>
<p>One can, of course, easily fool oneself into thinking one knows.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/#comments">89 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/azimuth/" rel="category tag">azimuth</a>, <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/" rel="bookmark" title="Permanent Link to What To Do? (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3231 post type-post status-publish format-standard hentry category-carbon-emissions category-climate category-energy category-risks" id="post-3231">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/04/21/stabilization-wedges-part-5/" rel="bookmark">Stabilization Wedges (Part&nbsp;5)</a></h2>
				<small>21 April, 2011</small><br />


				<div class="entry">
					<p>In 2004, Pacala and Socolow laid out a list of ways we can battle global warming using current technologies.  They said that to avoid serious trouble, we need to choose seven <a href="http://www.azimuthproject.org/azimuth/show/Stabilization+wedges">&#8216;stabilization wedges&#8217;</a>: that is, seven ways to cut carbon emissions by 1 gigatonne per year within 50 years.  They listed 15 wedges to choose from, and I&#8217;ve told you about them here:</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2010/11/16/stabilization-wedges/">Part 1</a> &#8211; efficiency and conservation.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2010/11/23/stabilization-wedges-part-2/">Part 2</a> &#8211; shifting from coal to natural gas, carbon capture and storage.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2010/12/17/stabilization-wedges-part-3/">Part 3</a> &#8211; nuclear power and renewable energy.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2011/01/16/stabilization-wedges-part-4/">Part 4</a> &#8211; reforestation, good soil management.</p>
<p>According to Pacala:</p>
<blockquote><p>
 The message was a very positive one: &#8220;gee, we can solve this problem: there are <i>lots</i> of ways to solve it, and lots of ways for the marketplace to solve it.&#8221;
</p></blockquote>
<p>I find that interesting, because to me each wedge seems like a gargantuan enterprise&mdash;and taken together, they seem like the Seven Labors of Hercules.  They&#8217;re technically feasible, but who has the stomach for them?  I fear things need to get worse before we come to our senses and take action at the scale that&#8217;s required.  </p>
<p>Anyway, that&#8217;s just me.  But three years ago, Pacala publicly reconsidered his ideas for a very different reason.  Based on new evidence, he gave a <a>talk at Stanford</a> where he said:</p>
<blockquote><p>
 It&#8217;s at least possible that we&#8217;ve already let this thing go too far, and that the biosphere may start to fall apart on us, even if we do all this.  We may have to fall back on some sort of dramatic Plan B.  We have to stay vigilant as a species.
</p></blockquote>
<p>You can watch his talk here:</p>
<div align="center">
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="450" height="390" src="https://www.youtube.com/embed/2X2u7-R3Wrc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></div>
<p>It&#8217;s pretty damned interesting: he&#8217;s a good speaker.</p>
<p>Here&#8217;s a dry summary of a few key points.   I won&#8217;t try to add caveats: I&#8217;m sure he would add some himself in print, but I&#8217;d rather keep the message simple.  I also won&#8217;t try to update his information!  Not in this blog entry, anyway.  But I&#8217;ll ask some questions, and I&#8217;ll be delighted if you help me out on those.</p>
<h4>Emissions targets</h4>
<p>First, Pacala&#8217;s review of different carbon emissions targets.</p>
<p>The old scientific view, circa 1998: if we could keep the CO<sub>2</sub> from <i>doubling</i> from its preindustrial level of 280 parts per million, that would count as a success.   Namely, most of the &#8216;monsters behind the door&#8217; would not come out: continental ice sheets falling into the sea and swamping coastal cities, the collapse of the Atlantic ocean circulation, a <a href="http://www.ncbi.nlm.nih.gov/pubmed/16322101">drought in the Sahel region of Africa</a>, etcetera.</p>
<p>Many experts say we&#8217;d be lucky to get away with CO<sub>2</sub> merely doubling.  At current burn rates we&#8217;ll double it by 2050, and <i>quadruple</i> it by the end of this century. We&#8217;ve got enough fossil fuels to send it to <i>seven times</i> its preindustrial levels.</p>
<p>Doubling it would take us to 560 parts per million.  A lot of people think that&#8217;s too high to be safe.  But going for lower levels gets harder:</p>
<p>&bull; In Pacala and Socolow&#8217;s original paper, they talked about <b>keeping CO<sub>2</sub> below 500 ppm</b>.  This would require keeping CO<sub>2</sub> emissions constant until 2050.  This could be achieved by <b>a radical decarbonization of the economies of rich countries, while allowing carbon emissions in poor countries to grow almost freely until that time</b>.</p>
<p>&bull; For a long time the IPCC and many organizations advocated <b>keeping CO<sub>2</sub> below 450 ppm</b>.  This would require cutting CO<sub>2</sub> emissions by 50% by 2050, which could be achieved by <b>a radical decarbonization in rich countries, and moderate decarbonization in poor countries</b>.</p>
<p>&bull; But by 2008 the IPCC and many groups wanted a cap of 2&deg;C global warming, or <b>keeping CO<sub>2</sub> below 430 ppm</b>.  This would mean cutting CO<sub>2</sub> emissions by 80% by 2050, which would require <b>a radical decarbonization in both rich and poor countries</b>.</p>
<p>The difference here is what poor people have to do.  The rich countries need to radically cut carbon emissions in <i>all</i> these scenarios.  In the USA, the <a href="http://en.wikipedia.org/wiki/America%27s_Climate_Security_Act_of_2007">Lieberman-Warner bill</a> would have forced the complete decarbonization of the economy by 2050.  </p>
<p>Then, Pacala spoke about 3 things that make him nervous:</p>
<h4>1. Faster emissions growth </h4>
<p>A <a href="http://www.pnas.org/content/104/47/18866.full">2007 paper by Canadell <i>et al</i></a> pointed out that starting in 2000, fossil fuel emissions started growing at 3% per year instead of the earlier figure of 1.5%.  This could be due to China&#8217;s industrialization.  Will this keep up in years to come?  If so, the original Pacala-Socolow plan won&#8217;t work.</p>
<p>(How much, exactly, did the economic recession change this story?)</p>
<h4>2. The ocean sink</h4>
<p>Each year fossil fuel burning puts about 8 gigatons of carbon in the atmosphere.  The ocean absorbs about 2 gigatons and the land absorbs about 2, leaving about 4 gigatons in the atmosphere.</p>
<p>However, as CO<sub>2</sub> emissions rise, the oceanic CO<sub>2</sub> sink has been growing less than anticipated.  This seems to be due to a change in wind patterns, itself a consequence of global warming.  </p>
<p>(What&#8217;s the latest story here?)</p>
<h4>3. The land sink </h4>
<p>As the CO<sub>2</sub> levels go up, people expected plants to grow better and suck up more CO<sub>2</sub>.    In the third IPCC report, models predicted that by 2050, plants will be drawing down 6 gigatonnes more carbon per year than they do now!  The fourth IPCC report was similar.</p>
<p>This is huge: remember that right now we emit about 8 gigatonnes per year.  Indeed, this effect, called CO<sub>2</sub> fertilization, could be the difference between the land being a big carbon <i>sink</i> and a big carbon <i>source</i>.  Why a carbon source?  For one thing, without the plants sucking up CO<sub>2</sub>, temperatures will rise faster, and the Amazon rainforest may start to die, and permafrost in the Arctic may release more greenhouse gases (especially methane) as it melts.    </p>
<p>In a simulation run by Pacala, where he deliberately assumed that plants <i>fail</i> to suck up more carbon dioxide, these effects happened and the biosphere dumped a huge amount of extra CO<sub>2</sub> into the atmosphere: the equivalent of 26 stabilization wedges.</p>
<p>So, <b>plans based on the IPCC models are essentially counting on plants to save us from ourselves</b>.  </p>
<p>But is there any reason to think plants might <i>not</i> suck up CO<sub>2</sub> at the predicted rates?  </p>
<p>Maybe.  First, people have actually grown forests in doubled CO<sub>2</sub> conditions to see how much faster plants grow then.  But the <a href="http://face.env.duke.edu/description.cfm">classic experiment</a> along these lines used <i>young</i> trees.  In 2005, <a href="http://www.sciencemag.org/content/309/5739/1360">K&ouml;rner <i>et al</i></a> did an experiment using <i>mature</i> trees&#8230; and they didn&#8217;t see them growing any faster!    </p>
<p>Second, models in the third IPCC report assumed that as plants grew faster, they&#8217;d have no trouble getting all the <i>nitrogen</i> they need.  But <a href="http://www.sciencemag.org/content/302/5650/1512.summary">Hungate <i>et al</i></a> have argued otherwise.  On the other hand, <a href="http://www.princeton.edu/~lhedin/Project9.htm">Alexander Barron</a> discovered that some tropical plants were unexpectedly good at ramping up the rate at which they grab ahold of nitrogen from the atmosphere.  But on the third hand, that only applies to the tropics.  And on the fourth hand&mdash;a complicated problem like this requires one of those Indian gods with lots of hands&mdash;nitrogen isn&#8217;t the only limiting factor to worry about: there&#8217;s also phosphorus, for example.</p>
<p>Pacala goes on and discusses even more complicating factors.  But his main point is simple.  The details of CO<sub>2</sub> fertilization matter a <i>lot</i>.  It could make the difference between their original plan being roughly good enough&#8230; and being <i>nowhere near good enough!</i></p>
<p>(What&#8217;s the latest story here?)</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/21/stabilization-wedges-part-5/#comments">42 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/" rel="category tag">carbon emissions</a>, <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/energy/" rel="category tag">energy</a>, <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/21/stabilization-wedges-part-5/" rel="bookmark" title="Permanent Link to Stabilization Wedges (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2858 post type-post status-publish format-standard hentry category-risks" id="post-2858">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/04/01/lifeboat-foundation/" rel="bookmark">Lifeboat Foundation</a></h2>
				<small>1 April, 2011</small><br />


				<div class="entry">
					<p>I&#8217;ve been invited to join this organization.  But you can join too:</p>
<p>&bull; <a href="http://lifeboat.com/ex/main">Lifeboat Foundation</a>.</p>
<p>I hadn&#8217;t heard of it before.  Do you know anything about it?  Here&#8217;s their mission statement:</p>
<blockquote><p>
The Lifeboat Foundation is a nonprofit nongovernmental organization dedicated to encouraging scientific advancements while helping humanity survive <a href="http://lifeboat.com/ex/programs">existential risks</a> and possible misuse of increasingly powerful technologies, including genetic engineering, nanotechnology, and robotics/AI, as we move towards the <a href="http://en.wikipedia.org/wiki/Technological_singularity">Singularity</a>.</p>
<p>Lifeboat Foundation is pursuing a variety of options, including helping to accelerate the development of technologies to defend humanity, including new <a href="http://lifeboat.com/ex/bio.shield">methods to combat viruses</a> (such as RNA interference and new vaccine methods), effective <a href="http://lifeboat.com/ex/nano.shield">nanotechnological defensive strategies</a>, and even self-sustaining <a href="http://lifeboat.com/ex/ark_i">spacecolonies</a> in case the other defensive strategies fail.</p>
<p>We believe that, in some situations, it might be feasible to relinquish technological capacity in the public interest (for example, we are against the U.S. government <a href="http://www.kurzweilai.net/news/frame.html?main=news_single.html?id%3D4934">posting the recipe</a> for the 1918 flu virus on the Internet).</p>
<p>We have some of the best minds on the planet working on <a href="/ex/programs">programs</a> to enable our survival. We <a href="/invitation2.doc">invite</a> you to join our cause!</p>
</blockquote>
<p>It seems to have Nick Bostrom and Ray Kurzweil as two of its guiding figures: the overview features quotes from both.</p>
<blockquote><p>
<b>Overview</b></p>
<p>An <a href="http://en.wikipedia.org/wiki/Existential_risk">existential risk</a> is a risk that is both global and terminal. Nick Bostrom defines it as a risk &#8220;where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential&#8221;. The term is frequently used to describe disaster and doomsday scenarios caused by non-friendly superintelligence, misuse of molecular nanotechnology, or other sources of danger.</p>
<p>The Lifeboat Foundation was formed to prevent <a href="http://en.wikipedia.org/wiki/Human_extinction">existential events</a> from happening, as once they occur, humanity may have no possibility to correct the error. Unfortunately governments, and humanity in general, always react AFTER a disaster has happened, and some disasters will leave no survivors so we must react BEFORE they occur. We must be proactive.</p>
<p>The Lifeboat Foundation is developing programs to prevent existential events (&#8220;shields&#8221;) as well as programs to preserve civilization (&#8220;preservers&#8221;) to survive such events.</p>
<p><b>Quotes</b></p>
<p><i>&#8220;Our approach to existential risks cannot be one of trial-and-error. There is no opportunity to learn from errors. The reactive approach — see what happens, limit damages, and learn from experience — is unworkable. Rather, we must take a proactive approach. This requires foresight to anticipate new types of threats and a willingness to take decisive preventive action and to bear the costs (moral and economic) of such actions.&#8221;</i> &mdash; Nick Bostrom</p>
<p><i>&#8220;We cannot rely on trial-and-error approaches to deal with existential risks&#8230; We need to vastly increase our investment in developing specific defensive technologies&#8230; We are at the critical stage today for biotechnology, and we will reach the stage where we need to directly implement defensive technologies for nanotechnology during the late teen years of this century&#8230; A self-replicating pathogen, whether biological or nanotechnology based, could destroy our civilization in a matter of days or weeks.&#8221;</i> &mdash; Ray Kurzweil</p>
</blockquote>
<p>You&#8217;ll note there&#8217;s no mention here of global warming, mass extinction of species, oil depletion and other minor nuisances.  Some people consider these problems insufficiently severe to count as &#8220;existential threats&#8221;&#8230; and thus, perhaps, best left to others.  Some argue that there are already enough people worrying about these problem&mdash;while other threats need more attention than they&#8217;re getting.</p>
<p>That would be an interesting discussion to have.  But I&#8217;m afraid there&#8217;s a cultural divide between the &#8220;green crowd&#8221; and the &#8220;tech crowd&#8221; that hinders such a discussion.  The green crowd worries about things like global warming, the mass extinction that may currently be underway, and peak oil.  The tech crowd worries about things like nanotechnology, artificial intelligence and asteroids hitting the Earth.  Each crowd tends to think the other is a bit silly&#8230; and they don&#8217;t talk to each other enough.  Am I just imagining this?  I don&#8217;t think so.</p>
<p>Of course, any generalization this vast admits many exceptions.  I like <a href="https://johncarlosbaez.wordpress.com/2011/02/28/this-weeks-finds-week-310/">Gregory Benford</a> because he confounds naive expectations: he thinks global warming is a desperately urgent problem that overshadows all others, but he&#8217;s willing to contemplate high-tech solutions.  According to my theory, that should annoy <i>both</i> the green crowd <i>and</i> the tech crowd.</p>
<p>Personally I think all significant threats to civilization and biosphere should be evaluated and addressed in a unified way.  Setting some aside because they&#8217;re &#8220;non-existential&#8221; or overly studied seems just as dangerous as setting others aside because they seem improbable or science-fiction-esque.</p>
<p>For one thing, I can imagine scenarios where medium-sized problems snowball into big &#8220;existential&#8221; ones.  What&#8217;s the chance that in this century, global warming leads to droughts and famines which combined with oil shortages lead to political instability, the collapse of democratic governments, wars&#8230; and finally a world-wide nuclear or biological war?  Maybe low&#8230; but I bet it&#8217;s higher than the chance of an asteroid hitting the Earth in this century.</p>
<p>I&#8217;m pleased to see that the Lifeboat Foundation plans &#8220;future programs&#8221; that will appeal to the green crowd:</p>
<blockquote>
<p>ClimateShield<br />
To protect against global warming and other unwanted climate changes.</p>
<p>BioPreserver<br />
To preserve animal life and diversity on the planet.</p>
<p>EnergyPreserver<br />
If our civilization ran out of energy, it would grind to a halt, so Lifeboat Foundation is looking for solutions.
</p></blockquote>
<p>However, their current programs are strongly focused on issues that appeal to the tech crowd.  Maybe that&#8217;s okay, but maybe it&#8217;s a bit unbalanced:</p>
<blockquote><p>
<a href="http://lifeboat.com/ex/ai.shield">AIShield</a><br />
To protect against unfriendly AI (Artificial  Intelligence).</p>
<p><a href="http://lifeboat.com/ex/asteroid.shield">AsteroidShield</a><br />
To protect against devastating asteroid strikes.</p>
<p><a href="http://lifeboat.com/ex/bio.shield">BioShield</a><br />
To protect against bioweapons and pandemics.</p>
<p><a href="http://lifeboat.com/ex/internet.shield">InternetShield</a><br />
As the Internet grows in importance, an attack on it could cause  physical as well as informational damage. An attack today on hospital  systems or electric utilities could lead to deaths. In the future an attack could be used to alter the output that is produced by<br />
nanofactories worldwide leading to massive deaths.</p>
<p><a href="http://lifeboat.com/ex/life.shield.bunkers">LifeShield Bunkers</a><br />
Developing fallback positions on Earth in case programs such as our <a href="/ex/bio.shield">BioShield</a> and <a href="/ex/nano.shield">NanoShield</a> fail globally or locally. </p>
<p><a href="http://lifeboat.com/ex/nano.shield">NanoShield</a><br />
To protect against ecophages and nonreplicating<br />
nanoweapons.</p>
<p><a href="http://lifeboat.com/ex/scientific.freedom.shield">ScientificFreedomShield</a><br />
This shield strives to protect scientists from obstacles that would prevent latter day Max Plancks from completing their research.</p>
<p><a href="http://lifeboat.com/ex/security.preserver">SecurityPreserver</a><br />
To prevent nuclear, biological, and nanotechnological attacks from occurring by using surveillance and <a href="http://en.wikipedia.org/wiki/Sousveillance"><br />
sousveillance</a> to identify terrorists before they are able to launch their attacks.</p>
<p><a href="http://lifeboat.com/ex/space.habitats">Space Habitats</a><br />
To build fail-safes against global existential risks by encouraging the spread of sustainable human civilization beyond Earth.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/01/lifeboat-foundation/#comments">29 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/01/lifeboat-foundation/" rel="bookmark" title="Permanent Link to Lifeboat Foundation">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2830 post type-post status-publish format-standard hentry category-risks category-this-weeks-finds" id="post-2830">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/" rel="bookmark">This Week&#8217;s Finds (Week&nbsp;313)</a></h2>
				<small>25 March, 2011</small><br />


				<div class="entry">
					<p>Here&#8217;s the third and final part of my interview with <a href="http://yudkowsky.net/">Eliezer Yudkowsky</a>.  We&#8217;ll talk about three big questions&#8230; roughly these:</p>
<p> &bull; How do you get people to work on potentially risky projects in a safe way?</p>
<p> &bull; Do we understand ethics well enough to build <a href="http://en.wikipedia.org/wiki/Friendly_artificial_intelligence">&#8220;Friendly artificial intelligence&#8221;</a>?</p>
<p> &bull; What&#8217;s better to work on, artificial intelligence or environmental issues?</p>
<p>So, with no further ado:</p>
<p><b>JB</b>:  There are decent Wikipedia articles on <a href="http://en.wikipedia.org/wiki/Optimism_bias">&#8220;optimism bias&#8221;</a> and <a href="http://en.wikipedia.org/wiki/Positive_illusions">&#8220;positive illusions&#8221;</a>, which suggest that unrealistically optimistic people are more energetic, while more realistic estimates of success go hand-in-hand with mild depression. If this is true, I can easily imagine that most people working on challenging projects like quantum gravity (me, 10 years ago) or artificial intelligence (you) are unrealistically optimistic about our chances of success.</p>
<p>Indeed, I can easily imagine that the first researchers to create a truly powerful artificial intelligence will be people who underestimate its potential dangers.  It&#8217;s an interesting irony, isn&#8217;t it?  If most people who are naturally cautious avoid a certain potentially dangerous line of research, the people who pursue that line of research are likely to be less cautious than average.</p>
<p>I&#8217;m a bit worried about this when it comes to <a href="http://en.wikipedia.org/wiki/Geoengineering">&#8220;geoengineering&#8221;</a>, for example&mdash;attempts to tackle global warming by large engineering projects.  We have people who say &#8220;oh no, that&#8217;s too dangerous&#8221;, and turn their attention to approaches they consider less risky, but that may leave the field to people who underestimate the risks.</p>
<p>So I&#8217;m very glad you are thinking hard about how to avoid the potential dangers of artificial intelligence&mdash;and even trying to make this problem sound exciting, to attract ambitious and energetic young people to work on it.  Is that part of your explicit goal?  To make caution and rationality sound sexy?</p>
<p><b>EY</b>:  The really hard part of the problem isn&#8217;t getting a few smart people to work on cautious, rational AI.  It&#8217;s admittedly a harder problem than it should be, because there&#8217;s a whole system out there which is set up to funnel smart young people into all sorts of other things <i>besides</i> cautious rational long-term basic AI research.  But it isn&#8217;t the really hard part of the problem.</p>
<p>The scary thing about AI is that I would guess that the <i>first</i> AI to go over some critical threshold of self-improvement takes all the marbles&mdash;first mover advantage, winner take all.  The first pile of uranium to have an effective neutron multiplication factor greater than 1, or maybe the first AI smart enough to absorb all the poorly defended processing power on the Internet&mdash;there&#8217;s actually a number of different thresholds that could provide a critical first-mover advantage.</p>
<p>And it is always going to be fundamentally easier in some sense to go straight all out for AI and not worry about clean designs or stable self-modification or the problem where a near-miss on the value system destroys almost all of the actual value from our perspective.  (E.g., imagine aliens who shared every single term in the human utility function but lacked our notion of boredom.  Their civilization might consist of a single peak experience repeated over and over, which would make their civilization very boring from our perspective, compared to what it might have been.  That is, leaving a single aspect out of the value system can destroy almost all of the value.  So there&#8217;s a very large gap in the AI problem between trying to get the value system exactly right, versus throwing something at it that sounds vaguely good.)</p>
<p>You want to keep as much of an advantage as possible for the cautious rational AI developers over the crowd that is just gung-ho to solve this super interesting scientific problem and go down in the eternal books of fame.  Now there should in fact be some upper bound on the combination of intelligence, methodological rationality, and deep understanding of the problem which you can possess, and still walk directly into the whirling helicopter blades.  The problem is that it is probably a rather high upper bound.  And you are trying to outrace people who are trying to solve a fundamentally easier wrong problem.  So the question is not attracting people to the field in general, but rather getting the really smart competent people to <i>either</i> work for a cautious project <i>or</i> not go into the field at all.  You aren&#8217;t going to stop people from trying to develop AI.  But you can hope to have as many of the really smart people as possible working on cautious projects rather than incautious ones.</p>
<p>So yes, making caution look sexy.  But even more than that, trying to make incautious AI projects look merely stupid.  Not dangerous.  Dangerous is sexy.  As the old proverb goes, most of the damage is done by people who wish to feel themselves important.  Human psychology seems to be such that many ambitious people find it far less scary to think about destroying the world, than to think about never amounting to much of anything at all.  I have met people like this.  In fact all the people I have met who think they are going to win eternal fame through their AI projects have been like this.  The thought of potentially destroying the world is bearable; it confirms their own importance.  The thought of not being able to plow full steam ahead on their incredible amazing AI idea is <i>not</i> bearable; it threatens all their fantasies of wealth and fame.  </p>
<p>Now these people of whom I speak are not top-notch minds, not in the class of the top people in mainstream AI, like say <a href="http://norvig.com/">Peter Norvig</a> (to name someone I&#8217;ve had the honor of meeting personally).  And it&#8217;s possible that if and when self-improving AI starts to get real top-notch minds working on it, rather than people who were too optimistic about/attached to their amazing bright idea to be scared away by the field of skulls, then these real stars will not fall prey to the same sort of psychological trap.  And then again it is also plausible to me that top-notch minds will fall prey to <i>exactly</i> the same trap, because I have yet to learn from reading history that great scientific geniuses are always sane. </p>
<p>So what I would most like to see would be uniform looks of condescending scorn directed at people who claimed their amazing bright AI idea was going to lead to self-improvement and superintelligence, but who couldn&#8217;t mount an adequate defense of how their design would have a goal system stable after a billion sequential self-modifications, or how it would get the value system exactly right instead of mostly right.  In other words, making destroying the world look unprestigious and low-status, instead of leaving it to the default state of sexiness and importance-confirmingness.</p>
<p><b>JB</b>: &#8220;Get the value system exactly right&#8221;&mdash;now this phrase touches on another issue I&#8217;ve been wanting to talk about.  How do we know what it means for a value system to be exactly right?  It seems people are even further from agreeing on what it means to be good than on what it means to be rational.  Yet you seem to be suggesting we need to solve this problem before it&#8217;s safe to build a self-improving artificial intelligence!</p>
<p>When I was younger I worried a lot about the foundations of ethics.  I decided that you &#8220;can&#8217;t derive an ought from an is&#8221;&mdash;do you believe that?  If so, all logical arguments leading up to the conclusion that &#8220;you should do X&#8221; must involve an assumption of the form &#8220;you should do Y&#8221;&#8230; and attempts to &#8220;derive&#8221; ethics are all implicitly circular in some way.  This really bothered the heck out of me: how was I supposed to know what to do?  But of course I kept on doing things while I was worrying about this&#8230; and indeed, it was painfully clear that there&#8217;s no way out of making decisions: even deciding to &#8220;do nothing&#8221; or commit suicide counts as a decision.</p>
<p>Later I got more comfortable with the idea that making decisions about what to do needn&#8217;t paralyze me any more than making decisions about what is true.  But still, it seems that the business of designing ethical beings is going to provoke huge arguments, if and when we get around to that.</p>
<p>Do you spend as much time thinking about these issues as you do thinking about rationality?  Of course they&#8217;re linked&#8230;.</p>
<p><b>EY</b>:  Well, I probably spend as much time <i>explaining</i> these issues as I do rationality.  There are also an absolutely huge number of pitfalls that people stumble into when they try to think about, as I would put it, <a href="http://en.wikipedia.org/wiki/Friendly_artificial_intelligence">Friendly AI</a>.  Consider how many pitfalls people run into when they try to think about Artificial Intelligence.  Next consider how many pitfalls people run into when they try to think about morality.  Next consider how many pitfalls philosophers run into when they try to think about the nature of morality.  Next consider how many pitfalls people run into when they try to think about hypothetical extremely powerful agents, especially extremely powerful agents that are supposed to be extremely good.  Next consider how many pitfalls people run into when they try to imagine optimal worlds to live in or optimal rules to follow or optimal governments and so on.</p>
<p>Now imagine a subject matter which offers discussants a lovely opportunity to run into all of those pitfalls at the same time.</p>
<p>That&#8217;s what happens when you try to talk about Friendly Artificial Intelligence.</p>
<p>And it only takes one error for a chain of reasoning to end up in Outer Mongolia.  So one of the great motivating factors behind all the writing I did on rationality and all the <a href="http://wiki.lesswrong.com/wiki/Sequences">sequences</a> I wrote on <i><a href="http://lesswrong.com/">Less Wrong</a></i> was to actually make it possible, via two years worth of writing and probably something like a month&#8217;s worth of reading at least, to immunize people against <i>all</i> the usual mistakes.</p>
<p>Lest I appear to dodge the question entirely, I&#8217;ll try for very quick descriptions and google keywords that professional moral philosophers might recognize.</p>
<p>In terms of what I would advocate programming a very powerful AI to actually do, the keywords are <a href="#Mature Folk Morality">&#8220;mature folk morality&#8221;</a> and <a href="http://en.wikipedia.org/wiki/Reflective_equilibrium">&#8220;reflective equilibrium&#8221;</a>.  This means that you build a sufficiently powerful AI to do, not what people say they want, or even what people actually want, but what people <i>would</i> decide they wanted the AI to do, if they had all of the AI&#8217;s information, could think about for as long a subjective time as the AI, knew as much as the AI did about the real factors at work in their own psychology, and had no failures of self-control.</p>
<p>There&#8217;s a lot of important reasons why you would want to do exactly that and not, say, implement Asimov&#8217;s <a href="http://en.wikipedia.org/wiki/Three_Laws_of_Robotics">Three Laws of Robotics</a> (a purely fictional device, and if Asimov had depicted them as working well, he would have had no stories to write) or building a superpowerful AI which obeys people&#8217;s commands interpreted in literal English, or creating a god whose sole prime directive is to make people maximally happy, or any of the above plus a list of six different patches which guarantee that nothing can possibly go wrong, and various other things that seem like incredibly obvious failure scenarios but which I assure you I have heard seriously advocated over and over and over again.</p>
<p>In a nutshell, you want to use concepts like &#8220;mature folk morality&#8221; or &#8220;reflective equilibrium&#8221; because these are as close as moral philosophy has ever gotten to defining in concrete, computable terms what you could be wrong <i>about</i> when you order an AI to do the wrong thing.</p>
<p>For an attempt at nontechnical explanation of what one might want to program an AI to do and why, the best resource I can offer is an old essay of mine which is not written so as to offer good google keywords, but holds up fairly well nonetheless:</p>
<p>&bull; Eliezer Yudkowsky, <a href="http://singinst.org/upload/CEV.html">Coherent extrapolated volition</a>, May 2004.</p>
<p>You also raised some questions about <a href="http://en.wikipedia.org/wiki/Meta-ethics">metaethics</a>, where metaethics asks not &#8220;Which acts are moral?&#8221; but &#8220;What is the subject matter of our talk about &#8216;morality&#8217;?&#8221; i.e. &#8220;What are we talking <i>about</i> here anyway?&#8221;  In terms of Google keywords, my brand of metaethics is closest to <a href="http://www.google.com/search?q=analytic+descriptivism">analytic descriptivism</a> or <a href="http://www.google.com/search?q=moral+functionalism">moral functionalism</a>.  If I were to try to put that into a very brief nutshell, it would be something like &#8220;When we talk about &#8216;morality&#8217; or &#8216;goodness&#8217; or &#8216;right&#8217;, the subject matter we&#8217;re talking <i>about</i> is a sort of gigantic math question hidden under the simple word &#8216;right&#8217;, a math question that includes all of our emotions and all of what we use to process moral arguments and all the things we might want to change about ourselves if we could see our own source code and know what we were really thinking.&#8221;</p>
<p>The complete Less Wrong sequence on metaethics (with many dependencies to earlier ones) is:</p>
<p>&bull; Eliezer Yudkowsky, <a href="http://wiki.lesswrong.com/wiki/Metaethics_sequence">Metaethics sequence</a>, <i>Less Wrong</i>, 20 June to 22 August 2008.</p>
<p>And one of the better quick summaries is at:</p>
<p>&bull; Eliezer Yudkowsky, <a href="http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/">Inseparably right; or, joy in the merely good</a>, <i>Less Wrong</i>, 9 August 2008.</p>
<p>And if I am wise I shall not say any more.</p>
<p><b>JB</b>: I&#8217;ll help you be wise.  There are a hundred followup questions I&#8217;m tempted to ask, but this has been a long and grueling interview, so I won&#8217;t.  Instead, I&#8217;d like to raise one last big question.  It&#8217;s about time scales.</p>
<p>Self-improving artificial intelligence seems like a real possibility to me.  But when?  You see, I believe we&#8217;re in the midst of a global ecological crisis&mdash;a mass extinction event, whose effects will be painfully evident by the end of the century.  I want to do something about it.  I can&#8217;t do much, but I want to do <i>something</i>.  Even if we&#8217;re doomed to disaster, there are different sizes of disaster.  And if we&#8217;re going through a kind of bottleneck, where some species make it through and others go extinct, even small actions now can make a difference.</p>
<p>I can imagine some technological optimists&mdash;singularitarians, extropians and the like&mdash;saying: &#8220;Don&#8217;t worry, things will get better.  Things that seem hard now will only get easier.  We&#8217;ll be able to suck carbon dioxide from the atmosphere using nanotechnology, and revive species starting from their DNA.&#8221;  Or maybe even: &#8220;Don&#8217;t worry: we won&#8217;t miss those species.  We&#8217;ll be having too much fun doing things we can&#8217;t even conceive of now.&#8221;</p>
<p>But various things make me skeptical of such optimism.  One of them is the question of time scales. What if the world goes to hell before our technology saves us? What if artificial intelligence comes along toolate to make a big impact on the short-term problems I&#8217;m worrying about?   In that case, maybe I should focus on short-term solutions.</p>
<p>Just to be clear: this isn&#8217;t some veiled attack on your priorities.  I&#8217;m just trying to decide on my own.  One good thing about having billions of people on the planet is that we don&#8217;t all have to do the same thing.  Indeed, a multi-pronged approach is best.  But for my own decisions, I want some rough guess about how long various potentially revolutionary technologies will take to come online.</p>
<p>What do you think about all this?</p>
<p><b>EY</b>:  I&#8217;ll try to answer the question about timescales, but first let me explain in some detail why I don&#8217;t think the decision should be dominated by that question.</p>
<p>If you look up &#8220;<a href="http://lesswrong.com/lw/hw/scope_insensitivity/">Scope Insensitivity</a>&#8221; on <i>Less Wrong</i>, you&#8217;ll see that when three different groups of subjects were asked how much they would pay in increased taxes to save 2,000 / 20,000 / 200,000 birds from drowning in uncovered oil ponds, the respective average answers were $80 / $78 / $88.  People asked questions like this visualize one bird, wings slicked with oil, struggling to escape, and that creates some amount of emotional affect which determines willingness to pay, and the quantity gets tossed out the window since no one can visualize 200,000 of anything.  Another hypothesis to explain the data is &#8220;purchase of moral satisfaction&#8221;, which says that people give enough money to create a &#8220;warm glow&#8221; inside themselves, and the amount required might have something to do with your personal financial situation, but it has nothing to do with birds.  Similarly, residents of four US states were only willing to pay 22% more to protect all 57 wilderness areas in those states than to protect one area.  The result I found most horrifying was that subjects were willing to contribute more when a set amount of money was needed to save one child&#8217;s life, compared to the same amount of money saving eight lives&mdash;because, of course, focusing your attention on a single person makes the feelings stronger, less diffuse.</p>
<p>So while it may make sense to <i>enjoy</i> the warm glow of doing good deeds <i>after</i> we do them, we cannot possibly allow ourselves to choose between altruistic causes based on the relative amounts of warm glow they generate, because our intuitions are quantitatively insane.</p>
<p>And two antidotes that absolutely must be applied in choosing between altruistic causes are conscious appreciation of scope and conscious appreciation of marginal impact.</p>
<p>By its nature, your brain flushes right out the window the all-important distinction between saving one life and saving a million lives.  You&#8217;ve got to compensate for that using conscious, verbal deliberation.  The Society For Curing Rare Diseases in Cute Puppies has got great warm glow, but the fact that these diseases are <i>rare</i> should call a screeching halt right there&mdash;which you&#8217;re going to have to do consciously, not intuitively.  Even before you realize that, contrary to the relative warm glows, it&#8217;s really hard to make a moral case for trading off human lives against cute puppies.  I suppose if you could save a billion puppies using one dollar I wouldn&#8217;t scream at someone who wanted to spend the dollar on that instead of cancer research.</p>
<p>And similarly, if there are a hundred thousand researchers and billions of dollars annually that are <i>already</i> going into saving species from extinction&mdash;because it&#8217;s a prestigious and popular cause that has an easy time generating warm glow in lots of potential funders&mdash;then you have to ask about the marginal value of putting your effort there, where so many other people are already working, compared to a project that isn&#8217;t so popular.</p>
<p>I wouldn&#8217;t say &#8220;Don&#8217;t worry, we won&#8217;t miss those species&#8221;.  But consider the future intergalactic civilizations growing out of Earth-originating intelligent life.  Consider the whole history of a universe which contains this world of Earth and this present century, and also billions of years of future intergalactic civilization continuing until the universe dies, or maybe forever if we can think of some ingenious way to carry on.  Next consider the interval in utility between a universe-history in which Earth-originating intelligence survived and thrived and managed to save 95% of the non-primate biological species now alive, versus a universe-history in which only 80% of those species are alive.  That utility interval is not very large compared to the utility interval between a universe in which intelligent life thrived and intelligent life died out.  Or the utility interval between a universe-history filled with sentient beings who experience happiness and have empathy for each other and get bored when they do the same thing too many times, versus a universe-history that grew out of various failures of Friendly AI.</p>
<p>(The really scary thing about universes that grow out of a loss of human value is not that they are different, but that they are, from our standpoint, boring.  The human utility function says that once you&#8217;ve made a piece of art, it&#8217;s <i>more</i> fun to make a <i>different</i> piece of art next time.  But that&#8217;s just us.  Most random utility functions will yield instrumental strategies that spend some of their time and resources exploring for the patterns with the highest utility at the beginning of the problem, and then use the rest of their resources to implement the pattern with the highest utility, over and over and over.  This sort of thing will surprise a human who expects, on some deep level, that all minds are made out of human parts, and who thinks, &#8220;Won&#8217;t the AI see that its utility function is boring?&#8221; But the AI is not a little spirit that looks over its code and decides whether to obey it; the AI <i>is</i> the code.  If the code doesn&#8217;t say to get bored, it won&#8217;t get bored.  A strategy of exploration followed by exploitation is implicit in most utility functions, but boredom is not.  If your utility function does not already contain a term for boredom, then you don&#8217;t care; it&#8217;s not something that emerges as an instrumental value from most terminal values.  For more on this see: &#8220;<a href="http://lesswrong.com/lw/xr/in_praise_of_boredom/">In Praise of Boredom</a>&#8221; in the <a href="http://lesswrong.com/lw/xy/the_fun_theory_sequence/">Fun Theory Sequence</a> on <i>Less Wrong</i>.)</p>
<p>Anyway:  In terms of expected utility maximization, even <i>large</i> probabilities of jumping the interval between a universe-history in which 95% of existing biological species survive Earth&#8217;s 21st century, versus a universe-history where 80% of species survive, are just about impossible to trade off against <i>tiny</i> probabilities of jumping the interval between interesting universe-histories, versus boring ones where intelligent life goes extinct, or the wrong sort of AI self-improves.</p>
<p>I honestly don&#8217;t see how a rationalist can avoid this conclusion:  At this absolutely critical hinge in the history of the universe&mdash;Earth in the 21st century&mdash;rational altruists should devote their marginal attentions to risks that threaten to terminate intelligent life or permanently destroy a part of its potential.  Those problems, which <a href="http://www.nickbostrom.com/">Nick Bostrom</a> named &#8220;<a href="http://www.nickbostrom.com/existential/risks.html">existential risks</a>&#8220;, have got <i>all</i> the scope.  And when it comes to marginal impact, there are major risks outstanding that practically no one is working on.  Once you get the stakes on a gut level it&#8217;s hard to see how doing anything else could be <i>sane</i>.</p>
<p>So how do you go about protecting the future of intelligent life?  Environmentalism?  After all, there are environmental catastrophes that could knock over our civilization&#8230; but then if you want to put the whole universe at stake, it&#8217;s not enough for one civilization to topple, you have to argue that our civilization is above average in its chances of building a positive galactic future compared to whatever civilization would rise again a century or two later.  Maybe if there were ten people working on environmentalism and millions of people working on Friendly AI, I could see sending the next marginal dollar to environmentalism.  But with millions of people working on environmentalism, and major existential risks that are completely ignored&#8230; if you add a marginal resource that can, rarely, be steered by expected utilities instead of warm glows, devoting that resource to environmentalism <i>does not make sense</i>.</p>
<p>Similarly with other short-term problems.  Unless they&#8217;re little-known and unpopular problems, the marginal impact is not going to make sense, because millions of other people will already be working on them.  And even if you argue that some short-term problem leverages existential risk, it&#8217;s not going to be perfect leverage and some quantitative discount will apply, probably a large one.  I would be suspicious that the decision to work on a short-term problem was driven by warm glow, status drives, or simple conventionalism.</p>
<p>With that said, there&#8217;s also such a thing as <a href="http://en.wikipedia.org/wiki/Comparative_advantage">comparative advantage</a>&mdash;the old puzzle of the lawyer who works an hour in the soup clinic instead of working an extra hour as a lawyer and donating the money. Personally I&#8217;d say you can work an hour in the soup clinic to keep yourself going if you like, but you should <i>also</i> be working extra lawyer-hours and donating the money to the soup clinic, or better yet, to something with more scope.  (See <a href="http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/">&#8220;Purchase Fuzzies and Utilons Separately&#8221;</a> on <i>Less Wrong</i>.)  Most people can&#8217;t work effectively on Artificial Intelligence (some would question if <i>anyone</i> can, but at the very least it&#8217;s not an easy problem).  But there&#8217;s a variety of existential risks to choose from, plus a general background job of spreading sufficiently high-grade rationality and existential risk awareness.  One really should look over those before going into something short-term and conventional.  Unless your master plan is just to work the extra hours and donate them to the cause with the highest marginal expected utility per dollar, which is perfectly respectable.</p>
<p>Where should you go in life?  I don&#8217;t know exactly, but I think I&#8217;ll go ahead and say &#8220;not environmentalism&#8221;.  There&#8217;s just no way that the product of scope, marginal impact, and John Baez&#8217;s comparative advantage is going to end up being maximal at that point.</p>
<p>Which brings me to AI timescales.</p>
<p>If I knew exactly how to make a Friendly AI, and I knew exactly how many people I had available to do it, I still couldn&#8217;t tell you how long it would take because of Product Management Chaos.</p>
<p>As it stands, this is a basic research problem&mdash;which will always <i>feel</i> very hard, because we don&#8217;t understand it, and that means when our brain checks for solutions, we don&#8217;t see any solutions available. But this ignorance is not to be confused with the positive knowledge that the problem will take a long time to solve once we know how to solve it.  It could be that some fundamental breakthrough will dissolve our confusion and then things will look relatively easy.  Or it could be that some fundamental breakthrough will be followed by the realization that, now that we <i>know</i> what to do, it&#8217;s going to take at<br />
least another 20 years to do it.</p>
<p>I seriously have no idea when AI is going to show up, although I&#8217;d be genuinely and deeply shocked if it took another century (barring a collapse of civilization in the meanwhile).</p>
<p>If you were to tell me that as a Bayesian I <i>have</i> to put probability distributions on things on pain of having my behavior be inconsistent and inefficient, well, I would actually suspect that my behavior <i>is</i> inconsistent.  But if you were to try and induce from my behavior a median expected time where I spend half my effort planning for less and half my effort planning for more, it would probably look something like 2030.</p>
<p>But that doesn&#8217;t really matter to my decisions.  Among all existential risks I know about, Friendly AI has the single largest absolute scope&mdash;it affects everything, and the problem <i>must</i> be solved at some point for worthwhile intelligence to thrive.  It also has the largest product of scope of marginal impact, because practically no one is working on it, even compared to other existential risks.  And my abilities seem applicable to it.  So I may not like my uncertainty about timescales, but my decisions are not unstable with respect to that uncertainty.</p>
<p><b>JB</b>: Ably argued!  If I think of an interesting reply, I&#8217;ll put it in the blog discussion.  Thanks for your time.</p>
<hr />
<p><i>The best way to predict the future is to invent it.</i> &#8211; Alan Kay</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/#comments">75 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>, <a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/" rel="category tag">this week's finds</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/" rel="bookmark" title="Permanent Link to This Week&#8217;s Finds (Week&nbsp;313)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/category/risks/page/4/" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the risks category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/risks/page/5/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

	<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jksOwjAMRC9EarWLSl0gzpImVuXQfLAd2t6eVAIJsWA1nrGePbAV43JSTApBwOOTHJa9C3KBr1Wspqx1oSRgfaRkZssQrShym4yydXf5hdq98KjIx1u6rbgcTeG8H4axZaIfhpJbq0c5oWYxzui79uhPkY38giogdRbHVJRyOjvc4rUfh2nqh2Hswwtq4FN0'></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTY5SFc3aUU1L0xnSXBRYkdYP2syVFJfblNQL0pKUHJxVEdLflpYZlR8d0ZWL2dTd0lPZ0JNcy5QX1JhdW1ZdkNRQVRMPTdFOHBRZVFFTXZ3TnImLXpafFJmQ11EXWc/NW5VWn5aUj9mP35ndG5hWGI5R34mbXYlRHosZlkxN0RJdG0yOXk9a0gvRmhLZiY9ZVt+aH45MTQzTSZGJk1QTjR3My9Ic2o2TDByZk4ufGFTYmpHS343dm5PSGNaK1FFcltpaFYrKz1p'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>