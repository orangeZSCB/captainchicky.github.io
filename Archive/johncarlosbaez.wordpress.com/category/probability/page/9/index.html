<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>probability | Azimuth | Page 9</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; probability Category Feed" href="https://johncarlosbaez.wordpress.com/category/probability/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IlqVOdbxwfBYOEVOBZSBYeXuz1e6sVrveMDl9yU9AHrOwUyJIJGMTOTSPqUpMz5iQ+mIMttYbuVFMLxChytyMPJXFTBfcGfKNXQPFc6aAfFNq2A9KmobBSRMm+yoCmqJLl5V6gKURJhua4zGHKiM41BB46qxo5eSgOxQRwGvbh4hpG+fc2v8G/S3+pJSbAWU9S9Z9aiR8QfdD9r9bFE2YfN3A7fSF7QY18t4cVlqC4nd29WTzzjkes768+BXsiM4DMV7PtiB4v45kHiOMyQVqFXxGbFF8/pSZe4qP6u5W3T/sx3F3+ACgX/YH?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F10%2F18%2Fthe-decline-effect%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/probability\/page\/9\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Fprobability%2Fpage%2F9%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F10%2F18%2Fthe-decline-effect%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="probability &#8211; Page 9 &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/probability/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about probability written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive paged category category-probability category-10451 paged-9 category-paged-9 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-5613 post type-post status-publish format-standard hentry category-probability category-the-practice-of-science" id="post-5613">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/10/18/the-decline-effect/" rel="bookmark">The Decline Effect</a></h2>
				<small>18 October, 2011</small><br />


				<div class="entry">
					<p>I bumped into a surprising article recently:</p>
<p>&bull; Jonah Lehrer, <a href="http://www.neurofly.com/NeuroSeminar_files/2011-2.pdf">Is there something wrong with the scientific method?</a>, <i>New Yorker</i>, 13 December 2010.</p>
<p>It starts with a bit of a bang:</p>
<blockquote><p>
Before the effectiveness of a drug can be confirmed, it must be tested and tested again. Different scientists in different labs need to repeat the protocols and publish their results. The test of replicability, as it’s known, is the foundation of modern research. Replicability is how the community enforces itself. It’s a safeguard for the creep of subjectivity. Most of the time, scientists know what results they want, and that can influence the results they get. The premise of replicability is that the scientific community can correct for these flaws.</p>
<p>But now all sorts of well-established, multiply confirmed findings have started to look increasingly uncertain. <b>It’s as if our facts were losing their truth: claims that have been enshrined in textbooks are suddenly unprovable. This phenomenon doesn’t yet have an official name, but it’s occurring across a wide range of fields, from psychology to ecology.</b> In the field of medicine, the phenomenon seems extremely widespread, affecting not only antipsychotics but also therapies ranging from cardiac stents to vitamin E and antidepressants: Davis has a forthcoming analysis demonstrating that the efficacy of antidepressants has gone down as much as threefold in recent decades.
</p></blockquote>
<p>This phenomenon does have a name now: it&#8217;s called the <b>decline effect</b>.  The article tells some amazing stories about it.  If you&#8217;re in the mood for some fun, I suggest going to your favorite couch or caf&eacute; now, and reading them!</p>
<p>For example: <a href="http://en.wikipedia.org/wiki/John_P._A._Ioannidis">John Ioannides</a> is the author of the most heavily downloaded paper in the open-access journal <i>PLoS Medicine</i>.  It&#8217;s called <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327/?tool=pmcentrez"><i>Why most published research findings are false</i></a>.  </p>
<p>In it, Ioannides took three prestigious medical journals and looked at the 49 most cited clinical research studies. 45 of them used randomized controlled trials and reported positive results.  But of the 34 that people tried to replicate, 41% were either directly contradicted or had their effect sizes significantly downgraded. </p>
<p>For more examples, read the article or listen to this radio show:</p>
<p>&bull; <a href="http://www.radiolab.org/blogs/radiolab-blog/2011/may/03/cosmic-habituation/">Cosmic Habituation</a>, Radiolab, May 3, 2011.</p>
<p>It&#8217;s a bit sensationalistic&#8230; but it&#8217;s fun.  It features Jonathan Schooler, who discovered a famous effect in psychology, called <a href="http://lesswrong.com/lw/dl/verbal_overshadowing_and_the_art_of_rationality/">verbal overshadowing</a>.    It doesn&#8217;t really matter what this effect is.  What matters is that it showed up very strongly in his first experiments&#8230; but as he and others continued to study it, it gradually diminished over time!  He got freaked out.  And then looked around, and saw that this sort of decline happened all over the place, in <i>lots</i> of cases.</p>
<p>What could cause this &#8216;decline effect&#8217;?  There are lots of possible explanations.</p>
<p>At one extreme, maybe the decline effect doesn&#8217;t really exist.  Maybe this sort of decline just happens sometimes purely by chance.   Maybe there are equally many cases where effects seem to get <i>stronger</i> each time they&#8217;re measured!  </p>
<p>At the other extreme, a very disturbing possibility has been proposed by Jonathan Schooler.  He suggests that somehow the laws of reality change when they&#8217;re studied, in such a way that initially strong effects gradually get weaker.  </p>
<p>I don&#8217;t believe this.  It&#8217;s logically possible, but there are lots of less radical explanations to rule out first.</p>
<p>But if it were true, maybe we could make the decline effect go away by studying it.  <i>The decline effect would itself decline!</i>   </p>
<p>Unless of course, you started <i>studying</i> the decline of the decline effect.</p>
<div align="center"><img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/spiral_eyes.gif" alt="" /></div>
<p>Okay.  On to some explanations that are interesting but less far-out.</p>
<p>One plausible explanation is <b>significance chasing</b>.  Scientists work really hard to find something that&#8217;s <a href="http://en.wikipedia.org/wiki/Statistical_significance">&#8216;statistically significant&#8217;</a> according to the widely-used criterion of having a <a href="http://en.wikipedia.org/wiki/P-value">p-value</a> of less than 0.05.  </p>
<p>That sounds technical, but basically all it means is this: there was at most a 5% chance of having found a deviation from the expected situation that&#8217;s as big as the one you found.  </p>
<p>(To play this game, you have to say ahead of time what the &#8216;expected situation&#8217; is: this is your <a href="http://en.wikipedia.org/wiki/Null_hypothesis"><b>null hypothesis</b></a>.)</p>
<p>Why is significance chasing dangerous?  How can it lead to the decline effect?</p>
<p>Well, here&#8217;s how to write a paper with a statistically significant result.  Go through 20 different colors of jelly bean and see if people who eat them have more acne than average.  There&#8217;s a good chance that one of your experiments will say &#8216;yes&#8217; with a p-value of less than 0.05, just because 0.05 = 1/20.    If so, this experiment gives a statistically significant result!</p>
<p>I took this example from Randall Munroe&#8217;s cartoon strip xkcd:</p>
<div align="center"><a href="http://imgs.xkcd.com/comics/significant.png"><img width="450" src="https://i2.wp.com/imgs.xkcd.com/comics/significant.png" /></a></div>
<p>It&#8217;s funny&#8230; but it&#8217;s actually sad: some testing of drugs is not much better than this!  Clearly a result obtained this way is junk, so when you try to replicate it, the &#8216;decline effect&#8217; will kick in.</p>
<p>Another possible cause of the decline effect is <b><a href="http://en.wikipedia.org/wiki/Publication_bias">publication bias</a></b>: scientists and journals prefer positive results over null results, where no effect is found.  And surely there are other explanations, too: for starters, all the ways people can fool themselves into thinking they&#8217;ve discovered something interesting.</p>
<p>For suggestions on how to avoid the evils of &#8216;publication bias&#8217;, try these:</p>
<p>&bull; Jonathan Schooler, <a href="http://www.nature.com/news/2011/110223/full/470437a.html">Unpublished results hide the decline effect</a>, <i>Nature</i> <b>470</b> (2011), 437.</p>
<p>Putting an end to &#8216;significance chasing&#8217; may require people to learn more about statistics:</p>
<p>&bull; Geoff Cumming, <a href="http://www.abc.net.au/rn/ockhamsrazor/stories/2011/3333636.htm">Significant does not equal important: why we need the new statistics</a>, 9 October 2011.</p>
<p>He explains the problem in simple language:</p>
<blockquote><p>
Consider a psychologist who&#8217;s investigating a new therapy for anxiety. She randomly assigns anxious clients to the therapy group, or a control group. You might think the most informative result would be an estimate of the benefit of therapy &#8211; the average improvement as a number of points on the anxiety scale-together with the amount that&#8217;s the confidence interval around that average. But psychology typically uses significance testing rather than estimation.</p>
<p>Introductory statistics books often introduce significance testing as a step-by-step recipe:</p>
<p><b>Step 1.</b> Assume the new therapy has zero effect. You don&#8217;t believe this and you fervently hope it&#8217;s not true, but you assume it.</p>
<p><b>Step 2.</b> You use that assumption to calculate a strange thing called a &#8216;p value&#8217;, which is the probability that, if the therapy really has zero effect, the experiment would have given a difference as large as you observed, or even larger.</p>
<p><b>Step 3.</b> If the p value is small, in particular less than the hallowed criterion of .05 (that&#8217;s 1 chance in 20), you are permitted to reject your initial assumption&#8212;which you never believed anyway&#8212;and declare that the therapy has a &#8216;significant&#8217; effect.</p>
<p>If that&#8217;s confusing, you&#8217;re in good company. Significance testing relies on weird backward logic. No wonder countless students every year are bamboozled by their introduction to statistics! Why this strange ritual they ask, and what does a p value actually mean? Why don&#8217;t we focus on how large an improvement the therapy gives, and whether people actually find it helpful?  These are excellent questions, and estimation gives the best answers.</p>
<p>For half a century distinguished scholars have published damning critiques of significance testing, and explained how it hampers research progress. There&#8217;s also extensive evidence that students, researchers, and even statistics teachers often don&#8217;t understand significance testing correctly. Strangely, the critiques of significance testing have hardly prompted any defences by its supporters. Instead, psychology and other disciplines have simply continued with the significance testing ritual, which is now deeply entrenched. It&#8217;s used in more than 90% of published research in psychology, and taught in every introductory textbook.
</p></blockquote>
<p>For more discussion and references, try my co-blogger:</p>
<p>&bull; Tom Leinster, <a href="http://golem.ph.utexas.edu/category/2010/09/fetishizing_pvalues.html">Fetishizing p-values</a>, <i>n</i>-Category Caf&eacute;.</p>
<p>He gives some good examples of how significance testing can lead us astray.  Anyone who uses the p-test should read these!  He also discusses this book:</p>
<p>&bull; Stephen T. Ziliak and Deirdre N. McCloskey, <i>The Cult of Statistical Significance</i>, University of Michigan Press, Ann Arbor, 2008.  (Online summary <a href="www.statlit.org/pdf/2009ZiliakMcCloskeyASA.pdf">here</a>.)</p>
<p>Now, back to the provocative title of that <i>New Yorker</i> article: &#8220;Is there something wrong with the scientific method?&#8221;  </p>
<p>The answer is <i><b>yes</b></i> if we mean <i>science as actually practiced, now</i>.  Lots of scientists are using cookbook recipes they learned in statistics class without understanding them, or investigating the alternatives.  Worse, some are treating statistics as a necessary but unpleasant piece of bureaucratic red tape, and then doing whatever it takes to achieve the <i>appearance</i> of a significant result!</p>
<p>This is a bit depressing.   There&#8217;s a student I know, who is taking an introductory statistics course.  After she read about this stuff she said:</p>
<blockquote><p>
So, what I&#8217;m gleaning here is that what I&#8217;m studying is basically bull. It struck me as bull to start with, admittedly, but since my grade depended on it, I grinned and swallowed. At least my eyes are open now, I guess.
</p></blockquote>
<p>But there&#8217;s some good news, buried in her last sentence.  Science has the marvelous ability to notice and correct its own mistakes.  It&#8217;s <i>scientists</i> who noticed the decline effect and significance chasing.   They&#8217;ll eventually figure out what&#8217;s going on, and learn how to fix any mistakes that they&#8217;ve been making.  So ultimately, I don&#8217;t find this story depressing.  It&#8217;s actually inspiring!</p>
<p>The scientific method is not a fixed rulebook handed down from on high.  It&#8217;s a work in progress.  It&#8217;s only been around for a few centuries&#8212;not very long, in the grand scheme of things.  The widespread use of statistics in science has been around for less than <i>one</i> century.  And computers, which make heavy-duty number-crunching easy, have only been cheap for <i>30 years!</i>  No wonder people still use primitive cookbook methods for analyzing data, when they could do better.</p>
<p>So science is still evolving.  And I think that&#8217;s fun, because it means <i>we can help it along</i>.   If you see someone claim their results are statistically significant, you can ask them what they mean, exactly&#8230; and what they had to do to get those results.</p>
<hr />
<p>I thank a lot of people on Google+ for discussions on this topic, including (but not limited to) John Forbes, Roko Mijic, Heather Vandagriff, and Willie Wong. </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/18/the-decline-effect/#comments">43 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>, <a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/" rel="category tag">the practice of science</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/18/the-decline-effect/" rel="bookmark" title="Permanent Link to The Decline Effect">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-5525 post type-post status-publish format-standard hentry category-mathematics category-networks category-physics category-probability" id="post-5525">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/10/11/network-theory-part-13/" rel="bookmark">Network Theory (Part&nbsp;13)</a></h2>
				<small>11 October, 2011</small><br />


				<div class="entry">
					<p>Unlike some recent posts, this will be very short.  I merely want to show you the quantum and stochastic versions of Noether&#8217;s theorem, side by side.  </p>
<p>Having made my sacrificial offering to the math gods last time by explaining how everything generalizes when we replace our finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> of states by an infinite set or an even more general <a href="http://en.wikipedia.org/wiki/Measure_space">measure space</a>, I&#8217;ll now relax and state Noether&#8217;s theorem only for a finite set.  If you&#8217;re the sort of person who finds that unsatisfactory, you can do the generalization yourself.</p>
<h4> Two versions of Noether&#8217;s theorem </h4>
<p>Let me write the quantum and stochastic Noether&#8217;s theorem so they look almost the same:</p>
<p><b>Theorem.</b> Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be a finite set.  Suppose <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is a self-adjoint operator on <img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X)" class="latex" />, and let <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> be an observable.  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = 0 " class="latex" /></p>
<p>if and only if for all states <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> obeying Schr&ouml;dinger&#8217;s equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+-i+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = -i H &#92;psi(t) } " class="latex" /></p>
<p>the expected value of <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> does not change with <img src="https://s0.wp.com/latex.php?latex=t.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t." class="latex" /></p>
<p><b>Theorem.</b>  Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be a finite set.  Suppose <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is an infinitesimal stochastic operator on <img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X)" class="latex" />, and let <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> be an observable.  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] =0 " class="latex" /></p>
<p>if and only if for all states <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> obeying the master equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = H &#92;psi(t) } " class="latex" /></p>
<p>the expected values of <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=O%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O^2" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> do not change with <img src="https://s0.wp.com/latex.php?latex=t.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t." class="latex" /></p>
<p>This makes the big difference stick out like a sore thumb: in the quantum version we only need the expected value of <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" />, while in the stochastic version we need the expected values of <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=O%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O^2" class="latex" />!</p>
<p>Brendan Fong proved the stochastic version of Noether&#8217;s theorem in <a href="https://johncarlosbaez.wordpress.com/2011/10/04/network-theory-part-11/">Part 11</a>.  Now let&#8217;s do the quantum version.</p>
<h4> Proof of the quantum version </h4>
<p>My statement of the quantum version was silly in a couple of ways.  First, I spoke of the Hilbert space <img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X)" class="latex" /> for a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />, but any finite-dimensional Hilbert space will do equally well.  Second, I spoke of the &#8220;self-adjoint operator&#8221; <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> and the &#8220;observable&#8221; <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" />, but in quantum mechanics an observable is the <i>same thing</i> as a self-adjoint operator! </p>
<p>Why did I talk in such a silly way?  Because I was attempting to emphasize the similarity between quantum mechanics and stochastic mechanics.   But they&#8217;re somewhat different.  For example, in stochastic mechanics we have two very different concepts: infinitesimal stochastic operators, which <i>generate symmetries</i>, and functions on our set <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> which <i>are observables</i>.  But in quantum mechanics something wonderful happens: self-adjoint operators both <i>generate symmetries</i> and <i>are observables!</i>  So, my attempt was a bit strained.  </p>
<p>Let me state and prove a less silly quantum version of Noether&#8217;s theorem, which implies the one above:</p>
<p><b>Theorem.</b> Suppose <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> are self-adjoint operators on a finite-dimensional Hilbert space.  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = 0 " class="latex" /></p>
<p>if and only if for all states <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> obeying Schr&ouml;dinger&#8217;s equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+-i+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = -i H &#92;psi(t) } " class="latex" /></p>
<p>the expected value of <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> does not change with <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Clangle+%5Cpsi%28t%29%2C+O+%5Cpsi%28t%29+%5Crangle+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;langle &#92;psi(t), O &#92;psi(t) &#92;rangle = 0 } " class="latex" /></p>
<p><b>Proof.</b> The trick is to compute the time derivative I just wrote down.  Using Schr&ouml;dinger&#8217;s equation, the product rule, and the fact that <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is self-adjoint we get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Clangle+%5Cpsi%28t%29%2C+O+%5Cpsi%28t%29+%5Crangle+%7D+%26%3D%26+++%5Clangle+-i+H+%5Cpsi%28t%29+%2C+O+%5Cpsi%28t%29+%5Crangle+%2B+%5Clangle+%5Cpsi%28t%29+%2C+O+%28-+i+H+%5Cpsi%28t%29%29+%5Crangle+%5C%5C++%5C%5C++%26%3D%26+i+%5Clangle+%5Cpsi%28t%29+%2C+H+O+%5Cpsi%28t%29+%5Crangle+-i+%5Clangle+%5Cpsi%28t%29+%2C+O+H+%5Cpsi%28t%29%29+%5Crangle+%5C%5C++%5C%5C++%26%3D%26+-+i+%5Clangle+%5Cpsi%28t%29%2C+%5BO%2CH%5D+%5Cpsi%28t%29+%5Crangle++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;frac{d}{d t} &#92;langle &#92;psi(t), O &#92;psi(t) &#92;rangle } &amp;=&amp;   &#92;langle -i H &#92;psi(t) , O &#92;psi(t) &#92;rangle + &#92;langle &#92;psi(t) , O (- i H &#92;psi(t)) &#92;rangle &#92;&#92;  &#92;&#92;  &amp;=&amp; i &#92;langle &#92;psi(t) , H O &#92;psi(t) &#92;rangle -i &#92;langle &#92;psi(t) , O H &#92;psi(t)) &#92;rangle &#92;&#92;  &#92;&#92;  &amp;=&amp; - i &#92;langle &#92;psi(t), [O,H] &#92;psi(t) &#92;rangle  &#92;end{array} " class="latex" /></p>
<p>So, if <img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = 0" class="latex" />, clearly the above time derivative vanishes.  Conversely, if this time derivative vanishes for all states <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> obeying Schr&ouml;dinger&#8217;s equation, we know </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi%2C+%5BO%2CH%5D+%5Cpsi+%5Crangle+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi, [O,H] &#92;psi &#92;rangle = 0 " class="latex" /></p>
<p>for all states <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> and thus all vectors in our Hilbert space.  Does this imply <img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = 0" class="latex" />?  Yes, because <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> times a commutator of a self-adjoint operators is self-adjoint, and for any self-adjoint operator <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cforall+%5Cpsi++%5C%3B+%5C%3B+%5Clangle+%5Cpsi%2C+A+%5Cpsi+%5Crangle+%3D+0+%5Cqquad+%5CRightarrow+%5Cqquad+A+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;forall &#92;psi  &#92;; &#92;; &#92;langle &#92;psi, A &#92;psi &#92;rangle = 0 &#92;qquad &#92;Rightarrow &#92;qquad A = 0 " class="latex" /></p>
<p>This is a well-known fact whose proof goes like this.  Assume <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi%2C+A+%5Cpsi+%5Crangle+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi, A &#92;psi &#92;rangle = 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" /> Then to show <img src="https://s0.wp.com/latex.php?latex=A+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A = 0," class="latex" /> it is enough to show <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cphi%2C+A+%5Cpsi+%5Crangle+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;phi, A &#92;psi &#92;rangle = 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" />.  But we have a marvelous identity:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Clangle+%5Cphi%2C+A+%5Cpsi+%5Crangle+%26%3D%26+%5Cfrac%7B1%7D%7B4%7D+%5Cleft%28+%5Clangle+%5Cphi+%2B+%5Cpsi%2C+%5C%2C+A+%28%5Cphi+%2B+%5Cpsi%29+%5Crangle+%5C%3B+-+%5C%3B+%5Clangle+%5Cpsi+-+%5Cphi%2C+%5C%2C+A+%28%5Cpsi+-+%5Cphi%29+%5Crangle+%5Cright.+%5C%5C+%26%26+%5Cleft.+%2Bi+%5Clangle+%5Cpsi+%2B+i+%5Cphi%2C+%5C%2C+A+%28%5Cpsi+%2B+i+%5Cphi%29+%5Crangle+%5C%3B+-+%5C%3B+i%5Clangle+%5Cpsi+-+i+%5Cphi%2C+%5C%2C+A+%28%5Cpsi+-+i+%5Cphi%29+%5Crangle+%5Cright%29+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;langle &#92;phi, A &#92;psi &#92;rangle &amp;=&amp; &#92;frac{1}{4} &#92;left( &#92;langle &#92;phi + &#92;psi, &#92;, A (&#92;phi + &#92;psi) &#92;rangle &#92;; - &#92;; &#92;langle &#92;psi - &#92;phi, &#92;, A (&#92;psi - &#92;phi) &#92;rangle &#92;right. &#92;&#92; &amp;&amp; &#92;left. +i &#92;langle &#92;psi + i &#92;phi, &#92;, A (&#92;psi + i &#92;phi) &#92;rangle &#92;; - &#92;; i&#92;langle &#92;psi - i &#92;phi, &#92;, A (&#92;psi - i &#92;phi) &#92;rangle &#92;right) &#92;end{array} " class="latex" /></p>
<p>and all four terms on the right vanish by our assumption.   &nbsp; &#9608;</p>
<p>The marvelous identity up there is called the <b><a href="http://en.wikipedia.org/wiki/Polarization_identity">polarization identity</a></b>.  In plain English, it says: if you know the diagonal entries of a self-adjoint matrix in every basis, you can figure out <i>all</i> the entries of that matrix in every basis.  </p>
<p>Why is it called the &#8216;polarization identity&#8217;?  I think because it shows up in optics, in the study of polarized light.</p>
<h4> Comparison </h4>
<p>In both the quantum and stochastic cases, the time derivative of the expected value of an observable <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> is expressed in terms of its commutator with the Hamiltonian.  In the quantum case we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Clangle+%5Cpsi%28t%29%2C+O+%5Cpsi%28t%29+%5Crangle+%3D+-+i+%5Clangle+%5Cpsi%28t%29%2C+%5BO%2CH%5D+%5Cpsi%28t%29+%5Crangle+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;langle &#92;psi(t), O &#92;psi(t) &#92;rangle = - i &#92;langle &#92;psi(t), [O,H] &#92;psi(t) &#92;rangle } " class="latex" /></p>
<p>and for the right side to <i>always</i> vanish, we need <img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = 0 " class="latex" />latex , thanks to the polarization identity.  In the stochastic case, a perfectly analogous equation holds:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cint+O+%5Cpsi%28t%29+%3D+%5Cint+%5BO%2CH%5D+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;int O &#92;psi(t) = &#92;int [O,H] &#92;psi(t) } " class="latex" /></p>
<p>but now the right side can always vanish even without <img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = 0." class="latex" />  We saw a counterexample in <a href="https://johncarlosbaez.wordpress.com/2011/10/04/network-theory-part-11/">Part 11</a>.  There is nothing like the polarization identity to save us!  To get <img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = 0" class="latex" /> we need a supplementary hypothesis, for example the vanishing of </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cint+O%5E2+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;int O^2 &#92;psi(t) } " class="latex" /></p>
<p>Okay!  Starting next time we&#8217;ll change gears and look at some more examples of stochastic Petri nets and Markov processes, including some from chemistry.  After some more of that, I&#8217;ll move on to networks of other sorts.  There&#8217;s a really big picture here, and I&#8217;m afraid I&#8217;ve been getting caught up in the details of a tiny corner.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/11/network-theory-part-13/#comments">6 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/11/network-theory-part-13/" rel="bookmark" title="Permanent Link to Network Theory (Part&nbsp;13)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-5470 post type-post status-publish format-standard hentry category-mathematics category-networks category-physics category-probability" id="post-5470">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/10/09/network-theory-part-12/" rel="bookmark">Network Theory (Part&nbsp;12)</a></h2>
				<small>9 October, 2011</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2011/10/04/network-theory-part-11/">Last time</a> we proved a version of Noether&#8217;s theorem for stochastic mechanics.  Now I want to compare that to the more familiar quantum version.  </p>
<p>But to do this, I need to say more about the analogy between stochastic mechanics and quantum mechanics.    And whenever I try, I get pulled toward explaining some technical issues involving analysis: whether sums converge, whether derivatives exist, and so on.  I&#8217;ve been trying to avoid such stuff&mdash;not because I dislike it, but because I&#8217;m afraid <i>you</i> might.  But the more I put off discussing these issues, the more they fester and make me unhappy.  In fact, that&#8217;s why it&#8217;s taken so long for me to write this post!</p>
<p>So, this time I will gently explore some of these issues.  But don&#8217;t be scared: I&#8217;ll <i>mainly</i> talk about some simple big ideas.  Next time I&#8217;ll discuss Noether&#8217;s theorem.  I hope that by getting the technicalities out of my system, I&#8217;ll feel okay about hand-waving whenever I want.</p>
<p>And if you&#8217;re an expert on analysis, maybe you can help me with a question.</p>
<h4>Stochastic mechanics versus quantum mechanics </h4>
<p>First, we need to recall the analogy we began sketching in <a href="https://johncarlosbaez.wordpress.com/2011/04/11/network-theory-part-5/">Part 5</a>, and push it a bit further.   The idea is that stochastic mechanics differs from quantum mechanics in two big ways:</p>
<p>&bull; First, instead of complex amplitudes, stochastic mechanics uses nonnegative real probabilities.  The complex numbers form a <a href="http://en.wikipedia.org/wiki/Ring_%28mathematics%29">ring</a>; the nonnegative real numbers form a mere <a href="http://ncatlab.org/nlab/show/rig">rig</a>, which is a &#8216;ri<b>n</b>g without <b>n</b>egatives&#8217;.  Rigs are much neglected in the typical math curriculum, but unjustly so: they&#8217;re almost as good as rings in many ways, and there are lots of important examples, like the natural numbers <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{N}" class="latex" /> and the nonnegative real numbers, <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" />.  For probability theory, we should learn to love rigs.  </p>
<p>But there are, alas, situations where we need to subtract probabilities, even when the answer comes out negative: namely when we&#8217;re taking the <i>time derivative</i> of a probability.  So sometimes we need <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> instead of just <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" />.</p>
<p>&bull; Second, while in quantum mechanics a state is described using a &#8216;wavefunction&#8217;, meaning a complex-valued function obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+%7C%5Cpsi%7C%5E2+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int |&#92;psi|^2 = 1 " class="latex" /></p>
<p>in stochastic mechanics it&#8217;s described using a &#8216;probability distribution&#8217;, meaning a nonnegative real function obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cpsi+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int &#92;psi = 1 " class="latex" /></p>
<p>So, let&#8217;s try our best to present the theories in close analogy, while respecting these two differences.</p>
<h4>States</h4>
<p>We&#8217;ll start with a set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> whose points are <b>states</b> that a system can be in.  Last time I assumed <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> was a finite set, but this post is so mathematical I might as well let my hair down and assume it&#8217;s a <a href="http://en.wikipedia.org/wiki/Measure_%28mathematics%29">measure space</a>.  A measure space lets you do integrals, but a finite set is a special case, and then these integrals are just sums.   So, I&#8217;ll write things like </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int f " class="latex" /></p>
<p>and mean the integral of the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> over the measure space <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />, but if <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set this just means </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+X%7D+f%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in X} f(x) " class="latex" /></p>
<p>Now, I&#8217;ve already defined the word &#8216;state&#8217;, but both quantum and stochastic mechanics need a more general concept of state.  Let&#8217;s call these &#8216;quantum states&#8217; and &#8216;stochastic states&#8217;:</p>
<p>&bull; In <b>quantum mechanics</b>, the system has an <b>amplitude</b> <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(x)" class="latex" /> of being in any state <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />.  These amplitudes are complex numbers with </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+%7C+%5Cpsi+%7C%5E2+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int | &#92;psi |^2 = 1 " class="latex" /></p>
<p>We call <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%3A+X+%5Cto+%5Cmathbb%7BC%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi: X &#92;to &#92;mathbb{C}" class="latex" /> obeying this equation a <b>quantum state</b>.</p>
<p>&bull; In <b>stochastic mechanics</b>, the system has a <b>probability</b> <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(x)" class="latex" /> of being in any state <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />.  These probabilities are nonnegative real numbers with </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cpsi+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int &#92;psi = 1 " class="latex" /></p>
<p>We call <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi: X &#92;to [0,&#92;infty)" class="latex" /> obeying this equation a <b>stochastic state</b>.</p>
<p>In quantum mechanics we often use this abbreviation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cphi%2C+%5Cpsi+%5Crangle+%3D+%5Cint+%5Coverline%7B%5Cphi%7D+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;phi, &#92;psi &#92;rangle = &#92;int &#92;overline{&#92;phi} &#92;psi " class="latex" /></p>
<p>so that a quantum state has</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi%2C+%5Cpsi+%5Crangle+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi, &#92;psi &#92;rangle = 1 " class="latex" /></p>
<p>Similarly, we could introduce this notation in stochastic mechanics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%5Crangle+%3D+%5Cint+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi &#92;rangle = &#92;int &#92;psi " class="latex" /></p>
<p>so that a stochastic state has</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%5Crangle+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi &#92;rangle = 1 " class="latex" /></p>
<p>But this notation is a bit risky, since angle brackets of this sort often stand for expectation values of observables.  So, I&#8217;ve been writing <img src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int &#92;psi" class="latex" />, and I&#8217;ll keep on doing this.</p>
<p>In quantum mechanics, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cphi%2C+%5Cpsi+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;phi, &#92;psi &#92;rangle" class="latex" /> is well-defined whenever both <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> live in the vector space</p>
<p><img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29+%3D+%5C%7B+%5Cpsi%3A+X+%5Cto+%5Cmathbb%7BC%7D+%5C%3B+%3A+%5C%3B+%5Cint+%7C%5Cpsi%7C%5E2+%3C+%5Cinfty+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X) = &#92;{ &#92;psi: X &#92;to &#92;mathbb{C} &#92;; : &#92;; &#92;int |&#92;psi|^2 &lt; &#92;infty &#92;} " class="latex" /></p>
<p>In stochastic mechanics, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi &#92;rangle" class="latex" /> is well-defined whenever <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> lives in the vector space</p>
<p><img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29+%3D++%5C%7B+%5Cpsi%3A+X+%5Cto+%5Cmathbb%7BR%7D+%5C%3B+%3A+%5C%3B+%5Cint+%7C%5Cpsi%7C+%3C+%5Cinfty+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X) =  &#92;{ &#92;psi: X &#92;to &#92;mathbb{R} &#92;; : &#92;; &#92;int |&#92;psi| &lt; &#92;infty &#92;} " class="latex" /></p>
<p>You&#8217;ll notice I wrote <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> rather than <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> here. That&#8217;s because in some calculations we&#8217;ll need functions that take negative values, even though our stochastic states are nonnegative.</p>
<h4> Observables </h4>
<p>A state is a way our system can be.  An observable is something we can measure about our system.   They fit together: we can measure an observable when our system is in some state.   If we repeat this we may get different answers, but there&#8217;s a nice formula for average or &#8216;expected&#8217; answer.</p>
<p>&bull; In quantum mechanics, an <b>observable</b> is a <a href="http://en.wikipedia.org/wiki/Self-adjoint_operator">self-adjoint operator</a> <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X)" class="latex" />.   The <b>expected value</b> of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi%2C+A+%5Cpsi+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi, A &#92;psi &#92;rangle " class="latex" /></p>
<p>Here I&#8217;m assuming that we can apply <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> and get a new vector <img src="https://s0.wp.com/latex.php?latex=A+%5Cpsi+%5Cin+L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;psi &#92;in L^2(X)" class="latex" />.   This is automatically true when <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set, but in general we need to be more careful.  </p>
<p>&bull; In stochastic mechanics, an <b>observable</b> is a real-valued function <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.    The <b>expected value</b> of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+A+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int A &#92;psi " class="latex" /></p>
<p>Here we&#8217;re using the fact that we can multiply <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> and get a new vector <img src="https://s0.wp.com/latex.php?latex=A+%5Cpsi+%5Cin+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;psi &#92;in L^1(X)" class="latex" />, at least if <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is bounded.  Again, this is automatic if <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set, but not otherwise.</p>
<h4> Symmetries </h4>
<p>Besides states and observables, we need &#8216;symmetries&#8217;, which are transformations that map states to states.  We use these to describe how our system changes when we wait a while, for example.</p>
<p>&bull; In quantum mechanics, an <b>isometry</b> is a linear map <img src="https://s0.wp.com/latex.php?latex=U%3A+L%5E2%28X%29+%5Cto+L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U: L^2(X) &#92;to L^2(X)" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+U+%5Cphi%2C+U+%5Cpsi+%5Crangle+%3D+%5Clangle+%5Cphi%2C+%5Cpsi+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle U &#92;phi, U &#92;psi &#92;rangle = &#92;langle &#92;phi, &#92;psi &#92;rangle " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%2C+%5Cphi+%5Cin+L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi, &#92;phi &#92;in L^2(X)" class="latex" />.  If <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is an isometry and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is a quantum state, then <img src="https://s0.wp.com/latex.php?latex=U+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U &#92;psi" class="latex" /> is again a quantum state.</p>
<p>&bull; In stochastic mechanics, a <b>stochastic operator</b> is a linear map <img src="https://s0.wp.com/latex.php?latex=U%3A+L%5E1%28X%29+%5Cto+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U: L^1(X) &#92;to L^1(X)" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+U+%5Cpsi+%3D+%5Cint+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int U &#92;psi = &#92;int &#92;psi " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cge+0+%5C%3B+%5C%3B+%5CRightarrow+%5C%3B+%5C%3B+U+%5Cpsi+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;ge 0 &#92;; &#92;; &#92;Rightarrow &#92;; &#92;; U &#92;psi &#92;ge 0 " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in L^1(X)" class="latex" />.  If <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is stochastic and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is a stochastic state, then <img src="https://s0.wp.com/latex.php?latex=U+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U &#92;psi" class="latex" /> is again a stochastic state.</p>
<p>In quantum mechanics we are mainly interested in invertible isometries, which are called <b>unitary</b> operators.  There are lots of these, and their inverses are always isometries.   There are, however, very few stochastic operators whose inverses are stochastic:</p>
<p><b>Puzzle 1.</b> Suppose <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set.  Show that every isometry <img src="https://s0.wp.com/latex.php?latex=U%3A+L%5E2%28X%29+%5Cto+L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U: L^2(X) &#92;to L^2(X)" class="latex" /> is invertible, and its inverse is again an isometry.</p>
<p><b>Puzzle 2.</b> Suppose <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set.  Which stochastic operators <img src="https://s0.wp.com/latex.php?latex=U%3A+L%5E1%28X%29+%5Cto+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U: L^1(X) &#92;to L^1(X)" class="latex" /> have stochastic inverses?</p>
<p>This is why we usually think of time evolution as being reversible quantum mechanics, but not in stochastic mechanics!  In quantum mechanics we often describe time evolution using a &#8216;1-parameter group&#8217;, while in stochastic mechanics we describe it using a 1-parameter <i>semi</i>group&#8230; meaning that we can run time forwards, but not backwards.</p>
<p>But let&#8217;s see how this works in detail! </p>
<h4> Time evolution in quantum mechanics </h4>
<p>In quantum mechanics there&#8217;s a beautiful relation between observables and symmetries, which goes like this.   Suppose that for each time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> we want a unitary operator <img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3A++L%5E2%28X%29+%5Cto+L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) :  L^2(X) &#92;to L^2(X)" class="latex" /> that describes time evolution.  Then it makes a lot of sense to demand that these operators form a 1-parameter group:</p>
<p><b>Definition.</b> A collection of linear operators U(t) (<img src="https://s0.wp.com/latex.php?latex=t+%5Cin+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;in &#92;mathbb{R}" class="latex" />) on some vector space forms a <b>1-parameter group</b> if</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%280%29+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(0) = 1 " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28s%2Bt%29+%3D+U%28s%29+U%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(s+t) = U(s) U(t) " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=s%2Ct+%5Cin+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s,t &#92;in &#92;mathbb{R}" class="latex" />.</p>
<p>Note that these conditions force all the operators <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" /> to be invertible.  </p>
<p>Now suppose our vector space is a Hilbert space, like <img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X)." class="latex" />  Then we call a 1-parameter group a <b>1-parameter unitary group</b> if the operators involved are all unitary.  </p>
<p>It turns out that 1-parameter unitary groups are either continuous in a certain way, or so pathological that you can&#8217;t even prove they exist without the axiom of choice!  So, we always focus on the continuous case:</p>
<p><b>Definition.</b> A 1-parameter unitary group is <b><a href="http://en.wikipedia.org/wiki/C0-semigroup">strongly continuous</a></b> if <img src="https://s0.wp.com/latex.php?latex=U%28t%29+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) &#92;psi" class="latex" /> depends continuously on <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi," class="latex" /> in this sense:</p>
<p><img src="https://s0.wp.com/latex.php?latex=t_i+%5Cto+t+%5C%3B%5C%3B+%5CRightarrow+%5C%3B+%5C%3B%5C%7CU%28t_i%29+%5Cpsi+-+U%28t%29+%5Cpsi+%5C%7C+%5Cto+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_i &#92;to t &#92;;&#92;; &#92;Rightarrow &#92;; &#92;;&#92;|U(t_i) &#92;psi - U(t) &#92;psi &#92;| &#92;to 0 " class="latex" /></p>
<p>Then we get a classic result proved by Marshall Stone back in the early 1930s.  You may not know him, but he was so influential at the University of Chicago during this period that it&#8217;s often called the &#8220;Stone Age&#8221;.  And here&#8217;s one reason why:</p>
<p><b><a href="http://en.wikipedia.org/wiki/Stone%27s_theorem_on_one-parameter_unitary_groups">Stone&#8217;s Theorem</a>.</b>  There is a one-to-one correspondence between strongly continuous 1-parameter unitary groups on a Hilbert space and self-adjoint operators on that Hilbert space, given as follows.  Given a strongly continuous 1-parameter unitary group <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" /> we can always write </p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3D+%5Cexp%28-i+t+H%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) = &#92;exp(-i t H) " class="latex" /></p>
<p>for a unique self-adjoint operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" />.  Conversely, any self-adjoint operator determines a strongly continuous 1-parameter group this way.  For all vectors <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> for which <img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi" class="latex" /> is well-defined, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%7D%7Bd+t%7D+U%28t%29+%5Cpsi+%5Cright%7C_%7Bt+%3D+0%7D+%3D+-i+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d}{d t} U(t) &#92;psi &#92;right|_{t = 0} = -i H &#92;psi } " class="latex" /></p>
<p>Moreover, for any of these vectors, if we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29+%3D+%5Cexp%28-i+t+H%29+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t) = &#92;exp(-i t H) &#92;psi " class="latex" /></p>
<p>we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+-+i+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = - i H &#92;psi(t) } " class="latex" /></p>
<p>When <img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3D+%5Cexp%28-i+t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) = &#92;exp(-i t H)" class="latex" /> describes the evolution of a system in time, <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is is called the <b>Hamiltonian</b>, and it has the physical meaning of &#8216;energy&#8217;.  The equation I just wrote down is then called <b>Schr&ouml;dinger&#8217;s equation</b>.</p>
<p>So, simply put, in quantum mechanics we have a correspondence between observables and nice one-parameter groups of symmetries.  Not surprisingly, our favorite observable, energy, corresponds to our favorite symmetry: time evolution!</p>
<p>However, if you were paying attention, you noticed that I carefully avoided explaining how we define <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-+i+t+H%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(- i t H)." class="latex" />  I didn&#8217;t even say what a self-adjoint operator is.    This is where the technicalities come in: they arise when <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is <a href="http://en.wikipedia.org/wiki/Unbounded_operator">unbounded</a>, and not defined on all vectors in our Hilbert space.</p>
<p>Luckily, these technicalities evaporate for <i>finite-dimensional</i> Hilbert spaces, such as <img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X)" class="latex" /> for a finite set <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  Then we get:</p>
<p><b>Stone&#8217;s Theorem (Baby Version).</b>  Suppose we are given a <i>finite-dimensional</i> Hilbert space.  In this case, a linear operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> on this space is self-adjoint iff it&#8217;s defined on the whole space and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cphi+%2C+H+%5Cpsi+%5Crangle+%3D+%5Clangle+H+%5Cphi%2C+%5Cpsi+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;phi , H &#92;psi &#92;rangle = &#92;langle H &#92;phi, &#92;psi &#92;rangle " class="latex" /></p>
<p>for all vectors <img src="https://s0.wp.com/latex.php?latex=%5Cphi%2C+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi, &#92;psi" class="latex" />.  Given a strongly continuous 1-parameter unitary group <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" /> we can always write</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3D+%5Cexp%28-+i+t+H%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) = &#92;exp(- i t H) " class="latex" /> </p>
<p>for a unique self-adjoint operator <img src="https://s0.wp.com/latex.php?latex=H%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H," class="latex" /> where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cexp%28-i+t+H%29+%5Cpsi+%3D+%5Csum_%7Bn+%3D+0%7D%5E%5Cinfty+%5Cfrac%7B%28-i+t+H%29%5En%7D%7Bn%21%7D+%5Cpsi+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;exp(-i t H) &#92;psi = &#92;sum_{n = 0}^&#92;infty &#92;frac{(-i t H)^n}{n!} &#92;psi }" class="latex" /></p>
<p>with the sum converging for all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi. " class="latex" />  Conversely, any self-adjoint operator on our space determines a strongly continuous 1-parameter group this way.  For all vectors <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> in our space we then have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%7D%7Bd+t%7D+U%28t%29+%5Cpsi+%5Cright%7C_%7Bt+%3D+0%7D+%3D+-i+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d}{d t} U(t) &#92;psi &#92;right|_{t = 0} = -i H &#92;psi } " class="latex" /></p>
<p>and if we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29+%3D+%5Cexp%28-i+t+H%29+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t) = &#92;exp(-i t H) &#92;psi " class="latex" /></p>
<p>we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+-+i+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = - i H &#92;psi(t) } " class="latex" /></p>
<h4> Time evolution in stochastic mechanics </h4>
<p>We&#8217;ve seen that in quantum mechanics, time evolution is usually described by a 1-parameter group of operators that comes from an observable: the Hamiltonian.  Stochastic mechanics is different!   </p>
<p>First, since stochastic operators aren&#8217;t usually invertible, we typically describe time evolution by a mere &#8216;semigroup&#8217;:</p>
<p><b>Definition.</b>  A collection of linear operators <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=t+%5Cin+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;in [0,&#92;infty)" class="latex" />) on some vector space forms a <b>1-parameter semigroup</b> if</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%280%29+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(0) = 1 " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28s%2Bt%29+%3D+U%28s%29+U%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(s+t) = U(s) U(t) " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=s%2C+t+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s, t &#92;ge 0" class="latex" />.</p>
<p>Now suppose this vector space is <img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X)" class="latex" /> for some measure space <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  We want to focus on the case where the operators <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" /> are stochastic and depend continuously on <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> in the same sense we discussed earlier.  </p>
<p><b>Definition.</b>  A 1-parameter strongly continuous semigroup of stochastic operators <img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3A+L%5E1%28X%29+%5Cto+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) : L^1(X) &#92;to L^1(X)" class="latex" /> is called a <b>Markov semigroup</b>.</p>
<p>What&#8217;s the analogue of Stone&#8217;s theorem for Markov semigroups?  I don&#8217;t know a fully satisfactory answer!  If you know, please tell me.</p>
<p>Later I&#8217;ll say what I <i>do</i> know&mdash;I&#8217;m not <i>completely</i> clueless&mdash;but for now let&#8217;s look at the &#8216;baby&#8217; case where <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set.  Then the story is neat and complete:</p>
<p><b>Theorem.</b>  Suppose we are given a <i>finite</i> set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  In this case, a linear operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X)" class="latex" /> is <b>infinitesimal stochastic</b> iff it&#8217;s defined on the whole space,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint+H+%5Cpsi+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int H &#92;psi = 0 " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in L^1(X)" class="latex" />, and the matrix of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> in terms of the obvious basis obeys</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7Bi+j%7D+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{i j} &#92;ge 0 " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=j+%5Cne+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j &#92;ne i" class="latex" />.  Given a Markov semigroup <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X)" class="latex" />, we can always write</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3D+%5Cexp%28t+H%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) = &#92;exp(t H) " class="latex" /></p>
<p>for a unique infinitesimal stochastic operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" />, where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cexp%28t+H%29+%5Cpsi+%3D+%5Csum_%7Bn+%3D+0%7D%5E%5Cinfty+%5Cfrac%7B%28t+H%29%5En%7D%7Bn%21%7D+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;exp(t H) &#92;psi = &#92;sum_{n = 0}^&#92;infty &#92;frac{(t H)^n}{n!} &#92;psi } " class="latex" /></p>
<p>with the sum converging for all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" /> Conversely, any infinitesimal stochastic operator on our space determines a Markov semigroup this way.  For all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in L^1(X)" class="latex" /> we then have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%7D%7Bd+t%7D+U%28t%29+%5Cpsi+%5Cright%7C_%7Bt+%3D+0%7D+%3D+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d}{d t} U(t) &#92;psi &#92;right|_{t = 0} = H &#92;psi } " class="latex" /></p>
<p>and if we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29+%3D+%5Cexp%28t+H%29+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t) = &#92;exp(t H) &#92;psi " class="latex" /></p>
<p>we have the <b>master equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = H &#92;psi(t) } " class="latex" /></p>
<p>In short, time evolution in stochastic mechanics is a lot like time evolution in quantum mechanics, except it&#8217;s typically not invertible, and the Hamiltonian is typically <i>not an observable</i>.  </p>
<p>Why not?  Because we defined an observable to be a function <img src="https://s0.wp.com/latex.php?latex=A%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A: X &#92;to &#92;mathbb{R}" class="latex" />.  We can think of this as giving an operator on <img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X)" class="latex" />, namely the operator of multiplication by <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />.  That&#8217;s a nice trick, which we used to good effect <a href="https://johncarlosbaez.wordpress.com/2011/10/04/network-theory-part-11/">last time</a>.  However, at least when <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set, this operator will be diagonal in the obvious basis consisting of functions that equal 1 at one point of <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and zero elsewhere.  So, it can only be infinitesimal stochastic if it&#8217;s zero!</p>
<p><b>Puzzle 3.</b> If <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set, show that any operator on <img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X)" class="latex" /> that&#8217;s both diagonal and infinitesimal stochastic must be zero.</p>
<h4> The Hille&#8211;Yosida theorem </h4>
<p>I&#8217;ve now told you everything you really need to know&#8230; but not everything I want to say.  What happens when <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is not a finite set?  What are Markov semigroups like then?  I can&#8217;t abide letting this question go unresolved!  Unfortunately I only know a partial answer.</p>
<p>We can get a certain distance using the Hille-Yosida theorem, which is much more general. </p>
<p><b>Definition.</b> A <a href="http://en.wikipedia.org/wiki/Banach_space">Banach space</a> is vector space with a <a href="http://en.wikipedia.org/wiki/Norm_%28mathematics%29#Definition">norm</a> such that any Cauchy sequence converges.</p>
<p>Examples include Hilbert spaces like <img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X)" class="latex" /> for any measure space, but also other spaces like <img src="https://s0.wp.com/latex.php?latex=L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(X)" class="latex" /> for any measure space!</p>
<p><b>Definition.</b> If <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> is a Banach space, a 1-parameter semigroup of operators <img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3A+V+%5Cto+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) : V &#92;to V" class="latex" /> is called a <b>contraction semigroup</b> if it&#8217;s strongly continuous and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7C+U%28t%29+%5Cpsi+%5C%7C+%5Cle+%5C%7C+%5Cpsi+%5C%7C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;| U(t) &#92;psi &#92;| &#92;le &#92;| &#92;psi &#92;| " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=t+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;ge 0 " class="latex" /> and all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in V" class="latex" />.</p>
<p>Examples include strongly continuous 1-parameter unitary groups, but also Markov semigroups!</p>
<p><b>Puzzle 4.</b>  Show any Markov semigroup is a contraction semigroup.</p>
<p>The Hille&#8211;Yosida theorem generalizes Stone&#8217;s theorem to contraction semigroups.  In my misspent youth, I spent a lot of time carrying around Yosida&#8217;s book <i>Functional Analysis</i>.  Furthermore, Einar Hille was the advisor of my thesis advisor, Irving Segal.  Segal generalized the Hille&#8211;Yosida theorem to <i>nonlinear</i> operators, and I used this generalization a lot back when I studied nonlinear partial differential equations.  So, I feel compelled to tell you this theorem:</p>
<p><b><a href="http://en.wikipedia.org/wiki/Hille%E2%80%93Yosida_theorem">Hille-Yosida Theorem</a>.</b> Given a contraction semigroup <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" /> we can always write </p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3D+%5Cexp%28t+H%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) = &#92;exp(t H) " class="latex" /> </p>
<p>for some <a href="http://en.wikipedia.org/wiki/Densely_defined_operator">densely defined</a> operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=H+-+%5Clambda+I+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H - &#92;lambda I " class="latex" /> has an inverse and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5C%7C+%28H+-+%5Clambda+I%29%5E%7B-1%7D+%5Cpsi+%5C%7C+%5Cle+%5Cfrac%7B1%7D%7B%5Clambda%7D+%5C%7C+%5Cpsi+%5C%7C+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;| (H - &#92;lambda I)^{-1} &#92;psi &#92;| &#92;le &#92;frac{1}{&#92;lambda} &#92;| &#92;psi &#92;| } " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda &gt; 0 " class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in V" class="latex" />.  Conversely, any such operator determines a strongly continuous 1-parameter group.  For all vectors <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> for which <img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi" class="latex" /> is well-defined, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7Bd%7D%7Bd+t%7D+U%28t%29+%5Cpsi+%5Cright%7C_%7Bt+%3D+0%7D+%3D+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{d}{d t} U(t) &#92;psi &#92;right|_{t = 0} = H &#92;psi } " class="latex" /></p>
<p>Moreover, for any of these vectors, if we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29+%3D+U%28t%29+%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t) = U(t) &#92;psi " class="latex" /></p>
<p>we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = H &#92;psi(t) } " class="latex" /></p>
<p>If you like, you can take the stuff at the end of this theorem to be what we mean by saying <img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3D+%5Cexp%28t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) = &#92;exp(t H)" class="latex" />.   When <img src="https://s0.wp.com/latex.php?latex=U%28t%29+%3D+%5Cexp%28t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t) = &#92;exp(t H)" class="latex" />, we say that <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> <b>generates</b> the semigroup <img src="https://s0.wp.com/latex.php?latex=U%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U(t)" class="latex" />.  </p>
<p>But now suppose <img src="https://s0.wp.com/latex.php?latex=V+%3D+L%5E1%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V = L^1(X)" class="latex" />.  Besides the conditions in the Hille&#8211;Yosida theorem, what <i>extra</i> conditions on <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> are necessary and sufficient for it to generate a Markov semigroup?  In other words, what&#8217;s a definition of &#8216;infinitesimal stochastic operator&#8217; that&#8217;s suitable not only when <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set, but an arbitrary measure space?</p>
<p>I asked this question on <a href="http://mathoverflow.net/questions/61270/infinitesimal-generators-of-stochastic-processes">Mathoverflow</a> a few months ago, and so far the answers have not been completely satisfactory.  </p>
<p>Some people mentioned the Hille&#8211;Yosida theorem, which is surely a step in the right direction, but not the full answer.  </p>
<p>Others discussed the special case when <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(t H)" class="latex" /> extends to a bounded self-adjoint operator on <img src="https://s0.wp.com/latex.php?latex=L%5E2%28X%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(X)." class="latex" />   When <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is a finite set, this special case happens precisely when the matrix <img src="https://s0.wp.com/latex.php?latex=H_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{i j}" class="latex" /> is <i>symmetric</i>: the probability of hopping from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> equals the probability of hopping from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />.  This is a fascinating special case, not least because when <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is both infinitesimal stochastic <i>and</i> self-adjoint, we can use it as a Hamiltonian for both stochastic mechanics <i>and</i> quantum mechanics!  Someday I want to discuss this.  However, it&#8217;s just a special case.</p>
<p>After grabbing people by the collar and insisting that I wanted to know the answer to the question I actually asked&mdash;not some vaguely similar question&mdash;the best answer seems to be Martin Gisser&#8217;s reference to this book:</p>
<p>&bull; Zhi-Ming Ma and Michael R&ouml;ckner, <i>Introduction to the Theory of (Non-Symmetric) Dirichlet Forms</i>, Springer, Berlin, 1992.</p>
<p>This book provides a very nice self-contained proof of the Hille-Yosida theorem.  On the other hand, it does <i>not</i> answer my question in general, but only when the skew-symmetric part of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is dominated (in a certain sense) by the symmetric part.  </p>
<p>So, I&#8217;m stuck on this front, but that needn&#8217;t bring the whole project to a halt.  We&#8217;ll just sidestep this question.</p>
<p>For a good well-rounded introduction to Markov semigroups and what they&#8217;re good for, try:</p>
<p>&bull; Ryszard Rudnicki, Katarzyna Pich&oacute;r and Marta Tyran-Kam&iacute;nska, <a href="http://www.impan.pl/~rams/r48-ladek.pdf">Markov semigroups and their applications</a>.  </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/09/network-theory-part-12/#comments">16 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/09/network-theory-part-12/" rel="bookmark" title="Permanent Link to Network Theory (Part&nbsp;12)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-5047 post type-post status-publish format-standard hentry category-mathematics category-networks category-probability" id="post-5047">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/09/13/network-theory-part-9/" rel="bookmark">Network Theory (Part&nbsp;9)</a></h2>
				<small>13 September, 2011</small><br />


				<div class="entry">
					<p><i><b>jointly written with <a href="http://www.azimuthproject.org/azimuth/show/Brendan+Fong">Brendan Fong</a></b></i></p>
<p><a href="https://johncarlosbaez.wordpress.com/2011/09/09/network-theory-part-8/">Last time</a> we reviewed the rate equation and the master equation.   Both of them describe processes where things of various kinds can react and turn into other things.  But:</p>
<p>&bull; In the rate equation, we assume the number of things varies continuously and is known precisely.  </p>
<p>&bull; In the master equation, we assume the number of things varies discretely and is known only probabilistically.</p>
<p>This should remind you of the difference between classical mechanics and quantum mechanics. But the master equation is not <i>quantum</i>, it&#8217;s <i>stochastic</i>: it involves probabilities, but there&#8217;s no uncertainty principle going on.  </p>
<p>Still, a lot of the math is similar.  </p>
<p>Now, given an equilibrium solution to the rate equation&mdash;one that doesn&#8217;t change with time&mdash;we&#8217;ll try to find a solution to the master equation with the same property.  We won&#8217;t <i>always</i> succeed&mdash;but  we often can!  The theorem saying how was proved here:</p>
<p>&bull; D. F. Anderson, G. Craciun and T. G. Kurtz, <a href="http://www.arxiv.org/abs/0803.3042">Product-form stationary distributions for deficiency zero chemical reaction networks</a>.</p>
<p>To emphasize the analogy to quantum mechanics, we&#8217;ll translate their proof into the language of annihilation and creation operators.  In particular, our equilibrium solution of the master equation is just like what people call a <a href="http://en.wikipedia.org/wiki/Coherent_states">&#8216;coherent state&#8217;</a> in quantum mechanics.  </p>
<p>So, if you know about quantum mechanics and coherent states, you should be happy.  But if you don&#8217;t, fear not!&mdash;we&#8217;re not assuming you do.</p>
<h4> The rate equation </h4>
<p>To construct our equilibrium solution of the master equation, we need a special type of solution to our rate equation.  We call this type a &#8216;complex balanced solution&#8217;.  This means that not only is the net rate of production of each species zero, but the net rate of production of each possible <i>bunch</i> of species is zero.  </p>
<p>Before we make this more precise, let&#8217;s remind ourselves of the basic setup. </p>
<p>We&#8217;ll consider a stochastic Petri net with a finite set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> of species and a finite set <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> of transitions.  For convenience let&#8217;s take <img src="https://s0.wp.com/latex.php?latex=S+%3D+%5C%7B1%2C%5Cdots%2C+k%5C%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = &#92;{1,&#92;dots, k&#92;}," class="latex" /> so our species are numbered from 1 to <img src="https://s0.wp.com/latex.php?latex=k.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k." class="latex" />  Then each transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" /> has an input vector <img src="https://s0.wp.com/latex.php?latex=m%28%5Ctau%29+%5Cin+%5Cmathbb%7BN%7D%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m(&#92;tau) &#92;in &#92;mathbb{N}^k" class="latex" /> and output vector <img src="https://s0.wp.com/latex.php?latex=n%28%5Ctau%29+%5Cin+%5Cmathbb%7BN%7D%5Ek.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n(&#92;tau) &#92;in &#92;mathbb{N}^k." class="latex" />  These say how many things of each species go in, and how many go out.  Each transition also has rate constant <img src="https://s0.wp.com/latex.php?latex=r%28%5Ctau%29+%5Cin+%5B0%2C%5Cinfty%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r(&#92;tau) &#92;in [0,&#92;infty)," class="latex" /> which says how rapidly it happens.</p>
<p>The rate equation concerns a vector <img src="https://s0.wp.com/latex.php?latex=x%28t%29+%5Cin+%5B0%2C%5Cinfty%29%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) &#92;in [0,&#92;infty)^k" class="latex" /> whose <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th component is the number of things of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species at time <img src="https://s0.wp.com/latex.php?latex=t.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t." class="latex" />   Note: we&#8217;re assuming this number of things varies continuously and is known precisely!  This should remind you of classical mechanics.  So, we&#8217;ll call <img src="https://s0.wp.com/latex.php?latex=x%28t%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t)," class="latex" /> or indeed any vector in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5Ek%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^k," class="latex" /> a <b>classical state</b>.  </p>
<p>The <b>rate equation</b> says how the classical state <img src="https://s0.wp.com/latex.php?latex=x%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t)" class="latex" /> changes with time:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29%5C%2C+%28n%28%5Ctau%29-m%28%5Ctau%29%29+%5C%2C+x%5E%7Bm%28%5Ctau%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d x}{d t} = &#92;sum_{&#92;tau &#92;in T} r(&#92;tau)&#92;, (n(&#92;tau)-m(&#92;tau)) &#92;, x^{m(&#92;tau)} } " class="latex" /></p>
<p>You may wonder what <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bm%28%5Ctau%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x^{m(&#92;tau)}" class="latex" /> means: after all, we&#8217;re taking a vector to a vector power!  It&#8217;s just an abbreviation, which we&#8217;ve seen plenty of times before.  If <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;mathbb{R}^k" class="latex" /> is a list of numbers and <img src="https://s0.wp.com/latex.php?latex=m+%5Cin+%5Cmathbb%7BN%7D%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m &#92;in &#92;mathbb{N}^k" class="latex" /> is a list of natural numbers, we define</p>
<p><img src="https://s0.wp.com/latex.php?latex=x%5Em+%3D+x_1%5E%7Bm_1%7D+%5Ccdots+x_k%5E%7Bm_k%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x^m = x_1^{m_1} &#92;cdots x_k^{m_k} " class="latex" /> </p>
<p>We&#8217;ll also use this notation when <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is a list of <i>operators</i>.</p>
<h4> Complex balance </h4>
<p>The vectors <img src="https://s0.wp.com/latex.php?latex=m%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m(&#92;tau)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=n%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n(&#92;tau)" class="latex" /> are examples of what chemists call <b>complexes</b>.  A complex is a bunch of things of each species.  For example, if the set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> consists of three species, the complex <img src="https://s0.wp.com/latex.php?latex=%281%2C0%2C5%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1,0,5)" class="latex" /> is a bunch consisting of one thing of the first species, none of the second species, and five of the third species.  </p>
<p>For our Petri net, the set of complexes is the set <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{N}^k" class="latex" />, and the complexes of particular interest are the <b>input complex</b> <img src="https://s0.wp.com/latex.php?latex=m%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m(&#92;tau)" class="latex" /> and the <b>output complex</b> <img src="https://s0.wp.com/latex.php?latex=n%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n(&#92;tau)" class="latex" /> of each transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" />.</p>
<p>We say a classical state <img src="https://s0.wp.com/latex.php?latex=c+%5Cin+%5B0%2C%5Cinfty%29%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;in [0,&#92;infty)^k" class="latex" /> is <b>complex balanced</b> if for all complexes <img src="https://s0.wp.com/latex.php?latex=%5Ckappa+%5Cin+%5Cmathbb%7BN%7D%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa &#92;in &#92;mathbb{N}^k" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7B%5C%7B%5Ctau+%3A+m%28%5Ctau%29+%3D+%5Ckappa%5C%7D%7D+r%28%5Ctau%29+c%5E%7Bm%28%5Ctau%29%7D+%3D%5Csum_%7B%5C%7B%5Ctau+%3A+n%28%5Ctau%29+%3D+%5Ckappa%5C%7D%7D+r%28%5Ctau%29+c%5E%7Bm%28%5Ctau%29%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{&#92;{&#92;tau : m(&#92;tau) = &#92;kappa&#92;}} r(&#92;tau) c^{m(&#92;tau)} =&#92;sum_{&#92;{&#92;tau : n(&#92;tau) = &#92;kappa&#92;}} r(&#92;tau) c^{m(&#92;tau)}  } " class="latex" /></p>
<p>The left hand side of this equation, which sums over the transitions with  <i>input complex </i> <img src="https://s0.wp.com/latex.php?latex=%5Ckappa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa" class="latex" />, gives the rate of consumption of the complex <img src="https://s0.wp.com/latex.php?latex=%5Ckappa.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa." class="latex" />  The right hand side, which sums over the transitions with <i> output complex </i> <img src="https://s0.wp.com/latex.php?latex=%5Ckappa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa" class="latex" />, gives the rate of production of <img src="https://s0.wp.com/latex.php?latex=%5Ckappa+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa ." class="latex" />  So, this equation requires that the net rate of production of the complex <img src="https://s0.wp.com/latex.php?latex=%5Ckappa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa" class="latex" /> is zero in the classical state <img src="https://s0.wp.com/latex.php?latex=c+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c ." class="latex" /></p>
<p><b>Puzzle.</b>  Show that if a classical state <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> is complex balanced, and we set <img src="https://s0.wp.com/latex.php?latex=x%28t%29+%3D+c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) = c" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=t%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t," class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=x%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t)" class="latex" /> is a solution of the rate equation.</p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=x%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t)" class="latex" /> doesn&#8217;t change with time here, we call it an <b>equilibrium solution</b> of the rate equation.  Since <img src="https://s0.wp.com/latex.php?latex=x%28t%29+%3D+c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) = c" class="latex" /> is complex balanced, we call it <b>complex balanced</b> equilibrium solution.</p>
<h4> The master equation </h4>
<p>We&#8217;ve seen that any complex balanced classical state gives an equilibrium solution of the <i>rate</i> equation.  The Anderson&ndash;Craciun&ndash;Kurtz theorem says that it also gives an equilibrium solution of the <i>master</i> equation.  </p>
<p>The master equation concerns a formal power series </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi%28t%29+%3D+%5Csum_%7Bn+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Cpsi_n%28t%29+z%5En+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi(t) = &#92;sum_{n &#92;in &#92;mathbb{N}^k} &#92;psi_n(t) z^n } " class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=z%5En+%3D+z_1%5E%7Bn_1%7D+%5Ccdots+z_k%5E%7Bn_k%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z^n = z_1^{n_1} &#92;cdots z_k^{n_k} " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_n%28t%29+%3D+%5Cpsi_%7Bn_1%2C+%5Cdots%2Cn_k%7D%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_n(t) = &#92;psi_{n_1, &#92;dots,n_k}(t) " class="latex" /> </p>
<p>is the probability that at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> we have <img src="https://s0.wp.com/latex.php?latex=n_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1" class="latex" /> things of the first species,  <img src="https://s0.wp.com/latex.php?latex=n_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_2" class="latex" /> of the second species, and so on.  </p>
<p>Note: now we&#8217;re assuming this number of things varies discretely and is known only probabilistically!  So, we&#8217;ll call <img src="https://s0.wp.com/latex.php?latex=%5CPsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi(t)" class="latex" />, or indeed any formal power series where the coefficients are probabilities summing to 1, a <b>stochastic state</b>.  Earlier we just called it a &#8216;state&#8217;, but that would get confusing now: we&#8217;ve got classical states and stochastic states, and we&#8217;re trying to relate them.</p>
<p>The <b>master equation</b> says how the stochastic state <img src="https://s0.wp.com/latex.php?latex=%5CPsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi(t)" class="latex" /> changes with time:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5CPsi%28t%29+%3D+H+%5CPsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;Psi(t) = H &#92;Psi(t) } " class="latex" /></p>
<p>where the <b>Hamiltonian</b> <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H+%3D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29+%5C%2C+%5Cleft%28%7Ba%5E%5Cdagger%7D%5E%7Bn%28%5Ctau%29%7D+-+%7Ba%5E%5Cdagger%7D%5E%7Bm%28%5Ctau%29%7D+%5Cright%29+%5C%2C+a%5E%7Bm%28%5Ctau%29%7D++%5Clabel%7Bmaster%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H = &#92;sum_{&#92;tau &#92;in T} r(&#92;tau) &#92;, &#92;left({a^&#92;dagger}^{n(&#92;tau)} - {a^&#92;dagger}^{m(&#92;tau)} &#92;right) &#92;, a^{m(&#92;tau)}  &#92;label{master} } " class="latex" /></p>
<p>The notation here is designed to neatly summarize some big products of annihilation and creation operators.  For any vector <img src="https://s0.wp.com/latex.php?latex=n+%5Cin+%5Cmathbb%7BN%7D%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;in &#92;mathbb{N}^k" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=a%5En+%3D+a_1%5E%7Bn_1%7D+%5Ccdots++a_k%5E%7Bn_k%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a^n = a_1^{n_1} &#92;cdots  a_k^{n_k} " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%7Ba%5E%5Cdagger%7D%5En+%3D+%7Ba_1%5E%5Cdagger+%7D%5E%7Bn_1%7D+%5Ccdots++%7Ba_k%5E%5Cdagger%7D%5E%7Bn_k%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  {a^&#92;dagger}^n = {a_1^&#92;dagger }^{n_1} &#92;cdots  {a_k^&#92;dagger}^{n_k} } " class="latex" /></p>
<h4> Coherent states </h4>
<p>Now suppose <img src="https://s0.wp.com/latex.php?latex=c+%5Cin+%5B0%2C%5Cinfty%29%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;in [0,&#92;infty)^k" class="latex" /> is a complex balanced equilibrium solution of the rate equation.  We want to get an equilibrium solution of the master equation.  How do we do it?</p>
<p>For any <img src="https://s0.wp.com/latex.php?latex=c+%5Cin+%5B0%2C%5Cinfty%29%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;in [0,&#92;infty)^k" class="latex" /> there is a stochastic state called a <b>coherent state</b>, defined by </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi_c+%3D+%5Cfrac%7Be%5E%7Bc+z%7D%7D%7Be%5Ec%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi_c = &#92;frac{e^{c z}}{e^c} } " class="latex" /></p>
<p>Here we are using some very terse abbreviations.  Namely, we are defining</p>
<p><img src="https://s0.wp.com/latex.php?latex=e%5E%7Bc%7D+%3D+e%5E%7Bc_1%7D+%5Ccdots+e%5E%7Bc_k%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{c} = e^{c_1} &#92;cdots e^{c_k} " class="latex" /></p>
<p>and </p>
<p><img src="https://s0.wp.com/latex.php?latex=e%5E%7Bc+z%7D+%3D+e%5E%7Bc_1+z_1%7D+%5Ccdots+e%5E%7Bc_k+z_k%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{c z} = e^{c_1 z_1} &#92;cdots e^{c_k z_k} " class="latex" /></p>
<p>Equivalently,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+e%5E%7Bc+z%7D+%3D+%5Csum_%7Bn+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Cfrac%7Bc%5En%7D%7Bn%21%7Dz%5En+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ e^{c z} = &#92;sum_{n &#92;in &#92;mathbb{N}^k} &#92;frac{c^n}{n!}z^n } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=c%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c^n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=z%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z^n" class="latex" /> are defined as products in our usual way, and</p>
<p><img src="https://s0.wp.com/latex.php?latex=n%21+%3D+n_1%21+%5C%2C+%5Ccdots+%5C%2C+n_k%21+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n! = n_1! &#92;, &#92;cdots &#92;, n_k! " class="latex" /></p>
<p>Either way, if you unravel the abbrevations, here&#8217;s what you get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5CPsi_c+%3D+e%5E%7B-%28c_1+%2B+%5Ccdots+%2B+c_k%29%7D+%5C%2C+%5Csum_%7Bn+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Cfrac%7Bc_1%5E%7Bn_1%7D+%5Ccdots+c_k%5E%7Bn_k%7D%7D+%7Bn_1%21+%5C%2C+%5Ccdots+%5C%2C+n_k%21+%7D+%5C%2C+z_1%5E%7Bn_1%7D+%5Ccdots+z_k%5E%7Bn_k%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;Psi_c = e^{-(c_1 + &#92;cdots + c_k)} &#92;, &#92;sum_{n &#92;in &#92;mathbb{N}^k} &#92;frac{c_1^{n_1} &#92;cdots c_k^{n_k}} {n_1! &#92;, &#92;cdots &#92;, n_k! } &#92;, z_1^{n_1} &#92;cdots z_k^{n_k} } " class="latex" /></p>
<p>Maybe now you see why we like the abbreviations.</p>
<p>The name <a href="http://en.wikipedia.org/wiki/Coherent_states">&#8216;coherent state&#8217;</a> comes from quantum mechanics.  In quantum mechanics, we think of a coherent state <img src="https://s0.wp.com/latex.php?latex=%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi_c" class="latex" /> as the &#8216;quantum state&#8217; that best approximates the classical state <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" />.  But we&#8217;re not doing quantum mechanics now, we&#8217;re doing probability theory.  <img src="https://s0.wp.com/latex.php?latex=%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi_c" class="latex" /> isn&#8217;t a &#8216;quantum state&#8217;, it&#8217;s a stochastic state.  </p>
<p>In probability theory, people like Poisson distributions.  In the state <img src="https://s0.wp.com/latex.php?latex=%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi_c" class="latex" />, the probability of having <img src="https://s0.wp.com/latex.php?latex=n_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i" class="latex" /> things of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is equal to </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++e%5E%7B-c_i%7D+%5C%2C+%5Cfrac%7Bc_i%5E%7Bn_i%7D%7D%7Bn_i%21%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  e^{-c_i} &#92;, &#92;frac{c_i^{n_i}}{n_i!} } " class="latex" /></p>
<p>This is precisely the definition of a <a href="http://en.wikipedia.org/wiki/Poisson_distribution"><b>Poisson distribution</b></a> with mean equal to <img src="https://s0.wp.com/latex.php?latex=c_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c_i" class="latex" />.   We can multiply a bunch of factors like this, one for each species, to get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+e%5E%7B-c%7D+%5Cfrac%7Bc%5En%7D%7Bn%21%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ e^{-c} &#92;frac{c^n}{n!} } " class="latex" /></p>
<p>This is the probability of having <img src="https://s0.wp.com/latex.php?latex=n_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1" class="latex" /> things of the first species, <img src="https://s0.wp.com/latex.php?latex=n_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_2" class="latex" /> things of the second, and so on, in the state <img src="https://s0.wp.com/latex.php?latex=%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi_c" class="latex" />.  So, the state <img src="https://s0.wp.com/latex.php?latex=%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi_c" class="latex" /> is a product of independent Poisson distributions.  In particular, knowing how many things there are of one species  says <i>nothing all about</i> how many things there are of any other species!</p>
<p>It is remarkable that such a simple state can give an equilibrium solution of the master equation, even for very complicated stochastic Petri nets.  But it&#8217;s true&mdash;at least if <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> is complex balanced.</p>
<h4> The Anderson&ndash;Craciun&ndash;Kurtz theorem </h4>
<p>Now we&#8217;re ready to state and prove the big result:</p>
<p><b>Theorem (Anderson&ndash;Craciun&ndash;Kurtz).</b>  Suppose <img src="https://s0.wp.com/latex.php?latex=c+%5Cin+%5B0%2C%5Cinfty%29%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;in [0,&#92;infty)^k" class="latex" />  is a complex balanced equilibrium solution of the rate equation.    Then <img src="https://s0.wp.com/latex.php?latex=H+%5CPsi_c+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;Psi_c = 0" class="latex" />.</p>
<p>It follows that <img src="https://s0.wp.com/latex.php?latex=%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi_c" class="latex" /> is an equilibrium solution of the master equation.  In other words, if we take <img src="https://s0.wp.com/latex.php?latex=%5CPsi%28t%29+%3D+%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi(t) = &#92;Psi_c" class="latex" /> for all times <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />, the master equation holds:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5CPsi%28t%29+%3D+H+%5CPsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;Psi(t) = H &#92;Psi(t) } " class="latex" /></p>
<p>since both sides are zero. </p>
<p><b>Proof.</b> To prove the Anderson&ndash;Craciun&ndash;Kurtz theorem, we just need to show that <img src="https://s0.wp.com/latex.php?latex=H+%5CPsi_c+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;Psi_c = 0" class="latex" />.  Since <img src="https://s0.wp.com/latex.php?latex=%5CPsi_c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi_c" class="latex" /> is a constant times <img src="https://s0.wp.com/latex.php?latex=e%5E%7Bc+z%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{c z}" class="latex" />, it suffices to show <img src="https://s0.wp.com/latex.php?latex=H+e%5E%7Bc+z%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H e^{c z} = 0" class="latex" />.  Remember that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H+e%5E%7Bc+z%7D+%3D++%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29+%5Cleft%28+%7Ba%5E%5Cdagger%7D%5E%7Bn%28%5Ctau%29%7D+-%7Ba%5E%5Cdagger%7D%5E%7Bm%28%5Ctau%29%7D+%5Cright%29+%5C%2C+a%5E%7Bm%28%5Ctau%29%7D+%5C%2C+e%5E%7Bc+z%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H e^{c z} =  &#92;sum_{&#92;tau &#92;in T} r(&#92;tau) &#92;left( {a^&#92;dagger}^{n(&#92;tau)} -{a^&#92;dagger}^{m(&#92;tau)} &#92;right) &#92;, a^{m(&#92;tau)} &#92;, e^{c z} } " class="latex" /></p>
<p>Since the annihilation operator <img src="https://s0.wp.com/latex.php?latex=a_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_i" class="latex" /> is given by differentiation with respect to <img src="https://s0.wp.com/latex.php?latex=z_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z_i" class="latex" />, while the creation operator <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cdagger_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a^&#92;dagger_i" class="latex" /> is just multiplying by <img src="https://s0.wp.com/latex.php?latex=z_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z_i" class="latex" />, we have:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H+e%5E%7Bc+z%7D+%3D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29+%5C%2C+c%5E%7Bm%28%5Ctau%29%7D+%5Cleft%28+z%5E%7Bn%28%5Ctau%29%7D+-+z%5E%7Bm%28%5Ctau%29%7D+%5Cright%29+e%5E%7Bc+z%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H e^{c z} = &#92;sum_{&#92;tau &#92;in T} r(&#92;tau) &#92;, c^{m(&#92;tau)} &#92;left( z^{n(&#92;tau)} - z^{m(&#92;tau)} &#92;right) e^{c z} } " class="latex" /></p>
<p>Expanding out <img src="https://s0.wp.com/latex.php?latex=e%5E%7Bc+z%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{c z}" class="latex" /> we get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H+e%5E%7Bc+z%7D+%3D+%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D%5Cleft%28z%5E%7Bn%28%5Ctau%29%7D%5Cfrac%7Bc%5Ei%7D%7Bi%21%7Dz%5Ei+-+z%5E%7Bm%28%5Ctau%29%7D%5Cfrac%7Bc%5Ei%7D%7Bi%21%7Dz%5Ei%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H e^{c z} = &#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;tau &#92;in T} r(&#92;tau)c^{m(&#92;tau)}&#92;left(z^{n(&#92;tau)}&#92;frac{c^i}{i!}z^i - z^{m(&#92;tau)}&#92;frac{c^i}{i!}z^i&#92;right) } " class="latex" /></p>
<p>Shifting indices and defining negative powers to be zero:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H+e%5E%7Bc+z%7D++%3D+%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D%5Cleft%28%5Cfrac%7Bc%5E%7Bi-n%28%5Ctau%29%7D%7D%7B%28i-n%28%5Ctau%29%29%21%7Dz%5Ei+-+%5Cfrac%7Bc%5E%7Bi-m%28%5Ctau%29%7D%7D%7B%28i-m%28%5Ctau%29%29%21%7Dz%5Ei%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H e^{c z}  = &#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;tau &#92;in T} r(&#92;tau)c^{m(&#92;tau)}&#92;left(&#92;frac{c^{i-n(&#92;tau)}}{(i-n(&#92;tau))!}z^i - &#92;frac{c^{i-m(&#92;tau)}}{(i-m(&#92;tau))!}z^i&#92;right) } " class="latex" /></p>
<p>So, to show <img src="https://s0.wp.com/latex.php?latex=H+e%5E%7Bc+z%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H e^{c z} = 0" class="latex" />, we need to show this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D%5Cfrac%7Bc%5E%7Bi-n%28%5Ctau%29%7D%7D%7B%28i-n%28%5Ctau%29%29%21%7Dz%5Ei+%3D%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D%5Cfrac%7Bc%5E%7Bi-m%28%5Ctau%29%7D%7D%7B%28i-m%28%5Ctau%29%29%21%7Dz%5Ei+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;tau &#92;in T} r(&#92;tau)c^{m(&#92;tau)}&#92;frac{c^{i-n(&#92;tau)}}{(i-n(&#92;tau))!}z^i =&#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;tau &#92;in T} r(&#92;tau)c^{m(&#92;tau)}&#92;frac{c^{i-m(&#92;tau)}}{(i-m(&#92;tau))!}z^i } " class="latex" /></p>
<p>We do this by splitting the sum over <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> according to output and then input complexes, making use of the complex balanced condition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ckappa+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5C%7B%5Ctau+%3A+n%28%5Ctau%29%3D%5Ckappa%5C%7D%7D++r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D%5Cfrac%7Bc%5E%7Bi-n%28%5Ctau%29%7D%7D%7B%28i-n%28%5Ctau%29%29%21%7D+%5C%2C+z%5Ei++%3D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;kappa &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;{&#92;tau : n(&#92;tau)=&#92;kappa&#92;}}  r(&#92;tau)c^{m(&#92;tau)}&#92;frac{c^{i-n(&#92;tau)}}{(i-n(&#92;tau))!} &#92;, z^i  = } " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ckappa+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Cfrac%7Bc%5E%7Bi-%5Ckappa%7D%7D%7B%28i-%5Ckappa%29%21%7D%5C%2C+z%5Ei+%5Csum_%7B%5C%7B%5Ctau+%3A+n%28%5Ctau%29+%3D+%5Ckappa%5C%7D%7D++r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D+%3D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;kappa &#92;in &#92;mathbb{N}^k} &#92;frac{c^{i-&#92;kappa}}{(i-&#92;kappa)!}&#92;, z^i &#92;sum_{&#92;{&#92;tau : n(&#92;tau) = &#92;kappa&#92;}}  r(&#92;tau)c^{m(&#92;tau)} = } " class="latex" /> </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ckappa+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Cfrac%7Bc%5E%7Bi-%5Ckappa%7D%7D%7B%28i-%5Ckappa%29%21%7D%5C%2C+z%5Ei+%5Csum_%7B%5C%7B%5Ctau+%3A+m%28%5Ctau%29+%3D+%5Ckappa%5C%7D%7D++r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D++%3D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;kappa &#92;in &#92;mathbb{N}^k} &#92;frac{c^{i-&#92;kappa}}{(i-&#92;kappa)!}&#92;, z^i &#92;sum_{&#92;{&#92;tau : m(&#92;tau) = &#92;kappa&#92;}}  r(&#92;tau)c^{m(&#92;tau)}  = } " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5Ckappa+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Csum_%7B%5C%7B%5Ctau+%3A+m%28%5Ctau%29+%3D+%5Ckappa%5C%7D%7D++r%28%5Ctau%29c%5E%7Bm%28%5Ctau%29%7D%5Cfrac%7Bc%5E%7Bi-m%28%5Ctau%29%7D%7D%7B%28i-m%28%5Ctau%29%29%21%7D%5C%2C+z%5Ei+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;kappa &#92;in &#92;mathbb{N}^k} &#92;sum_{&#92;{&#92;tau : m(&#92;tau) = &#92;kappa&#92;}}  r(&#92;tau)c^{m(&#92;tau)}&#92;frac{c^{i-m(&#92;tau)}}{(i-m(&#92;tau))!}&#92;, z^i } " class="latex" /></p>
<p>This completes the proof!  It&#8217;s just algebra, but it seems a bit magical, so we&#8217;re trying to understand it better.</p>
<p>I hope you see how amazing this result is.  If you know quantum mechanics and coherent states you&#8217;ll understand what I mean.  A coherent state is the &quot;best quantum approximation&quot; to a classical state, but we don&#8217;t expect this quantum state to be <i>exactly</i> time-independent when the corresponding classical state is, <i>except</i> in very special cases, like when the Hamiltonian is quadratic in the creation and annihilation operators.  Here we are getting a result like that much more generally&#8230; but only given the &quot;complex balanced&quot; condition.</p>
<h4> An example </h4>
<p>We&#8217;ve already seen one example of the theorem, back in <a href="https://johncarlosbaez.wordpress.com/2011/04/27/network-theory-part-7/">Part 7</a>.  We had this stochastic Petri net:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/amoeba.png" alt="" />
</div>
<p>We saw that the rate equation is just the logistic equation, familiar from population biology.  The equilibrium solution is complex balanced, because pairs of amoebas are getting created at the same rate as they&#8217;re getting destroyed, and <i>single</i> amoebas are  getting created at the same rate as <i>they&#8217;re</i> getting destroyed.  </p>
<p>So, the Anderson&ndash;Craciun&ndash;Kurtz theorem guarantees that there&#8217;s an equilibrium solution of the master equation where the number of amoebas is distributed according to a Poisson distribution.  And, we actually checked that this was true!</p>
<p>Next time we&#8217;ll look at another example.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/09/13/network-theory-part-9/#comments">56 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/09/13/network-theory-part-9/" rel="bookmark" title="Permanent Link to Network Theory (Part&nbsp;9)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-4881 post type-post status-publish format-standard hentry category-networks category-physics category-probability" id="post-4881">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/09/09/network-theory-part-8/" rel="bookmark">Network Theory (Part&nbsp;8)</a></h2>
				<small>9 September, 2011</small><br />


				<div class="entry">
					<p>Summer vacation is over.  Time to get back to work!  </p>
<p>This month, before he goes to Oxford to begin a master&#8217;s program in Mathematics and the Foundations of Computer Science, Brendan Fong is visiting the Centre for Quantum Technologies and working with me on stochastic Petri nets.  He&#8217;s proved two interesting results, which he wants to explain.  </p>
<p>To understand what he&#8217;s done, you need to know how to get the rate equation and the master equation from a stochastic Petri net.  We&#8217;ve <i>almost</i> seen how.  But it&#8217;s been a long time since the last article in this series, so today I&#8217;ll start with some review.  And at the end, just for fun, I&#8217;ll say a bit more about how Feynman diagrams show up in this theory.</p>
<p>Since I&#8217;m an experienced teacher, I&#8217;ll assume you&#8217;ve forgotten <i>everything I ever said</i>.  </p>
<p>(This has some advantages.  I can change some of my earlier terminology&mdash;improve it a bit here and there&mdash;and you won&#8217;t even notice.)</p>
<h4> Stochastic Petri nets </h4>
<p><b>Definition.</b>  A <b>Petri net</b> consists of a set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> of <b>species</b> and a set <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> of <b>transitions</b>, together with a function </p>
<p><img src="https://s0.wp.com/latex.php?latex=i+%3A+S+%5Ctimes+T+%5Cto+%5Cmathbb%7BN%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i : S &#92;times T &#92;to &#92;mathbb{N} " class="latex" /></p>
<p>saying how many things of each species appear in the <b>input</b> for each transition, and a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=o%3A+S+%5Ctimes+T+%5Cto+%5Cmathbb%7BN%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="o: S &#92;times T &#92;to &#92;mathbb{N} " class="latex" /></p>
<p>saying how many things of each species appear in the <b>output</b>.</p>
<p>We can draw pictures of Petri nets.  For example, here&#8217;s a Petri net with two species and three transitions:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/wolf-rabbit.png" />
</div>
<p>It should be clear that the transition &#8216;predation&#8217; has one wolf and one rabbit as input, and two wolves as output.  </p>
<p>A &#8216;stochastic&#8217; Petri net goes further: it also says the rate at which each transition occurs.</p>
<p><b>Definition.</b> A <b>stochastic Petri net</b> is a Petri net together with a function </p>
<p><img src="https://s0.wp.com/latex.php?latex=r%3A+T+%5Cto+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r: T &#92;to [0,&#92;infty) " class="latex" /></p>
<p>giving a <b>rate constant</b> for each transition.</p>
<h4> Master equation versus rate equation </h4>
<p>Starting from any stochastic Petri net, we can get two things.  First:</p>
<p>&bull; The <a href="http://en.wikipedia.org/wiki/Master_equation"><b>master equation</b></a>.  This says how the <i>probability that we have a given number of things of each species</i> changes with time.     </p>
<p>&bull; The <a href="http://en.wikipedia.org/wiki/Rate_equation"><b>rate equation</b></a>.  This says how the <i>expected number of things of each species</i> changes with time.  </p>
<p>The master equation is stochastic: it describes how probabilities change with time.  The rate equation is deterministic.  </p>
<p>The master equation is more fundamental.  It&#8217;s like the equations of quantum electrodynamics, which describe the amplitudes for creating and annihilating individual photons.  The rate equation is less fundamental.  It&#8217;s like the classical Maxwell equations, which describe changes in the electromagnetic field in a deterministic way.  The classical Maxwell equations are an approximation to quantum electrodynamics.  This approximation gets good <i>in the limit</i> where there are lots of photons all piling on top of each other to form nice waves. </p>
<p>Similarly, the rate equation can be derived from the master equation <i>in the limit</i> where the number of things of each species become large, and the fluctuations in these numbers become negligible.  </p>
<p>But I won&#8217;t do this derivation today!  Nor will I probe more deeply into the analogy with quantum field theory, even though that&#8217;s my ultimate goal.  Today I&#8217;m content to remind you what the master equation and rate equation <i>are</i>.  </p>
<p>The rate equation is simpler, so let&#8217;s do that first.</p>
<h4> The rate equation </h4>
<p>Suppose we have a stochastic Petri net with <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> different species.  Let <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> be the number of things of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.   Then the rate equation looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x_i%7D%7Bd+t%7D+%3D+%3F%3F%3F+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x_i}{d t} = ??? } " class="latex" /></p>
<p>It&#8217;s really a bunch of equations, one for each <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+i+%5Cle+k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;le i &#92;le k" class="latex" />.  But what is the right-hand side?</p>
<p>The right-hand side is a sum of terms, one for each transition in our Petri net.  So, let&#8217;s start by assuming our Petri net has just one transition.   </p>
<p>Suppose the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species appears as input to this transition <img src="https://s0.wp.com/latex.php?latex=m_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m_i" class="latex" /> times, and as output <img src="https://s0.wp.com/latex.php?latex=n_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i" class="latex" /> times.  Then the rate equation is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x_i%7D%7Bd+t%7D+%3D+r+%28n_i+-+m_i%29+x_1%5E%7Bm_1%7D+%5Ccdots+x_k%5E%7Bm_k%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x_i}{d t} = r (n_i - m_i) x_1^{m_1} &#92;cdots x_k^{m_k} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> is the rate constant for this transition.  </p>
<p>That&#8217;s really all there is to it!  But we can make it look nicer.  Let&#8217;s make up a vector </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%28x_1%2C+%5Cdots+%2C+x_k%29+%5Cin+%5B0%2C%5Cinfty%29%5Ek+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = (x_1, &#92;dots , x_k) &#92;in [0,&#92;infty)^k " class="latex" /></p>
<p>that says how many things  there are of each species.  Similarly let&#8217;s make up an <b>input vector</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=m+%3D+%28m_1%2C+%5Cdots%2C+m_k%29+%5Cin+%5Cmathbb%7BN%7D%5Ek+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m = (m_1, &#92;dots, m_k) &#92;in &#92;mathbb{N}^k " class="latex" /></p>
<p>and an <b>output vector</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%3D+%28n_1%2C+%5Cdots%2C+n_k%29+%5Cin+%5Cmathbb%7BN%7D%5Ek+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = (n_1, &#92;dots, n_k) &#92;in &#92;mathbb{N}^k " class="latex" /></p>
<p>for our transition.  To be cute, let&#8217;s also define</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+x%5Em+%3D+x_1%5E%7Bm_1%7D+%5Ccdots+x_k%5E%7Bm_k%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ x^m = x_1^{m_1} &#92;cdots x_k^{m_k} } " class="latex" /></p>
<p>Then we can write the rate equation for a single transition like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+r+%28n-m%29+x%5Em+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x}{d t} = r (n-m) x^m } " class="latex" /></p>
<p>Next let&#8217;s do a general stochastic Petri net, with lots of transitions.  Let&#8217;s write <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> for the set of transitions and <img src="https://s0.wp.com/latex.php?latex=r%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r(&#92;tau)" class="latex" /> for the rate constant of the transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau+%5Cin+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau &#92;in T" class="latex" />.  Let <img src="https://s0.wp.com/latex.php?latex=n%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n(&#92;tau)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=m%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m(&#92;tau)" class="latex" /> be the input and output vectors of the transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" />.  Then the rate equation is:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29+%28n%28%5Ctau%29+-+m%28%5Ctau%29%29+x%5E%7Bm%28%5Ctau%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x}{d t} = &#92;sum_{&#92;tau &#92;in T} r(&#92;tau) (n(&#92;tau) - m(&#92;tau)) x^{m(&#92;tau)} } " class="latex" /></p>
<p>For example, consider our rabbits and wolves:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/wolf-rabbit.png" />
</div>
<p>Suppose </p>
<p>&bull; the rate constant for &#8216;birth&#8217; is <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />,</p>
<p>&bull; the rate constant for &#8216;predation&#8217; is <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" />,</p>
<p>&bull; the rate constant for &#8216;death&#8217; is <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta" class="latex" />.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=x_1%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1(t)" class="latex" /> be the number of rabbits and <img src="https://s0.wp.com/latex.php?latex=x_2%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_2(t)" class="latex" /> the number of wolves at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.  Then the rate equation looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x_1%7D%7Bd+t%7D+%3D+%5Cbeta+x_1+-+%5Cgamma+x_1+x_2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x_1}{d t} = &#92;beta x_1 - &#92;gamma x_1 x_2 } " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x_2%7D%7Bd+t%7D+%3D+%5Cgamma+x_1+x_2+-+%5Cdelta+x_2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x_2}{d t} = &#92;gamma x_1 x_2 - &#92;delta x_2 } " class="latex" /></p>
<p>If you stare at this, and think about it, it should make perfect sense.  If it doesn&#8217;t, go back and read <a href="http://math.ucr.edu/home/baez/networks/networks_2.html">Part 2</a>.</p>
<h4> The master equation </h4>
<p>Now let&#8217;s do something new.  In <a href="http://math.ucr.edu/home/baez/networks/networks_6.html">Part 6</a> I explained how to write down the master equation for a stochastic Petri net with just <i>one</i> species.  Now let&#8217;s generalize that.  Luckily, the ideas are exactly the same.</p>
<p>So, suppose we have a stochastic Petri net with <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> different species. Let <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bn_1%2C+%5Cdots%2C+n_k%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_{n_1, &#92;dots, n_k}" class="latex" /> be the probability that we have <img src="https://s0.wp.com/latex.php?latex=n_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1" class="latex" /> things of the first species, <img src="https://s0.wp.com/latex.php?latex=n_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_2" class="latex" /> of the second species, and so on.   The master equation will say how all these probabilities change with time.</p>
<p>To keep the notation clean, let&#8217;s introduce a vector</p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%3D+%28n_1%2C+%5Cdots%2C+n_k%29+%5Cin+%5Cmathbb%7BN%7D%5Ek+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = (n_1, &#92;dots, n_k) &#92;in &#92;mathbb{N}^k " class="latex" /></p>
<p>and let</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_n+%3D+%5Cpsi_%7Bn_1%2C+%5Cdots%2C+n_k%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_n = &#92;psi_{n_1, &#92;dots, n_k} " class="latex" /></p>
<p>Then, let&#8217;s take all these probabilities and cook up a formal power series that has them as coefficients: as we&#8217;ve seen, this is a powerful trick.    To do this, we&#8217;ll bring in some variables <img src="https://s0.wp.com/latex.php?latex=z_1%2C+%5Cdots%2C+z_k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z_1, &#92;dots, z_k" class="latex" /> and write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+z%5En+%3D+z_1%5E%7Bn_1%7D+%5Ccdots+z_k%5E%7Bn_k%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ z^n = z_1^{n_1} &#92;cdots z_k^{n_k} } " class="latex" /></p>
<p>as a convenient abbreviation.  Then any formal power series in these variables looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi+%3D+%5Csum_%7Bn+%5Cin+%5Cmathbb%7BN%7D%5Ek%7D+%5Cpsi_n+z%5En+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi = &#92;sum_{n &#92;in &#92;mathbb{N}^k} &#92;psi_n z^n } " class="latex" /></p>
<p>We call <img src="https://s0.wp.com/latex.php?latex=%5CPsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi" class="latex" /> a <b>state</b> if the probabilities sum to 1 as they should:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+%5Cpsi_n+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n &#92;psi_n = 1 }" class="latex" /></p>
<p>The simplest example of a state is a monomial:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++z%5En+%3D+z_1%5E%7Bn_1%7D+%5Ccdots+z_k%5E%7Bn_k%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  z^n = z_1^{n_1} &#92;cdots z_k^{n_k} } " class="latex" /></p>
<p>This is a state where we are 100% sure that there are <img src="https://s0.wp.com/latex.php?latex=n_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1" class="latex" /> things of the first species, <img src="https://s0.wp.com/latex.php?latex=n_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_2" class="latex" /> of the second species, and so on.  We call such a state a <b>pure state</b>, since physicists use this term to describe a state where we know for sure exactly what&#8217;s going on.  Sometimes a general state, one that might not be pure, is called <b>mixed</b>.</p>
<p>The master equation says how a state evolves in time.  It looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5CPsi%28t%29+%3D+H+%5CPsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;Psi(t) = H &#92;Psi(t) } " class="latex" /></p>
<p>So, I just need to tell you what <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is!  </p>
<p>It&#8217;s called the <b>Hamiltonian</b>.  It&#8217;s a linear operator built from special operators that annihilate and create things of various species.  Namely, for each state <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+i+%5Cle+k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;le i &#92;le k" class="latex" /> we have an <b>annihilation operator</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_i+%5CPsi+%3D+%5Cfrac%7Bd%7D%7Bd+z_i%7D+%5CPsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_i &#92;Psi = &#92;frac{d}{d z_i} &#92;Psi } " class="latex" /></p>
<p>and a <b>creation operator</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_i%5E%5Cdagger+%5CPsi+%3D+z_i+%5CPsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_i^&#92;dagger &#92;Psi = z_i &#92;Psi } " class="latex" /></p>
<p>How do we build <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> from these?   Suppose we&#8217;ve got a stochastic Petri net whose set of transitions is <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />.  As before, write <img src="https://s0.wp.com/latex.php?latex=r%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r(&#92;tau)" class="latex" /> for the rate constant of the transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau+%5Cin+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau &#92;in T" class="latex" />, and let <img src="https://s0.wp.com/latex.php?latex=n%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n(&#92;tau)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=m%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m(&#92;tau)" class="latex" /> be the input and output vectors of this transition.  Then:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H+%3D+%5Csum_%7B%5Ctau+%5Cin+T%7D+r%28%5Ctau%29+%5C%2C+%28%7Ba%5E%5Cdagger%7D%5E%7Bn%28%5Ctau%29%7D+-+%7Ba%5E%5Cdagger%7D%5E%7Bm%28%5Ctau%29%7D%29+%5C%2C+a%5E%7Bm%28%5Ctau%29%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H = &#92;sum_{&#92;tau &#92;in T} r(&#92;tau) &#92;, ({a^&#92;dagger}^{n(&#92;tau)} - {a^&#92;dagger}^{m(&#92;tau)}) &#92;, a^{m(&#92;tau)}  } " class="latex" /></p>
<p>where as usual we&#8217;ve introduce some shorthand notations to keep from going insane.  For example:</p>
<p><img src="https://s0.wp.com/latex.php?latex=a%5E%7Bm%28%5Ctau%29%7D+%3D+a_1%5E%7Bm_1%28%5Ctau%29%7D+%5Ccdots++a_k%5E%7Bm_k%28%5Ctau%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a^{m(&#92;tau)} = a_1^{m_1(&#92;tau)} &#92;cdots  a_k^{m_k(&#92;tau)} " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7Ba%5E%5Cdagger%7D%5E%7Bm%28%5Ctau%29%7D+%3D+%7Ba_1%5E%5Cdagger+%7D%5E%7Bm_1%28%5Ctau%29%7D+%5Ccdots++%7Ba_k%5E%5Cdagger%7D%5E%7Bm_k%28%5Ctau%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{a^&#92;dagger}^{m(&#92;tau)} = {a_1^&#92;dagger }^{m_1(&#92;tau)} &#92;cdots  {a_k^&#92;dagger}^{m_k(&#92;tau)} " class="latex" /></p>
<p>Now, it&#8217;s not surprising that each transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" /> contributes a term to <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" />.  It&#8217;s also not surprising that this term is proportional to the rate constant <img src="https://s0.wp.com/latex.php?latex=r%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r(&#92;tau)" class="latex" />.   The only tricky thing is the expression </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%28%7Ba%5E%5Cdagger%7D%5E%7Bn%28%5Ctau%29%7D+-+%7Ba%5E%5Cdagger%7D%5E%7Bm%28%5Ctau%29%7D%29%5C%2C+a%5E%7Bm%28%5Ctau%29%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ ({a^&#92;dagger}^{n(&#92;tau)} - {a^&#92;dagger}^{m(&#92;tau)})&#92;, a^{m(&#92;tau)} }" class="latex" /></p>
<p>How can we understand it?  The basic idea is this.   We&#8217;ve got two terms here.  The first term:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%7Ba%5E%5Cdagger%7D%5E%7Bn%28%5Ctau%29%7D+a%5E%7Bm%28%5Ctau%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  {a^&#92;dagger}^{n(&#92;tau)} a^{m(&#92;tau)} } " class="latex" /></p>
<p>describes how <img src="https://s0.wp.com/latex.php?latex=m_i%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m_i(&#92;tau)" class="latex" /> things of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species get annihilated, and <img src="https://s0.wp.com/latex.php?latex=n_i%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i(&#92;tau)" class="latex" /> things of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species get created.   Of course this happens thanks to our transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" />.  The second term:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++-+%7Ba%5E%5Cdagger%7D%5E%7Bm%28%5Ctau%29%7D+a%5E%7Bm%28%5Ctau%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  - {a^&#92;dagger}^{m(&#92;tau)} a^{m(&#92;tau)} } " class="latex" /></p>
<p>is a bit harder to understand, but it says how the probability that <i>nothing</i> happens&mdash;that we remain in the same pure state&mdash;<i>decreases</i> as time passes.   Again this happens due to our transition <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" />.</p>
<p>In fact, the second term must take precisely the form it does to ensure &#8216;conservation of total probability&#8217;.  In other words: if the probabilities <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_n" class="latex" /> sum to 1 at time zero, we want these probabilities to still sum to 1 at any later time.  And for this, we need that second term to be what it is!  In <a href="http://math.ucr.edu/home/baez/networks/networks_6.html">Part 6</a> we saw this in the special case where there&#8217;s only one species.  The general case works the same way.</p>
<p>Let&#8217;s look at an example.  Consider our rabbits and wolves yet again:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/wolf-rabbit.png" />
</div>
<p>and again suppose the rate constants for birth, predation and death are <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta" class="latex" />, respectively.  We have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi+%3D+%5Csum_n+%5Cpsi_n+z%5En+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi = &#92;sum_n &#92;psi_n z^n } " class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+z%5En+%3D+z_1%5E%7Bn_1%7D+z_2%5E%7Bn_2%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ z^n = z_1^{n_1} z_2^{n_2} } " class="latex" /></p>
<p>and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_n+%3D+%5Cpsi_%7Bn_1%2C+n_2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_n = &#92;psi_{n_1, n_2}" class="latex" /> is the probability of having <img src="https://s0.wp.com/latex.php?latex=n_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1" class="latex" /> rabbits and <img src="https://s0.wp.com/latex.php?latex=n_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_2" class="latex" /> wolves.  These probabilities evolve according to the equation </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5CPsi%28t%29+%3D+H+%5CPsi%28t%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;Psi(t) = H &#92;Psi(t) }" class="latex" /></p>
<p>where the Hamiltonian is </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cbeta+B+%2B+%5Cgamma+C+%2B+%5Cdelta+D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;beta B + &#92;gamma C + &#92;delta D " class="latex" /></p>
<p>and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> are operators describing birth, predation and death, respectively.  (<img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> stands for birth, <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> stands for death&#8230; and you can call predation &#8216;consumption&#8217; if you want something that starts with <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" />.  Besides, &#8216;consumer&#8217; is a nice euphemism for &#8216;predator&#8217;.)  What are these operators?  Just follow the rules I described:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+B+%3D+%7Ba_1%5E%5Cdagger%7D%5E2+a_1+-+a_1%5E%5Cdagger+a_1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ B = {a_1^&#92;dagger}^2 a_1 - a_1^&#92;dagger a_1 }" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+C+%3D+%7Ba_2%5E%5Cdagger%7D%5E2+a_1+a_2+-+a_1%5E%5Cdagger+a_2%5E%5Cdagger+a_1+a_2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ C = {a_2^&#92;dagger}^2 a_1 a_2 - a_1^&#92;dagger a_2^&#92;dagger a_1 a_2 } " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D+%3D+a_2+-++a_2%5E%5Cdagger+a_2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D = a_2 -  a_2^&#92;dagger a_2 } " class="latex" /></p>
<p>In each case, the first term is easy to understand:</p>
<p>&bull; Birth annihilates one rabbit and creates two rabbits.</p>
<p>&bull; Predation annihilates one rabbit and one wolf and creates two wolves.</p>
<p>&bull; Death annihilates one wolf.</p>
<p>The second term is trickier, but I told you how it works.  </p>
<h4> Feynman diagrams </h4>
<p>How do we solve the master equation?  If we don&#8217;t worry about mathematical rigor too much, it&#8217;s easy.  The solution of </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5CPsi%28t%29+%3D+H+%5CPsi%28t%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;Psi(t) = H &#92;Psi(t) }" class="latex" /></p>
<p>should be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi%28t%29+%3D+e%5E%7Bt+H%7D+%5CPsi%280%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi(t) = e^{t H} &#92;Psi(0) }" class="latex" /></p>
<p>and we can hope that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+e%5E%7Bt+H%7D+%3D+1+%2B+t+H+%2B+%5Cfrac%7B%28t+H%29%5E2%7D%7B2%21%7D+%2B+%5Ccdots+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ e^{t H} = 1 + t H + &#92;frac{(t H)^2}{2!} + &#92;cdots } " class="latex" /></p>
<p>so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi%28t%29+%3D+%5CPsi%280%29+%2B+t+H+%5CPsi%280%29+%2B+%5Cfrac%7Bt%5E2%7D%7B2%21%7D+H%5E2+%5CPsi%280%29+%2B+%5Ccdots+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi(t) = &#92;Psi(0) + t H &#92;Psi(0) + &#92;frac{t^2}{2!} H^2 &#92;Psi(0) + &#92;cdots } " class="latex" /></p>
<p>Of course there&#8217;s always the question of whether this power series converges.  In many contexts it doesn&#8217;t, but that&#8217;s not necessarily a disaster: the series can still be <a href="http://en.wikipedia.org/wiki/Asymptotic_expansion">asymptotic</a> to the right answer, or even better, <a href="http://en.wikipedia.org/wiki/Borel_summation">Borel summable</a> to the right answer.  </p>
<p>But let&#8217;s not worry about these subtleties yet!  Let&#8217;s just imagine our rabbits and wolves, with Hamiltonian </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cbeta+B+%2B+%5Cgamma+C+%2B+%5Cdelta+D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;beta B + &#92;gamma C + &#92;delta D " class="latex" /></p>
<p>Now, imagine working out</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPsi%28t%29+%3D+%5Cdisplaystyle%7B+%5CPsi%280%29+%2B+t+H+%5CPsi%280%29+%2B+%5Cfrac%7Bt%5E2%7D%7B2%21%7D+H%5E2+%5CPsi%280%29+%2B+%5Cfrac%7Bt%5E3%7D%7B3%21%7D+H%5E3+%5CPsi%280%29+%2B+%5Ccdots+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi(t) = &#92;displaystyle{ &#92;Psi(0) + t H &#92;Psi(0) + &#92;frac{t^2}{2!} H^2 &#92;Psi(0) + &#92;frac{t^3}{3!} H^3 &#92;Psi(0) + &#92;cdots } " class="latex" /></p>
<p>We&#8217;ll get lots of terms involving products of <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> hitting our original state <img src="https://s0.wp.com/latex.php?latex=%5CPsi%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi(0)" class="latex" />.   And we can draw these as diagrams!  For example, suppose we start with one rabbit and one wolf.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPsi%280%29+%3D+z_1+z_2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi(0) = z_1 z_2 " class="latex" /></p>
<p>And suppose we want to compute </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H%5E3+%5CPsi%280%29+%3D+%28%5Cbeta+B+%2B+%5Cgamma+C+%2B+%5Cdelta+D%29%5E3+%5CPsi%280%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H^3 &#92;Psi(0) = (&#92;beta B + &#92;gamma C + &#92;delta D)^3 &#92;Psi(0) } " class="latex" /></p>
<p>as part of the task of computing <img src="https://s0.wp.com/latex.php?latex=%5CPsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Psi(t)" class="latex" />.  Then we&#8217;ll get lots of terms: 27, in fact, though many will turn out to be zero.  Let&#8217;s take one of these terms, for example the one proportional to:</p>
<p><img src="https://s0.wp.com/latex.php?latex=D+C+B+%5CPsi%280%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D C B &#92;Psi(0) " class="latex" /></p>
<p>We can draw this as a sum of Feynman diagrams, including this:</p>
<div align="center"><img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/feynman-diagram.png" /></div>
<p>In this diagram, we start with one rabbit and one wolf at top.  As we read the diagram from top to bottom, first a rabbit is born (<img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />), then predation occur (<img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" />), and finally a wolf dies (<img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />).  The end result is again a rabbit and a wolf.  </p>
<p>This is just one of four Feynman diagrams we should draw in our sum for <img src="https://s0.wp.com/latex.php?latex=D+C+B+%5CPsi%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D C B &#92;Psi(0)" class="latex" />, since either of the two rabbits could have been eaten, and either wolf could have died.  So, the end result of computing</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H%5E3+%5CPsi%280%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H^3 &#92;Psi(0) }" class="latex" /></p>
<p>will involve a lot of Feynman diagrams&#8230; and of course computing </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi%28t%29+%3D+%5CPsi%280%29+%2B+t+H+%5CPsi%280%29+%2B+%5Cfrac%7Bt%5E2%7D%7B2%21%7D+H%5E2+%5CPsi%280%29+%2B+%5Cfrac%7Bt%5E3%7D%7B3%21%7D+H%5E3+%5CPsi%280%29+%2B+%5Ccdots+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi(t) = &#92;Psi(0) + t H &#92;Psi(0) + &#92;frac{t^2}{2!} H^2 &#92;Psi(0) + &#92;frac{t^3}{3!} H^3 &#92;Psi(0) + &#92;cdots }" class="latex" /></p>
<p>will involve even more, even if we get tired and give up after the first few terms.  So, this Feynman diagram business may seem quite tedious&#8230; and it may not be obvious how it helps.  </p>
<p>But it does, sometimes!  </p>
<p>Now is not the time for me to describe &#8216;practical&#8217; benefits of Feynman diagrams.  Instead, I&#8217;ll just point out one conceptual benefit.  We started with what seemed like a purely computational chore, namely computing </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CPsi%28t%29+%3D+%5CPsi%280%29+%2B+t+H+%5CPsi%280%29+%2B+%5Cfrac%7Bt%5E2%7D%7B2%21%7D+H%5E2+%5CPsi%280%29+%2B+%5Ccdots+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Psi(t) = &#92;Psi(0) + t H &#92;Psi(0) + &#92;frac{t^2}{2!} H^2 &#92;Psi(0) + &#92;cdots }" class="latex" /></p>
<p>But then we saw&mdash;at least roughly&mdash;how this series has a clear meaning!  It can be written as a sum over diagrams, each of which represents a <i>possible history</i> of rabbits and wolves.  So, it&#8217;s what physicists call a <a href="http://en.wikipedia.org/wiki/Path_integral_formulation">&#8216;sum over histories&#8217;</a>.  </p>
<p>Feynman invented the idea of a sum over histories in the context of quantum field theory.  At the time this idea seemed quite mind-blowing, for various reasons.   First, it involved elementary particles instead of everyday things like rabbits and wolves.  Second, it involved complex &#8216;amplitudes&#8217; instead of real probabilities.  Third, it actually involved integrals instead of sums.  And fourth, a lot of these integrals diverged, giving infinite answers that needed to be &#8216;cured&#8217; somehow!</p>
<p>Now we&#8217;re seeing a sum over histories in a more down-to-earth context without all these complications.  A lot of the underlying math is analogous&#8230; but now there&#8217;s nothing mind-blowing about it: it&#8217;s quite easy to understand.  So, we can use this analogy to demystify quantum field theory a bit.  On the other hand, thanks to this analogy, all sorts of clever ideas invented by quantum field theorists will turn out to have applications to biology and chemistry!  So it&#8217;s a double win.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/09/09/network-theory-part-8/#comments">42 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/09/09/network-theory-part-8/" rel="bookmark" title="Permanent Link to Network Theory (Part&nbsp;8)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-4572 post type-post status-publish format-standard hentry category-probability category-risks" id="post-4572">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/08/19/bayesian-computations-of-expected-utility/" rel="bookmark">Bayesian Computations of Expected&nbsp;Utility</a></h2>
				<small>19 August, 2011</small><br />


				<div class="entry">
					<p><a href="http://www.givewell.org/">GiveWell</a> is an organization that rates charities.  They&#8217;ve met people who argue that</p>
<blockquote>
<p>charities working on reducing the risk of sudden human extinction must be the best ones to support, since the value of saving the human race is so high that “any imaginable probability of success” would lead to a higher expected value for these charities than for others.</p>
</blockquote>
<p>For example, say I have a dollar to spend on charity.  One charity says that with this dollar they can save the life of one child in Somalia.   Another says that with this dollar they can increase by .000001% our chance of saving 1 billion people from the effects of a massive asteroid colliding with the Earth.</p>
<p>Naively, in terms of the expected number of lives saved, the latter course of action seems 10 times better, since</p>
<div align="center">
.000001% &times; 1 billion = 10
</div>
<p>But is it really better?</p>
<p>It&#8217;s a subtle question, with all sorts of complicating factors, like <i>why should I trust these guys?</i>  </p>
<p>I&#8217;m not ready to present a thorough analysis of this sort of question today.   But I would like to hear what you think about it.  And I&#8217;d like you to read what the founder of Givewell has to say about it:</p>
<p>&bull; Holden Karnofsky, <a href="http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/">Why we can’t take expected value estimates literally (even when they’re unbiased)</a>, 18 August 2011.</p>
<p>He argues against what he calls an &#8216;explicit expected value&#8217; or &#8216;EEV&#8217; approach:</p>
<blockquote><p>
The mistake (we believe) is estimating the “expected value” of a donation (or other action) based solely on a fully explicit, quantified formula, many of whose inputs are guesses or very rough estimates. We believe that any estimate along these lines needs to be adjusted using a “Bayesian prior”; that this adjustment can rarely be made (reasonably) using an explicit, formal calculation; and that most attempts to do the latter, even when they seem to be making very conservative downward adjustments to the expected value of an opportunity, are not making nearly large enough downward adjustments to be consistent with the proper Bayesian approach.
</p></blockquote>
<p>His focus, in short, is on the fact that anyone saying &#8220;this money can increase by .000001% our chance of saving 1 billion people from an asteroid impact&#8221; is likely to be <i>pulling those numbers from thin air</i>.  If they can&#8217;t really back up their numbers with a lot of hard evidence, then our lack of confidence in their estimate should be taken into account somehow. </p>
<p>His article spends a lot of time analyzing a less complex but still very interesting example:</p>
<blockquote><p>
It seems fairly clear that a restaurant with 200 Yelp reviews, averaging 4.75 stars, ought to outrank a restaurant with 3 Yelp reviews, averaging 5 stars. Yet this ranking can’t be justified in an explicit expected utility framework, in which options are ranked by their estimated average/expected value.
</p></blockquote>
<p>This is the only question I really want to talk about today.  Actually I&#8217;ll focus on a similar question that Tim van Beek posed <a href="https://johncarlosbaez.wordpress.com/2011/07/15/rationality-in-humans-and-monkeys/#comment-6751">on this blog</a>:</p>
<blockquote><p>
You have two kinds of fertilizer, A and B. You know that of 4 trees who got A, three thrived and one died. Of 36 trees that got B, 24 thrived and 12 died. Which fertilizer would you buy?
</p></blockquote>
<p>So, 3/4 of the trees getting fertilizer A thrived, while only 2/3 of those getting fertilizer B thrived.  That makes fertilizer A seem better.  However, the sample size is considerably larger for fertilizer B, so we may feel <i>more confident</i> about the results in this case.  Which should we choose?</p>
<p>Nathan Urban <a href="https://johncarlosbaez.wordpress.com/2011/07/15/rationality-in-humans-and-monkeys/#comment-6790">tackled the problem in an interesting way</a>.  Let me sketch what he did before showing you his detailed work.  </p>
<p>Suppose that before doing any experiments at all, we assume the probability <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> that a fertilizer will make a tree thrive is a number <i>uniformly distributed between 0 and 1</i>.  This assumption is our &#8220;Bayesian prior&#8221;. </p>
<p>Note: I&#8217;m not saying this prior is &#8220;correct&#8221;.  You are allowed to choose a different prior!  Choosing a different prior will change your answer to this puzzle.  That can&#8217;t be helped.  We need to make some assumption to answer this kind of puzzle; we are simply making it explicit here.</p>
<p>Starting from this prior, Nathan works out the probability that <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> has some value <i>given that when we apply the fertilizer to 4 trees, 3 thrive</i>.   That&#8217;s the black curve below.   He also works out the probability that <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> has some value  <i>given that when we apply the fertilizer to 36 trees, 24 thrive</i>.  That&#8217;s the red curve:</p>
<div align="center"><a href="http://img30.imageshack.us/img30/1774/binomial.png" rel="nofollow"><img width="450" src="https://i2.wp.com/img30.imageshack.us/img30/1774/binomial.png" /></a></div>
<p>The red curve, corresponding to the experiment with 36 trees, is much more sharply peaked.  That makes sense.   It means that <i>when we do more experiments, we become more confident that we know what&#8217;s going on</i>.</p>
<p>We still have to choose a criterion to decide which fertilizer is best!  This is where &#8216;decision theory&#8217; comes in.  For example, suppose we want to maximize the expected number of the trees that thrive.  Then Nathan shows that fertilizer A is slightly better, despite the smaller sample size.</p>
<p>However, he also shows that if fertilizer A succeeded 4 out of 5 times, while fertilizer B succeeded 7 out of 9 times, the same evaluation procedure would declare fertilizer B better!  Its percentage success rate is less: about 78% instead of 80%.  However, the sample size is larger.  And in this particular case, <i>given our particular Bayesian prior</i> and <i>given what we are trying to maximize</i>, that&#8217;s enough to make fertilizer B win.</p>
<p>So if someone is trying to get you to contribute to a charity, there are many interesting issues involved in deciding if their arguments are valid or just a bunch of&#8230; fertilizer.</p>
<p>Here is Nathan&#8217;s detailed calculation:</p>
<blockquote>
<p>It&#8217;s fun to work out an official &#8216;correct&#8217; answer mathematically, as John suggested.  Of course, this ends up being a long way of confirming the obvious&#8212;and the answer is only as good as the assumptions&#8212;but I think it&#8217;s interesting anyway.  In this case, I&#8217;ll work it out by maximizing expected utility in Bayesian decision theory, for one choice of utility function.  This dodges the whole risk aversion point, but it opens discussion for how the assumptions might be modified to account for more real-world considerations.  Hopefully others can spot whether I&#8217;ve made mistakes in the derivations.</p>
<p>In Bayesian decision theory, the first thing you do is write down the data-generating process and then compute a posterior distribution for what is unknown.</p>
<p>In this case, we may assume the data-generating process (likelihood function) is a binomial distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BBin%7D%28s%2Cn%7C%5Cpi%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Bin}(s,n|&#92;pi)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> successes in <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> trials, given a probability of success <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" />.  Fertilizer A corresponds to <img src="https://s0.wp.com/latex.php?latex=s%3D3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s=3" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=n%3D4&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n=4" class="latex" /> and fertilizer B corresponds to <img src="https://s0.wp.com/latex.php?latex=s%3D24&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s=24" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=n%3D36&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n=36" class="latex" />.</p>
<p>The probability of success <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> is unknown, and we want to infer its posterior conditional on the data, <img src="https://s0.wp.com/latex.php?latex=p%28%5Cpi%7Cs%2Cn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;pi|s,n)" class="latex" />.  To compute a posterior we need to assume a prior on <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" />.</p>
<p>It turns out that the <a href="http://en.wikipedia.org/wiki/Beta_distribution" rel="nofollow">Beta</a> distribution is <a href="http://en.wikipedia.org/wiki/Conjugate_prior" rel="nofollow">conjugate</a> to a binomial likelihood, meaning that if we assume a Beta distributed prior, the then the posterior is also Beta distributed.  If the prior is <img src="https://s0.wp.com/latex.php?latex=%5Cpi+%5Csim+%5Cmathrm%7BBeta%7D%28%5Calpha_0%2C%5Cbeta_0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi &#92;sim &#92;mathrm{Beta}(&#92;alpha_0,&#92;beta_0)" class="latex" /> then the posterior is </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi+%5Csim+%5Cmathrm%7BBeta%7D%28%5Calpha%3D%5Calpha_0%2Bs%2C%5Cbeta%3D%5Cbeta_0%2Bn-s%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi &#92;sim &#92;mathrm{Beta}(&#92;alpha=&#92;alpha_0+s,&#92;beta=&#92;beta_0+n-s)." class="latex" /></p>
<p>One choice for a prior is a uniform prior on <img src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,1]" class="latex" />, which corresponds to a <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BBeta%7D%281%2C1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Beta}(1,1)" class="latex" /> distribution.  There are of course other prior choices which will lead to different conclusions.  For this prior, the posterior is <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BBeta%7D%28%5Cpi%3B+s%2B1%2C+n-s%2B1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Beta}(&#92;pi; s+1, n-s+1)" class="latex" />.  The posterior mode is </p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5Calpha-1%29%2F%28%5Calpha%2B%5Cbeta-2%29+%3D+s%2Fn&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;alpha-1)/(&#92;alpha+&#92;beta-2) = s/n" class="latex" /> </p>
<p>and the posterior mean is </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha%2F%28%5Calpha%2B%5Cbeta%29+%3D+%28s%2B1%29%2F%28n%2B2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha/(&#92;alpha+&#92;beta) = (s+1)/(n+2)." class="latex" /></p>
<p>So, what is the inference for fertilizers A and B?  I made a graph of the posterior distributions.  You can see that the inference for fertilizer B is sharper, as expected, since there is more data.  But the inference for fertilizer A tends towards higher success rates, which can be quantified.</p>
<div align="center"><a href="http://img30.imageshack.us/img30/1774/binomial.png" rel="nofollow"><img width="450" src="https://i2.wp.com/img30.imageshack.us/img30/1774/binomial.png" /></a></div>
<p>Fertilizer A has a posterior mode of 3/4 = 0.75 and B has a mode of 2/3 = 0.667, corresponding to the sample proportions.  The mode isn&#8217;t the only measure of central tendency we could use.  The means are 0.667 for A and 0.658 for B; the medians are 0.686 for A and 0.661 for B.  No matter which of the three statistics we choose, fertilizer A looks better than fertilizer B.</p>
<p>But we haven&#8217;t really done &#8220;decision theory&#8221; yet.  We&#8217;ve just compared point estimators.  Actually, we have done a little decision theory, implicitly.  It turns out that picking the mean corresponds to the estimator which minimizes the expected squared error in <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" />, where &#8220;squared error&#8221; can be thought of formally as a loss function in decision theory.  Picking the median corresponds to minimizing the expected absolute loss, and picking the mode corresponds to minimizing the minimizing the 0-1 loss (where you lose nothing if you guess <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> exactly and lose 1 otherwise).</p>
<p>Still, these don&#8217;t really correspond to a decision theoretic view of the problem.  We don&#8217;t care about the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> at all, let alone some point estimator of it.  We only care about <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> indirectly, insofar as it helps us predict something about what the fertilizer will do to new trees.  For that, we have to move from the posterior distribution <img src="https://s0.wp.com/latex.php?latex=p%28%5Cpi%7Cs%2Cn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;pi|s,n)" class="latex" /> to the predictive distribution </p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28y%7Cs%2Cn%29+%3D+%5Cint+p%28y%7C%5Cpi%2Cn%29%5C%2Cp%28%5Cpi%7Cs%2Cn%29%5C%2Cd%5Cpi+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(y|s,n) = &#92;int p(y|&#92;pi,n)&#92;,p(&#92;pi|s,n)&#92;,d&#92;pi ," class="latex" /> </p>
<p>where <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> is a random variable indicating whether a new tree will thrive under treatment.  Here I assume that the success of new trees follows the same binomial distribution as in the experimental group.</p>
<p>For a Beta posterior, the predictive distribution is beta-binomial, and the expected number of successes for a new tree is equal to the mean of the Beta distribution for <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> &#8211; i.e. the posterior mean we computed before, <img src="https://s0.wp.com/latex.php?latex=%28s%2B1%29%2F%28n%2B2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(s+1)/(n+2)" class="latex" />.  If we introduce a utility function such that we are rewarded 1 util for a thriving tree and 0 utils for non-thriving tree, then the expected utility is equal to the expected success rate.  Therefore, under these assumptions, we should choose the fertilizer that maximizes the quantity <img src="https://s0.wp.com/latex.php?latex=%28s%2B1%29%2F%28n%2B2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(s+1)/(n+2)" class="latex" />, which, as we&#8217;ve seen, favors fertilizer A (0.667) over fertilizer B (0.658).</p>
<p>An interesting mathematical question is, does this ever work out to a &#8220;non-obvious&#8221; conclusion?  That is, if fertilizer A has a sample success rate which is greater than fertilizer B&#8217;s sample success rate, but expected utility maximization prefers fertilizer B?  Mathematically, we&#8217;re looking for a set <img src="https://s0.wp.com/latex.php?latex=%7Bs%2Cs%27%2Cn%2Cn%27%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{s,s&#039;,n,n&#039;}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=s%2Fn%3Es%27%2Fn%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s/n&gt;s&#039;/n&#039;" class="latex" /> but <img src="https://s0.wp.com/latex.php?latex=%28s%2B1%29%2F%28n%2B2%29+%3C+%28s%27%2B1%29%2F%28n%27%2B2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(s+1)/(n+2) &lt; (s&#039;+1)/(n&#039;+2)" class="latex" />.  (Also there are obvious constraints on <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=s%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s&#039;" class="latex" />.)  The answer is yes.  For example, if fertilizer A has 4 of 5 successes while fertilizer B has 7 of 9 successes.</p>
</blockquote>
<p>By the way, on a quite different note: <a href="http://neo.jpl.nasa.gov/risk/a99942.html">NASA</a> currently rates the chances of the asteroid <a href="http://en.wikipedia.org/wiki/99942_Apophis">Apophis</a> colliding with the Earth in 2036 at 4.3 &times; 10<sup></sup><sup>-6</sup>.  It estimates that the energy of such a collision would be comparable with a 510-megatonne thermonuclear bomb.  This is ten times larger than the largest bomb actually exploded, the <a href="http://en.wikipedia.org/wiki/Tsar_Bomba">Tsar Bomba</a>.  The Tsar Bomba, in turn, was ten times larger than all the explosives used in World War II.</p>
<div align="center">
<a href="http://en.wikipedia.org/wiki/File:Tsar_photo11.jpg"><br />
<img src="http://upload.wikimedia.org/wikipedia/en/c/c9/Tsar_photo11.jpg" /><br />
</a>
</div>
<p>There&#8217;s an interesting <a href="http://www.technologyreview.com/blog/arxiv/27088/?ref=rss">Chinese plan</a> to deflect Apophis if that should prove necessary.  It is, however, quite a sketchy plan.  I expect people will make more detailed plans shortly before Apophis comes close to the Earth in 2029.  </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/08/19/bayesian-computations-of-expected-utility/#comments">25 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>, <a href="https://johncarlosbaez.wordpress.com/category/risks/" rel="category tag">risks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/08/19/bayesian-computations-of-expected-utility/" rel="bookmark" title="Permanent Link to Bayesian Computations of Expected&nbsp;Utility">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3832 post type-post status-publish format-standard hentry category-information-and-entropy category-probability" id="post-3832">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/" rel="bookmark">A Characterization of&nbsp;Entropy</a></h2>
				<small>2 June, 2011</small><br />


				<div class="entry">
					<p><a href="http://golem.ph.utexas.edu/category/2011/05/an_operadic_introduction_to_en.html">Over at the <i>n</i>-Category Café</a> some of us have been trying an experiment: writing a math paper in full public view, both on that blog and on its associated wiki: the <a href="http://nlab.mathforge.org/nlab/show/HomePage"><i>n</i>Lab</a>.  One great thing about doing things this way is that  people can easily chip in with helpful suggestions.  It&#8217;s also more fun!  Both these tend to speed the process.</p>
<p>Like Frankenstein&#8217;s monster, our paper&#8217;s main result was initially jolted into life by huge blasts of power: in this case, not lightning but category theory.  It was awesome to behold, but too scary for <i>this</i> blog.</p>
<div align="center">
<img width="440" src="https://bosquechica.files.wordpress.com/2009/02/frankenstein11.jpg?w=440" />
</div>
<p>First <a href="http://www.maths.gla.ac.uk/~tl/">Tom Leinster</a> realized that the concept of entropy fell out &#8212; unexpectedly, but very naturally &#8212; from considerations involving <a href="http://en.wikipedia.org/wiki/Operad_theory">&#8216;operads&#8217;</a>, which are collections of abstract operations.  He was looking at a particular operad where the operations are &#8216;convex linear combinations&#8217;, and he discovered that this operad has entropy lurking in its heart.  Then <a href="http://users.icfo.es/Tobias.Fritz/">Tobias Fritz</a> figured out a nice way to state Tom&#8217;s result without mentioning operads.  By now we&#8217;ve taught the monster table manners, found it shoes that fit, and it&#8217;s ready for polite society:</p>
<p>• John Baez, Tobias Fritz and Tom Leinster, <a href="http://arxiv.org/abs/1106.1791">A characterization of entropy in terms of information loss</a>,  <a href="http://www.mdpi.com/1099-4300/13/11/1945"><i>Entropy</i></a> <b>13</b> (2011), 1945&ndash;1957.</p>
<p>The idea goes like this.   Say you&#8217;ve got a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with a <b><a href="http://en.wikipedia.org/wiki/Probability_measure">probability measure</a></b> <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on it, meaning a number <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+p_i+%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le p_i &#92;le 1" class="latex" /> for each point <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" />, obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi+%5Cin+X%7D+p_i+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i &#92;in X} p_i = 1" class="latex" /></p>
<p>Then the <b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a></b> of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%2C+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{i &#92;in X} p_i &#92;, &#92;ln(p_i) " class="latex" /></p>
<p>This funny-looking formula can be justified in many ways.  Our new way involves focusing not on entropy itself, but on <i>changes</i> in entropy.  This makes sense for lots of reasons.  For example, in physics we don&#8217;t usually measure entropy directly.  Instead, we measure changes in entropy, using the fact that a system at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> absorbing a tiny amount of heat <img src="https://s0.wp.com/latex.php?latex=%5CDelta+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta Q" class="latex" /> in a reversible way will experience an entropy change of <img src="https://s0.wp.com/latex.php?latex=%5CDelta+Q+%2F+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta Q / T" class="latex" />.  But our real reason for focusing on changes in entropy is that it gives a really slick theorem.</p>
<p>Suppose we have two finite sets with probability measures, say <img src="https://s0.wp.com/latex.php?latex=%28X%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,p)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(Y,q)" class="latex" />.  Then we define a <b>morphism</b> <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" /> to be a measure-preserving function: in other words, one for which the probability <img src="https://s0.wp.com/latex.php?latex=q_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_j" class="latex" /> of any point in <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> is the sum of the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> of the points in <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=f%28i%29+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(i) = j" class="latex" />.</p>
<p>A morphism of this sort is a deterministic process that carries one random situation to another.  For example, if I have a random integer between -10 and 10, chosen according to some probability distribution, and I square it, I get a random integer between 0 and 100.   A process of this sort always <i>decreases</i> the entropy: given any morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28q%29+%5Cle+S%28p%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q) &#92;le S(p) " class="latex" /></p>
<p>Since the second law of thermodynamics says that entropy always <i>increases</i>, this may seem counterintuitive or even paradoxical!   But there&#8217;s no paradox here.  It makes more intuitive sense if you think of entropy as information, and the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> as some kind of data processing that doesn&#8217;t introduce any additional randomness. Such a process can only decrease the amount of information.  For example, squaring the number -5 gives the same answer as squaring 5, so if I tell you &#8220;this number squared is 25&#8221;, I&#8217;m giving you less information than if I said &#8220;this number is -5&#8221;.</p>
<p>For this reason, we call the difference <img src="https://s0.wp.com/latex.php?latex=S%28p%29+-+S%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) - S(q)" class="latex" /> the <b>information loss</b> of the morphism <img src="https://s0.wp.com/latex.php?latex=f+%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : (X,p) &#92;to (Y,q)" class="latex" />.  And here&#8217;s our characterization of Shannon entropy in terms of information loss:</p>
<p>First, let&#8217;s write a morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=f+%3A+p+%5Cto+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : p &#92;to q" class="latex" /> for short.  Suppose <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> is a function that assigns to any such morphism a number <img src="https://s0.wp.com/latex.php?latex=F%28f%29+%5Cin+%5B0%2C%5Cinfty%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f) &#92;in [0,&#92;infty)," class="latex" /> which we think of as its information loss.  And suppose that <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> obeys three axioms:</p>
<ol>
<li>Functoriality.  Whenever we can compose morphisms <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />, we demand that </li>
</ol>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f+%5Ccirc+g%29+%3D+F%28f%29+%2B+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f &#92;circ g) = F(f) + F(g)" class="latex" /></p>
<p>In other words: <b>when we do a process consisting of two stages, the amount of information lost in the whole process is the sum of the amounts lost in each stage!</b></p>
<ol>
<li>Convex linearity.   Suppose we have two finite sets equipped with probability measures, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, and a real number <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5B0%2C+1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda &#92;in [0, 1]" class="latex" />.  Then there is a probability measure <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p+%5Coplus+%281+-+%5Clambda%29+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p &#92;oplus (1 - &#92;lambda) q" class="latex" /> on the disjoint union of the two sets, obtained by weighting the two measures by <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=1+-+%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 - &#92;lambda" class="latex" />, respectively.  Similarly, given morphisms <img src="https://s0.wp.com/latex.php?latex=f%3A+p+%5Cto+p%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: p &#92;to p&#039;" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g%3A+q+%5Cto+q%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g: q &#92;to q&#039;" class="latex" /> there is an obvious morphism from <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p+%5Coplus+%281+-+%5Clambda%29+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p &#92;oplus (1 - &#92;lambda) q" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p%27+%5Coplus+%281+-+%5Clambda%29+q%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p&#039; &#92;oplus (1 - &#92;lambda) q&#039;" class="latex" />.   Let&#8217;s call this morphism <img src="https://s0.wp.com/latex.php?latex=%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda f &#92;oplus (1 - &#92;lambda) g" class="latex" />.  We demand that </li>
</ol>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g%29+%3D+%5Clambda+F%28f%29+%2B+%281+-+%5Clambda%29+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(&#92;lambda f &#92;oplus (1 - &#92;lambda) g) = &#92;lambda F(f) + (1 - &#92;lambda) F(g)" class="latex" /></p>
<p>In other words: <b>if we flip a probability-λ coin to decide whether to do one process or another, the information lost is λ times the information lost by the first process plus (1 &#8211; λ) times the information lost by the second!</b></p>
<ol>
<li>Continuity.  The same function between finite sets can be thought of as a measure-preserving map in different ways, by changing the measures on these sets.   In this situation the quantity <img src="https://s0.wp.com/latex.php?latex=F%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f)" class="latex" /> should depend continuously on the measures in question.</li>
</ol>
<p>In other words: <b>if we slightly change what we do a process to, the information it loses changes only slightly</b>.</p>
<p>Then we conclude that there exists a constant <img src="https://s0.wp.com/latex.php?latex=c+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;ge 0" class="latex" /> such that for any morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f%29+%3D+c%28S%28p%29+-+S%28q%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f) = c(S(p) - S(q)) " class="latex" /></p>
<p>In other words: <b>the information loss is some multiple of the change in Shannon entropy!</b></p>
<p>What&#8217;s pleasing about this theorem is that the three axioms are pretty natural, and it&#8217;s hard to see the formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%2C+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{i &#92;in X} p_i &#92;, &#92;ln(p_i) " class="latex" /></p>
<p>hiding in them&#8230; but it&#8217;s actually there.</p>
<p>(We also prove a version of this theorem for <a href="http://en.wikipedia.org/wiki/Tsallis_entropy">Tsallis entropy</a>, in case you care.   This obeys a mutant version of axiom 2, namely:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g%29+%3D+%5Clambda%5E%5Calpha+F%28f%29+%2B+%281+-+%5Clambda%29%5E%5Calpha+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(&#92;lambda f &#92;oplus (1 - &#92;lambda) g) = &#92;lambda^&#92;alpha F(f) + (1 - &#92;lambda)^&#92;alpha F(g)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> is a parameter with <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &lt; &#92;alpha&lt; &#92;infty" class="latex" />.  Tsallis entropy is a close relative of Rényi entropy, which I discussed here <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">earlier</a>.  Just as  Rényi entropy is a kind of <i>q</i>-derivative of the free energy, the Tsallis entropy is a <i>q</i>-derivative of the partition function.  I&#8217;m not sure either of them are really important, but when you&#8217;re trying to uniquely characterize Shannon entropy, it&#8217;s nice for it to have some competitors to fight against, and these are certainly the main two.  Both of them depend on a parameter and reduce to the Shannon entropy at a certain value of that parameter.)</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comments">32 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/" rel="bookmark" title="Permanent Link to A Characterization of&nbsp;Entropy">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2963 post type-post status-publish format-standard hentry category-mathematics category-networks category-physics category-probability" id="post-2963">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/" rel="bookmark">Network Theory (Part&nbsp;4)</a></h2>
				<small>6 April, 2011</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2011/04/03/network-theory-part-3/">Last time</a> I explained the rate equation of a stochastic Petri net.  But now let&#8217;s get serious: let&#8217;s see what&#8217;s really <i>stochastic</i>&mdash; that is, random&mdash;about a stochastic Petri net.  For this we need to forget the rate equation (temporarily) and learn about the &#8216;master equation&#8217;.  This is where ideas from quantum field theory start showing up!</p>
<p>A Petri net has a bunch of <b>states</b> and a bunch of <b>transitions</b>.  Here&#8217;s an example we&#8217;ve already seen, from chemistry:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/chemistryNetBasicA.png" alt="" />
</div>
<p>The states are in yellow, the transitions in blue.  A <b>labelling</b> of our Petri net is a way of putting some number of <b>things</b> in each state.  We can draw these things as little black dots:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/chemistryNetDot1A.png" alt="" />
</div>
<p>In this example there are only 0 or 1 things in each state: we&#8217;ve got one atom of carbon, one molecule of oxygen, one molecule of sodium hydroxide, one molecule of hydrochloric acid, and nothing else.  But in general, we can have any natural number of things in each state.</p>
<p>In a stochastic Petri net, the transitions occur randomly as time passes.  For example, as time passes we could see a sequence of transitions like this:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/chemistryNetDot1A.png" alt="" />
</div>
<p>&nbsp; </p>
<p>&nbsp; </p>
<div align="center">
<img width="450" src="https://i1.wp.com/www.azimuthproject.org/azimuth/files/chemistryNetDot2A.png" alt="" />
</div>
<p>&nbsp; </p>
<p>&nbsp; </p>
<div align="center">
<img width="450" src="https://i1.wp.com/www.azimuthproject.org/azimuth/files/chemistryNetDot3.png" alt="" />
</div>
<p>&nbsp; </p>
<p>&nbsp; </p>
<div align="center">
<img width="450" src="https://i1.wp.com/www.azimuthproject.org/azimuth/files/chemistryNetDot4.png" alt="" />
</div>
<p>Each time a transition occurs, the number of things in each state changes in an obvious way.</p>
<h4> The Master Equation </h4>
<p>Now, I said the transitions occur &#8216;randomly&#8217;, but that doesn&#8217;t mean there&#8217;s no rhyme or reason to them!  The miracle of probability theory is that it lets us state precise laws about random events.   The law governing the random behavior of a stochastic Petri net is called the &#8216;master equation&#8217;.  </p>
<p>In a stochastic Petri net, each transition has a <b>rate constant</b>, a nonnegative real number.  Roughly speaking, this determines the probability of that transition.</p>
<p>A bit more precisely: suppose we have a Petri net that is labelled in some way at some moment.  Then the probability that a given transition occurs in a short time <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t" class="latex" /> is approximately:</p>
<p>&bull; the rate constant for that transition, times</p>
<p>&bull; the time <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t" class="latex" />, times</p>
<p>&bull; the number of ways the transition can occur.</p>
<p>More precisely still: this formula is correct up to terms of order <img src="https://s0.wp.com/latex.php?latex=%28%5CDelta+t%29%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Delta t)^2" class="latex" />.  So, taking the limit as <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta t &#92;to 0" class="latex" />, we get a differential equation describing precisely how the probability of the Petri net having a given labelling changes with time!  And this is the <b>master equation</b>.</p>
<p>Now, you might be impatient to actually <i>see</i> the master equation, but that would be rash.    The true master doesn&#8217;t need to see the master equation.   It sounds like a Zen proverb, but it&#8217;s true. The raw beginner in mathematics wants to see the solutions of an equation.  The more advanced student is content to prove that the solution exists.  But the master is content to prove that the <i>equation</i> exists.  </p>
<p>A bit more seriously: what matters is understanding the rules that inevitably lead to some equation: actually <i>writing it down</i> is then straightforward.  </p>
<p>And you see, there&#8217;s something I haven&#8217;t explained yet: &#8220;the number of ways the transition can occur&#8221;.  This involves a bit of counting.  Consider, for example, this Petri net:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/wolf-rabbit.png" alt="" />
</div>
<p>Suppose there are 10 rabbits and 5 wolves.  </p>
<p>&bull; How many ways can the <b>birth</b> transition occur?  Since birth takes one rabbit as input, it can occur in 10 ways.</p>
<p>&bull; How many ways can <b>predation</b> occur?  Since predation takes one rabbit and one wolf as inputs, it can occur in 10 &times; 5 = 50 ways.</p>
<p>&bull; How many ways can <b>death</b> occur?  Since death takes one wolf as input, it can occur in 5 ways.  </p>
<p>Or consider this one:</p>
<div align="center">
<img width="250" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/H2O-II.png" alt="" />
</div>
<p>Suppose there are 10 hydrogen atoms and 5 oxygen atoms.  How many ways can they form a water molecule?  There are 10 ways to pick the first hydrogen, 9 ways to pick the second hydrogen, and 5 ways to pick the oxygen.  So, there are</p>
<p><img src="https://s0.wp.com/latex.php?latex=10+%5Ctimes+9+%5Ctimes+5+%3D+450+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="10 &#92;times 9 &#92;times 5 = 450 " class="latex" /></p>
<p>ways.   </p>
<p>Note that we&#8217;re treating the hydrogen atoms as <i>distinguishable</i>, so there are <img src="https://s0.wp.com/latex.php?latex=10+%5Ctimes+9&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="10 &#92;times 9" class="latex" /> ways to pick them, not <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B10+%5Ctimes+9%7D%7B2%7D+%3D+%5Cbinom%7B9%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{10 &#92;times 9}{2} = &#92;binom{9}{2}" class="latex" />.  In general, the number of ways to choose <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> distinguishable things from a collection of <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> is the <b><a href="http://www.stanford.edu/~dgleich/publications/finite-calculus.pdf#page=6">falling power</a></b> </p>
<p><img src="https://s0.wp.com/latex.php?latex=L%5E%7B%5Cunderline%7BM%7D%7D+%3D+L+%5Ccdot+%28L+-+1%29+%5Ccdots+%28L+-+M+%2B+1%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^{&#92;underline{M}} = L &#92;cdot (L - 1) &#92;cdots (L - M + 1) " class="latex" /></p>
<p>where there are <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> factors in the product, but each is 1 less than the preceding one&mdash;hence the term &#8216;falling&#8217;.</p>
<p>Okay, now I&#8217;ve given you all the raw ingredients to work out the master equation for any stochastic Petri net.  The previous paragraph was a big fat hint.  One more nudge and you&#8217;re on your own:</p>
<p><b>Puzzle.</b> Suppose we have a stochastic Petri net with <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> states and just one transition, whose rate constant is <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" />.  Suppose the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state appears <img src="https://s0.wp.com/latex.php?latex=m_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m_i" class="latex" /> times as the input of this transition and <img src="https://s0.wp.com/latex.php?latex=n_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i" class="latex" /> times as the output.   A labelling of this stochastic Petri net is a <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" />-tuple of natural numbers <img src="https://s0.wp.com/latex.php?latex=%5Cell+%3D+%28%5Cell_1%2C+%5Cdots%2C+%5Cell_k%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell = (&#92;ell_1, &#92;dots, &#92;ell_k)" class="latex" /> saying how many things are in each state.  Let <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Cell%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_&#92;ell(t)" class="latex" /> be the probability that the labelling is <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.  Then the <b>master equation</b> looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi_%7B%5Cell%7D%28t%29++%3D+%5Csum_%7B%5Cell%27%7D+H_%7B%5Cell+%5Cell%27%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{d t} &#92;psi_{&#92;ell}(t)  = &#92;sum_{&#92;ell&#039;} H_{&#92;ell &#92;ell&#039;} &#92;psi_{&#92;ell&#039;}(t) " class="latex" /></p>
<p>for some matrix of real numbers <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell+%5Cell%27%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell &#92;ell&#039;}." class="latex" />  What is this matrix?</p>
<p>You can write down a formula for this matrix using what I&#8217;ve told you.    And then, if you have a stochastic Petri net with more transitions, you can just compute the matrix for each transition using this formula, and add them all up.</p>
<p>Someday I&#8217;ll tell you the answer to this puzzle, but I want to get there by a strange route: I want to <i>guess</i> the master equation using ideas from <i>quantum field theory!</i></p>
<h4>Some clues</h4>
<p>Why?</p>
<p>Well, if we think about a stochastic Petri net whose labelling undergoes random transitions as I&#8217;ve described, you&#8217;ll see that any possible &#8216;history&#8217; for the labelling can be drawn in a way that looks like a <a href="http://en.wikipedia.org/wiki/Feynman_diagram">Feynman diagram</a>.  In quantum field theory, Feynman diagrams show how things interact and turn into other things.  But that&#8217;s what stochastic Petri nets do, too! </p>
<p>For example, if our Petri net looks like this:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/wolf-rabbit.png" alt="" />
</div>
<p>then a typical history can be drawn like this:</p>
<div align="center">
<img width="300" src="https://i2.wp.com/math.ucr.edu/home/baez/networks/feynman-diagram.png" alt="" />
</div>
<p>Some rabbits and wolves come in on top.  They undergo some transitions as time passes, and go out on the bottom. The vertical coordinate is time, while the horizontal coordinate doesn&#8217;t really mean anything: it just makes the diagram easier to draw.</p>
<p>If we ignore all the artistry that makes it cute, this Feynman diagram is just a graph with states as edges and transitions as vertices.    Each transition occurs at a specific time.</p>
<p>We can use these Feynman diagrams to compute the probability that if we start it off with some labelling at time <img src="https://s0.wp.com/latex.php?latex=t_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_1" class="latex" />, our stochastic Petri net will wind up with some other labelling at time <img src="https://s0.wp.com/latex.php?latex=t_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_2" class="latex" />.   To do this, we just take a sum over Feynman diagrams that start and end with the given labellings.  For each Feynman diagram, we integrate over all possible times at which the transitions occur.  And what do we integrate?  Just the product of the rate constants for those transitions!</p>
<p>That was a bit of a mouthful, and it doesn&#8217;t really matter if you followed it in detail.  What matters is that it <i>sounds a lot like stuff you learn when you study quantum field theory!</i>   </p>
<p>That&#8217;s one clue that something cool is going on here.  Another is the master equation itself:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bdt%7D+%5Cpsi_%7B%5Cell%7D%28t%29+%3D+%5Csum_%7B%5Cell%27%7D+H_%7B%5Cell+%5Cell%27%7D+%5Cpsi_%7B%5Cell%27%7D%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{dt} &#92;psi_{&#92;ell}(t) = &#92;sum_{&#92;ell&#039;} H_{&#92;ell &#92;ell&#039;} &#92;psi_{&#92;ell&#039;}(t) " class="latex" /></p>
<p>This looks a lot like <a href="http://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation">Schr&ouml;dinger&#8217;s equation</a>, the basic equation describing how a quantum system changes with the passage of time.  </p>
<p>We can make it look even more like Schr&ouml;dinger&#8217;s equation if we create a vector space with the labellings <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> as a basis.  The numbers <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Cell%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_&#92;ell(t)" class="latex" /> will be the components of some vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t)" class="latex" /> in this vector space.   The numbers <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cell+%5Cell%27%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;ell &#92;ell&#039;}" class="latex" /> will be the matrix entries of some operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> on that vector space.  And the master equation becomes:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+H+%5Cpsi%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{d t} &#92;psi(t) = H &#92;psi(t) " class="latex" /></p>
<p>Compare Schr&ouml;dinger&#8217;s equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=i+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+H+%5Cpsi%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;frac{d}{d t} &#92;psi(t) = H &#92;psi(t) " class="latex" /></p>
<p>The only visible difference is that factor of <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />!  </p>
<p>But of course this is linked to another big difference: in the master equation <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> describes probabilities, so it&#8217;s a vector in a <i>real</i> vector space.   In quantum theory <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> describes <a href="http://en.wikipedia.org/wiki/Probability_amplitude">probability amplitudes</a>, so it&#8217;s a vector in a <i>complex</i> Hilbert space.</p>
<p>Apart from this huge difference, everything is a lot like quantum field theory.  In particular, our vector space is a lot like the <a href="http://en.wikipedia.org/wiki/Fock_space">Fock space</a> one sees in quantum field theory.   Suppose we have a quantum particle that can be in <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> different states.  Then its Fock space is the Hilbert space we use to describe an arbitrary collection of such particles.  It has an orthonormal basis denoted</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C+%5Cell_1+%5Ccdots+%5Cell_k+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="| &#92;ell_1 &#92;cdots &#92;ell_k &#92;rangle " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cell_1%2C+%5Cdots%2C+%5Cell_k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell_1, &#92;dots, &#92;ell_k" class="latex" /> are natural numbers saying how many particles there are in each state.  So, any vector in Fock space looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%3D+%5Csum_%7B%5Cell_1%2C+%5Cdots%2C+%5Cell_k%7D+%5C%3B+%5Cpsi_%7B%5Cell_1+%2C+%5Cdots%2C+%5Cell_k%7D+%5C%2C++%7C+%5Cell_1+%5Ccdots+%5Cell_k+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi = &#92;sum_{&#92;ell_1, &#92;dots, &#92;ell_k} &#92;; &#92;psi_{&#92;ell_1 , &#92;dots, &#92;ell_k} &#92;,  | &#92;ell_1 &#92;cdots &#92;ell_k &#92;rangle " class="latex" /></p>
<p>But if write the whole list <img src="https://s0.wp.com/latex.php?latex=%5Cell_1%2C+%5Cdots%2C+%5Cell_k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell_1, &#92;dots, &#92;ell_k" class="latex" /> simply as <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" />, this becomes</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%3D+%5Csum_%7B%5Cell%7D+%5Cpsi_%7B%5Cell%7D+++%7C+%5Cell+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi = &#92;sum_{&#92;ell} &#92;psi_{&#92;ell}   | &#92;ell &#92;rangle " class="latex" /></p>
<p>This is almost like what we&#8217;ve been doing with Petri nets!&mdash;except I hadn&#8217;t gotten around to giving names to the basis vectors.</p>
<p>In quantum field theory class, I learned lots of interesting operators on Fock space: annihilation and creation operators, number operators, and so on.  So, when I bumped into this master equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+H+%5Cpsi%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d}{d t} &#92;psi(t) = H &#92;psi(t) " class="latex" /></p>
<p>it seemed natural to take the operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> and write it in terms of these.  There was an obvious first guess, which didn&#8217;t quite work&#8230; but thinking a bit harder eventually led to the right answer.   Later, it turned out people had already thought about similar things.  So, I want to explain this.</p>
<p>When I first started working on this stuff, I was focused on the difference between collections of <i>indistinguishable</i> things, like bosons or fermions, and collections of <i>distinguishable</i> things, like rabbits or wolves. </p>
<p>But with the benefit of hindsight, it&#8217;s even more important to think about the difference between ordinary quantum theory, which is all about <i>probability amplitudes</i>, and the game we&#8217;re playing now, which is all about <i>probabilities</i>.  So, next time I&#8217;ll explain how we need to modify quantum theory so that it&#8217;s about probabilities.  This will make it easier to guess a nice formula for <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" />.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/#comments">44 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/06/network-theory-part-4/" rel="bookmark" title="Permanent Link to Network Theory (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2517 post type-post status-publish format-standard hentry category-biology category-mathematics category-probability category-this-weeks-finds" id="post-2517">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/02/17/this-weeks-finds-week-309/" rel="bookmark">This Week&#8217;s Finds (Week&nbsp;309)</a></h2>
				<small>17 February, 2011</small><br />


				<div class="entry">
					<p>In the next issues of This Week&#8217;s Finds, I&#8217;ll return to interviewing people who are trying to help humanity deal with some of the risks we face.</p>
<p>First I&#8217;ll talk to the science fiction author and astrophysicist <a href="http://www.gregorybenford.com/">Gregory Benford</a>.  I&#8217;ll ask him about his ideas on <a href="http://en.wikipedia.org/wiki/Geoengineering">&#8220;geoengineering&#8221;</a> — proposed ways of deliberately manipulating the Earth&#8217;s climate to counteract the effects of global warming.</p>
<p>After that, I&#8217;ll spend a few weeks asking <a href="http://yudkowsky.net/">Eliezer Yudkowsky</a> about his ideas on rationality and <a href="http://en.wikipedia.org/wiki/Friendly_artificial_intelligence">&#8220;friendly artificial intelligence&#8221;</a>.  Yudkowsky believes that the possibility of dramatic increases in intelligence, perhaps leading to a <a href="http://en.wikipedia.org/wiki/Technological_singularity">technological singularity</a>, should command more of our attention than it does.</p>
<p>Needless to say, all these ideas are controversial.  They&#8217;re exciting to some people — and infuriating, terrifying or laughable to others.    But I want to study lots of scenarios and lots of options in a calm, level-headed way without rushing to judgement.  I hope you enjoy it.</p>
<p>This week, I want to say a bit more about the Hopf bifurcation!</p>
<p>Last week I talked about applications of this mathematical concept to climate cycles like the El Niño &#8211; Southern Oscillation.  But over on the Azimuth Project, Graham Jones has explained an application of the same math to a very different subject:</p>
<p>• <a href="http://www.azimuthproject.org/azimuth/show/Quantitative+ecology">Quantitative ecology</a>, Azimuth Project.</p>
<p>That&#8217;s one thing that&#8217;s cool about math: the same patterns show up in different places.  So, I&#8217;d like to take advantage of his hard work and show you how a Hopf bifurcation shows up in a simple model of predator-prey interactions.</p>
<p>Suppose we have some rabbits that reproduce endlessly, with their numbers growing at a rate proportional to their population.  Let <img src="https://s0.wp.com/latex.php?latex=x%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t)" class="latex" /> be the number of animals at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.  Then we have:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+r+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d x}{d t} = r x" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> is the growth rate.  This gives exponential growth: it has solutions like</p>
<p><img src="https://s0.wp.com/latex.php?latex=x%28t%29+%3D+x_0+e%5E%7Br+t%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) = x_0 e^{r t}" class="latex" /></p>
<p>To get a slightly more realistic model, we can add &#8216;limits to growth&#8217;.  Instead of a constant growth rate, let&#8217;s try a growth rate that decreases as the population increases.  Let&#8217;s say it decreases in a linear way, and drops to zero when the population hits some value <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" />.  Then we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+r+%281-x%2FK%29+x++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d x}{d t} = r (1-x/K) x  " class="latex" /></p>
<p>This is called the <a href="http://www.azimuthproject.org/azimuth/show/Logistic+equation">&#8220;logistic equation&#8221;</a>.  <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> is known as the <a href="http://en.wikipedia.org/wiki/Carrying_capacity">&#8220;carrying capacity&#8221;</a>.  The idea is that the environment has enough resources to support this population.  If the population is less, it&#8217;ll grow; if it&#8217;s more, it&#8217;ll shrink.</p>
<p>If you know some calculus you can solve the logistic equation by hand by separating the variables and integrating both sides; it&#8217;s a textbook exercise.  The solutions are called <a href="http://en.wikipedia.org/wiki/Logistic_function">&#8220;logistic functions&#8221;</a>, and they look sort of like this:</p>
<div><a href="http://en.wikipedia.org/wiki/Logistic_function"><br />
<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/450px-Logistic-curve.svg.png" border="none" alt="" /><br />
</a></div>
<p>The above graph shows the simplest solution:</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%5Cfrac%7Be%5Et%7D%7Be%5Et+%2B+1%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = &#92;frac{e^t}{e^t + 1} " class="latex" /></p>
<p>of the simplest logistic equation in the world:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B+d+x%7D%7Bd+t%7D+%3D+%281+-+x%29x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{ d x}{d t} = (1 - x)x " class="latex" /></p>
<p>Here the carrying capacity is 1.  Populations less than 1 sound a bit silly, so think of it as 1 million rabbits.  You can see how the solution starts out growing almost exponentially and then levels off.  There&#8217;s a very different-looking solution where the population starts off above the carrying capacity and decreases.  There&#8217;s also a silly solution involving negative populations.  But whenever the population starts out positive, it approaches the carrying capacity.</p>
<p>The solution where the population just stays at the carrying capacity:</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = 1 " class="latex" /></p>
<p>is called a &#8220;stable equilibrium&#8221;, because it&#8217;s constant in time and nearby solutions approach it.</p>
<p>But now let&#8217;s introduce another species: some wolves, which eat the rabbits!  So, let <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> be the number of rabbits, and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> the number of wolves.  Before the rabbits meet the wolves, let&#8217;s assume they obey the logistic equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B+d+x%7D%7Bd+t%7D+%3D+x%281-x%2FK%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{ d x}{d t} = x(1-x/K)  " class="latex" /></p>
<p>And before the wolves meet the rabbits, let&#8217;s assume they obey this equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B+d+y%7D%7Bd+t%7D+%3D+-y++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{ d y}{d t} = -y  " class="latex" /></p>
<p>so that their numbers would decay exponentially to zero if there were nothing to eat.</p>
<p>So far, not very interesting.  But now let&#8217;s include a term that describes how predators eat prey.  Let&#8217;s say that on top of the above effect, the predators grow in numbers, and the prey decrease, at a rate proportional to:</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+y%2F%281%2Bx%29.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x y/(1+x). " class="latex" /></p>
<p>For small numbers of prey and predators, this means that predation increases nearly linearly with both <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />.   But if you have one wolf surrounded by a million rabbits in a small area, the rate at which it eats rabbits won&#8217;t double if you double the number of rabbits!  So, this formula includes a limit on predation as the number of prey increases.</p>
<p>Okay, so let&#8217;s try these equations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B+d+x%7D%7Bd+t%7D+%3D+x%281-x%2FK%29+-+4x+y%2F%28x%2B1%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{ d x}{d t} = x(1-x/K) - 4x y/(x+1) " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B+d+y%7D%7Bd+t%7D+%3D+-y+%2B+2x+y%2F%28x%2B1%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{ d y}{d t} = -y + 2x y/(x+1)  " class="latex" /></p>
<p>The constants 4 and 2 here have been chosen for simplicity rather than realism.</p>
<p>Before we plunge ahead and get a computer to solve these equations, let&#8217;s see what we can do by hand.  Setting <img src="https://s0.wp.com/latex.php?latex=d+x%2Fd+t+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x/d t = 0" class="latex" /> gives the interesting parabola</p>
<p><img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cfrac%7B1%7D%7B4%7D%281-x%2FK%29%28x%2B1%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y = &#92;frac{1}{4}(1-x/K)(x+1) " class="latex" /></p>
<p>together with the boring line <img src="https://s0.wp.com/latex.php?latex=x+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = 0" class="latex" />. (If you start with no prey, that&#8217;s how it will stay.  It takes bunny to make bunny.)</p>
<p>Setting <img src="https://s0.wp.com/latex.php?latex=d+y%2Fd+t+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d y/d t = 0" class="latex" /> gives the interesting line</p>
<p><img src="https://s0.wp.com/latex.php?latex=x%3D1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x=1 " class="latex" /></p>
<p>together with the boring line <img src="https://s0.wp.com/latex.php?latex=y+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y = 0" class="latex" />.</p>
<p>The interesting parabola and the interesting line separate the <img src="https://s0.wp.com/latex.php?latex=x+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x y" class="latex" /> plane into four parts, so these curves are called <a href="http://en.wikipedia.org/wiki/Separatrix">separatrices</a>.  They meet at the point</p>
<p><img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cfrac%7B1%7D%7B2%7D+%281+-+1%2FK%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y = &#92;frac{1}{2} (1 - 1/K)  " class="latex" /></p>
<p>which of course is an equilibrium, since <img src="https://s0.wp.com/latex.php?latex=d+x+%2F+d+t+%3D+d+y+%2F+d+t+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x / d t = d y / d t = 0" class="latex" /> there.  But when <img src="https://s0.wp.com/latex.php?latex=K+%3C+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K &lt; 1" class="latex" /> this equilibrium occurs at a negative value of <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />, and negative populations make no sense.</p>
<p>So, if <img src="https://s0.wp.com/latex.php?latex=K+%3C+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K &lt; 1" class="latex" /> there is no equilibrium population, and with a bit more work one can see the problem: the wolves die out.  For larger values of <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> there is an equilibrium population.  But the nature of this equilibrium depends on <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" />: that&#8217;s the interesting part.</p>
<p>We could figure this out analytically, but let&#8217;s look at two of Graham&#8217;s plots.  Here&#8217;s a solution when <img src="https://s0.wp.com/latex.php?latex=K+%3D+2.5&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K = 2.5" class="latex" />:</p>
<div align="center"><img src="https://i1.wp.com/www.azimuthproject.org/azimuth/files/predator-prey-point.png" alt="" width="450" /></div>
<p>The gray curves are the separatrices.  The red curve shows a solution of the equations, with the numbers showing the passage of time.  So, you can see that the solution spirals in towards the equilibrium.   That&#8217;s what you expect of a stable equilibrium.</p>
<p>Here&#8217;s a picture when <img src="https://s0.wp.com/latex.php?latex=K+%3D+3.5&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K = 3.5" class="latex" />:</p>
<div align="center"><img src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/predator-prey-cycle.png" alt="" width="450" /></div>
<p>The red and blue curves are two solutions, again numbered to show how time passes.  The red curve spirals in towards the dotted gray curve.  The blue one spirals out towards it.  The gray curve is also a solution.  It&#8217;s called a &#8220;stable limit cycle&#8221; because it&#8217;s periodic, and nearby solutions move closer and closer to it.</p>
<p>With a bit more work, we could show analytically that whenever <img src="https://s0.wp.com/latex.php?latex=1+%3C+K+%3C+3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &lt; K &lt; 3" class="latex" /> there is a stable equilibrium.   As we increase <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" />, when <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> passes 3 this stable equilibrium suddenly becomes a tiny stable limit cycle.  This is a <a href="http://en.wikipedia.org/wiki/Hopf_bifurcation">Hopf bifurcation</a>!</p>
<p>Now, what if we add noise?  We saw the answer last week: where we before had a stable equilibrium, we now can get irregular cycles — because the noise keeps pushing the solution away from the equilibrium!</p>
<p>Here&#8217;s how it looks for <img src="https://s0.wp.com/latex.php?latex=K%3D2.5&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K=2.5" class="latex" /> with white noise added:</p>
<div align="center"><img src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/predator-prey-noise.png" alt="" width="450" /></div>
<p>The following graph shows a longer run in the noisy <img src="https://s0.wp.com/latex.php?latex=K%3D2.5&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K=2.5" class="latex" /> case, with rabbits (<img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />) in black and wolves (<img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />) in gray.  Click on the picture to make it bigger:</p>
<div align="center"><a href="http://www.azimuthproject.org/azimuth/files/predator-prey-noise-long.png"><br />
<img src="https://i2.wp.com/math.ucr.edu/home/baez/predator-prey-noise-long_small.jpg" alt="" width="450" /><br />
</a></div>
<p>There is irregular periodicity — and as you&#8217;d expect, the predators tends to lag behind the prey.  A burst in the rabbit population causes a rise in the wolf population; a lot of wolves eat a lot of rabbits; a crash in rabbits causes a crash in wolves.</p>
<p>This sort of phenomenon is actually seen in nature sometimes.  The most famous case involves the snowshoe hare and the lynx in Canada.  It was first noted by MacLulich:</p>
<p>• D. A. MacLulich, Fluctuations in the Numbers of the Varying Hare (<em>Lepus americanus</em>), University of Toronto Studies Biological Series <strong>43</strong>, University of Toronto Press, Toronto, 1937.</p>
<p>The <a href="http://en.wikipedia.org/wiki/Snowshoe_Hare">snowshoe hare</a> is also known as the &#8220;varying hare&#8221;, because its coat varies in color quite dramatically.  In the summer it looks like this:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/File:Lepus_americanus_5459.JPG"><br />
<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Lepus_americanus_5459.JPG/800px-Lepus_americanus_5459.JPG" alt="" width="450" /><br />
</a></div>
<p>In the winter it looks like this:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/File:Snowshoe_hare.jpg"><br />
<img src="http://upload.wikimedia.org/wikipedia/commons/e/ee/Snowshoe_hare.jpg" alt="" /><br />
</a></div>
<p>The <a href="http://en.wikipedia.org/wiki/Canada_Lynx">Canada lynx</a> is an impressive creature:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/File:Lynx_Canadensis.jpg"><br />
<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Lynx_Canadensis.jpg/800px-Lynx_Canadensis.jpg" alt="" width="450" /><br />
</a></div>
<p>But don&#8217;t be too scared: it only weighs 8-11 kilograms, nothing like a tiger or lion.</p>
<p>Down in the United States, the same species lynx went extinct in  Colorado around 1973 — but now it&#8217;s back!</p>
<p>• Colorado Division of Wildlife, <a href="http://wildlife.state.co.us/Research/Mammal/Lynx/">Success of the Lynx Reintroduction Program</a>, 27 September, 2010.</p>
<p>In Canada, at least, the lynx rely for the snowshoe hare for 60% to 97% of their diet.  I suppose this is one reason the hare has evolved such magnificent protective coloration.  This is also why the hare and lynx populations are tightly coupled.  They rise and crash in irregular cycles that look a bit like what we saw in our simplified model:</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/lynx-hare_cycle.jpg"><br />
<img src="https://i2.wp.com/math.ucr.edu/home/baez/lynx-hare_cycle.jpg" alt="" width="450" /><br />
</a></div>
<p>This cycle looks a bit more strongly periodic than Graham&#8217;s graph, so to fit this data, we might want to choose parameters that give a limit cycle rather than a stable equilibrium.</p>
<p>But I should warn you, in case it&#8217;s not obvious: everything about population biology is infinitely more complicated than the models I&#8217;ve showed you so far!  Some obvious complications: snowshoe hare breed in the spring, their diet varies dramatically over the course of year, and the lynx also eat rodents and birds, carrion when it&#8217;s available, and sometimes even deer.   Some less obvious ones: the hare will eat dead mice and even dead hare when they&#8217;re available, and the lynx can control the size of their litter depending on the abundance of food.  And I&#8217;m sure all these facts are just the tip of the iceberg.  So, it&#8217;s best to think of models here as crude caricatures designed to illustrate a few features of a very complex system.</p>
<p>I hope someday to say a bit more and go a bit deeper.  Do any of you know good books or papers to read, or fascinating tidbits of information?   Graham Jones recommends this book for some mathematical aspects of ecology:</p>
<p>• Michael R. Rose, <em>Quantitative Ecological Theory</em>, Johns Hopkins University Press, Maryland, 1987.</p>
<p>Alas, I haven&#8217;t read it yet.</p>
<p>Also: you can get Graham&#8217;s <a href="http://www.azimuthproject.org/azimuth/files/r-code-for-predator-prey.r">R code for predator-prey simulations</a> at the Azimuth Project.</p>
<hr />
<p><i>Under carefully controlled experimental circumstances, the organism will behave as it damned well pleases.</i> &#8211; the Harvard Law of Animal Behavior </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/02/17/this-weeks-finds-week-309/#comments">37 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>, <a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/" rel="category tag">this week's finds</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/02/17/this-weeks-finds-week-309/" rel="bookmark" title="Permanent Link to This Week&#8217;s Finds (Week&nbsp;309)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1238 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-1238">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/" rel="bookmark">Algorithmic Thermodynamics (Part&nbsp;1)</a></h2>
				<small>12 October, 2010</small><br />


				<div class="entry">
					<p>My grad student <a href="http://www.cs.auckland.ac.nz/~msta039/">Mike Stay</a> and I have published this paper:</p>
<p>• John Baez and Mike Stay, <a href="http://arxiv.org/abs/1010.2067">Algorithmic thermodynamics</a>, <i>Mathematical Structures in Computer Science</i> <b>22</b> (2012), 771&ndash;787.</p>
<p>Mike has a masters degree in computer science, and he&#8217;s working for Google on a project called <a href="http://en.wikipedia.org/wiki/Caja_project">Caja</a>.  This is a system for letting people write programs in JavaScript while protecting the end users from dirty tricks the programmers might have tried.  With me, he&#8217;s mainly working on the intersection of <a href="http://arxiv.org/abs/0903.0340">computer science and category theory</a>, trying to bring 2-categories into the game.  But Mike also knows a lot about <a href="http://en.wikipedia.org/wiki/Algorithmic_information_theory">algorithmic information theory</a>, a subject with fascinating connections to thermodynamics.  So, it was natural for us to work on that too.</p>
<p>Let me just tell you a little about what we did.</p>
<div align="center">
<a href="http://mediaresetproject.blogspot.com/2009/11/information-man.html"><br />
<img src="https://johncarlosbaez.files.wordpress.com/2010/10/shannon2.jpg?w=234" /><br />
</a>
</div>
<p>Around 1948, the electrical engineer <a href="http://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> came up with a mathematical theory of information.  Here is a quote that gives a flavor of his thoughts:</p>
<blockquote><p>
The whole point of a message is that it should contain something new.
</p></blockquote>
<p>Say you have a source of information, for example a mysterious radio transmission from an extraterrestrial civilization.  Suppose every day you get a signal sort of like this:</p>
<div align="center">
00101101011111101010101011011101110
</div>
<p>How much information are you getting?  If the message always looks like this, presumably not much:</p>
<div align="center">
00000000000000000000000000000000000
</div>
<p>Shannon came up with a precise formula for the information.  But beware: it&#8217;s not really a formula for the information of a <i>particular</i> message. It&#8217;s a formula for the <i>average</i> information of a message chosen from some probability distribution.  It&#8217;s this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Csum_i+p_i+%5C%3B%5Cmathrm%7Blog%7D%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;sum_i p_i &#92;;&#92;mathrm{log}(p_i) " class="latex" /></p>
<p>where we sum over all possible messages, and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the probability of the <i>i </i>th message.</p>
<p>So, for example, suppose you keep getting the same message.  Then every message has probability 0 except for one message, which has probability 1.  Then either <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Cln%28p_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(p_i)" class="latex" /> is zero, so the information is zero.</p>
<p>That seems vaguely plausible in the example where every day we get this message:</p>
<div align="center">
000000000000000000000000000000000000000
</div>
<p>It may seem less plausible if every day we get this message:</p>
<div align="center">
011010100010100010100010000010100000100
</div>
<p>It looks like the aliens are trying to tell us which numbers are prime!  1 is not prime, 2 is, 3 is, 4 is not, 5 is, and so on.  Aren&#8217;t we getting some information?</p>
<p>Maybe so: this could be considered a defect of <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon information</a>.  On the other hand, you might be willing to admit that if we keep getting the same message every day, the second time we get it we&#8217;re not getting any <i>new</i> information.  Or, you might <i>not</i> be willing to admit this — it&#8217;s actually a subtle issue, and I don&#8217;t feel like arguing.</p>
<p>But at the very least, you&#8217;ll probably admit that the second time you get the same message, you get less <i>new</i> information than the first time.  The third time you get even less, and so on.  So it&#8217;s quite believable that in the long run, the average amount of new information per message approaches 0 in this case.  For Shannon, information means <i>new</i> information.</p>
<p>On the other hand, suppose we are absolutely unable to to predict each new bit we get from the aliens.  Suppose our ability to predict the next bit is no better than our ability to predict whether a fair coin comes up heads or tails.  Then Shannon&#8217;s formula says we are getting the same amount of new information with every bit: namely, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Blog%7D%282%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{log}(2)." class="latex" /></p>
<p>If we take the logarithm using base 2 here, we get 1 — so we say we&#8217;re getting <i>one bit of information</i>.  If we take it using base e, as physicists prefer, we get <img src="https://s0.wp.com/latex.php?latex=%5Cln%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(2)" class="latex" /> — and we say we&#8217;re getting <img src="https://s0.wp.com/latex.php?latex=%5Cln%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(2)" class="latex" /> <a>nats</a> of information.   One bit equals <img src="https://s0.wp.com/latex.php?latex=%5Cln%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln(2)" class="latex" /> nats.</p>
<p>There&#8217;s a long road from these reflections to a full justification of the specific formula for Shannon information!  To begin marching down that road you can read his original paper, <a href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">A mathematical theory of communication</a>.</p>
<p>Anyway: it soon became clear that Shannon&#8217;s formula was almost the same as the usual formula for &#8220;entropy&#8221;, which goes back to <a href="http://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">Josiah Willard Gibbs</a>.  Gibbs was actually the first engineer to get a Ph.D. in the United States, back in 1863&#8230; but he&#8217;s mainly famous as a mathematician, physicist and chemist.</p>
<div align="center">
<a href="http://en.wikipedia.org/wiki/Josiah_Willard_Gibbs"><br />
<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Josiah_Willard_Gibbs_-from_MMS-.jpg/235px-Josiah_Willard_Gibbs_-from_MMS-.jpg" /><br />
</a>
</div>
<p>Entropy is a measure of disorder. Suppose we have a box of stuff — solid, liquid, gas, whatever.   There are many possible states this stuff can be in: for example, the atoms can be in different positions, and have different velocities.  Suppose we only know the <i>probability</i> that the stuff is in any one of the allowed states.  If the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state is occupied with probability <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> Gibbs said the <a href="http://en.wikipedia.org/wiki/Entropy">entropy</a> of our box of stuff is</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+k+%5Csum_i+p_i+%5C%3B+%5Cmathrm%7Blog%7D%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- k &#92;sum_i p_i &#92;; &#92;mathrm{log}(p_i) " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> is a constant called <a href="http://en.wikipedia.org/wiki/Boltzmann_constant">Boltzmann&#8217;s constant</a>.</p>
<p>There&#8217;s a wonderful story here, which I don&#8217;t have time to tell in detail.  The way I wrote Shannon&#8217;s formula for information and Gibbs&#8217; formula for entropy, you&#8217;d think only a moron would fail to instantly grasp that they&#8217;re basically the same.  But historically, it took some work.</p>
<p>The appearance of Bolzmann&#8217;s constant hints at why.  It shows up because people had ideas about entropy, and the closely related concept of <i>temperature</i>, long before they realized the full  mathematical meaning of these concepts!   So entropy traditionally came in units of &#8220;joules/kelvin&#8221;, and physicists had a visceral understanding of it.   But dividing by Boltzmann&#8217;s constant, we can translate that notion of entropy into the modern, more abstract way of thinking of entropy as information!</p>
<p>Henceforth I&#8217;ll work in units where <img src="https://s0.wp.com/latex.php?latex=k+%3D+1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k = 1," class="latex" /> as modern mathematical physicists do, and treat entropy and information as the same concept.</p>
<p>Closely related to information and entropy is a third concept, which I will call <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a>, though it was developed by many people — including Martin-Löf, Solomonoff, Kolmogorov, Levin, and Chaitin — and it has many names, including descriptive complexity, Kolmogorov-Chaitin complexity, algorithmic entropy, and program-size complexity.  You may be intimidated by all these names, but you shouldn&#8217;t be: when lots of people keep rediscovering something and giving it new names, it&#8217;s usually because this thing is pathetically simple.</p>
<p>So, what is Kolmogorov complexity?   It&#8217;s a way of measuring the information in a <i>single</i> message rather than a probability distribution of messages.  And it&#8217;s just <i>the length of the shortest computer program that prints out this message</i>.</p>
<p>I suppose some clarification may be needed here:</p>
<p>1) I&#8217;m only talking about programs without any input, that either calculate, print out a message, and halt&#8230; or calculate forever and never halt.</p>
<p>2) Of course the length of the shortest program that prints out the desired message depends on the programming language.  But there are theorems saying it doesn&#8217;t depend &#8220;too much&#8221; on the language.  So don&#8217;t worry about it: just pick your favorite language and stick with that.</p>
<p>If you think about it, Kolmogorov complexity is a really nice concept.  The Kolmogorov complexity of a string of a million 0&#8217;s is a lot less than a million: you can write a short program that says &#8220;print a million 0&#8217;s&#8221;.  But the Kolmogov complexity of a highly unpredictable string of a million 0&#8217;s and 1&#8217;s is about a million: you basically need to include that string in your program and then say &#8220;print this string&#8221;.  Those are the two extremes, but in general the complexity will be somewhere in between.</p>
<p>Various people — the bigshots listed above, and others too — soon realized that Kolmogorov complexity is deeply connected to Shannon information.  They have similar properties, but they&#8217;re also directly related.   It&#8217;s another great story, and I urge you to learn it.  For that, I recommend:</p>
<p>•  Ming Li and Paul Vitanyi, <i>An Introduction to Kolmogorov Complexity and Its Applications</i>, Springer, Berlin, 2008.</p>
<p>To understand the relation a bit better, Mike and I started thinking about <i>probability measures on the set of programs</i>.  People had already thought about this — but we thought about it a bit more the way physicists do.</p>
<p>Physicists like to talk about something called a <a href="http://en.wikipedia.org/wiki/Canonical_ensemble">Gibbs ensemble</a>.  Suppose we have a set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F: X &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>Then the Gibbs ensemble is the probability distribution on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> that maximizes entropy subject to the condition that <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> has some specified average, or &#8220;expected value&#8221;.</p>
<p>So, to find the Gibbs ensemble, we need to find a probability distribution <img src="https://s0.wp.com/latex.php?latex=p+%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : X &#92;to &#92;mathbb{R}" class="latex" /> that maximizes</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%3B+%5Cmathrm%7Blog%7D%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;sum_{i &#92;in X} p_i &#92;; &#92;mathrm{log}(p_i) " class="latex" /></p>
<p>subject to the constraint that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%3B+F%28i%29+%3D+%5Clangle+F+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i &#92;in X} p_i &#92;; F(i) = &#92;langle F &#92;rangle " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clangle+F+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle F &#92;rangle " class="latex" /> is some number, the expected value of <img src="https://s0.wp.com/latex.php?latex=F.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F." class="latex" />  Finding the probability distribution that does the job is an exercise in <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">Lagrange multipliers</a>.  I won&#8217;t do it.    There&#8217;s a nice formula for the answer, but we won&#8217;t need it here.</p>
<p>What you really need to know is something more important: why Gibbs invented the Gibbs ensemble!  He invented it to solve some puzzles that sound completely impossible at first.</p>
<p>For example, suppose you have a box of stuff and you don&#8217;t know which state it&#8217;s in.  Suppose you only know the expected value of its energy, say <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle." class="latex" /> What&#8217;s the probability that this stuff is in its <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th state?</p>
<p>This may sound like an insoluble puzzle: how can we possibly know?   But Gibbs proposed an answer!  He said, basically: find the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> that maximizes entropy subject to the constraint that the mean value of energy is <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle." class="latex" />  Then the answer is <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /></p>
<p>In other words: <i>use the Gibbs ensemble.</i></p>
<p>Now let&#8217;s come back to Kolmogorov complexity.</p>
<p>Imagine randomly picking a program out of a hat.  What&#8217;s the probability that you pick a certain program?   Again, this sounds like an impossible puzzle.  But you can answer it if you know the expected value of the length of this program!   Then you just use the Gibbs ensemble.</p>
<p>What does this have to do with Kolmogorov complexity?  Here I&#8217;ll be a bit fuzzy, because the details are in our paper, and I want you to read that.</p>
<p>Suppose we start out with the Gibbs ensemble I just mentioned.  In other words, we have a program in a hat, but all you know is the expected value of its length.</p>
<p>But now suppose I tell you the message that this program prints out.  Now you know more.  How much more information do you have now?  <i>The Kolmogorov complexity of the message</i> — that&#8217;s how much!</p>
<p>(Well, at least this is correct up to some error bounded by a constant.)</p>
<p>The main fuzzy thing in what I just said is &#8220;how much more information do you have?&#8221;  You see, I&#8217;ve explained information, but I haven&#8217;t explained <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">&#8220;information gain&#8221;</a>.  Information is a quantity you compute from <i>one</i> probability distribution.  Information gain is a quantity you compute from <i>two</i>.  More precisely,</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Csum_i+p_i+%5C%3B+%5Cmathrm%7Blog%7D%28p_i%2Fq_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;sum_i p_i &#92;; &#92;mathrm{log}(p_i/q_i) " class="latex" /></p>
<p>is the information you gain when you <i>thought</i> the probability distribution was <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> but then someone comes along and tells you it&#8217;s <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>In fact, we argue that information gain is more fundamental than information.   This is a Bayesian idea: <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is your <a href="http://en.wikipedia.org/wiki/Prior_probability">&#8220;prior&#8221;</a>, the probability distribution you <i>thought</i> was true, and the information you get upon hearing the distribution is <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> should be defined relative to this prior.  When people think they&#8217;re talking about information without any prior, they are really using a prior that&#8217;s so bland that they don&#8217;t notice it&#8217;s there: a so-called <a href="http://en.wikipedia.org/wiki/Uninformative_prior#Uninformative_priors">&#8220;uninformative prior&#8221;</a>.</p>
<p>But I digress.  To help you remember the story so far, let me repeat myself.  Up to a bounded error, <i>the Kolmogorov complexity of a message is the information gained when you start out only knowing the expected length of a program, and then learn which message the program prints out.</i></p>
<p>But this is just the beginning of the story.  We&#8217;ve seen how Kolmogorov complexity is related to Gibbs ensembles.  Now that we have Gibbs ensembles running around, we can go ahead and do thermodynamics!  We can talk about quantities analogous to temperature, pressure, and so on&#8230; and all the usual thermodynamic relations hold!  We can even take the ideas about steam engines and apply them to programs!</p>
<div align="center">
<img src="https://i2.wp.com/math.ucr.edu/home/baez/piston_tiny.jpg" />
</div>
<p>But for that, please read <a href="http://arxiv.org/abs/1010.2067">our paper</a>.  Here&#8217;s the abstract, which says what we really do:</p>
<blockquote><p>
Algorithmic entropy can be seen as a special case of entropy as studied in statistical mechanics. This viewpoint allows us to apply many techniques developed for use in thermodynamics to the subject of algorithmic information theory. In particular, suppose we fix a universal prefix-free Turing machine and let X be the set of programs that halt for this machine. Then we can regard X as a set of &#8216;microstates&#8217;, and treat any function on X as an &#8216;observable&#8217;. For any collection of observables, we can study the Gibbs ensemble that maximizes entropy subject to constraints on expected values of these observables. We illustrate this by taking the log runtime, length, and output of a program as observables analogous to the energy E, volume V and number of molecules N in a container of gas. The conjugate variables of these observables allow us to define quantities which we call the &#8216;algorithmic temperature&#8217; T, &#8216;algorithmic pressure&#8217; P and `algorithmic potential&#8217; μ, since they are analogous to the temperature, pressure and chemical potential. We derive an analogue of the fundamental thermodynamic relation dE = T dS &#8211; P d V + μ dN, and use it to study thermodynamic cycles analogous to those for heat engines. We also investigate the values of T, P and μ for which the partition function converges. At some points on the boundary of this domain of convergence, the partition function becomes uncomputable. Indeed, at these points the partition function itself has nontrivial algorithmic entropy.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/#comments">24 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/" rel="bookmark" title="Permanent Link to Algorithmic Thermodynamics (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/category/probability/page/10/" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/category/probability/page/8/" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the probability category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/probability/page/9/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

				<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = JSON.parse( decodeURIComponent( '%7B%22settings%22%3A%7B%22id%22%3A%22content%22%2C%22ajaxurl%22%3A%22https%3A%5C%2F%5C%2Fjohncarlosbaez.wordpress.com%5C%2F%3Finfinity%3Dscrolling%22%2C%22type%22%3A%22scroll%22%2C%22wrapper%22%3Atrue%2C%22wrapper_class%22%3A%22infinite-wrap%22%2C%22footer%22%3Atrue%2C%22click_handle%22%3A%221%22%2C%22text%22%3A%22Older%20posts%22%2C%22totop%22%3A%22Scroll%20back%20to%20top%22%2C%22currentday%22%3A%2212.10.10%22%2C%22order%22%3A%22DESC%22%2C%22scripts%22%3A%5B%5D%2C%22styles%22%3A%5B%5D%2C%22google_analytics%22%3Afalse%2C%22offset%22%3A9%2C%22history%22%3A%7B%22host%22%3A%22johncarlosbaez.wordpress.com%22%2C%22path%22%3A%22%5C%2Fcategory%5C%2Fprobability%5C%2Fpage%5C%2F%25d%5C%2F%22%2C%22use_trailing_slashes%22%3Atrue%2C%22parameters%22%3A%22%22%7D%2C%22query_args%22%3A%7B%22paged%22%3A9%2C%22category_name%22%3A%22probability%22%2C%22error%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A0%2C%22post_parent%22%3A%22%22%2C%22subpost%22%3A%22%22%2C%22subpost_id%22%3A%22%22%2C%22attachment%22%3A%22%22%2C%22attachment_id%22%3A0%2C%22name%22%3A%22%22%2C%22pagename%22%3A%22%22%2C%22page_id%22%3A0%2C%22second%22%3A%22%22%2C%22minute%22%3A%22%22%2C%22hour%22%3A%22%22%2C%22day%22%3A0%2C%22monthnum%22%3A0%2C%22year%22%3A0%2C%22w%22%3A0%2C%22tag%22%3A%22%22%2C%22cat%22%3A10451%2C%22tag_id%22%3A%22%22%2C%22author%22%3A%22%22%2C%22author_name%22%3A%22%22%2C%22feed%22%3A%22%22%2C%22tb%22%3A%22%22%2C%22meta_key%22%3A%22%22%2C%22meta_value%22%3A%22%22%2C%22preview%22%3A%22%22%2C%22s%22%3A%22%22%2C%22sentence%22%3A%22%22%2C%22title%22%3A%22%22%2C%22fields%22%3A%22%22%2C%22menu_order%22%3A%22%22%2C%22embed%22%3A%22%22%2C%22category__in%22%3A%5B%5D%2C%22category__not_in%22%3A%5B%5D%2C%22category__and%22%3A%5B%5D%2C%22post__in%22%3A%5B%5D%2C%22post__not_in%22%3A%5B%5D%2C%22post_name__in%22%3A%5B%5D%2C%22tag__in%22%3A%5B%5D%2C%22tag__not_in%22%3A%5B%5D%2C%22tag__and%22%3A%5B%5D%2C%22tag_slug__in%22%3A%5B%5D%2C%22tag_slug__and%22%3A%5B%5D%2C%22post_parent__in%22%3A%5B%5D%2C%22post_parent__not_in%22%3A%5B%5D%2C%22author__in%22%3A%5B%5D%2C%22author__not_in%22%3A%5B%5D%2C%22lazy_load_term_meta%22%3Afalse%2C%22posts_per_page%22%3A10%2C%22ignore_sticky_posts%22%3Afalse%2C%22suppress_filters%22%3Afalse%2C%22cache_results%22%3Afalse%2C%22update_post_term_cache%22%3Atrue%2C%22update_post_meta_cache%22%3Atrue%2C%22post_type%22%3A%22%22%2C%22nopaging%22%3Afalse%2C%22comments_per_page%22%3A%22100%22%2C%22no_found_rows%22%3Afalse%2C%22order%22%3A%22DESC%22%7D%2C%22query_before%22%3A%222021-09-26%2017%3A28%3A42%22%2C%22last_post_date%22%3A%222010-10-12%2009%3A52%3A09%22%2C%22body_class%22%3A%22infinite-scroll%20neverending%22%2C%22loading_text%22%3A%22Loading%20new%20page%22%2C%22stats%22%3A%22blog%3D12777403%26v%3Dwpcom%26tz%3D0%26user_id%3D0%26subd%3Djohncarlosbaez%26x_pagetype%3Dinfinite%22%7D%7D' ) );
		//]]>
		</script>
		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>
		<div id="infinite-footer">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://johncarlosbaez.wordpress.com/" rel="home">
						Azimuth					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a> 				</div>
			</div>
		</div><!-- #infinite-footer -->
		
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s1.wp.com/_static/??-eJyFy00OQDAQQOELGUP8xUKcpWoiZVTTadO4vVhY2LB8L/kwOdCHDWQD7hEcx8VYQa38EYUYJRlHHqZoZ6Zci2T4I1YKTukNnvGFgmGaYVHM5M933Wzch7Ktir5p+rpbL+lvP34=?cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2010%2F10%2F12%2Falgorithmic-thermodynamics%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s0.wp.com/_static/??-eJyNkN1uwyAMhV9oDkouKvVi2rMQcCNT/oZN07z9yNRMbTZFu8LH8J2DreYMJkXBKMqxsngjg/neOX5TT1ehQvZ1osiK4oUiyfJTHLzVNlCEURcVNAuWVoEUba68h1q2+6xYlsfRzdmkALmk+wIFW49lYygaXy3yCjWJYUTbtaCDj8xkJxRWXEc2hbJQivyXH8+XNDo0sjfbnG5kMSnNvNq5h84F+ddIT/FGl1QZvXIouY0PW+OAEfJoYdLer1t5Uf9Igu/97WTjPsJ7fxrO534YTr37Ai2cwB8='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			(function() {
				var extend = function(out) {
					out = out || {};

					for (var i = 1; i < arguments.length; i++) {
						if (!arguments[i])
						continue;

						for (var key in arguments[i]) {
						if (arguments[i].hasOwnProperty(key))
							out[key] = arguments[i][key];
						}
					}

					return out;
				};
				extend( window.infiniteScroll.settings.scripts, ["postmessage","mobile-useragent-info","rlt-proxy","jquery-core","jquery-migrate","jquery","wpcom-actionbar-placeholder","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","wpcom-masterbar-tracks-js","jquery.wpcom-proxy-request","wp-embed","jetpack-subscriptions-js","swfobject","videopress","jetpack-carousel","tiled-gallery","carousel-wpcom"] );
				extend( window.infiniteScroll.settings.styles, ["the-neverending-homepage","infinity-contempt","wp-block-library","mediaelement","wp-mediaelement","jetpack-layout-grid","jetpack-ratings","coblocks-frontend","wpcom-core-compat-playlist-styles","wpcom-text-widget-styles","wpcom-bbpress2-staff-css","contempt","geo-location-flair","reblogging","a8c-global-print","h4-global","global-styles","jetpack-global-styles-frontend-style","jetpack-carousel-swiper-css","jetpack-carousel","tiled-gallery"] );
			})();
		</script>
				<span id="infinite-aria" aria-live="polite"></span>
		<script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZneFhaUTVPOXd0MXRdUmJDYn54WlVLT0RuOVBZZTZsU2JSPV16TGlJPUgvOWFaOEd1cFpzaUZ0WFVQSC01U05UW0Jbd1R1NnVaUC1hU1pycT1LU3E3bHxrX1MrL2pSY3Q/RGtRcUt3SThXamVITlgrSzRkTStmL3FyfnV5c1ZkRUJrTV9BLGc3NUxffkklRTIyPSwrbDMwUXFlfFphfH4yQS1jUUp+cndoNjFuVW4vUkhOMVsvb2FIWW45XW9nLlk1VmZzJUh2'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>