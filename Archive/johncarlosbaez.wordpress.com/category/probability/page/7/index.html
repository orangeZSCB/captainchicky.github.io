<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>probability | Azimuth | Page 7</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; probability Category Feed" href="https://johncarlosbaez.wordpress.com/category/probability/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IlqVOdbxwfBYOEVOBZSBYeXuz1e6sVrveMDl9yU9AHrOwUyJIJGMTOTSPqUpMz5iQ+mIMttYbuVFMLxChytyMPJXFTBfcGfKNXQPFc6aAfFNq2A9KmobBSRMm+yoCmqJLl5V6gKURJhua4zGHKiM41BB46qxo5eSgOxQRwGvbh4hpG+fc2v8G/S3+pJSbAWU9S9Z9aiR8QfdD9r9bFE2YfN3A7fSF7QY18t4cVlqC4nd29WTzzjkes768+BXsiM4DMV7PtiB4v45kHiOMyQVqFXxGbFF8/pSZe4qP6u5W3T/sx3F3+ACgX/YH?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F08%2F21%2Fnetwork-theory-part-23%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/probability\/page\/7\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Fprobability%2Fpage%2F7%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F08%2F21%2Fnetwork-theory-part-23%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="probability &#8211; Page 7 &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/probability/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about probability written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive paged category category-probability category-10451 paged-7 category-paged-7 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-11611 post type-post status-publish format-standard hentry category-chemistry category-mathematics category-networks category-probability" id="post-11611">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/08/21/network-theory-part-23/" rel="bookmark">Network Theory (Part&nbsp;23)</a></h2>
				<small>21 August, 2012</small><br />


				<div class="entry">
					<p>We&#8217;ve been looking at reaction networks, and we&#8217;re getting ready to find equilibrium solutions of the equations they give.  To do this, we&#8217;ll need to connect them to another kind of network we&#8217;ve studied.   A reaction network is something like this:</p>
<div align="center"><img width="200" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/chemical_reaction_network_part_20_III.png" alt="" /></div>
<p>It&#8217;s a bunch of <b>complexes</b>, which are sums of basic building-blocks called <b>species</b>, together with arrows called <b>transitions</b> going between the complexes.  If we know a number  for each transition describing the rate at which it occurs, we get an equation called the &#8216;rate equation&#8217;.  This describes how the amount of each species changes with time.   We&#8217;ve been talking about this equation ever since the start of this series!   <a href="http://math.ucr.edu/home/baez/networks/networks_22.html">Last time</a>, we wrote it down in a new very compact form:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+Y+H+x%5EY++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x}{d t} = Y H x^Y  } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is a vector whose components are the amounts of each species, while <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> are certain matrices.</p>
<p>But now suppose we forget how each complex is made of species!  Suppose we just think of them as abstract things in their own right, like numbered boxes:</p>
<div align="center"><img width="200" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/markov_process_vs_reaction_network_1.png" alt="" /></div>
<p>We can use these boxes to describe <b>states</b> of some system.  The arrows still describe <b>transitions</b>, but now we think of these as ways for the system to hop from one state to another.   Say we know a number for each transition describing the probability per time at which it occurs:</p>
<div align="center"><img width="200" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/markov_process_vs_reaction_network_2.png" alt="" /></div>
<p>Then we get a &#8216;Markov process&#8217;&#8212;or in other words, a random walk where our system hops from one state to another.  If <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is the probability distribution saying how likely the system is to be in each state, this Markov process is described by this equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+%5Cpsi%7D%7Bd+t%7D+%3D+H+%5Cpsi++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d &#92;psi}{d t} = H &#92;psi  } " class="latex" /></p>
<p>This is simpler than the rate equation, because it&#8217;s linear.  But the matrix <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is the same&mdash;we&#8217;ll see that explicitly later on today.</p>
<p>What&#8217;s the point?  Well, our ultimate goal is to prove the deficiency zero theorem, which gives equilibrium solutions of the rate equation.  That means finding <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> with </p>
<p><img src="https://s0.wp.com/latex.php?latex=Y+H+x%5EY+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y H x^Y = 0 " class="latex" /></p>
<p>Today we&#8217;ll find all equilibria for the Markov process, meaning all <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi = 0 " class="latex" /></p>
<p>Then next time we&#8217;ll show some of these have the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%3D+x%5EY+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi = x^Y " class="latex" /></p>
<p>So, we&#8217;ll get </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+x%5EY+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H x^Y = 0 " class="latex" /></p>
<p>and thus</p>
<p><img src="https://s0.wp.com/latex.php?latex=Y+H+x%5EY+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y H x^Y = 0 " class="latex" /></p>
<p>as desired!</p>
<p>So, let&#8217;s get to to work.</p>
<h3> The Markov process of a graph with rates </h3>
<p>We&#8217;ve been looking at stochastic reaction networks, which are things like this:</p>
<div align="center">
<img src="https://i2.wp.com/math.ucr.edu/home/baez/networks/reaction_network_diagram_1.png" alt="" />
</div>
<p>However, we can build a Markov process starting from just part of this information:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/networks/markov_process_diagram_1.png" alt="" />
</div>
<p>Let&#8217;s call this thing a &#8216;graph with rates&#8217;, for lack of a better name.  We&#8217;ve been calling the things in <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> &#8216;complexes&#8217;, but now we&#8217;ll think of them as &#8216;states&#8217;.  So:</p>
<p><b>Definition.</b>  A <b>graph with rates</b> consists of:</p>
<p>&bull; a finite set of <b>states</b> <img src="https://s0.wp.com/latex.php?latex=K%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K," class="latex" /></p>
<p>&bull; a finite set of <b>transitions</b> <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /></p>
<p>&bull; a map <img src="https://s0.wp.com/latex.php?latex=r%3A+T+%5Cto+%280%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r: T &#92;to (0,&#92;infty)" class="latex" /> giving a <b>rate constant</b> for each transition,</p>
<p>&bull; <b>source</b> and <b>target</b> maps <img src="https://s0.wp.com/latex.php?latex=s%2Ct+%3A+T+%5Cto+K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s,t : T &#92;to K" class="latex" /> saying where each transition starts and ends.</p>
<p>Starting from this, we can get a <a href="http://math.ucr.edu/home/baez/networks/networks_11.html">Markov process</a> describing how a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> on our set of states will change with time.  As usual, this Markov process is described by a master equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+%5Cpsi%7D%7Bd+t%7D+%3D+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d &#92;psi}{d t} = H &#92;psi } " class="latex" /></p>
<p>for some Hamiltonian:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3A+%5Cmathbb%7BR%7D%5EK+%5Cto+%5Cmathbb%7BR%7D%5EK+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H : &#92;mathbb{R}^K &#92;to &#92;mathbb{R}^K " class="latex" /></p>
<p>What is this Hamiltonian, exactly?  Let&#8217;s think of it as a matrix where <img src="https://s0.wp.com/latex.php?latex=H_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{i j}" class="latex" /> is the probability per time for our system to hop from the state <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to the state <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />   This looks backwards, but don&#8217;t blame me&mdash;blame the guys who invented the usual conventions for matrix algebra.  Clearly if <img src="https://s0.wp.com/latex.php?latex=i+%5Cne+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;ne j" class="latex" /> this probability per time should be the sum of the rate constants of all transitions going from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+i+%5Cne+j+%5Cquad+%5CRightarrow+%5Cquad+H_%7Bi+j%7D+%3D++%5Csum_%7B%5Ctau%3A+j+%5Cto+i%7D+r%28%5Ctau%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ i &#92;ne j &#92;quad &#92;Rightarrow &#92;quad H_{i j} =  &#92;sum_{&#92;tau: j &#92;to i} r(&#92;tau) } " class="latex" /></p>
<p>where we write <img src="https://s0.wp.com/latex.php?latex=%5Ctau%3A+j+%5Cto+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau: j &#92;to i" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" /> is a transition with source <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> and target <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" /></p>
<p>Now, we saw in <a href="http://math.ucr.edu/home/baez/networks/networks_11.html">Part 11</a> that for a probability distribution to remain a probability distribution as it evolves in time according to the master equation, we need <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> to be <b>infinitesimal stochastic</b>: its off-diagonal entries must be nonnegative, and the sum of the entries in each column must be zero.  </p>
<p>The first condition holds already, and the second one tells us what the diagonal entries must be.  So, we&#8217;re basically done describing <img src="https://s0.wp.com/latex.php?latex=H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H." class="latex" />  But we can summarize it this way:</p>
<p><b>Puzzle 1.</b>  Think of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^K" class="latex" /> as the vector space consisting of finite linear combinations of elements <img src="https://s0.wp.com/latex.php?latex=%5Ckappa+%5Cin+K.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa &#92;in K." class="latex" />  Then show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++H+%5Ckappa+%3D+%5Csum_%7Bs%28%5Ctau%29+%3D+%5Ckappa%7D+r%28%5Ctau%29+%28t%28%5Ctau%29+-+s%28%5Ctau%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  H &#92;kappa = &#92;sum_{s(&#92;tau) = &#92;kappa} r(&#92;tau) (t(&#92;tau) - s(&#92;tau)) } " class="latex" /> </p>
<h3>  Equilibrium solutions of the master equation </h3>
<p>Now we&#8217;ll classify <b>equilibrium solutions</b> of the master equation, meaning <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{R}^K" class="latex" /> with </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi = 0 " class="latex" /></p>
<p>We&#8217;ll do only do this when our graph with rates is &#8216;weakly reversible&#8217;.  This concept doesn&#8217;t actually depend on the rates, so let&#8217;s be general and say:</p>
<p><b>Definition.</b> A graph is <b>weakly reversible</b> if for every edge <img src="https://s0.wp.com/latex.php?latex=%5Ctau+%3A+i+%5Cto+j%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau : i &#92;to j," class="latex" /> there is <b>directed path</b> going back from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> meaning that we have edges</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctau_1+%3A+j+%5Cto+j_1+%2C+%5Cquad+%5Ctau_2+%3A+j_1+%5Cto+j_2+%2C+%5Cquad+%5Cdots%2C+%5Cquad+%5Ctau_n%3A+j_%7Bn-1%7D+%5Cto+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau_1 : j &#92;to j_1 , &#92;quad &#92;tau_2 : j_1 &#92;to j_2 , &#92;quad &#92;dots, &#92;quad &#92;tau_n: j_{n-1} &#92;to i " class="latex" /></p>
<p>This graph with rates is <i>not</i> weakly reversible:</p>
<div align="center"><img width="200" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/markov_process_vs_reaction_network_2.png" alt="" /></div>
<p>but this one is:</p>
<div align="center"><img width="200" src="https://i1.wp.com/math.ucr.edu/home/baez/networks/markov_process_vs_reaction_network_3.png" alt="" /></div>
<p>The good thing about the weakly reversible case is that we get one equilibrium solution of the master equation for each component of our graph, and all equilibrium solutions are linear combinations of these.   This is <i>not</i> true in general!  For example, this guy is not weakly reversible:</p>
<div align="center"><img width="240" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/markov_process_not_weakly_reversible.png" alt="" /></div>
<p>It has only one component, but the master equation has two linearly independent equilibrium solutions: one that vanishes except at the state 0, and one that vanishes except at the state 2.  </p>
<p>The idea of a &#8216;component&#8217; is supposed to be fairly intuitive&#8212;our graph falls apart into pieces called components&#8212;but we should make it precise.  As explained in <a href="http://math.ucr.edu/home/baez/networks/networks_21.html">Part 21</a>, the graphs we&#8217;re using here are directed multigraphs, meaning things like</p>
<p><img src="https://s0.wp.com/latex.php?latex=s%2C+t+%3A+E+%5Cto+V++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s, t : E &#92;to V  " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> is the set of <b>edges</b> (our transitions) and <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> is the set of <b>vertices</b> (our states).    There are actually two famous concepts of &#8216;component&#8217; for graphs of this sort: &#8216;strongly connected&#8217; components and &#8216;connected&#8217; components.   We only need connected components, but let me explain both concepts, in a futile attempt to slake your insatiable thirst for knowledge.</p>
<p>Two vertices <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> of a graph lie in the same <b>strongly connected component</b> iff you can find a directed path of edges from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> and also one from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> back to <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />  </p>
<p>Remember, a directed path from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=i+%5Cto+a+%5Cto+b+%5Cto+c+%5Cto+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;to a &#92;to b &#92;to c &#92;to j " class="latex" /></p>
<p>Here&#8217;s a path from <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> that is not directed:</p>
<p><img src="https://s0.wp.com/latex.php?latex=i+%5Cto+a+%5Cleftarrow+b+%5Cto+c+%5Cto+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;to a &#92;leftarrow b &#92;to c &#92;to j " class="latex" /></p>
<p>and I hope you can write down the obvious but tedious definition of an &#8216;undirected path&#8217;, meaning a path made of edges that don&#8217;t necessarily point in the correct direction.   Given that, we say two vertices <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> lie in the same <b>connected component</b> iff you can find an <i>undirected</i> path going from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />  In this case, there will automatically also be an undirected path going from <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" /></p>
<p>For example, <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> lie in the same connected component here, but not the same strongly connected component:</p>
<p><img src="https://s0.wp.com/latex.php?latex=i+%5Cto+a+%5Cleftarrow+b+%5Cto+c+%5Cto+j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;to a &#92;leftarrow b &#92;to c &#92;to j " class="latex" /></p>
<p>Here&#8217;s a graph with one connected component and 3 strongly connected components, which are marked in blue:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/Strongly_connected_component"><img src="https://i2.wp.com/math.ucr.edu/home/baez/networks/strongly_connected_component.png" /></a></div>
<p>For the theory we&#8217;re looking at now, <i>we only care about connected components, not strongly connected components!</i>   However:</p>
<p><b>Puzzle 2.</b>  Show that for weakly reversible graph, the connected components are the same as the strongly connected components.  </p>
<p>With these definitions out of way, we can state today&#8217;s big theorem:</p>
<p><b>Theorem.</b>  Suppose <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is the Hamiltonian of a weakly reversible graph with rates:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/networks/markov_process_diagram_1.png" alt="" />
</div>
<p>Then for each connected component <img src="https://s0.wp.com/latex.php?latex=C+%5Csubseteq+K%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C &#92;subseteq K," class="latex" /> there exists a unique probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_C+%5Cin+%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_C &#92;in &#92;mathbb{R}^K" class="latex" /> that is positive on that component, zero elsewhere, and is an equilibrium solution of the master equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi_C+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi_C = 0 " class="latex" /></p>
<p>Moreover, these probability distributions <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_C" class="latex" /> form a basis for the space of equilibrium solutions of the master equation.  So, the dimension of this space is the number of components of <img src="https://s0.wp.com/latex.php?latex=K.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K." class="latex" /></p>
<p><i>Proof.</i>   We start by assuming our graph has one connected component.  We use the Perron&ndash;Frobenius theorem, as explained in <a href="http://math.ucr.edu/home/baez/networks/networks_20.html">Part 20</a>.  This applies to &#8216;nonnegative&#8217; matrices, meaning those whose entries are all nonnegative.  That is not true of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> itself, but only its diagonal entries can be negative, so if we choose a large enough number <img src="https://s0.wp.com/latex.php?latex=c+%3E+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &gt; 0," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=H+%2B+c+I&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H + c I" class="latex" /> will be nonnegative.  </p>
<p>Since our graph is weakly reversible and has one connected component, it follows straight from the definitions that the operator <img src="https://s0.wp.com/latex.php?latex=H+%2B+c+I&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H + c I" class="latex" /> will also be &#8216;irreducible&#8217; in the sense of <a href="http://math.ucr.edu/home/baez/networks/networks_20.html">Part 20</a>.  The Perron&ndash;Frobenius theorem then swings into action, and we instantly conclude several things.</p>
<p>First, <img src="https://s0.wp.com/latex.php?latex=H+%2B+c+I&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H + c I" class="latex" /> has a positive real eigenvalue <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> such that any other eigenvalue, possibly complex, has absolute value <img src="https://s0.wp.com/latex.php?latex=%5Cle+r.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;le r." class="latex" />  Second, there is an eigenvector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> with eigenvalue <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> and all positive components.  Third, any other eigenvector with eigenvalue <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> is a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" />  </p>
<p>Subtracting <img src="https://s0.wp.com/latex.php?latex=c%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c," class="latex" /> it follows that <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+r+-+c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = r - c" class="latex" /> is the eigenvalue of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> with the largest real part.  We have <img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi+%3D+%5Clambda+%5Cpsi%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi = &#92;lambda &#92;psi," class="latex" /> and any other vector with this property is a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" />   </p>
<p>We can show that in fact <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = 0." class="latex" />  To do this we copy an argument from <a href="http://math.ucr.edu/home/baez/networks/networks_20.html">Part 20</a>.   First, since <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is positive we can normalize it to be a probability distribution:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+K%7D+%5Cpsi_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in K} &#92;psi_i = 1 } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is infinitesimal stochastic, <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(t H)" class="latex" /> sends probability distributions to probability distributions:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+K%7D+%28%5Cexp%28t+H%29+%5Cpsi%29_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in K} (&#92;exp(t H) &#92;psi)_i = 1 } " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=t+%5Cge+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;ge 0." class="latex" />  On the other hand,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi+%5Cin+K%7D+%28%5Cexp%28t+H%29%5Cpsi%29_i+%3D+%5Csum_%7Bi+%5Cin+K%7D+e%5E%7Bt+%5Clambda%7D+%5Cpsi_i+%3D+e%5E%7Bt+%5Clambda%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i &#92;in K} (&#92;exp(t H)&#92;psi)_i = &#92;sum_{i &#92;in K} e^{t &#92;lambda} &#92;psi_i = e^{t &#92;lambda} } " class="latex" /></p>
<p>so we must have <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = 0." class="latex" />  </p>
<p>We conclude that when our graph has one connected component, there is a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{R}^K" class="latex" /> that is positive everywhere and has <img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi = 0." class="latex" />  Moreover, any <img src="https://s0.wp.com/latex.php?latex=%5Cphi+%5Cin+%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi &#92;in &#92;mathbb{R}^K" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=H+%5Cphi+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;phi = 0" class="latex" /> is a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" /></p>
<p>When <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> has several components, the matrix <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is block diagonal, with one block for each component.  So, we can run the above argument on each component <img src="https://s0.wp.com/latex.php?latex=C+%5Csubseteq+K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C &#92;subseteq K" class="latex" /> and get a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_C+%5Cin+%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_C &#92;in &#92;mathbb{R}^K" class="latex" /> that is positive on <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" />  We can then check that <img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi_C+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi_C = 0" class="latex" /> and that every <img src="https://s0.wp.com/latex.php?latex=%5Cphi+%5Cin+%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi &#92;in &#92;mathbb{R}^K" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=H+%5Cphi+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;phi = 0" class="latex" /> can be expressed as a linear combination of these probability distributions <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_C" class="latex" /> in a unique way.   &nbsp;  &#9608;</p>
<p>This result must be absurdly familiar to people who study Markov processes, but I haven&#8217;t bothered to look up a reference yet.  Do you happen to know a good one?  I&#8217;d like to see one that generalizes this theorem to graphs that aren&#8217;t weakly reversible.  I think I see how it goes.  We don&#8217;t need that generalization right now, but it would be good to have around.</p>
<h3> The Hamiltonian, revisited </h3>
<p>One last small piece of business: <a href="http://math.ucr.edu/home/baez/networks/networks_22.html">last time</a> I showed you a very slick formula for the Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H." class="latex" />  I&#8217;d like to prove it agrees with the formula I gave this time.</p>
<p>We start with any graph with rates:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/networks/markov_process_diagram_1.png" alt="" />
</div>
<p>We extend <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> to linear maps between vector spaces:</p>
<div align="center">
<img src="https://i0.wp.com/math.ucr.edu/home/baez/networks/markov_process_diagram_4.png" /></div>
<p>We define the <b>boundary operator</b> just as we did last time:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial+%3D+t+-+s+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial = t - s " class="latex" /></p>
<p>Then we put an inner product on the vector spaces <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5ET&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^T" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5EK.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^K." class="latex" />  So, for <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5EK&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^K" class="latex" /> we let the elements of <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> be an orthonormal basis, but for <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5ET&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^T" class="latex" /> we define the inner product in a more clever way involving the rate constants:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Ctau%2C+%5Ctau%27+%5Crangle+%3D+%5Cfrac%7B1%7D%7Br%28%5Ctau%29%7D+%5Cdelta_%7B%5Ctau%2C+%5Ctau%27%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;tau, &#92;tau&#039; &#92;rangle = &#92;frac{1}{r(&#92;tau)} &#92;delta_{&#92;tau, &#92;tau&#039;} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Ctau%2C+%5Ctau%27+%5Cin+T.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau, &#92;tau&#039; &#92;in T." class="latex" />  This lets us define adjoints of the maps <img src="https://s0.wp.com/latex.php?latex=s%2C+t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s, t" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpartial%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial," class="latex" /> via formulas like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+s%5E%5Cdagger+%5Cphi%2C+%5Cpsi+%5Crangle+%3D+%5Clangle+%5Cphi%2C+s+%5Cpsi+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle s^&#92;dagger &#92;phi, &#92;psi &#92;rangle = &#92;langle &#92;phi, s &#92;psi &#92;rangle " class="latex" /></p>
<p>Then:</p>
<p><b>Theorem.</b>  The Hamiltonian for a graph with rates is given by </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cpartial+s%5E%5Cdagger+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;partial s^&#92;dagger " class="latex" /></p>
<p><i>Proof.</i>  It suffices to check that this formula agrees with the formula for <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> given in Puzzle 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+++H+%5Ckappa+%3D+%5Csum_%7Bs%28%5Ctau%29+%3D+%5Ckappa%7D+r%28%5Ctau%29+%28t%28%5Ctau%29+-+s%28%5Ctau%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{   H &#92;kappa = &#92;sum_{s(&#92;tau) = &#92;kappa} r(&#92;tau) (t(&#92;tau) - s(&#92;tau)) } " class="latex" /> </p>
<p>Here we are using the complex <img src="https://s0.wp.com/latex.php?latex=%5Ckappa+%5Cin+K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa &#92;in K" class="latex" /> as a name for one of the standard basis vectors of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5EK.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^K." class="latex" />   Similarly shall we write things like <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Ctau%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau&#039;" class="latex" /> for basis vectors of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5ET.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^T." class="latex" /></p>
<p>First, we claim that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+s%5E%5Cdagger+%5Ckappa+%3D+%5Csum_%7B%5Ctau%3A+%5C%3B+s%28%5Ctau%29+%3D+%5Ckappa%7D+r%28%5Ctau%29+%5C%2C+%5Ctau+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ s^&#92;dagger &#92;kappa = &#92;sum_{&#92;tau: &#92;; s(&#92;tau) = &#92;kappa} r(&#92;tau) &#92;, &#92;tau } " class="latex" /></p>
<p>To prove this it&#8217;s enough to check that taking the inner products of either sides with any basis vector <img src="https://s0.wp.com/latex.php?latex=%5Ctau%27%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau&#039;," class="latex" /> we get results that agree.  On the one hand:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Clangle+%5Ctau%27+%2C+s%5E%5Cdagger+%5Ckappa+%5Crangle+%26%3D%26+++%5Clangle+s+%5Ctau%27%2C+%5Ckappa+%5Crangle+%5C%5C++%5C%5C++%26%3D%26+%5Cdelta_%7Bs%28%5Ctau%27%29%2C+%5Ckappa%7D++++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;langle &#92;tau&#039; , s^&#92;dagger &#92;kappa &#92;rangle &amp;=&amp;   &#92;langle s &#92;tau&#039;, &#92;kappa &#92;rangle &#92;&#92;  &#92;&#92;  &amp;=&amp; &#92;delta_{s(&#92;tau&#039;), &#92;kappa}    &#92;end{array} " class="latex" /></p>
<p>On the other hand:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Clangle+%5Ctau%27%2C+%5Csum_%7B%5Ctau%3A+%5C%3B+s%28%5Ctau%29+%3D+%5Ckappa%7D+r%28%5Ctau%29+%5C%2C+%5Ctau+%5Crangle+%7D+%26%3D%26++%5Csum_%7B%5Ctau%3A+%5C%3B+s%28%5Ctau%29+%3D+%5Ckappa%7D+r%28%5Ctau%29+%5C%2C+%5Clangle+%5Ctau%27%2C+%5Ctau+%5Crangle+++%5C%5C++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7B%5Ctau%3A+%5C%3B+s%28%5Ctau%29+%3D+%5Ckappa%7D+%5Cdelta_%7B%5Ctau%27%2C+%5Ctau%7D+%7D+++%5C%5C++%5C%5C++%26%3D%26+++%5Cdelta_%7Bs%28%5Ctau%27%29%2C+%5Ckappa%7D+++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;langle &#92;tau&#039;, &#92;sum_{&#92;tau: &#92;; s(&#92;tau) = &#92;kappa} r(&#92;tau) &#92;, &#92;tau &#92;rangle } &amp;=&amp;  &#92;sum_{&#92;tau: &#92;; s(&#92;tau) = &#92;kappa} r(&#92;tau) &#92;, &#92;langle &#92;tau&#039;, &#92;tau &#92;rangle   &#92;&#92;  &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_{&#92;tau: &#92;; s(&#92;tau) = &#92;kappa} &#92;delta_{&#92;tau&#039;, &#92;tau} }   &#92;&#92;  &#92;&#92;  &amp;=&amp;   &#92;delta_{s(&#92;tau&#039;), &#92;kappa}   &#92;end{array} " class="latex" /></p>
<p>where the factor of <img src="https://s0.wp.com/latex.php?latex=1%2Fr%28%5Ctau%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/r(&#92;tau)" class="latex" /> in the inner product on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5ET&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^T" class="latex" /> cancels the visible factor of <img src="https://s0.wp.com/latex.php?latex=r%28%5Ctau%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r(&#92;tau)." class="latex" />    So indeed the results match.</p>
<p>Using this formula for <img src="https://s0.wp.com/latex.php?latex=s%5E%5Cdagger+%5Ckappa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s^&#92;dagger &#92;kappa" class="latex" />  we now see that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++H+%5Ckappa+%26%3D%26+%5Cpartial+s%5E%5Cdagger+%5Ckappa+++%5C%5C++%5C%5C++%26%3D%26+%5Cpartial+%5Cdisplaystyle%7B+%5Csum_%7B%5Ctau%3A+%5C%3B+s%28%5Ctau%29+%3D+%5Ckappa%7D+r%28%5Ctau%29+%5C%2C+%5Ctau+%7D++++%5C%5C++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7B%5Ctau%3A+%5C%3B+s%28%5Ctau%29+%3D+%5Ckappa%7D+r%28%5Ctau%29+%5C%2C+%28t%28%5Ctau%29+-+s%28%5Ctau%29%29+%7D++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  H &#92;kappa &amp;=&amp; &#92;partial s^&#92;dagger &#92;kappa   &#92;&#92;  &#92;&#92;  &amp;=&amp; &#92;partial &#92;displaystyle{ &#92;sum_{&#92;tau: &#92;; s(&#92;tau) = &#92;kappa} r(&#92;tau) &#92;, &#92;tau }    &#92;&#92;  &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_{&#92;tau: &#92;; s(&#92;tau) = &#92;kappa} r(&#92;tau) &#92;, (t(&#92;tau) - s(&#92;tau)) }  &#92;end{array} " class="latex" /></p>
<p>which is precisely what we want.  &nbsp;  &#9608;</p>
<p>I hope you see through the formulas to their intuitive meaning.  As usual, the formulas are just a way of precisely saying something that makes plenty of sense.  If <img src="https://s0.wp.com/latex.php?latex=%5Ckappa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa" class="latex" /> is some state of our Markov process, <img src="https://s0.wp.com/latex.php?latex=s%5E%5Cdagger+%5Ckappa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s^&#92;dagger &#92;kappa" class="latex" /> is the sum of all transitions starting at this state, weighted by their rates.  Applying <img src="https://s0.wp.com/latex.php?latex=%5Cpartial&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial" class="latex" /> to a transition tells us what change in state it causes.  So <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+s%5E%5Cdagger+%5Ckappa&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial s^&#92;dagger &#92;kappa" class="latex" /> tells us the rate at which things change when we start in the state <img src="https://s0.wp.com/latex.php?latex=%5Ckappa.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;kappa." class="latex" /> That&#8217;s why <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+s%5E%5Cdagger&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial s^&#92;dagger" class="latex" /> is the Hamiltonian for our Markov process.  After all, the Hamiltonian tells us how things change:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+%5Cpsi%7D%7Bd+t%7D+%3D+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d &#92;psi}{d t} = H &#92;psi } " class="latex" /></p>
<p>Okay, we&#8217;ve got all the machinery in place.  Next time we&#8217;ll prove the deficiency zero theorem!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/21/network-theory-part-23/#comments">9 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/chemistry/" rel="category tag">chemistry</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/21/network-theory-part-23/" rel="bookmark" title="Permanent Link to Network Theory (Part&nbsp;23)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-11311 post type-post status-publish format-standard hentry category-mathematics category-physics category-probability" id="post-11311">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/08/06/network-theory-part-20/" rel="bookmark">Network Theory (Part&nbsp;20)</a></h2>
				<small>6 August, 2012</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://www.azimuthproject.org/azimuth/show/Jacob+Biamonte">Jacob Biamonte</a></b></i></p>
<p>We&#8217;re in the middle of a battle: in addition to our typical man versus equation scenario, it&#8217;s a battle between two theories.  For those good patrons following the <a href="http://math.ucr.edu/home/baez/networks/">network theory series</a>, you know the two opposing forces well.  It&#8217;s our old friends, at it again:</p>
<div align="center">
<i> Stochastic Mechanics vs Quantum Mechanics! </i>
</div>
<p>Today we&#8217;re reporting live from a crossroads, and we&#8217;re facing a skirmish that gives rise to what some might consider a paradox.  Let me sketch the main thesis before we get our hands dirty with the gory details.   </p>
<p>First I need to tell you that the battle takes place at the intersection of stochastic and quantum mechanics.  We recall from <a href="https://johncarlosbaez.wordpress.com/2011/11/04/network-theory-part-16/">Part 16</a> that there is a class of operators called &#8216;Dirichlet operators&#8217; that are valid Hamiltonians for both stochastic and quantum mechanics.  In other words, you can use them to generate time evolution both for old-fashioned random processes and for quantum processes! </p>
<p>Staying inside this class allows the theories to fight it out on the same turf.  We will be considering a special subclass of Dirichlet operators, which we call &#8216;irreducible Dirichlet operators&#8217;.  These are the ones where starting in any state in our favorite basis of states, we have a nonzero chance of winding up in any other.   When considering this subclass, we found something interesting: </p>
<p><b>Thesis.</b> Let <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> be an irreducible Dirichlet operator with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> eigenstates.  In stochastic mechanics, there is only one valid state that is an eigenvector of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" />: the unique so-called &#8216;Perron&#8211;Frobenius state&#8217;.  The other <img src="https://s0.wp.com/latex.php?latex=n-1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n-1" class="latex" /> eigenvectors are forbidden states of a stochastic system: the stochastic system is either in the Perron&#8211;Frobenius state, or in a superposition of at least two eigensvectors.  In quantum mechanics, all <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> eigenstates of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> are valid states.  </p>
<p>This might sound like a riddle, but today as we&#8217;ll prove, riddle or not, it&#8217;s a fact.  If it makes sense, well that&#8217;s another issue.  As John might have said, it&#8217;s like a bone kicked down from the gods up above: we can either choose to chew on it, or let it be.  Today we are going to do a bit of chewing.  </p>
<p>One of the many problems with this post is that John had <i>a nut loose on his keyboard</i>.  It was not broken!  I&#8217;m saying he wrote enough blog posts on this stuff to turn them into a book.  I&#8217;m supposed to be compiling the blog articles into a massive LaTeX file, but I wrote this instead.  </p>
<p>Another problem is that this post somehow seems to use just about everything said before, so I&#8217;m going to have to do my best to make things self-contained.  Please bear with me as I try to recap what&#8217;s been done.  For those of you familiar with the series, a good portion of the background for what we&#8217;ll cover today can be found in <a href="https://johncarlosbaez.wordpress.com/2011/10/09/network-theory-part-12/">Part 12</a> and <a href="https://johncarlosbaez.wordpress.com/2011/11/04/network-theory-part-16/">Part 16</a>.</p>
<h3> At the intersection of two theories </h3>
<p>As John has mentioned in his <a href="http://math.ucr.edu/home/baez/prob/">recent talks</a>, the <i>typical</i> view of how quantum mechanics and probability theory come into contact looks like this:</p>
<div align="center">
<img width="400" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/quantum-vs-probability-theory-I.png" />
</div>
<p>The idea is that quantum theory generalizes classical probability theory by considering observables that don&#8217;t commute.</p>
<p>That&#8217;s perfectly valid, but we&#8217;ve been exploring an alternative view in this series.  Here quantum theory doesn&#8217;t subsume probability theory, but they intersect:</p>
<div align="center">
<img width="400" src="https://i1.wp.com/math.ucr.edu/home/baez/networks/quantum-vs-probability-theory-III.png" />
</div>
<p>What goes in the middle you might ask?  As odd as it might sound at first, John showed in <a href="https://johncarlosbaez.wordpress.com/2011/11/04/network-theory-part-16/">Part 16</a> that electrical circuits made of resistors constitute the intersection!    </p>
<div align="center">
<img width="400" src="https://i0.wp.com/math.ucr.edu/home/baez/networks/quantum-vs-probability-theory-IV.png" />
</div>
<p>For example, a circuit like this:</p>
<div align="center">
<img width="400" src="https://i1.wp.com/math.ucr.edu/home/baez/networks/resistor-graph-5.png" />
</div>
<p>gives rise to a Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> that&#8217;s good both for stochastic mechanics and quantum mechanics.   Indeed, he found that the power dissipated by a circuit made of resistors is related to the familiar quantum theory concept known as the expectation value of the Hamiltonian!</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7Bpower%7D+%3D+-2+%5Clangle+%5Cpsi%2C+H+%5Cpsi+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;textrm{power} = -2 &#92;langle &#92;psi, H &#92;psi &#92;rangle " class="latex" /> </p>
<p>Oh&#8212;and you might think we made a mistake and wrote our  (ohm) symbols upside down.  We didn&#8217;t.  It happens that  is the symbol for a mho&#8212;a unit of conductance thats the reciprocal of an ohm. Check out <a href="https://johncarlosbaez.wordpress.com/2011/11/04/network-theory-part-16/">Part 16</a> for the details.  </p>
<h3> Stochastic mechanics versus quantum mechanics </h3>
<p>Let&#8217;s recall how states, time evolution, symmetries and observables work in the two theories.     Today we&#8217;ll fix a basis for our vector space of states, and we&#8217;ll assume it&#8217;s finite-dimensional so that all vectors have <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> components over either the complex numbers <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{C}" class="latex" /> or the reals <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}." class="latex" />   In other words, we&#8217;ll treat our space as either <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{C}^n" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n." class="latex" />  In this fashion, linear operators that map such spaces to themselves will be represented as square matrices.  </p>
<p>Vectors will be written as <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i" class="latex" /> where the index <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> runs from 1 to <img src="https://s0.wp.com/latex.php?latex=n%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n," class="latex" /> and we think of each choice of the index as a <b>state</b> of our system&#8212;but since we&#8217;ll be using that word in other ways too, let&#8217;s call it a <b>configuration</b>.  It&#8217;s just a basic way our system can be.</p>
<h4> States </h4>
<p>Besides the configurations <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C%5Cdots%2C+n%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,&#92;dots, n," class="latex" /> we have more general states that tell us the probability or amplitude of finding our system in one of these configurations:</p>
<p>&bull; <b>Stochastic states</b> are <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-tuples of nonnegative real numbers:  </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i+%5Cin+%5Cmathbb%7BR%7D%5E%2B+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i &#92;in &#92;mathbb{R}^+ " class="latex" /></p>
<p>The probability of finding the system in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th configuration is defined to be <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i." class="latex" />  For these probabilities to sum to one, <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i" class="latex" /> needs to be normalized like this: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+%5Cpsi_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i &#92;psi_i = 1 } " class="latex" /></p>
<p>or in the notation we&#8217;re using in these articles:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Cpsi+%5Crangle+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;psi &#92;rangle = 1 } " class="latex" /></p>
<p>where we define</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Cpsi+%5Crangle+%3D+%5Csum_i+%5Cpsi_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;psi &#92;rangle = &#92;sum_i &#92;psi_i } " class="latex" /></p>
<p>&bull; <b>Quantum states</b> are <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-tuples of complex numbers:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i+%5Cin+%5Cmathbb%7BC%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i &#92;in &#92;mathbb{C} " class="latex" /></p>
<p>The probability of finding a state in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th configuration is defined to be <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%28x%29%7C%5E2.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|&#92;psi(x)|^2." class="latex" />  For these probabilities to sum to one, <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> needs to be normalized like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+%7C%5Cpsi_i%7C%5E2+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i |&#92;psi_i|^2 = 1 }" class="latex" /></p>
<p>or in other words </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi%2C++%5Cpsi+%5Crangle+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi,  &#92;psi &#92;rangle = 1 " class="latex" /></p>
<p>where the <b>inner product</b> of two vectors <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Cpsi%2C+%5Cphi+%5Crangle+%3D+%5Csum_i+%5Coverline%7B%5Cpsi%7D_i+%5Cphi_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;psi, &#92;phi &#92;rangle = &#92;sum_i &#92;overline{&#92;psi}_i &#92;phi_i } " class="latex" /></p>
<p>Now, the usual way to turn a quantum state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> into a stochastic state is to take the absolute value of each number <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i" class="latex" /> and then square it.  However, if the numbers <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i" class="latex" /> happen to be nonnegative, we can also turn <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> into a stochastic state simply by multiplying it by a number to ensure <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%5Crangle+%3D+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi &#92;rangle = 1." class="latex" /></p>
<p>This is very unorthodox, but it lets us evolve the same vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> either stochastically or quantum-mechanically, using the recipes I&#8217;ll describe next.  In physics jargon these correspond to evolution in &#8216;real time&#8217; and &#8216;imaginary time&#8217;.  But don&#8217;t ask me which is which: from a quantum viewpoint stochastic mechanics uses imaginary time, but from a stochastic viewpoint it&#8217;s the other way around! </p>
<h4> Time evolution </h4>
<p>Time evolution works similarly in stochastic and quantum mechanics, but with a few big differences:</p>
<p>&bull; In stochastic mechanics the state changes in time according to the <b>master equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = H &#92;psi(t) } " class="latex" /></p>
<p>which has the solution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29+%3D+%5Cexp%28t+H%29+%5Cpsi%280%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t) = &#92;exp(t H) &#92;psi(0) " class="latex" /></p>
<p>&bull;  In quantum mechanics the state changes in time according to <b>Schr&ouml;dinger&#8217;s equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+%5Cpsi%28t%29+%3D+-i+H+%5Cpsi%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} &#92;psi(t) = -i H &#92;psi(t) } " class="latex" /></p>
<p>which has the solution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28t%29+%3D+%5Cexp%28-i+t+H%29+%5Cpsi%280%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi(t) = &#92;exp(-i t H) &#92;psi(0) " class="latex" /></p>
<p>The operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is called the <b>Hamiltonian</b>.  The properties it must have depend on whether we&#8217;re doing stochastic mechanics or quantum mechanics:</p>
<p>&bull;  We need <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> to be <b>infinitesimal stochastic</b> for time evolution given by <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(t H)" class="latex" /> to send stochastic states to stochastic states.  In other words, we need that (i) its columns sum to zero and (ii) its off-diagonal entries are real and nonnegative:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+H_%7Bi+j%7D%3D0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i H_{i j}=0 } " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j+%5CRightarrow+H_%7Bi+j%7D%5Cgeq+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i&#92;neq j &#92;Rightarrow H_{i j}&#92;geq 0 " class="latex" /></p>
<p>&bull; We need <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> to be <b>self-adjoint</b> for time evolution given by <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-i+t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-i t H)" class="latex" /> to send quantum states to quantum states.  So, we need</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+H%5E%5Cdagger+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = H^&#92;dagger " class="latex" /></p>
<p>where we recall that the adjoint of a matrix is the conjugate of its transpose:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%28H%5E%5Cdagger%29_%7Bi+j%7D+%3A%3D+%5Coverline%7BH%7D_%7Bj+i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ (H^&#92;dagger)_{i j} := &#92;overline{H}_{j i} } " class="latex" /></p>
<p>We are concerned with the case where the operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> generates both a valid quantum evolution and also a valid stochastic one:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is a <b>Dirichlet operator</b> if it&#8217;s both self-adjoint and infinitesimal stochastic.</p>
<p>We will soon go further and zoom in on this intersection!  But first let&#8217;s finish our review.  </p>
<h4> Symmetries </h4>
<p>As John explained in <a href="https://johncarlosbaez.wordpress.com/2011/10/09/network-theory-part-12/">Part 12</a>, besides states and observables we need symmetries, which are transformations that map states to states.  These include the evolution operators which we only briefly discussed in the preceding subsection.</p>
<p>&bull;  A linear map <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> that sends quantum states to quantum states is called an  <b>isometry</b>, and isometries are characterized by this property:</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%5E%5Cdagger+U+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U^&#92;dagger U = 1" class="latex" /></p>
<p>&bull; A linear map <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> that sends stochastic states to stochastic states is called a <b>stochastic operator</b>, and stochastic operators are characterized by these properties:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+U_%7Bi+j%7D+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i U_{i j} = 1 } " class="latex" /></p>
<p>and </p>
<p><img src="https://s0.wp.com/latex.php?latex=U_%7Bi+j%7D+%5Cgeq+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U_{i j} &#92;geq 0 " class="latex" /></p>
<p>A notable difference here is that in our finite-dimensional situation, isometries are always invertible, but stochastic operators may not be!  If <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix that&#8217;s an isometry, <img src="https://s0.wp.com/latex.php?latex=U%5E%5Cdagger&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U^&#92;dagger" class="latex" /> is its inverse.  So, we also have</p>
<p><img src="https://s0.wp.com/latex.php?latex=U+U%5E%5Cdagger+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U U^&#92;dagger = 1" class="latex" /></p>
<p>and we say <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is <b>unitary</b>.  But if <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is stochastic, it may not have an inverse&#8212;and even if it does, its inverse is rarely stochastic.   This explains why in stochastic mechanics time evolution is often not reversible, while in quantum mechanics it always is.</p>
<p><b>Puzzle 1.</b> Suppose <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is a stochastic <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix whose inverse is stochastic.  What are the possibilities for <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" />?</p>
<p>It is quite hard for an operator to be a symmetry in both stochastic and quantum mechanics, especially in our finite-dimensional situation:</p>
<p><b>Puzzle 2.</b> Suppose <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix that is both stochastic and unitary.  What are the possibilities for <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" />?</p>
<h4> Observables </h4>
<p>&#8216;Observables&#8217; are real-valued quantities that can be measured, or predicted, given a specific theory.  </p>
<p>&bull; In quantum mechanics, an observable is given by a self-adjoint matrix <img src="https://s0.wp.com/latex.php?latex=O%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O," class="latex" /> and the expected value of the observable <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> in the quantum state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Cpsi+%2C+O+%5Cpsi+%5Crangle+%3D+%5Csum_%7Bi%2Cj%7D+%5Coverline%7B%5Cpsi%7D_i+O_%7Bi+j%7D+%5Cpsi_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;psi , O &#92;psi &#92;rangle = &#92;sum_{i,j} &#92;overline{&#92;psi}_i O_{i j} &#92;psi_j } " class="latex" /> </p>
<p>&bull; In stochastic mechanics, an observable <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> has a value <img src="https://s0.wp.com/latex.php?latex=O_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O_i" class="latex" /> in each configuration <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> and the expected value of the observable <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> in the stochastic state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+O+%5Cpsi+%5Crangle+%3D+%5Csum_i+O_i+%5Cpsi_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle O &#92;psi &#92;rangle = &#92;sum_i O_i &#92;psi_i } " class="latex" /></p>
<p>We can turn an observable in stochastic mechanics into an observable in quantum mechanics by making a diagonal matrix whose diagonal entries are the numbers <img src="https://s0.wp.com/latex.php?latex=O_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O_i." class="latex" /></p>
<h3> From graphs to matrices </h3>
<p>Back in <a href="https://johncarlosbaez.wordpress.com/2011/11/04/network-theory-part-16/">Part 16</a>, John explained how a graph with positive numbers on its edges gives rise to a Hamiltonian in both quantum and stochastic mechanics&#8212;in other words, a Dirichlet operator.</p>
<p>Here&#8217;s how this works.  We&#8217;ll consider <b><a href="http://en.wikipedia.org/wiki/Simple_graph#Simple_graph">simple graphs</a></b>: graphs without arrows on their edges, with at most one edge from one vertex to another, with no edges from a vertex to itself.  And we&#8217;ll only look at graphs with finitely many vertices and edges.  We&#8217;ll assume each edge is labelled by a positive number, like this:</p>
<div align="center"><img width="300" src="https://i1.wp.com/math.ucr.edu/home/baez/networks/complete-graph-5-zero.png" alt="" /></div>
<p>If our graph has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> vertices, we can create an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=A_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{i j}" class="latex" /> is the number labelling the edge from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j," class="latex" /> if there is such an edge, and 0 if there&#8217;s not.  This matrix is symmetric, with real entries, so it&#8217;s self-adjoint.   So <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is a valid Hamiltonian in quantum mechanics.  </p>
<p>How about stochastic mechanics?   Remember that a Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> in stochastic mechanics needs to be &#8216;infinitesimal stochastic&#8217;.  So, its off-diagonal entries must be nonnegative, which is indeed true for our <img src="https://s0.wp.com/latex.php?latex=A%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A," class="latex" /> but also the sums of its columns must be zero, which is not true when our <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is nonzero.</p>
<p>But now comes the best news you&#8217;ve heard all day: we can improve <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> to a stochastic operator in a way that is completely determined by <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> itself!  This is done by subtracting a diagonal matrix <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> whose entries are the sums of the columns of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=L_%7Bi+i%7D+%3D+%5Csum_i+A_%7Bi+j%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L_{i i} = &#92;sum_i A_{i j} " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=i+%5Cne+j+%5CRightarrow+L_%7Bi+j%7D+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;ne j &#92;Rightarrow L_{i j} = 0 " class="latex" /></p>
<p>It&#8217;s easy to check that</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+A+-+L+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = A - L " class="latex" /></p>
<p>is still self-adjoint, but now also infinitesimal stochastic.  So, it&#8217;s a <i>Dirichlet operator</i>: a good Hamiltonian for <i>both</i> stochastic and quantum mechanics!  </p>
<p>In <a href="https://johncarlosbaez.wordpress.com/2011/11/04/network-theory-part-16/">Part 16</a>, we saw a bit more: <i>every</i> Dirichlet operator arises this way.  It&#8217;s easy to see.  You just take your Dirichlet operator and make a graph with one edge for each nonzero off-diagonal entry.  Then you label the edge with this entry.</p>
<p>So, Dirichlet operators are essentially the same as finite simple graphs with edges labelled by positive numbers. </p>
<p>Now, a simple graph can consist of many separate &#8216;pieces&#8217;, called <b>components</b>.  Then there&#8217;s no way for a particle hopping along the edges to get from one component to another, either in stochastic or quantum mechanics.  So we might as well focus our attention on graphs with just one component.  These graphs are called &#8216;connected&#8217;.  In other words:</p>
<p><b>Definition.</b> A simple graph is <b>connected</b> if it is nonempty and there is a path of edges connecting any vertex to any other.  </p>
<p>Our goal today is to understand more about Dirichlet operators coming from connected graphs.  For this we need to learn the Perron&#8211;Frobenius theorem.  But let&#8217;s start with something easier.</p>
<h3> Perron&#8217;s theorem </h3>
<p>In quantum mechanics it&#8217;s good to think about observables that have positive expected values:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi%2C+O+%5Cpsi+%5Crangle+%3E+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;psi, O &#92;psi &#92;rangle &gt; 0 " class="latex" /></p>
<p>for every quantum state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BC%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{C}^n." class="latex" />  These are called <b>positive definite</b>.  But in stochastic mechanics it&#8217;s good to think about matrices that are positive in a more naive sense:</p>
<p><b>Definition.</b> An <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> real matrix <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is <b>positive</b> if all its entries are positive: </p>
<p><img src="https://s0.wp.com/latex.php?latex=T_%7Bi+j%7D+%3E+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_{i j} &gt; 0 " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+i%2C+j+%5Cle+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;le i, j &#92;le n." class="latex" /></p>
<p>Similarly: </p>
<p><b>Definition.</b> A vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{R}^n" class="latex" /> is <b>positive</b> if all its components are positive:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_i+%3E+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_i &gt; 0 " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+i+%5Cle+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;le i &#92;le n." class="latex" /></p>
<p>We&#8217;ll also define <b>nonnegative</b> matrices and vectors in the same way, replacing <img src="https://s0.wp.com/latex.php?latex=%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&gt; 0" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=%5Cge+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ge 0." class="latex" />  A good example of a nonnegative vector is a stochastic state.</p>
<p>In 1907, Perron proved the following fundamental result about positive matrices:</p>
<p><b>Perron&#8217;s Theorem.</b> Given a positive square matrix <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /> there is a positive real number <img src="https://s0.wp.com/latex.php?latex=r%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r," class="latex" /> called the <b>Perron&#8211;Frobenius eigenvalue</b> of <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> is an eigenvalue of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> and any other eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> has <img src="https://s0.wp.com/latex.php?latex=%7C%5Clambda%7C+%3C+r.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|&#92;lambda| &lt; r." class="latex" />  Moreover, there is a positive vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{R}^n" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=T+%5Cpsi+%3D+r+%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;psi = r &#92;psi." class="latex" />  Any other vector with this property is a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" />  Furthermore, any nonnegative vector that is an eigenvector of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> must be a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" /></p>
<p>In other words, if <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is positive, it has a unique eigenvalue with the largest absolute value.  This eigenvalue is positive.  Up to a constant factor, it has an unique eigenvector.  We can choose this eigenvector to be positive.  And then, up to a constant factor, it&#8217;s the <i>only</i> nonnegative eigenvector of <img src="https://s0.wp.com/latex.php?latex=T.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T." class="latex" /></p>
<h3> From matrices to graphs </h3>
<p>The conclusions of Perron&#8217;s theorem don&#8217;t hold for matrices that are merely nonnegative.  For example, these matrices</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%28+%5Cbegin%7Barray%7D%7Bcc%7D+1+%26+0+%5C%5C+0+%26+1+%5Cend%7Barray%7D+%5Cright%29+%2C+%5Cqquad+%5Cleft%28+%5Cbegin%7Barray%7D%7Bcc%7D+0+%26+1+%5C%5C+0+%26+0+%5Cend%7Barray%7D+%5Cright%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left( &#92;begin{array}{cc} 1 &amp; 0 &#92;&#92; 0 &amp; 1 &#92;end{array} &#92;right) , &#92;qquad &#92;left( &#92;begin{array}{cc} 0 &amp; 1 &#92;&#92; 0 &amp; 0 &#92;end{array} &#92;right) " class="latex" /></p>
<p>are nonnegative, but they violate lots of the conclusions of Perron&#8217;s theorem.  </p>
<p>Nonetheless, in 1912 Frobenius published an impressive generalization of Perron&#8217;s result.  In its strongest form, it doesn&#8217;t apply to <i>all</i> nonnegative matrices; only to those that are &#8216;irreducible&#8217;.  So, let us define those.</p>
<p>We&#8217;ve seen how to build a matrix from a graph.  Now we need to build a graph from a matrix!  Suppose we have an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=T.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T." class="latex" />  Then we can build a graph <img src="https://s0.wp.com/latex.php?latex=G_T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G_T" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> vertices where there is an edge from the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th vertex to the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th vertex if and only if <img src="https://s0.wp.com/latex.php?latex=T_%7Bi+j%7D+%5Cne+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_{i j} &#92;ne 0." class="latex" />  </p>
<p>But watch out: this is a different kind of graph!  It&#8217;s a <b><a href="http://en.wikipedia.org/wiki/Directed_graph">directed graph</a></b>, meaning the edges have directions, there&#8217;s at most one edge going from any vertex to any vertex, and we do allow an edge going from a vertex to itself.  There&#8217;s a stronger concept of &#8216;connectivity&#8217; for these graphs:</p>
<p><b>Definition.</b> A directed graph is <b>strongly connected</b> if there is a directed path of edges going from any vertex to any other vertex.</p>
<p>So, you have to be able to walk along edges from any vertex to any other vertex, but always following the direction of the edges!  Using this idea we define irreducible matrices:</p>
<p><b>Definition.</b> A nonnegative square matrix <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is <b>irreducible</b> if its graph <img src="https://s0.wp.com/latex.php?latex=G_T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G_T" class="latex" /> is strongly connected.  </p>
<h3> The Perron&#8211;Frobenius theorem </h3>
<p>Now we are ready to state:</p>
<p><b>The Perron-Frobenius Theorem.</b> Given an irreducible nonnegative square matrix <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /> there is a positive real number <img src="https://s0.wp.com/latex.php?latex=r%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r," class="latex" /> called the <b>Perron&#8211;Frobenius eigenvalue</b> of <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> is an eigenvalue of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> and any other eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> has <img src="https://s0.wp.com/latex.php?latex=%7C%5Clambda%7C+%5Cle+r.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|&#92;lambda| &#92;le r." class="latex" />  Moreover, there is a positive vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{R}^n" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=T%5Cpsi+%3D+r+%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T&#92;psi = r &#92;psi." class="latex" />  Any other vector with this property is a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" />  Furthermore, any nonnegative vector that is an eigenvector of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> must be a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" /></p>
<p>The only conclusion of this theorem that&#8217;s weaker than those of Perron&#8217;s theorem is that there may be other eigenvalues with <img src="https://s0.wp.com/latex.php?latex=%7C%5Clambda%7C+%3D+r.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|&#92;lambda| = r." class="latex" />  For example, this matrix is irreducible and nonnegative:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%28+%5Cbegin%7Barray%7D%7Bcc%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Barray%7D+%5Cright%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left( &#92;begin{array}{cc} 0 &amp; 1 &#92;&#92; 1 &amp; 0 &#92;end{array} &#92;right) " class="latex" /></p>
<p>Its Perron&#8211;Frobenius eigenvalue is 1, but it also has -1 as an eigenvalue.  In general, Perron-Frobenius theory says quite a lot about the other eigenvalues on the circle <img src="https://s0.wp.com/latex.php?latex=%7C%5Clambda%7C+%3D+r%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|&#92;lambda| = r," class="latex" /> but we won&#8217;t need that fancy stuff here.</p>
<p>Perron&#8211;Frobenius theory is useful in many ways, from highbrow math to ranking football teams.   We&#8217;ll need it not just today but also later in this series.  There are many books and other sources of information for those that want to take a closer look at this subject.  If you&#8217;re interested, you can <a href="https://www.google.com/search?q=perron-frobenius">search online</a> or take a look at these:</p>
<p>&bull; Dimitrious Noutsos, <a href="http://www.math.uoi.gr/~dnoutsos/Papers_pdf_files/slide_perron.pdf">Perron Frobenius theory and some extensions</a>, 2008.  (Includes proofs of the basic theorems.)</p>
<p>&bull; V. S. Sunder, <a href="http://www.imsc.res.in/~sunder/pf.pdf">Perron Frobenius theory</a>, 18 December 2009.  (Includes applications to graph theory, Markov chains and von Neumann algebras.)</p>
<p>&bull; Stephen Boyd, <a href="http://www.stanford.edu/class/ee363/lectures/pf.pdf">Lecture 17: Perron Frobenius theory</a>, Winter 2008-2009.  (Includes a max-min characterization of the Perron&#8211;Frobenius eigenvalue and applications to Markov chains, economics, population growth and power control.)</p>
<p>I have not taken a look myself, but if anyone is interested and can read German, the original work appears here:</p>
<p>&bull; Oskar Perron, Zur Theorie der Matrizen, <i>Math. Ann.</i> <b>64</b> (1907), 248263.</p>
<p>&bull; Georg Frobenius, &Uuml;ber Matrizen aus nicht negativen Elementen, <i>S.-B. Preuss Acad. Wiss. Berlin</i> (1912), 456477.   </p>
<p>And, of course, there&#8217;s this:</p>
<p>&bull; Wikipedia, <a href="http://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron&#8211;Frobenius theorem</a>.</p>
<p>It&#8217;s got a lot of information.</p>
<h3> Irreducible Dirichlet operators </h3>
<p>Now comes the payoff.  We saw how to get a Dirichlet operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> from any finite simple graph with edges labelled by positive numbers.  Now let&#8217;s apply Perron&#8211;Frobenius theory to prove our thesis.</p>
<p>Unfortunately, the matrix <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is rarely nonnegative.  If you remember how we built it, you&#8217;ll see its off-diagonal entries will always be nonnegative&#8230; but its diagonal entries can be negative.  </p>
<p>Luckily, we can fix this just by adding a big enough multiple of the identity matrix to <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" />!   The result is a nonnegative matrix</p>
<p><img src="https://s0.wp.com/latex.php?latex=T+%3D+H+%2B+c+I+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = H + c I " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=c+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &gt; 0" class="latex" /> is some large number.  This matrix <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> has the same eigenvectors as <img src="https://s0.wp.com/latex.php?latex=H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H." class="latex" />   The off-diagonal matrix entries of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> are the same as those of <img src="https://s0.wp.com/latex.php?latex=H%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H," class="latex" /> so <img src="https://s0.wp.com/latex.php?latex=T_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_{i j}" class="latex" /> is nonzero for <img src="https://s0.wp.com/latex.php?latex=i+%5Cne+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;ne j" class="latex" /> exactly when the graph we started with has an edge from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />  So, for <img src="https://s0.wp.com/latex.php?latex=i+%5Cne+j%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;ne j," class="latex" /> the graph <img src="https://s0.wp.com/latex.php?latex=G_T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G_T" class="latex" /> will have an directed edge going from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> precisely when our original graph had an edge from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />  And that means that if our original graph was connected, <img src="https://s0.wp.com/latex.php?latex=G_T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G_T" class="latex" /> will be strongly connected.  Thus, by definition, the matrix <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is irreducible!</p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is nonnegative and irreducible, the Perron&#8211;Frobenius theorem swings into action and we conclude:</p>
<p><b>Lemma.</b> Suppose <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is the Dirichlet operator coming from a connected finite simple graph with edges labelled by positive numbers.  Then the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> are real.  Let <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> be the largest eigenvalue.  Then there is a positive vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{R}^n" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=H%5Cpsi+%3D+%5Clambda+%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#92;psi = &#92;lambda &#92;psi." class="latex" />  Any other vector with this property is a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" />  Furthermore, any nonnegative vector that is an eigenvector of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> must be a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" /></p>
<p><i>Proof.</i>  The eigenvalues of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> are real since <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is self-adjoint.  Notice that if <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> is the Perron&#8211;Frobenius eigenvalue of <img src="https://s0.wp.com/latex.php?latex=T+%3D+H+%2B+c+I&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = H + c I" class="latex" /> and</p>
<p><img src="https://s0.wp.com/latex.php?latex=T+%5Cpsi+%3D+r+%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;psi = r &#92;psi" class="latex" /></p>
<p>then </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi+%3D+%28r+-+c%29%5Cpsi+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi = (r - c)&#92;psi " class="latex" /></p>
<p>By the Perron&#8211;Frobenius theorem the number <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> is positive, and it has the largest absolute value of any eigenvalue of <img src="https://s0.wp.com/latex.php?latex=T.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T." class="latex" />  Thanks to the subtraction, the eigenvalue <img src="https://s0.wp.com/latex.php?latex=r+-+c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r - c" class="latex" /> may not have the largest absolute value of any eigenvalue of <img src="https://s0.wp.com/latex.php?latex=H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H." class="latex" />  It is, however, the largest eigenvalue of <img src="https://s0.wp.com/latex.php?latex=H%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H," class="latex" /> so we take this as our <img src="https://s0.wp.com/latex.php?latex=%5Clambda.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda." class="latex" />  The rest follows from the Perron&#8211;Frobenius theorem.  &nbsp;  &#9608;</p>
<p>But in fact we can improve this result, since the largest eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> is just zero.  Let&#8217;s also make up a definition, to make our result sound more slick:</p>
<p><b>Definition.</b> A Dirichlet operator is <b>irreducible</b> if it comes from a connected finite simple graph with edges labelled by positive numbers.</p>
<p>This meshes nicely with our earlier definition of irreducibility for nonnegative matrices.  Now:</p>
<p><b>Theorem.</b>  Suppose <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is an irreducible Dirichlet operator.  Then <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> has zero as its largest real eigenvalue.  There is a positive vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi &#92;in &#92;mathbb{R}^n" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=H%5Cpsi+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#92;psi = 0." class="latex" />  Any other vector with this property is a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" />  Furthermore, any nonnegative vector that is an eigenvector of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> must be a scalar multiple of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi." class="latex" /></p>
<p><i>Proof.</i>  Choose <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> as in the Lemma, so that <img src="https://s0.wp.com/latex.php?latex=H%5Cpsi+%3D+%5Clambda+%5Cpsi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#92;psi = &#92;lambda &#92;psi." class="latex" />  Since <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is positive we can normalize it to be a stochastic state:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+%5Cpsi_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i &#92;psi_i = 1 } " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is a Dirichlet operator, <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28t+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(t H)" class="latex" /> sends stochastic states to stochastic states, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+%28%5Cexp%28t+H%29+%5Cpsi%29_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i (&#92;exp(t H) &#92;psi)_i = 1 } " class="latex" /> </p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=t+%5Cge+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;ge 0." class="latex" />  On the other hand,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+%28%5Cexp%28t+H%29%5Cpsi%29_i+%3D+%5Csum_i+e%5E%7Bt+%5Clambda%7D+%5Cpsi_i+%3D+e%5E%7Bt+%5Clambda%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i (&#92;exp(t H)&#92;psi)_i = &#92;sum_i e^{t &#92;lambda} &#92;psi_i = e^{t &#92;lambda} } " class="latex" /></p>
<p>so we must have <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = 0." class="latex" />  &nbsp;  &#9608;</p>
<p>What&#8217;s the point of all this?  One point is that there&#8217;s a unique stochastic state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> that&#8217;s an <b>equilibrium</b> state: since <img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi = 0," class="latex" /> it doesn&#8217;t change with time.  It&#8217;s also <b>globally stable</b>: since all the other eigenvalues of <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> are negative, all other stochastic states converge to this one as time goes forward.</p>
<h3>  An example </h3>
<p>There are many examples of irreducible Dirichlet operators.  For instance, in <a href="https://johncarlosbaez.wordpress.com/2011/10/26/network-theory-part-15/">Part 15</a> we talked about graph Laplacians.  The Laplacian of a connected simple graph is always irreducible.  But let us try a different sort of example, coming from the picture of the resistors we saw earlier:</p>
<div align="center">
<img width="320" src="https://i1.wp.com/math.ucr.edu/home/baez/networks/perron-frobenius-graph.png" />
</div>
<p>Let&#8217;s create a matrix <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> whose entry <img src="https://s0.wp.com/latex.php?latex=A_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{i j}" class="latex" /> is the number labelling the edge from <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> if there is such an edge, and zero otherwise:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cleft%28++%5Cbegin%7Barray%7D%7Bccccc%7D+++0+%26+2+%26+1+%26+0+%26+1+%5C%5C+++2+%26+0+%26+0+%26+1+%26+1+%5C%5C+++1+%26+0+%26+0+%26+2+%26+1+%5C%5C+++0+%26+1+%26+2+%26+0+%26+1+%5C%5C+++1+%26+1+%26+1+%26+1+%26+0++%5Cend%7Barray%7D++%5Cright%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A = &#92;left(  &#92;begin{array}{ccccc}   0 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &#92;&#92;   2 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &#92;&#92;   1 &amp; 0 &amp; 0 &amp; 2 &amp; 1 &#92;&#92;   0 &amp; 1 &amp; 2 &amp; 0 &amp; 1 &#92;&#92;   1 &amp; 1 &amp; 1 &amp; 1 &amp; 0  &#92;end{array}  &#92;right) " class="latex" /></p>
<p>Remember how the game works.  The matrix <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is already a valid Hamiltonian for quantum mechanics, since it&#8217;s self adjoint.  However, to get a valid Hamiltonian for both stochastic and quantum mechanics&#8212;in other words, a Dirichlet operator&#8212;we subtract the diagonal matrix <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> whose entries are the sums of the columns of <img src="https://s0.wp.com/latex.php?latex=A.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A." class="latex" /> In this example it just so happens that the column sums are all 4, so <img src="https://s0.wp.com/latex.php?latex=L+%3D+4+I%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L = 4 I," class="latex" /> and our Dirichlet operator is</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+A+-+4+I+%3D+%5Cleft%28++%5Cbegin%7Barray%7D%7Brrrrr%7D+++-4+%26+2+%26+1+%26+0+%26+1+%5C%5C+++2+%26+-4+%26+0+%26+1+%26+1+%5C%5C+++1+%26+0+%26+-4+%26+2+%26+1+%5C%5C+++0+%26+1+%26+2+%26+-4+%26+1+%5C%5C+++1+%26+1+%26+1+%26+1+%26+-4++%5Cend%7Barray%7D++%5Cright%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = A - 4 I = &#92;left(  &#92;begin{array}{rrrrr}   -4 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &#92;&#92;   2 &amp; -4 &amp; 0 &amp; 1 &amp; 1 &#92;&#92;   1 &amp; 0 &amp; -4 &amp; 2 &amp; 1 &#92;&#92;   0 &amp; 1 &amp; 2 &amp; -4 &amp; 1 &#92;&#92;   1 &amp; 1 &amp; 1 &amp; 1 &amp; -4  &#92;end{array}  &#92;right) " class="latex" /></p>
<p>We&#8217;ve set up this example so it&#8217;s easy to see that the vector <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%3D+%281%2C1%2C1%2C1%2C1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi = (1,1,1,1,1)" class="latex" /> has</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%5Cpsi+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H &#92;psi = 0 " class="latex" /></p>
<p>So, this is the unique eigenvector for the eigenvalue 0.  We can use Mathematica to calculate the remaining eigenvalues of <img src="https://s0.wp.com/latex.php?latex=H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H." class="latex" />  The set of eigenvalues is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C+-7%2C+-8%2C+-8%2C+-3+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{0, -7, -8, -8, -3 &#92;} " class="latex" /></p>
<p>As we expect from our theorem, the largest real eigenvalue is 0.   By design, the eigenstate associated to this eigenvalue is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C+v_0+%5Crangle+%3D+%281%2C+1%2C+1%2C+1%2C+1%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="| v_0 &#92;rangle = (1, 1, 1, 1, 1) " class="latex" /></p>
<p>(This funny notation for vectors is common in quantum mechanics, so don&#8217;t worry about it.)   All the other eigenvectors fail to be nonnegative, as predicted by the theorem.  They are:  </p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C+v_1+%5Crangle+%3D+%281%2C+-1%2C+-1%2C+1%2C+0%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="| v_1 &#92;rangle = (1, -1, -1, 1, 0) " class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%7C+v_2+%5Crangle+%3D+%28-1%2C+0%2C+-1%2C+0%2C+2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="| v_2 &#92;rangle = (-1, 0, -1, 0, 2) " class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%7C+v_3+%5Crangle+%3D+%28-1%2C+1%2C+-1%2C+1%2C+0%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="| v_3 &#92;rangle = (-1, 1, -1, 1, 0) " class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%7C+v_4+%5Crangle+%3D+%28-1%2C+-1%2C+1%2C+1%2C+0%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="| v_4 &#92;rangle = (-1, -1, 1, 1, 0) " class="latex" /> </p>
<p>To compare the quantum and stochastic states, consider first <img src="https://s0.wp.com/latex.php?latex=%7Cv_0%5Crangle.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|v_0&#92;rangle." class="latex" />  This is the only eigenvector that can be normalized to a stochastic state.  Remember, a stochastic state must have nonnegative components.  This rules out <img src="https://s0.wp.com/latex.php?latex=%7Cv_1%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|v_1&#92;rangle" class="latex" /> through to <img src="https://s0.wp.com/latex.php?latex=%7Cv_4%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="|v_4&#92;rangle" class="latex" /> as valid stochastic states, no matter how we normalize them!  However, these are allowed as states in quantum mechanics, once we normalize them correctly.  For a stochastic system to be in a state other than the Perron&#8211;Frobenius state, it must be a linear combination of least two eigenstates.  For instance, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi_a+%3D+%281-a%29%7Cv_0%5Crangle+%2B+a+%7Cv_1%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi_a = (1-a)|v_0&#92;rangle + a |v_1&#92;rangle " class="latex" /></p>
<p>can be normalized to give stochastic state only if <img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+a+%5Cleq+%5Cfrac%7B1%7D%7B2%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;leq a &#92;leq &#92;frac{1}{2}." class="latex" />  </p>
<p>And, it&#8217;s easy to see that it works this way for any irreducible Dirichlet operator, thanks to our theorem.  So, our thesis has been proved true!</p>
<h3> Puzzles on irreducibility </h3>
<p>Let us conclude with a couple more puzzles.  There are lots of ways to characterize irreducible nonnegative matrices; we don&#8217;t need to mention graphs.  Here&#8217;s one:</p>
<p><b>Puzzle 3.</b> Let <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> be a nonnegative <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix.  Show that <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is irreducible if and only if for all <img src="https://s0.wp.com/latex.php?latex=i%2Cj+%5Cge+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i,j &#92;ge 0," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=%28T%5Em%29_%7Bi+j%7D+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(T^m)_{i j} &gt; 0" class="latex" /> for some natural number <img src="https://s0.wp.com/latex.php?latex=m.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m." class="latex" /> </p>
<p>You may be confused because today we explained the usual concept of irreducibility for nonnegative matrices, but also defined a concept of irreducibility for Dirichlet operators.  Luckily there&#8217;s no conflict: Dirichlet operators aren&#8217;t nonnegative matrices, but if we add a big multiple of the identity to a Dirichlet operator it becomes a nonnegative matrix, and then:</p>
<p><b>Puzzle 4.</b>  Show that a Dirichlet operator <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is irreducible if and only if the nonnegative operator <img src="https://s0.wp.com/latex.php?latex=H+%2B+c+I&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H + c I" class="latex" /> (where <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> is any sufficiently large constant) is irreducible. </p>
<p>Irreducibility is also related to the nonexistence of interesting conserved quantities.  In <a href="http://math.ucr.edu/home/baez/networks/networks_11.html">Part 11</a> we saw a version of Noether&#8217;s Theorem for stochastic mechanics.  Remember that an observable <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> in stochastic mechanics assigns a number <img src="https://s0.wp.com/latex.php?latex=O_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O_i" class="latex" /> to each configuration <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C+%5Cdots%2C+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1, &#92;dots, n." class="latex" />  We can make a diagonal matrix with <img src="https://s0.wp.com/latex.php?latex=O_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O_i" class="latex" /> as its diagonal entries, and by abuse of language we call this <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> as well.  Then we say <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> is a <b>conserved quantity</b> for the Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> if the commutator <img src="https://s0.wp.com/latex.php?latex=%5BO%2CH%5D+%3D+O+H+-+H+O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[O,H] = O H - H O" class="latex" /> vanishes.</p>
<p><b>Puzzle 5.</b> Let <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> be a Dirichlet operator.  Show that <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is irreducible if and only if every conserved quantity <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is a constant, meaning that for some  <img src="https://s0.wp.com/latex.php?latex=c+%5Cin+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;in &#92;mathbb{R}" class="latex" /> we have <img src="https://s0.wp.com/latex.php?latex=O_i+%3D+c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O_i = c" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />  (Hint: examine the proof of Noether&#8217;s theorem.)</p>
<p>In fact this works more generally:</p>
<p><b>Puzzle 6.</b>  Let <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> be an infinitesimal stochastic matrix.  Show that <img src="https://s0.wp.com/latex.php?latex=H+%2B+c+I&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H + c I" class="latex" /> is an irreducible nonnegative matrix for all sufficiently large <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c" class="latex" /> if and only if every conserved quantity <img src="https://s0.wp.com/latex.php?latex=O&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> is a constant.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/06/network-theory-part-20/#comments">20 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/06/network-theory-part-20/" rel="bookmark" title="Permanent Link to Network Theory (Part&nbsp;20)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-11212 post type-post status-publish format-standard hentry category-climate category-probability category-software" id="post-11212">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/30/increasing-the-signal-to-noise-ratio-with-more-noise/" rel="bookmark">Increasing the Signal-to-Noise Ratio With More&nbsp;Noise</a></h2>
				<small>30 July, 2012</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://www.azimuthproject.org/azimuth/show/Glyn+Adgie">Glyn Adgie</a></b> and <b><a href="http://www.azimuthproject.org/azimuth/show/Tim+van+Beek">Tim van Beek</a></b></i></p>
<h3>Or: Are the Milankovich Cycles Causing the Ice Ages?</h3>
<p>The <a href="http://www.azimuthproject.org/azimuth/show/Milankovitch+cycle#Idea">Milankovich cycles</a> are periodic changes in how the Earth orbits the Sun.  One question is: can these changes can be responsible for the ice ages?  On the first sight it seems impossible, because the changes are simply too small. But it turns out that we can find a dynamical system where a small periodic external force is actually strengthened by random &#8216;noise&#8217; in the system. This phenomenon has been dubbed &#8216;stochastic resonance&#8217; and has been proposed as an explanation for the ice ages.</p>
<p>In this blog post we would like to provide an introduction to it. We will look at a bistable toy example. And you&#8217;ll even get to play with an online simulation that some members of the Azimuth Project created, namely Glyn Adgie, Allan Erskine and Jim Stuttard!</p>
<h3> Equations with noise </h3>
<p>In <i>This Weeks Finds</i>, back in <a href="https://johncarlosbaez.wordpress.com/2010/12/24/this-weeks-finds-week-308/">&#8216;week308&#8217;</a>, we had a look at a model with &#8216;noise&#8217;.  A lot of systems can be described by ordinary differential equations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+f%28x%2C+t%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x}{d t} = f(x, t)} " class="latex" /> </p>
<p>If <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is nice, the time evolution of the system will be a nice smooth function <img src="https://s0.wp.com/latex.php?latex=x%28t%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t)," class="latex" /> like the trajectory of a thrown stone. But there are situations where we have some kind of noise, a chaotic, fluctuating influence, that we would like to take into account. This could be, for example, turbulence in the air flow around a rocket. Or, in our case, short term fluctuations of the weather of the earth. If we take this into account, we get an equation of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+f%28x%2C+t%29+%2B+W%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x}{d t} = f(x, t) + W(t) } " class="latex" /></p>
<p>where the <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W" class="latex" /> is some model of noise. In all the examples above, the noise is actually a simplified way to take into account fast degrees of freedom of the system at hand. This way we do not have to explicitly model these degrees of freedom, which is often impossible. But we could still try to formulate a model where long term influences of these degrees of freedom are included.</p>
<p>A way to make the above mathematically precise is to write the equation in the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=d+x_t+%3D+f%28x%2C+t%29+d+t+%2B+g%28x%2C+t%29+d+W_t+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x_t = f(x, t) d t + g(x, t) d W_t " class="latex" /></p>
<p>as an <a href="http://en.wikipedia.org/wiki/It%C5%8D_calculus"><b>Ito stochastic differential equation</b></a> driven by a <a href="http://en.wikipedia.org/wiki/Wiener_process"><b>Wiener process</b></a>.</p>
<p>The mathematics behind this is somewhat sophisticated, but we don&#8217;t need to delve into it in this blog post. For those who are interested in it, we have gathered some material here:</p>
<p>&bull; <a href="http://www.azimuthproject.org/azimuth/show/Stochastic+differential+equation">Stochastic differential equation</a>, Azimuth Library.</p>
<p>Physicists and engineers who model systems with ordinary and partial differential equations should think of stochastic differential equations as another tool in the toolbox. With this tool, it is possible to easily model systems that exhibit new behavior.</p>
<h3> What is stochastic resonance? </h3>
<p>Listening to your friend who sits across the table, the noise around you may become a serious distraction, although humans are famous for their ability to filter the signal&#8212;what your friend is saying &#8211; from the noise&#8212;all other sounds, people&#8217;s chatter, dishes clattering etc. But could you imagine a situation where <i>more noise</i> makes it <i>easier</i> to detect the signal?</p>
<p>Picture a  marble that sits in a double well potential:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/marble_in_double_well_potential.jpg" alt="double well potential" />
</div>
<p>This graphic is taken from</p>
<p>&bull; Roberto Benzi, <a href="http://arxiv.org/abs/nlin/0702008">Stochastic resonance: from climate to biology</a>.</p>
<p>This review paper is also a nice introduction to the topic.</p>
<p>This simple model can be applied to various situations. Since we think about the ice ages, we can interpret the two metastable states at the two minima of the potential in this way: the minimum to the left at -1 corresponds to the Earth in an ice age.   The minimum to the right at 1 corresponds to the Earth at warmer temperatures, as we know it today.</p>
<p>Let us add a small periodic force to the model.  This periodic force corresponds to different solar input due to periodic changes in Earth&#8217;s orbit: the Milankovich cycles.  There are actually several different cycles with different periods, but in our toy model we will consider only one force with one period.</p>
<p>If the force itself is too small to get the marble over the well that is between the two potential minima, we will not observe any periodic change in the position of the marble.  But if we add noise to the motion, it may happen that the force together with the noise succeed in getting the marble from one minimum to the other!  This is more likely to happen when the force is pushing the right way. So, it should happen with roughly the same periodicity as the force: we should see a &#8216;quasi-periodic&#8217; motion of the marble.  </p>
<p>But if we increase the noise more and more, the influence of the external force will become unrecognizable, and we will not recognize any pattern at all.</p>
<p>There should be a noise strength somewhere in between that makes the Milankovich cycles as visible as possible from the Earth&#8217;s temperature record.  In other words, if we think of the periodic force as a &#8216;signal&#8217;, there should be some amount of noise that maximizes the &#8216;signal-to-noise ratio&#8217;.  Can we find out what it is? </p>
<p>In order to find out, let us make our model precise.  Let&#8217;s take the time-independent potential to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+V%28x%29+%3D+%5Cfrac%7B1%7D%7B4%7D+x%5E4+-+%5Cfrac%7B1%7D%7B2%7D+x%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ V(x) = &#92;frac{1}{4} x^4 - &#92;frac{1}{2} x^2 } " class="latex" /></p>
<p>This potential has two local minima at <img src="https://s0.wp.com/latex.php?latex=x+%3D+%5Cpm+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = &#92;pm 1." class="latex" />   Let&#8217;s take the time-dependent periodic forcing, or &#8216;signal&#8217;, to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28t%29+%3D+-+A+%5C%3B+%5Csin%28t%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(t) = - A &#92;; &#92;sin(t) " class="latex" /></p>
<p>The constant <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is the amplitude of the periodic forcing.  The time-dependent effective potential of the system is therefore:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+V%28x%2C+t%29+%3D+%5Cfrac%7B1%7D%7B4%7D+x%5E4+-+%5Cfrac%7B1%7D%7B2%7D+x%5E2+-+A+%5C%3B+%5Csin%28t%29+x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ V(x, t) = &#92;frac{1}{4} x^4 - &#92;frac{1}{2} x^2 - A &#92;; &#92;sin(t) x } " class="latex" /></p>
<p>Including noise leads to the equation of motion, which is usually called a <b>Langevin equation</b> by physicists and chemists:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+dX+%26%3D%26+-V%27%28X_t%2C+t%29+%5C%3B+dt+%2B+%5Csqrt%7B2D%7D+%5C%3B+dW_t+%5C%5C++%5C%5C++%26%3D%26+%28X_t+-+X_t%5E3+%2B+A+%5C%3B++%5Csin%28t%29%29+%5C%3B+dt++%2B+%5Csqrt%7B2D%7D+%5C%3B+dW_t++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} dX &amp;=&amp; -V&#039;(X_t, t) &#92;; dt + &#92;sqrt{2D} &#92;; dW_t &#92;&#92;  &#92;&#92;  &amp;=&amp; (X_t - X_t^3 + A &#92;;  &#92;sin(t)) &#92;; dt  + &#92;sqrt{2D} &#92;; dW_t  &#92;end{array} " class="latex" /></p>
<p>For more about what a Langevin equation is, see our article on <a href="http://www.azimuthproject.org/azimuth/show/Stochastic+differential+equation">stochastic differential equations</a>.</p>
<p>This model has two free parameters, <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D." class="latex" />   As already mentioned, <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is the amplitude of the forcing.  <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> is called the <b>diffusion coefficient</b> and represents the strength of noise. In our case it is a constant, but in more general models it could depend on space and time.</p>
<p>It is possible to write programs that generate simulations aka numerical approximations to solutions of stochastic differential equations. For those interested in how this works, there is an addendum at the bottom of the blog post that explains this.  We can use such a program to model the three cases of small, high and optimal noise level.  </p>
<p>In the following graphs, green is the external force, while red is the marble position.  Here is a simulation with a low level of noise:</p>
<div align="center">
<img width="450" src="https://i2.wp.com/www.azimuthproject.org/azimuth/files/stochres_weaknoise.jpg" alt="low noise level" />
</div>
<p>As you can see, within the time of the simulation there is no transition from the metastable state at 1 to the one at -1. That is, in this situation Earth stays in the warm state.</p>
<p>Here is a simulation with a high noise level:</p>
<div align="center">
<img width="450" src="https://i1.wp.com/math.ucr.edu/home/baez/Stochastic_Resonance_High_Noise_Level.JPG" alt="high noise level" />
</div>
<p>The marble jumps wildly between the states. By inspecting the graph with your eyes only, you don&#8217;t see any pattern in it, do you?</p>
<p>And finally, here is a simulation where the noise level seems to be about right to make the signal visible via a quasi-periodic motion:</p>
<div align="center">
<img width="450" src="https://i0.wp.com/www.azimuthproject.org/azimuth/files/stochres_intermednoise.jpg" alt="high noise level" />
</div>
<p>Sometimes the marble makes a transition in phase with the force, sometimes it does not, sometimes it has a phase lag. It can even happen that the marble makes a transition while the force acts in the opposite direction!</p>
<p>Some members of the Azimuth crew have created an online model that calculates simulations of the model, for different values of the involved constants.  You can see it here:</p>
<p>&bull; <a href="http://www.adgie.f9.co.uk/azimuth/stochastic-resonance/Javascript/StochasticResonanceEuler.html">Stochastic resonance example</a>, Azimuth Code Project.</p>
<p>We especially thank Allan Erskine and Jim Stuttard.</p>
<p>You can change the values using the sliders under the graphic and see what happens. You can also choose different &#8216;random seeds&#8217;, which means that the random numbers used in the simulation will be different.</p>
<p>Above, we asked if one could calculate for a fixed <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> the level of noise <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> that maximizes the signal to noise ratio.  We have defined the model system, but we still need to define &#8216;signal to noise ratio&#8217; in order to address the question.</p>
<p>There is no general definition that makes sense for all kinds of systems. For our model system, let&#8217;s assume that the expectation value of <img src="https://s0.wp.com/latex.php?latex=x%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t)" class="latex" /> has a Fourier expansion like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+x%28t%29+%5Crangle+%3D+%5Csum_%7Bn+%3D+-+%5Cinfty%7D%5E%7B%5Cinfty%7D++M_n+%5Cexp%7B%28i+n+t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle x(t) &#92;rangle = &#92;sum_{n = - &#92;infty}^{&#92;infty}  M_n &#92;exp{(i n t)} } " class="latex" /></p>
<p>Then one useful definition of the <b>signal to noise ratio</b> <img src="https://s0.wp.com/latex.php?latex=%5Ceta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;eta" class="latex" /> is </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Ceta+%3A%3D+%5Cfrac%7B%7CM_1%7C%5E2%7D%7BA%5E2%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;eta := &#92;frac{|M_1|^2}{A^2}} " class="latex" /></p>
<p>So, can one calculate the value of <img src="https://s0.wp.com/latex.php?latex=D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D," class="latex" /> depending on <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> that maximizes <img src="https://s0.wp.com/latex.php?latex=%5Ceta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;eta" class="latex" />? We don&#8217;t know! Do you?</p>
<p>If you would like to know more, have a look at this page:</p>
<p>&bull; <a href="http://www.azimuthproject.org/azimuth/show/Stochastic+resonance">Stochastic resonance</a>, Azimuth Library.</p>
<p>Also, read on for more about how we numerically solved our stochastic differential equation, and the design decisions we had to make to create a <a href="http://www.adgie.f9.co.uk/azimuth/stochastic-resonance/Javascript/StochasticResonanceEuler.html">simulation that runs on your web browser</a>.</p>
<h3> For &uuml;bernerds only </h3>
<h4> Numerically solving stochastic differential equations</h4>
<p>Maybe you are asking yourself what is behind the <a href="http://www.adgie.f9.co.uk/azimuth/stochastic-resonance/Javascript/StochasticResonanceEuler.html">online model</a>? How does one &#8216;numerically solve&#8217; a stochastic differential equation and in what sense are the graphs that you see there &#8216;close&#8217; to the exact solution?  No problem, we will explain it here!</p>
<p>First, you need C code like this:</p>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;math.h&gt;

#define znew ((z=36969*(z&amp;65535)+(z&gt;&gt;16)) &lt;&lt; 16)
#define wnew ((w = 18000*(w&amp;65535)+(w&gt;&gt;16))&amp;65535)

static unsigned long z = 362436069, w = 521288629;

float rnor() {
	float x, y, v;

	x = ((signed long) (znew + wnew)) * 1.167239E-9;

	if (fabs(x) &lt; 1.17741)
		return (x);

	y = (znew + wnew) * 2.328306E-10;

	if (log(y) &lt; 0.6931472 - 0.5 * x * x)
		return x;

	x = (x &gt; 0) ? 0.8857913 * (2.506628 - x) : -0.8857913 * (2.506628 + x);

	if (log(1.8857913 - y) &lt; 0.5718733 - 0.5 * (x * x))
		return (x);

	do {
		v = ((signed long) (znew + wnew)) * 4.656613E-10;

		x = -log(fabs(v)) * 0.3989423;

		y = -log((znew + wnew)* 2.328306E-10);

	} while (y + y &lt; x * x);

	return (v &gt; 0 ? 2.506628 + x : 2.506628 - x);
}

int main(void) {

	int index = 0;

	for(index = 0; index &lt; 10; index++) {
		printf(&quot;%f\n&quot;, rnor());
	}
	return EXIT_SUCCESS;
}

</pre>
<p>Pretty scary, huh? Do you see what it does? We don&#8217;t think anyone stands a chance to understand all the &#8216;magical constants&#8217; in it. We pasted it here so you can go around and show it to friends who claim that they know the programming language C and understand every program in seconds, and see what they can tell you about it. We&#8217;ll explain it below.</p>
<p>Since this section is for &uuml;bernerds, we will assume that you know stochastic processes in continuous time. Brownian motion on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> can be viewed as a probability distribution on the space of all continuous functions on a fixed interval, <img src="https://s0.wp.com/latex.php?latex=C%5B0%2C+T%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C[0, T]." class="latex" /> This is also true for solutions of stochastic differential equations in general. A &#8216;numerical approximation&#8217; to a stochastic differential equation is an discrete time approximation that samples this probability space. </p>
<p>Another way to think about it is to start with numerical approximations of ordinary differential equations. When you are handed such an equation of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+x%7D%7Bd+t%7D+%3D+f%28x%2C+t%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d x}{d t} = f(x, t) }" class="latex" /></p>
<p>with an initial condition <img src="https://s0.wp.com/latex.php?latex=x%280%29+%3D+x_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(0) = x_0," class="latex" /> you know that there are different discrete approximation schemes to calculate an approximate solution to it. The simplest one is the Euler scheme. You need to choose a step size <img src="https://s0.wp.com/latex.php?latex=h&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h" class="latex" /> and calculate values for <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> recursively using the formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=x_%7Bn+%2B+1%7D+%3D+x_n+%2B+f%28x_n%2C+t_n%29+%5C%3B+h+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_{n + 1} = x_n + f(x_n, t_n) &#92;; h " class="latex" /> </p>
<p>with <img src="https://s0.wp.com/latex.php?latex=t_%7Bn%2B1%7D+%3D+t_n+%2B+h.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_{n+1} = t_n + h." class="latex" />  Connect the discrete values of <img src="https://s0.wp.com/latex.php?latex=x_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_n" class="latex" /> with straight lines to get your approximating function <img src="https://s0.wp.com/latex.php?latex=x%5Eh.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x^h." class="latex" />  For <img src="https://s0.wp.com/latex.php?latex=h+%5Cto+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h &#92;to 0," class="latex" /> you get convergence for example in the supremum norm of <img src="https://s0.wp.com/latex.php?latex=x%5Eh%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x^h(t)" class="latex" /> to the exact solution <img src="https://s0.wp.com/latex.php?latex=x%28t%29%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t):" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Clim_%7Bh+%5Cto+0%7D+%5C%7C+x%5Eh+-+x+%5C%7C_%5Cinfty+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;lim_{h &#92;to 0} &#92;| x^h - x &#92;|_&#92;infty = 0 } " class="latex" /></p>
<p>It is possible to extend some schemes from ordinary differential equations to stochastic differential equations that involve certain random variables in every step. The best textbook on the subject that I know is this one: </p>
<p>&bull; Peter E. Kloeden and Eckhard Platen, <i>Numerical Solution of Stochastic Differential Equations</i>, Springer, Berlin, 2010.  (<a href="http://www.zentralblatt-math.org/zmath/en/advanced/?q=an:0752.60043">ZMATH review</a>.)</p>
<p>The extension of the Euler scheme is very simple, deceptively so!  It is much less straightforward to determine the extensions of higher Runge&#8211;Kutta schemes, for example. You have been warned! The Euler scheme for stochastic differential equations of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=d+X_t+%3D+f%28X%2C+t%29+d+t+%2B+g%28X%2C+t%29+d+W_t+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d X_t = f(X, t) d t + g(X, t) d W_t " class="latex" /></p>
<p>is simply</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_%7Bn+%2B+1%7D+%3D+X_n+%2B+f%28X_n%2C+t_n%29+%5C%3B+h+%2B+g%28X_n%2C+t_n%29+%5C%3B+w_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_{n + 1} = X_n + f(X_n, t_n) &#92;; h + g(X_n, t_n) &#92;; w_n " class="latex" /></p>
<p>where all the <img src="https://s0.wp.com/latex.php?latex=w_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="w_n" class="latex" /> are independent random variables with a normal distribution <img src="https://s0.wp.com/latex.php?latex=N%280%2C+h%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N(0, h)." class="latex" />  If you write a computer program that calculates approximations using this scheme, you need a <a href="http://en.wikipedia.org/wiki/Pseudorandom_number_generator">pseudorandom number generator</a>. This is what the code we posted above is about. Actually, it is an efficient algorithm to calculate pseudorandom numbers with a normal distribution. It&#8217;s from this paper:</p>
<p>&bull; George Marsaglia and Wai Wan Tsang, The Monty Python method for generating random variables, <i><a href="http://dl.acm.org/citation.cfm?id=292453">ACM Transactions on Mathematical Software</a></i> <b>24</b> (1998), 341&#8211;350.</p>
<p>While an approximation to an ordinary differential equation approximates a function, an approximation to a stochastic differential equation approximates a probability distribution on <img src="https://s0.wp.com/latex.php?latex=C%5B0%2C+T%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C[0, T]." class="latex" />  So we need a different concept of &#8216;convergence&#8217; of the approximation for <img src="https://s0.wp.com/latex.php?latex=h+%5Cto+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h &#92;to 0." class="latex" /> The two most important ones are called &#8216;strong&#8217; and &#8216;weak&#8217; convergence.  </p>
<p>A discrete approximation <img src="https://s0.wp.com/latex.php?latex=X%5Eh&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X^h" class="latex" /> to an Ito process <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> <b>converges strongly</b> at time <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bh+%5Cto+0%7D+%5C%3B+E%28%7CX_T%5Eh+-+X_T%7C%29+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lim_{h &#92;to 0} &#92;; E(|X_T^h - X_T|) = 0 " class="latex" /></p>
<p>It is said to be of <b>strong order</b> <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> if there is a constant <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28%7CX_T%5Eh+-+X_T%7C%29+%5Cleq+C+h%5E%7B%5Cgamma%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(|X_T^h - X_T|) &#92;leq C h^{&#92;gamma} " class="latex" /></p>
<p>As we see from the definition, a strong approximation needs to approximate the path of the original process <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> at the end of the time interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2C+T%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0, T]." class="latex" /> </p>
<p>What about weak convergence?</p>
<p>A discrete approximation <img src="https://s0.wp.com/latex.php?latex=X%5Eh&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X^h" class="latex" /> to an Ito process <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> <b>converges weakly</b> for a given class of functions <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" /> at a time <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> if for every function <img src="https://s0.wp.com/latex.php?latex=g+%5Cin+G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;in G" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bh+%5Cto+0%7D+%5C%3B+E%28g%28X_T%5Eh%29%29+-+E%28g%28X_T%29%29+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lim_{h &#92;to 0} &#92;; E(g(X_T^h)) - E(g(X_T)) = 0 " class="latex" /></p>
<p>where we assume that all appearing expectation values exist and are finite.</p>
<p>It is said to be of <b>weak order</b> <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" />  with respect to <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" /> if there is a constant <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> such that for every function <img src="https://s0.wp.com/latex.php?latex=g+%5Cin+G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;in G" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28g%28X_T%5Eh%29%29+-+E%28g%28X_T%29%29+%5Cleq+C+h%5E%7B%5Cgamma%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(g(X_T^h)) - E(g(X_T)) &#92;leq C h^{&#92;gamma} " class="latex" /></p>
<p>One speaks simply of &#8216;weak convergence&#8217; when <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="G" class="latex" /> is the set of all polynomials.</p>
<p>So weak convergence means that the approximation needs to approximate the probability distribution of the original process <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  Weak and strong convergence are indeed different concepts.  For example, the Euler scheme is of strong order 0.5 and of weak order 1.0.</p>
<p>Our simple online model implements the Euler scheme to calculate approximations to our model equation. So now you know it what sense it is indeed an approximation to the solution process!</p>
<h4> Implementation details of the interactive online model </h4>
<p>There appear to be three main approaches to implementing an interactive online model. It is assumed that all the main browsers understand Javascript.</p>
<p>1) Implement the model using a server-side programme that produces graph data in response to parameter settings, and use Javascript in the browser to plot the graphs and provide interaction inputs.</p>
<p>2) Pre-compute the graph data for various sets of parameter values, send these graphs to the browser, and use Javascript to select a graph and plot it. The computation of the graph data can be done on the designers machine, using their preferred language.</p>
<p>3) Implement the model entirely in Javascript.</p>
<p>Option 1) is only practical if one can run interactive programs on the server. While there is technology to do this, it is not something that most ISPs or hosting companies provide. You might be provided with Perl and PHP, but neither of these is a language of choice for numeric coding.</p>
<p>Option 2) has been tried. The problem with this approach is the large amount of pre-computed data required. For example, if we have 4 independent parameters, each of which takes on 10 values, then we need 10000 graphs. Also, it is not practical to provide much finer control of each parameter, as this increases the data size even more.</p>
<p>Option 3) looked doable. Javascript has some nice language features, such as closures, which are a good fit for implementing numerical methods for solving ODEs. The main question then is whether a typical Javascript engine can cope with the compute load.</p>
<p>One problem that arose with a pure Javascript implementation is the production of normal random deviates required for the numerical solution of an SDE. Javascript has a random() function, that produces uniform random deviates, and there are algorithms that could be implemented in Javascript to produce normal deviates from uniform deviates. The problems here are that there is no guarantee that all Javascript engines use the same PRNG, or that the PRNG is actually any good, and we cannot set a specific seed in Javascript. Even if the PRNG is adequate, we have the problem that different users will see different outputs for the same parameter settings. Also, reloading the page will reseed the PRNG from some unknown source, so a user will not be able to replicate a particular output.</p>
<p>The chosen solution was to pre-compute a sequence of random normal deviates, using a PRNG of known characteristics. This sequence is loaded from the server, and will naturally be the same for all users and sessions. This is the C++ code used to create the fixed Javascript array:</p>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;boost/random/mersenne_twister.hpp&gt;
#include &lt;boost/random/normal_distribution.hpp&gt;
#include &lt;iostream&gt;

int main(int argc, char * argv[])
{
    boost::mt19937 engine(1234); // PRNG with seed = 1234
    boost::normal_distribution&lt;double&gt; rng(0.0, 1.0); // Mean 0.0, standard deviation 1.0
    std::cout &lt;&lt; &quot;normals = [\n&quot;;
    int nSamples = 10001;
    while(nSamples--)
    {
        std::cout &lt;&lt; rng(engine) &lt;&lt; &quot;,\n&quot;;
    }
    std::cout &lt;&lt; &quot;];&quot;;
}
</pre>
<p>Deviates with a standard deviation other than 1.0 can be produced in Javascript by multiplying the pre-computed deviates by a suitable scaling factor.</p>
<p>The graphs only use 1000 samples from the Javascript array. To see the effects of different sample paths, the Javascript code selects one of ten slices from the pre-computed array. </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/30/increasing-the-signal-to-noise-ratio-with-more-noise/#comments">48 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>, <a href="https://johncarlosbaez.wordpress.com/category/software/" rel="category tag">software</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/30/increasing-the-signal-to-noise-ratio-with-more-noise/" rel="bookmark" title="Permanent Link to Increasing the Signal-to-Noise Ratio With More&nbsp;Noise">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-11087 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-probability" id="post-11087">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/" rel="bookmark">The Noisy Channel Coding&nbsp;Theorem</a></h2>
				<small>28 July, 2012</small><br />


				<div class="entry">
					<div align="center"><a href="http://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf;jsessionid=639715502BCB1045260D9FA914505CCE?sequence=1"><img src="https://lh5.googleusercontent.com/-v-pRcD4u6yc/T8v91X48mII/AAAAAAAAK-8/mxthvAIQMTs/w497-h373/shannon.jpg" /></a></div>
<p>Here&#8217;s a charming, easily readable tale of Claude Shannon and how he came up with information theory:</p>
<p> Erico Guizzo, <i><a href="http://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf;jsessionid=639715502BCB1045260D9FA914505CCE?sequence=1">The Essential Message: Claude Shannon and the Making of Information Theory</a></i>.</p>
<p>I hadn&#8217;t known his PhD thesis was on genetics!  His master&#8217;s thesis introduced Boolean logic to circuit design.  And as a kid,  he once set up a telegraph line to a friend&#8217;s house half a mile away.</p>
<p>So, he was perfectly placed to turn <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">information</a> into a mathematical quantity, deeply related to entropy, and prove some now-famous theorems about it.</p>
<p>These theorems set limits on how much information we can transmit through a noisy channel.   More excitingly, they say we can cook up coding schemes that let us come <i>as close as we want</i> to this limit, with an arbitrarily low probability of error.</p>
<p>As Erico Guizzo points out, these results are fundamental to the &#8216;information age&#8217; we live in today:</p>
<blockquote><p>
Can we transmit, say, a high-resolution picture over a telephone line? How long will that take? Is there a best way to do it?</p>
<p>Before Shannon, engineers had no clear answers to these questions. At that time, a wild zoo of technologies was in operation, each with a life of its own&#8212;telephone, telegraph, radio, television, radar, and a number of other systems developed during the war. Shannon came up with a unifying, general  theory of communication. It didn&#8217;t matter whether you transmitted signals using a copper wire, an optical fiber, or a parabolic dish. It didn&#8217;t matter if you were transmitting text, voice, or images. Shannon envisioned communication in abstract, mathematical terms; he defined what the once fuzzy concept of &#8220;information&#8221; meant for communication engineers and proposed a precise way to quantify it. According to him, the information content of any kind of message could be measured in binary digits, or just <b>bits</b>&#8212;a name suggested by a colleague at Bell Labs. Shannon took the bit as the fundamental unit in information theory. It was the first time that the term appeared in print.
</p></blockquote>
<p>So, I want to understand Shannon&#8217;s theorems and their proofs&#8212;especially because they clarify the relation between <i>information</i> and <i>entropy</i>, two concepts I&#8217;d like to be an expert on.  It&#8217;s sort of embarrassing that I don&#8217;t already know this stuff!  But I thought I&#8217;d post some preliminary remarks anyway, in case you too are trying to learn this stuff, or in case you can help me.</p>
<p>There are various different theorems I should learn.  For example:</p>
<p> The <a href="http://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">source coding theorem</a> says it&#8217;s impossible to compress a stream of data to make the average number of bits per symbol in the compressed data less than the <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon information</a> of the source, without some of the data almost certainly getting lost.  However, you can make the number of bits per symbols arbitrarily close to the Shannon entropy with a probability of error as small as you like.</p>
<p> the <a href="http://en.wikipedia.org/wiki/Noisy_channel_coding_theorem">noisy channel coding theorem</a> is a generalization to data sent over a noisy channel.</p>
<p>The proof of the noisy channel coding theorem seems not so bad&#8212;there&#8217;s a sketch of a proof in <a href="http://en.wikipedia.org/wiki/Noisy_channel_coding_theorem">the Wikipedia article on this theorem</a>.   But many theorems have a hard lemma at their heart, and for this one it&#8217;s a result in probability theory called the <a href="http://en.wikipedia.org/wiki/Asymptotic_equipartition_property">asymptotic equipartition property</a>.</p>
<p>You should not try to <i>dodge</i> the hard lemma at the heart of the theorem you&#8217;re trying to understand: there&#8217;s a reason it&#8217;s there.    So what&#8217;s the asymptotic equipartition property?</p>
<p>Here&#8217;s a somewhat watered-down statement that gets the basic idea across.   Suppose you have a method of randomly generating letters&#8212;for example, a probability distribution on the set of letters.  Suppose you randomly generate a string of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> letters, and compute <img src="https://s0.wp.com/latex.php?latex=-%281%2Fn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-(1/n)" class="latex" /> times the logarithm of the probability that you got that string.   Then as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty" class="latex" /> this number &#8216;almost surely&#8217; approaches some number <img src="https://s0.wp.com/latex.php?latex=S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S." class="latex" />  What&#8217;s this number <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />?  It&#8217;s the <i>entropy</i> of the probability distribution you used to generate those letters!</p>
<p>(<b><a href="http://en.wikipedia.org/wiki/Almost_surely">Almost surely</a></b> is probability jargon for &#8216;with probability 100%&#8217;, which is not the same as &#8216;always&#8217;.)</p>
<p>This result is really cool&#8212;definitely worth understanding in its own right!  It says that while many strings are possible, the ones you&#8217;re most likely to see lie in a certain &#8216;typical set&#8217;.  The &#8216;typical&#8217; strings are the ones where when you compute <img src="https://s0.wp.com/latex.php?latex=-%281%2Fn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-(1/n)" class="latex" /> times the log of their probability, the result is close to <img src="https://s0.wp.com/latex.php?latex=S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S." class="latex" />  How close?  Well, you get to pick that.</p>
<p>The typical strings are not individually the most probable strings!  But if you randomly generate a string, it&#8217;s very probable that it lies in the typical set.  That sounds a bit paradoxical, but if you think about it, you&#8217;ll see it&#8217;s not.  Think of repeatedly flipping a coin that has a 90% chance of landing heads up.    The most probable single outcome is that it lands heads up every time.  But the <i>typical</i> outcome is that it lands up close to 90% of the time.  And, there are lots of ways this can happen.   So, if you flip the coin a bunch of times, there&#8217;s a very high chance that the outcome is typical.</p>
<p>It&#8217;s easy to see how this result is the key to the noisy channel coding theorem.  In general, the &#8216;typical set&#8217; has few elements compared to the whole set of strings with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> letters.  So, you can make short codes for the strings in this set, and compress your message that way, and this works almost all the time.  How much you can compress your message depends on the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />.</p>
<p>(I said &#8216;in general&#8217; because there&#8217;s one exception: when every <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-letter string is equally probable, every string is in the typical set.  In this very special case, no compression is possible.)</p>
<p>So, we&#8217;re seeing the link between information and entropy!</p>
<p>The actual coding schemes that people use are a lot trickier than the simple scheme I&#8217;m hinting at here.  When you read about them, you see scary things like this:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/Turbo_code"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Turbo_decoder.svg/300px-Turbo_decoder.svg.png" /></a></div>
<p>But presumably they&#8217;re faster to implement, hence more practical.</p>
<p>The first coding schemes that come really close to the Shannon limit are the <a href="http://en.wikipedia.org/wiki/Turbo_code">turbo codes.</a>   Surprisingly, these codes were developed only in 1993!  They&#8217;re used in 3G mobile communications and deep space satellite communications.</p>
<p>One key trick is to use, not <i>one</i> decoder, but <i>two</i>.   These two decoders keep communicating with each other and improving their guesses about the signal they&#8217;re received, until they agree:</p>
<blockquote><p>
This iterative process continues until the two decoders come up with the same hypothesis for the m-bit pattern of the payload, typically in 15 to 18 cycles. An analogy can be drawn between this process and that of solving cross-reference puzzles like crossword or sudoku. Consider a partially completed, possibly garbled crossword puzzle. Two puzzle solvers (decoders) are trying to solve it: one possessing only the &#8220;down&#8221; clues (parity bits), and the other possessing only the &#8220;across&#8221; clues. To start, both solvers guess the answers (hypotheses) to their own clues, noting down how confident they are in each letter (payload bit). Then, they compare notes, by exchanging answers and confidence ratings with each other, noticing where and how they differ. Based on this new knowledge, they both come up with updated answers and confidence ratings, repeating the whole process until they converge to the same solution.
</p></blockquote>
<p>This can be seen as &#8220;an instance of loopy belief propagation in Bayesian networks.&#8221;</p>
<p>By the way, the picture I showed you above is a flowchart of the decoding scheme for a simple turbo code.  You can see the two decoders, and maybe the loop where data gets fed back to the decoders.</p>
<p>While I said this picture is &#8220;scary&#8221;, I actually like it because it&#8217;s an example of <a href="http://math.ucr.edu/home/baez/networks/">network theory</a> applied to real-life problems.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comments">18 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/" rel="bookmark" title="Permanent Link to The Noisy Channel Coding&nbsp;Theorem">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10580 post type-post status-publish format-standard hentry category-biodiversity category-information-and-entropy category-probability" id="post-10580">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;7)</a></h2>
				<small>12 July, 2012</small><br />


				<div class="entry">
					<p>How ignorant are you?  </p>
<p>Do you know?  </p>
<p><i>Do you know how much don&#8217;t you know?</i></p>
<p>It seems hard to accurately estimate your lack of knowledge.  It even seems hard to say precisely <i>how hard</i> it is.  But the cool thing is, we can actually extract an interesting math question from this problem.  And one answer to this question leads to the following conclusion:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate how ignorant you are.<br />
</b></p></blockquote>
<p>But the devil is in the details.  So let&#8217;s see the details!</p>
<p>The <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a> of a probability distribution is a way of measuring how ignorant we are when this probability distribution describes our knowledge. </p>
<p>For example, suppose all we care about is whether this ancient Roman coin will land heads up or tails up:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/File:PupienusSest.jpg"><img src="http://upload.wikimedia.org/wikipedia/commons/6/63/PupienusSest.jpg" /></a></div>
<p>If we know there&#8217;s a 50% chance of it landing heads up, that&#8217;s a Shannon entropy of 1 bit: we&#8217;re missing one bit of information.  </p>
<p>But suppose for some reason we know for sure it&#8217;s going to land heads up.  For example, suppose we know the guy on this coin is the emperor Pupienus Maximus, a egomaniac who had lead put on the back of all coins bearing his likeness, so his face would never hit the dirt!  Then the Shannon entropy is 0: we know what&#8217;s going to happen when we toss this coin.</p>
<p>Or suppose we know there&#8217;s a 90% it will land heads up, and a 10% chance it lands tails up.  Then the Shannon entropy is somewhere in between.  We can calculate it like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+0.9+%5Clog_2+%280.9%29+-+0.1+%5Clog_2+%280.1%29+%3D+0.46899...+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- 0.9 &#92;log_2 (0.9) - 0.1 &#92;log_2 (0.1) = 0.46899... " class="latex" /></p>
<p>so that&#8217;s how many bits of information we&#8217;re missing.</p>
<p>But now suppose we have no idea.  Suppose we just start flipping the coin over and over, and seeing what happens.  Can we <i>estimate</i> the Shannon entropy?</p>
<p>Here&#8217;s a naive way to do it.  First, use your experimental data to estimate the probability that that the coin lands heads-up.  Then, stick that probability into the formula for Shannon entropy.  For example, say we flip the coin 3 times and it lands head-up once.  Then we can <i>estimate</i> the probability of it landing heads-up as 1/3, and tails-up as 2/3.  So we can estimate that the Shannon entropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Cfrac%7B1%7D%7B3%7D+%5Clog_2+%28%5Cfrac%7B1%7D%7B3%7D%29+-%5Cfrac%7B2%7D%7B3%7D+%5Clog_2+%28%5Cfrac%7B2%7D%7B3%7D%29+%3D+0.918...+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;frac{1}{3} &#92;log_2 (&#92;frac{1}{3}) -&#92;frac{2}{3} &#92;log_2 (&#92;frac{2}{3}) = 0.918... } " class="latex" /></p>
<p>But it turns out that this approach systematically <i>underestimates</i> the Shannon entropy!  </p>
<p>Say we have a coin that lands up a certain fraction of the time, say <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />   And say we play this game: we flip our coin <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> times, see what we get, and estimate the Shannon entropy using the simple recipe I just illustrated.  </p>
<p>Of course, our estimate will depend on the luck of the game.  But on average, it will be <i>less</i> than the <i>actual</i> Shannon entropy, which is </p>
<p><img src="https://s0.wp.com/latex.php?latex=-+p+%5Clog_2+%28p%29+-+%281-p%29+%5Clog_2+%281-p%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- p &#92;log_2 (p) - (1-p) &#92;log_2 (1-p)  " class="latex" /></p>
<p>We can prove this mathematically.  But it shouldn&#8217;t be surprising.  After all, if <img src="https://s0.wp.com/latex.php?latex=n+%3D+1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 1," class="latex" /> we&#8217;re playing a game where we flip the coin just <i>once</i>.  And with this game, our naive estimate of the Shannon entropy will always be <i>zero!</i>  Each time we play the game, the coin will either land heads up 100% of the time, or tails up 100% of the time!  </p>
<p>If we play the game with more coin flips, the error gets less severe.  In fact it approaches zero as the number of coin flips gets ever larger, so that <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty." class="latex" />  The case where you flip the coin just once is an extreme case&#8212;but extreme cases can be good to think about, because they can indicate what may happen in less extreme cases.</p>
<p>One moral here is that naively generalizing on the basis of limited data can make you feel more sure you know what&#8217;s going on than you actually are.  </p>
<p>I hope you knew <i>that</i> already!</p>
<p>But we can also say, in a more technical way, that the naive way of estimating Shannon entropy is a <a href="http://en.wikipedia.org/wiki/Bias_of_an_estimator"><b>biased estimator</b></a>: the average value of the estimator is different from the value of the quantity being estimated.   </p>
<p>Here&#8217;s an example of an unbiased estimator.  Say you&#8217;re trying to estimate the probability that the coin will land heads up.  You flip it <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> times and see that it lands up <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m" class="latex" /> times.  You estimate that the probability is <img src="https://s0.wp.com/latex.php?latex=m%2Fn.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m/n." class="latex" />  That&#8217;s the obvious thing to do, and it turns out to be unbiased.  </p>
<p>Statisticians like to think about <a href="http://en.wikipedia.org/wiki/Estimator">estimators</a>, and being unbiased is one way an estimator can be &#8216;good&#8217;.  Beware: it&#8217;s not the only way!  There are estimators that are unbiased, but whose standard deviation is so huge that they&#8217;re almost useless.  It can be better to have an estimate of something that&#8217;s more accurate, even though on average it&#8217;s a bit too low.  So sometimes, a biased estimator can be more useful than an unbiased estimator.  </p>
<p>Nonetheless, my ears perked up when Lou Jost mentioned that there is no unbiased estimator for Shannon entropy.  In rough terms, the moral is that:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate how ignorant you are.</b>
</p></blockquote>
<p>I think this is important.  For example, it&#8217;s important because Shannon entropy is also used as a measure of <i>biodiversity</i>.  Instead of flipping a coin repeatedly and seeing which side lands up, now we go out and collect plants or animals, and see which species we find.  The relative abundance of different species defines a  probability distribution on the set of species.  In this language, the moral is:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate biodiversity.<br />
</b></p></blockquote>
<p>But of course, this doesn&#8217;t mean we should give up.  We may just have to settle for an estimator that&#8217;s a bit biased!  And people have spent a bunch of time looking for estimators that are less biased than the naive one I just described.  </p>
<p>By the way, equating &#8216;biodiversity&#8217; with &#8216;Shannon entropy&#8217; is sloppy: there are <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">many measures of biodiversity</a>.  The Shannon entropy is just a special case of the <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a>, which depends on a parameter <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />: we get Shannon entropy when <img src="https://s0.wp.com/latex.php?latex=q+%3D+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 1." class="latex" />  </p>
<p>As <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gets smaller, the R&eacute;nyi entropy gets more and more sensitive to rare species&#8212;or shifting back to the language of probability theory, rare events.  It&#8217;s the rare events that make Shannon entropy hard to estimate, so I imagine there should be theorems about estimators for R&eacute;nyi entropy, which say it gets harder to estimate as <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gets smaller.  Do you know such theorems?</p>
<p>Also, I should add that biodiversity is better captured by the &#8216;Hill numbers&#8217;, which are functions of the R&eacute;nyi entropy, than by the R&eacute;nyi entropy itself.  (See <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">here</a> for the formulas.)  Since these functions are nonlinear, the lack of an unbiased estimator for R&eacute;nyi entropy doesn&#8217;t instantly imply the same for the Hill numbers.  So there are also some obvious questions about unbiased estimators for Hill numbers.  Do you know answers to those?</p>
<p>Here are some papers on estimators for entropy.  Most of these focus on estimating the Shannon entropy of a probability distribution on a finite set.  </p>
<p>This old classic has a proof that the &#8216;naive&#8217; estimator of Shannon entropy is biased, and estimates on the bias:</p>
<p>&bull; Bernard Harris, <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a020217.pdf">The statistical estimation of entropy in the non-parametric case</a>, Army Research Office, 1975.</p>
<p>He shows the bias goes to zero as we increase the number of samples: the number I was calling <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> in my coin flip example.  In fact he shows the bias goes to zero like <img src="https://s0.wp.com/latex.php?latex=O%281%2Fn%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(1/n)." class="latex" />  This is big <a href="http://en.wikipedia.org/wiki/Big_O_notation">big O notation</a> which means that as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%2B%5Cinfty%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to +&#92;infty," class="latex" /> the bias is bounded by some constant times <img src="https://s0.wp.com/latex.php?latex=1%2Fn.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/n." class="latex" /> This constant depends on the size of our finite set&#8212;or, if you want to do better, the <b>class number</b>, which is the number of elements on which our probability distribution is nonzero. </p>
<p>Using this idea, he shows that you can find a less biased estimator if you have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> on a finite set and you know that exactly <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> of these probabilities are nonzero.   To do this, just take the &#8216;naive&#8217; estimator I described earlier and add <img src="https://s0.wp.com/latex.php?latex=%28k-1%29%2F2n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(k-1)/2n." class="latex" />  This is called the <b>Miller&#8211;Madow bias correction</b>.  The bias of this improved estimator goes to zero like <img src="https://s0.wp.com/latex.php?latex=O%281%2Fn%5E2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(1/n^2)." class="latex" /></p>
<p>The problem is that in practice you don&#8217;t know ahead of time how many probabilities are nonzero!  In applications to biodiversity this would amount to knowing ahead of time how many species exist, before you go out looking for them. </p>
<p>But what about the theorem that there&#8217;s no unbiased estimator for Shannon entropy?  The best reference I&#8217;ve found is this:</p>
<p>&bull; Liam Paninski, <a href="http://www.stat.columbia.edu/~liam/research/abstracts/info_est-nc-abs.html">Estimation of entropy and mutual information</a>, <i>Neural Computation</i> <b>15</b> (2003) 1191-1254. </p>
<p>In Proposition 8 of Appendix A, Paninski gives a quick proof that there is no unbiased estimator of Shannon entropy for probability distributions on a finite set.  But his paper goes far beyond this.  Indeed, it seems like a pretty definitive modern discussion of the whole subject of estimating entropy.  Interestingly, this subject is dominated by neurobiologists studying entropy of signals in the brain!  So, lots of his examples involve brain signals.</p>
<p>Another overview, with tons of references, is this:</p>
<p>&bull; J. Beirlant, E. J. Dudewicz, L. Gy&ouml;rfi, and E. C. van der Meulen, <a href="http://www.its.caltech.edu/~jimbeck/summerlectures/references/Entropy%20estimation.pdf">Nonparametric entropy estimation: an overview</a>.  </p>
<p>This paper focuses on the situation where don&#8217;t know ahead of time how many probabilities are nonzero:</p>
<p>&bull; Anne Chao and T.-J. Shen, <a href="http://wayback.archive.org/web/20110715000000*/http://chao.stat.nthu.edu.tw/paper/2003_eest_10_p429.pdf">Nonparametric estimation of Shannon&#8217;s index of diversity when there are unseen species in sample</a>, <i><a href="http://www.springerlink.com/content/j23110l474087421/">Environmental and Ecological Statistics</a></i> <b>10</b> (2003), 429&amp;&#8211;443.</p>
<p>In 2003 there was a conference on the problem of estimating entropy, whose webpage has useful information.  As you can see, it was dominated by neurobiologists:</p>
<p>&bull; <a href="http://menem.com/~ilya/pages/NIPS03/">Estimation of entropy and information of undersampled probability distributions: theory, algorithms, and applications to the neural code</a>, Whistler, British Columbia, Canada, 12 December 2003.</p>
<p>By the way, I was very confused for a while, because these guys claim to have found an unbiased estimator of Shannon entropy:</p>
<p>&bull; Stephen Montgomery Smith and Thomas Sch&uuml;rmann, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.6882&amp;rep=rep1&amp;type=pdf">Unbiased estimators for entropy and class number</a>.</p>
<p>However, their way of estimating entropy has a funny property: in the language of biodiversity, it&#8217;s only well-defined if our samples include at least one species of each organism.   So, we cannot compute this estimate for an <i>arbitary</i> list of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> samples.  This means it&#8217;s not <a href="http://en.wikipedia.org/wiki/Estimator">estimator</a> in the usual sense&#8212;the sense that Paninski is using!  So it doesn&#8217;t really contradict Paninski&#8217;s result.</p>
<p>To wrap up, let me state Paninski&#8217;s result in a mathematically precise way. Suppose <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a probability distribution on a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  Suppose <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is any number we can compute from <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />: that is, any real-valued function on the set of probability distributions.   We&#8217;ll be interested in the case where <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the <b>Shannon entropy</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-%5Csum_%7Bx+%5Cin+X%7D+p%28x%29+%5C%2C+%5Clog+p%28x%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = -&#92;sum_{x &#92;in X} p(x) &#92;, &#92;log p(x) }" class="latex" /></p>
<p>Here we can use whatever base for the logarithm we like: earlier I was using base 2, but that&#8217;s not sacred.  Define an <b>estimator</b> to be any function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D%3A+X%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}: X^n &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>The idea is that given <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> <b>samples</b> from the set <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> meaning points <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cdots%2C+x_n+%5Cin+X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1, &#92;dots, x_n &#92;in X," class="latex" /> the estimator gives a number <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D%28x_1%2C+%5Cdots%2C+x_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}(x_1, &#92;dots, x_n)" class="latex" />.   This number is supposed to estimate some feature of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />: for example, its entropy.   </p>
<p>If the samples are independent and distributed according to the distribution <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> the <b>sample mean of the estimator</b> will be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Chat%7BS%7D+%5Crangle+%3D+%5Csum_%7Bx_1%2C+%5Cdots%2C+x_n+%5Cin+X%7D+%5Chat%7BS%7D%28x_1%2C+%5Cdots%2C+x_n%29+%5C%2C+p%28x_1%29+%5Ccdots+p%28x_n%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;hat{S} &#92;rangle = &#92;sum_{x_1, &#92;dots, x_n &#92;in X} &#92;hat{S}(x_1, &#92;dots, x_n) &#92;, p(x_1) &#92;cdots p(x_n) } " class="latex" /></p>
<p>The <b>bias</b> of the estimator is the difference between the sample mean of the estimator and actual value of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Chat%7BS%7D+%5Crangle+-+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;hat{S} &#92;rangle - S " class="latex" /></p>
<p>The estimator <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}" class="latex" /> is <b>unbiased</b> if this bias is zero for all <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>Proposition 8 of Paninski&#8217;s paper says there exists no unbiased estimator for entropy!  The proof is very short&#8230; </p>
<p>Okay, that&#8217;s all for today.</p>
<p>I&#8217;m back in Singapore now; I learned so much at the <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx">Mathematics of Biodiversity</a> conference that there&#8217;s no way I&#8217;ll be able to tell you all that information.   I&#8217;ll try to write a few more blog posts, but please be aware that my posts so far give a hopelessly biased and idiosyncratic view of the conference, which would be almost unrecognizable to most of the participants.  There are a lot of important themes I haven&#8217;t touched on at all&#8230; while this business of entropy estimation barely came up: I just find it interesting!</p>
<p>If more of you blogged more, we wouldn&#8217;t have this problem. </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/#comments">26 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;7)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10541 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics category-physics category-probability" id="post-10541">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;5)</a></h2>
				<small>3 July, 2012</small><br />


				<div class="entry">
					<p>I&#8217;d be happy to get your feedback on these slides of the talk I&#8217;m giving the day after tomorrow:</p>
<p>&bull; John Baez, <a href="http://math.ucr.edu/home/baez/biodiversity/">Diversity, entropy and thermodynamics</a>, 6 July 2012, Exploratory Conference on the Mathematics of Biodiversity, Centre de Recerca Matemtica, Barcelona.</p>
<blockquote><p>
<b>Abstract:</b> As is well known, some popular measures of biodiversity are formally identical to measures of entropy developed by Shannon, Rnyi and others. This fact is part of a larger analogy between thermodynamics and the mathematics of biodiversity, which we explore here. Any probability distribution can be extended to a 1-parameter family of probability distributions where the parameter has the physical meaning of &#8216;temperature&#8217;. This allows us to introduce thermodynamic concepts such as energy, entropy, free energy and the partition function in any situation where a probability distribution is present&#8212;for example, the probability distribution describing the relative abundances of different species in an ecosystem. The Rnyi entropy of this probability distribution is closely related to the change in free energy with temperature. We give one application of thermodynamic ideas to population dynamics, coming from the work of Marc Harper: as a population approaches an &#8216;evolutionary optimum&#8217;, the amount of Shannon information it has &#8216;left to learn&#8217; is nonincreasing. This fact is closely related to the Second Law of Thermodynamics.
</p></blockquote>
<p>This talk is rather different than the one I&#8217;d envisaged giving!  There was a lot of interest in my work on R&eacute;nyi entropy and thermodynamics, because R&eacute;nyi entropies&#8212;and their exponentials, called the Hill numbers&#8212;are an <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">important measure of biodiversity</a>.  So, I decided to spend a lot of time talking about that.</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/biodiversity/"><img src="https://i2.wp.com/math.ucr.edu/home/baez/biodiversity/408px-Forest_fruits_from_Barro_Colorado.jpg" /></a></div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/#comments">12 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10340 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics category-probability" id="post-10340">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/" rel="bookmark">Information Geometry (Part&nbsp;13)</a></h2>
				<small>26 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/">Last time</a> I gave a sketchy overview of evolutionary game theory.  Now let&#8217;s get serious.</p>
<p>I&#8217;ll start by explaining &#8216;Nash equilibria&#8217; for 2-person games.  These are situations where neither player can profit by changing what they&#8217;re doing.  Then I&#8217;ll introduce &#8216;mixed strategies&#8217;, where the players can choose among several strategies with different probabilities.  Then I&#8217;ll introduce evolutionary game theory, where we think of each strategy as a <i>species</i>, and its probability as <i>the fraction of organisms that belong to that species</i>.</p>
<p>Back in <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/">Part 9</a>, I told you about the &#8216;replicator equation&#8217;, which says how these fractions change with time thanks to natural selection.   Now we&#8217;ll see how this leads to the idea of an &#8216;evolutionarily stable strategy&#8217;.   And finally, we&#8217;ll see that when evolution takes us toward such a stable strategy, the amount of information the organisms have &#8216;left to learn&#8217; keeps decreasing!</p>
<h3> Nash equilibria </h3>
<p>We can describe a certain kind of two-person game using a <b>payoff matrix</b>, which is an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> of real numbers.  We think of <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> as the payoff that either player gets if they choose strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and their opponent chooses strategy <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" /></p>
<p>Note that in this kind of game, there&#8217;s no significant difference between the &#8216;first player&#8217; and the &#8216;second player&#8217;: <i>either</i> player wins an amount <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> if they choose strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and their opponent chooses strategy <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />  So, this kind of game is called <b>symmetric</b> even though the matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> may not be symmetric.  Indeed, it&#8217;s common for this matrix to be antisymmetric, meaning <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D+%3D+-+A_%7Bji%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij} = - A_{ji}," class="latex" /> since in this case what one player wins, the other loses.  Games with this extra property are called <b>zero-sum games</b>.  But we won&#8217;t limit ourselves to those!</p>
<p>We say a strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> is a <b>symmetric Nash equilibrium</b> if</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7Bii%7D+%5Cge+A_%7Bji%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ii} &#92;ge A_{ji} " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />  This means that if both players use strategy <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> neither gains anything by switching to another strategy.</p>
<p>For example, suppose our matrix is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%28+%5Cbegin%7Barray%7D%7Brr%7D+-1+%26+-12+%5C%5C++0+%26+-3+%5Cend%7Barray%7D+%5Cright%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left( &#92;begin{array}{rr} -1 &amp; -12 &#92;&#92;  0 &amp; -3 &#92;end{array} &#92;right) " class="latex" /></p>
<p>Then we&#8217;ve got the Prisoner&#8217;s Dilemma exactly as described last time!  Here strategy 1 is <b>cooperate</b> and strategy 2 is <b>defect</b>.  If a player cooperates and so does his opponent, he wins</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B11%7D+%3D+-1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{11} = -1 " class="latex" /></p>
<p>meaning he gets one month in jail.   We include a minus sign because &#8216;winning a month in jail&#8217; is not a good thing.   If the player cooperates but his opponent defects, he gets a whole year in jail:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B12%7D+%3D+-12&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{12} = -12" class="latex" /></p>
<p>If he defects but his opponent cooperates, he doesn&#8217;t go to jail at all:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B21%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{21} = 0" class="latex" /></p>
<p>And if they both defect, they both get three months in jail:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B22%7D+%3D+-3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{22} = -3" class="latex" /></p>
<p>You can see that defecting is a Nash equilibrium, since</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B22%7D+%5Cge+A_%7B12%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{22} &#92;ge A_{12}" class="latex" /></p>
<p>So, oddly, if our prisoners know game theory and believe Nash equilibria are best, they&#8217;ll both be worse off than if they cooperate and don&#8217;t betray each other.</p>
<div align="center">
<img width="170" src="http://math.ucr.edu/home/baez/prisoner's_dilemma_left.jpg" /> &nbsp;&nbsp;&nbsp;&nbsp; <img width="170" src="http://math.ucr.edu/home/baez/prisoner's_dilemma_right.jpg" /></div>
<h3> Nash equilibria  for mixed strategies </h3>
<p>So far we&#8217;ve been assuming that with 100% certainty, each player chooses one strategy <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C2%2C3%2C%5Cdots%2C+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,2,3,&#92;dots, n." class="latex" />  Since we&#8217;ll be considering more general strategies in a minute, let&#8217;s call these <b>pure strategies</b>.</p>
<p>Now let&#8217;s throw some probability theory into the stew!  Let&#8217;s allow the players to pick different pure strategies with different probabilities.  So, we define a <b>mixed strategy</b> to be a probability distribution on the set of pure strategies.  In other words, it&#8217;s a list of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> nonnegative numbers</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;ge 0 " class="latex" /></p>
<p>that sum to one:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i = 1 } " class="latex" /></p>
<p>Say I choose the mixed strategy <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> while you, my opponent, choose the mixed strategy <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  Say our choices are made independently.  Then the probability that I choose the pure strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> while you chose <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+q_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i q_j" class="latex" /></p>
<p>so the expected value of my winnings is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2Cj+%3D+1%7D%5En+p_i+A_%7Bij%7D+q_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i,j = 1}^n p_i A_{ij} q_j }" class="latex" /></p>
<p>or using vector notation</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%5Ccdot+A+q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;cdot A q " class="latex" /></p>
<p>where the dot is the usual dot product on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n." class="latex" /></p>
<p>We can easily adapt the concept of Nash equilibrium to mixed strategies.  A mixed strategy <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a <b>symmetric  Nash equilibrium</b> if for any other mixed strategy <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge++p+%5Ccdot+A+q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge  p &#92;cdot A q " class="latex" /></p>
<p>This means that if both you and I are playing the mixed strategy <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> I can&#8217;t improve my expected winnings by unilaterally switching to the mixed strategy <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />  And neither can you, because the game is symmetric!</p>
<p>If this were a course on game theory, I would now do some examples.  But it&#8217;s not, so I&#8217;ll just send you to page 6 of <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">Sandholm&#8217;s paper</a>: he looks at some famous games like &#8216;hawks and doves&#8217; and &#8216;rock paper scissors&#8217;.</p>
<h3> Evolutionarily stable strategies </h3>
<p>We&#8217;re finally ready to discuss evolutionarily stable strategies.  To do this, let&#8217;s reinterpret the &#8216;pure strategies&#8217; <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C2%2C3%2C+%5Cdots+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,2,3, &#92;dots n" class="latex" /> as <b>species</b>.  Here I don&#8217;t necessarily mean species in the classic biological sense: I just mean different kinds of self-replicating entities, or <b>replicators</b>.  For example, they could be different <a href="http://en.wikipedia.org/wiki/Allele">alleles</a> of the same gene.</p>
<p>Similarly, we&#8217;ll reinterpret the &#8216;mixed strategy&#8217; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as describing a mixed population of replicators, where the fraction of replicators belonging to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />  These numbers are still probabilities: <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the probability that a randomly chosen replicator will belong to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.</p>
<p>We&#8217;ll reinterpret the payoff matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> as a <b>fitness matrix</b>.  In our earlier discussion of the replicator equation, we assumed that the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species grew according to the replicator equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+P_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) P_i }" class="latex" /></p>
<p>where the <b>fitness function</b> <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is any smooth function of the populations of each kind of replicator.</p>
<p>But in evolutionary game theory it&#8217;s common to start by looking at a simple special case where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7Bf_i%28P_1%2C+%5Cdots%2C+P_n%29+++%3D+%5Csum_%7Bj%3D1%7D%5En+A_%7Bij%7D+p_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{f_i(P_1, &#92;dots, P_n)   = &#92;sum_{j=1}^n A_{ij} p_j }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_j+%3D+%5Cfrac%7BP_j%7D%7B%5Csum_k+P_k%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_j = &#92;frac{P_j}{&#92;sum_k P_k} }" class="latex" /></p>
<p>is the fraction of replicators who belong to the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th species.</p>
<p>What does this mean?  The idea is that we have a well-mixed population of game players&#8212;or replicators.   Each one has its own pure strategy&#8212;or species.   Each one randomly roams around and &#8216;plays games&#8217; with each other replicator it meets.  It gets to reproduce at a rate proportional to its expected winnings.</p>
<p>This is unrealistic in all sorts of ways, but it&#8217;s mathematically cute, and it&#8217;s been studied a lot, so it&#8217;s good to know about.  Today I&#8217;ll explain evolutionarily stable strategies only in this special case.  Later I&#8217;ll go back to the general case.</p>
<p>Suppose that we select a sample of replicators from the overall population.  What is the mean fitness of the replicators in this sample?  For this, we need to know the probability that a replicator from this sample belongs to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  Say it&#8217;s <img src="https://s0.wp.com/latex.php?latex=q_j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_j." class="latex" />  Then the mean fitness of our sample is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2Cj%3D1%7D%5En+q_i+A_%7Bij%7D+p_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i,j=1}^n q_i A_{ij} p_j }" class="latex" /></p>
<p>This is just a weighted average of the fitnesses in our earlier formula.   But using the magic of vectors, we can write this sum as</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+p+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A p " class="latex" /></p>
<p>We already saw this type of expression in the last section!  It&#8217;s my expected winnings if I play the mixed strategy <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and you play the mixed strategy <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>John Maynard Smith defined <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to be <b>evolutionarily stable strategy</b> if when we add a small population of &#8216;invaders&#8217; distributed according to any other probability distribution <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> the original population is more fit than the invaders.</p>
<p>In simple terms: a small &#8216;invading&#8217; population will do worse than the population as a whole.</p>
<p>Mathematically, this means:</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+%28%281-%5Cepsilon%29q+%2B+%5Cepsilon+p%29+%3E++p+%5Ccdot+A+%28%281-%5Cepsilon%29q+%2B+%5Cepsilon+p%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A ((1-&#92;epsilon)q + &#92;epsilon p) &gt;  p &#92;cdot A ((1-&#92;epsilon)q + &#92;epsilon p) " class="latex" /></p>
<p>for all mixed strategies <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and all sufficiently small <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Cge+0+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon &#92;ge 0 ." class="latex" />   Here</p>
<p><img src="https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29q+%2B+%5Cepsilon+p+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1-&#92;epsilon)q + &#92;epsilon p " class="latex" /></p>
<p>is the population we get by replacing an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon" class="latex" />-sized portion of our original population by invaders.</p>
<p><b>Puzzle:</b>  Show that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an evolutionarily stable strategy if and only these two conditions hold for all mixed stategies <img src="https://s0.wp.com/latex.php?latex=p%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p:" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge+p+%5Ccdot+A+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge p &#92;cdot A q" class="latex" /></p>
<p>and also, for all <img src="https://s0.wp.com/latex.php?latex=q+%5Cne+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;ne p" class="latex" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%3D+p+%5Ccdot+A+q+%5C%3B+%5Cimplies+%5C%3B+q+%5Ccdot+A+p+%3E+p+%5Ccdot+A+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q = p &#92;cdot A q &#92;; &#92;implies &#92;; q &#92;cdot A p &gt; p &#92;cdot A p" class="latex" /></p>
<p>The first condition says that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a symmetric Nash equilibrium.  In other words, the invaders can&#8217;t on average be <i>better</i> playing against the original population than members of the original population are.    The second says that if the invaders are <i>just as good</i> at playing against the original population, they must be worse at playing against each other!  The combination of these conditions means the invaders won&#8217;t take over.</p>
<p>Again, I should do some examples&#8230; but instead I&#8217;ll refer you to page 9 of <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">Sandholm&#8217;s paper</a>, and also these course notes:</p>
<p>&bull; Samuel Alizon and Daniel Cownden, <a href="http://dcownden.files.wordpress.com/2009/01/notes6.pdf">Evolutionary games and evolutionarily stable strategies</a>.</p>
<p>&bull; Samuel Alizon and Daniel Cownden, <a href="http://dcownden.files.wordpress.com/2009/01/notes8.pdf">Replicator dynamics</a>.</p>
<h3> The decrease of relative information </h3>
<p>Now comes the punchline&#8230; but with a slight surprise twist at the end.  <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/">Last time</a> we let</p>
<p><img src="https://s0.wp.com/latex.php?latex=P+%3D+%28P_1%2C+%5Cdots+%2C+P_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P = (P_1, &#92;dots , P_n)" class="latex" /></p>
<p>be a population that evolves with time according to the replicator equation, and we let <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> be the corresponding probability distribution.  We supposed <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> was some fixed probability distribution.   We saw that the relative information</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Cdisplaystyle%7B+%5Csum_i+%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7B+p_i+%7D%5Cright%29+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;displaystyle{ &#92;sum_i &#92;ln &#92;left(&#92;frac{q_i}{ p_i }&#92;right) q_i } " class="latex" /></p>
<p>obeys</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%3D++%28p+-+q%29+%7D+%5Ccdot+f%28P%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) =  (p - q) } &#92;cdot f(P) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" /> is the vector of fitness functions.  So, this relative information can never increase if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p+-+q%29+%5Ccdot+f%28P%29+%5Cle+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p - q) &#92;cdot f(P) &#92;le 0 " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" />.</p>
<p>We can adapt this to the special case we&#8217;re looking at now.  Remember, right now we&#8217;re assuming</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7Bf_i%28P_1%2C+%5Cdots%2C+P_n%29+++%3D+%5Csum_%7Bj%3D1%7D%5En+A_%7Bij%7D+p_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{f_i(P_1, &#92;dots, P_n)   = &#92;sum_{j=1}^n A_{ij} p_j }" class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+%3D+A+p+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) = A p " class="latex" /></p>
<p>Thus, the relative information will never increase if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p+-+q%29+%5Ccdot+A+p+%5Cle+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p - q) &#92;cdot A p &#92;le 0" class="latex" /></p>
<p>or in other words,</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+p+%5Cge+p+%5Ccdot+A+p++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad++%281%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A p &#92;ge p &#92;cdot A p  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad  (1) " class="latex" /></p>
<p>Now, this looks very similar to the conditions for an evolutionary stable strategy as stated in the Puzzle above.  <i>But it&#8217;s not the same!</i>  That&#8217;s the surprise twist.</p>
<p>Remember, the Puzzle says that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an evolutionarily stable state if for all mixed strategies <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge+p+%5Ccdot+A+q++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge p &#92;cdot A q  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad (2)" class="latex" /></p>
<p>and also</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%3D+p+%5Ccdot+A+q+%5C%3B+%5Cimplies+%5C%3B+q+%5Ccdot+A+p+%3E+p+%5Ccdot+A+p++%5Cqquad+%5C%3B+%283%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q = p &#92;cdot A q &#92;; &#92;implies &#92;; q &#92;cdot A p &gt; p &#92;cdot A p  &#92;qquad &#92;; (3)" class="latex" /></p>
<p>Note that condition (1), the one we want, is <i>neither</i> condition (2) <i>nor</i> condition (3)!  This drove me crazy for almost a day.</p>
<div align="center">
<img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/spiral_eyes.gif" alt="" /></div>
<p>I kept thinking I&#8217;d made a mistake, like mixing up <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> somewhere.  You&#8217;ve got to mind your p&#8217;s and q&#8217;s in this game!</p>
<p>But the solution turned out to be this.  After Maynard Smith came up with his definition of &#8216;evolutionarily stable state&#8217;, another guy came up with a different definition:</p>
<p>&bull; Bernhard Thomas, On evolutionarily stable sets, <i><a href="http://www.springerlink.com/content/g7812u72h6110m26/?MUD=MP">J. Math. Biology</a></i> <b>22</b> (1985), 105&#8211;115.</p>
<p>For him, an <b>evolutionarily stable strategy</b> obeys</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge+p+%5Ccdot+A+q++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge p &#92;cdot A q  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad (2)" class="latex" /></p>
<p>and also</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+p+%5Cge+p+%5Ccdot+A+p++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad++%281%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A p &#92;ge p &#92;cdot A p  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad  (1) " class="latex" /></p>
<p>Condition (1) is stronger than condition (3), so he renamed Maynard Smith&#8217;s evolutionarily stable strategies <b>weakly  evolutionarily stable strategies</b>.  And condition (1) guarantees that the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> can never increase.  So, now we&#8217;re happy.</p>
<p>Except for one thing: why should we switch from Maynard Smith&#8217;s perfectly sensible concept of evolutionarily stable state to this new stronger one?  I don&#8217;t really know, except that</p>
<p>&bull; it&#8217;s not much stronger</p>
<p>and</p>
<p>&bull; it lets us prove the theorem we want!</p>
<p>So, it&#8217;s a small mystery for me to mull over.  If you have any good ideas, let me know.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/#comments">7 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;13)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10293 post type-post status-publish format-standard hentry category-biodiversity category-mathematics category-probability" id="post-10293">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/24/the-mathematics-of-biodiversity-part-2/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;2)</a></h2>
				<small>24 June, 2012</small><br />


				<div class="entry">
					<p>How likely is it that the next thing we see is one of a brand new kind?   That sounds like a hard question.  <a href="https://johncarlosbaez.wordpress.com/2012/06/21/the-mathematics-of-biodiversity-part-1/">Last time</a> I told you about the Good&#8211;Turing rule for answering this question.   </p>
<p>The discussion that blog entry triggered has been very helpful!   Among other things, it got Lou Jost more interested in this subject.  Two days ago, he showed me the following simple argument for the Good&#8211;Turing estimate.</p>
<p>Suppose there are finitely many species of orchid.  Suppose the fraction of orchids belonging to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /></p>
<p>Suppose we start collecting orchids.  Suppose each time we find one, the chance that it&#8217;s an orchid of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />   Of course this is not true in reality!  For example, it&#8217;s harder to find a tiny orchid, like this:  </p>
<div align="center"><a href="http://en.wikinews.org/wiki/American_botanist_Lou_Jost_discovers_world%27s_smallest_orchid"><img width="50" src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Platystele_P5313ruler2.jpg/220px-Platystele_P5313ruler2.jpg" /></a></div>
<p>than a big one.  But never mind.</p>
<p>Say we collect a total of <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> orchids.   What is the probability that we find no orchids of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species?  It is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%281+-+p_i%29%5EN&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1 - p_i)^N" class="latex" />  </p>
<p>Similarly, the probability that we find exactly one orchid of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is</p>
<p><img src="https://s0.wp.com/latex.php?latex=N+p_i+%281+-+p_i%29%5E%7BN-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N p_i (1 - p_i)^{N-1}" class="latex" />  </p>
<p>And so on: these are the first two terms in a binomial series.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=n_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1" class="latex" /> be the expected number of <b>singletons</b>: species for which we find exactly one orchid of that species.  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+n_1+%3D+%5Csum_i+N+p_i+%281+-+p_i%29%5E%7BN-1%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ n_1 = &#92;sum_i N p_i (1 - p_i)^{N-1} }" class="latex" /></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> be the <b>coverage deficit</b>: the expected fraction of the total population consisting of species that remain undiscovered.  Given our assumptions, this is the same as the chance that the <i>next</i> orchid we find will be of a brand new species.</p>
<p>Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D+%3D+%5Csum_i+p_i+%281-p_i%29%5EN+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D = &#92;sum_i p_i (1-p_i)^N }" class="latex" /></p>
<p>since <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the fraction of orchids belonging to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species and <img src="https://s0.wp.com/latex.php?latex=%281-p_i%29%5EN+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1-p_i)^N " class="latex" /> is the chance that this species remains undiscovered.</p>
<p>Lou Jost pointed out that the formulas for <img src="https://s0.wp.com/latex.php?latex=n_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> are very similar!  In particular, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bn_1%7D%7BN%7D+%3D+%5Csum_i+p_i+%281+-+p_i%29%5E%7BN-1%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{n_1}{N} = &#92;sum_i p_i (1 - p_i)^{N-1} } " class="latex" /></p>
<p>should be very close to </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D+%3D+%5Csum_i+p_i+%281+-+p_i%29%5EN+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D = &#92;sum_i p_i (1 - p_i)^N } " class="latex" /></p>
<p>when <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> is large.  So, we should have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+D+%5Capprox+%5Cfrac%7Bn_1%7D%7BN%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ D &#92;approx &#92;frac{n_1}{N} }" class="latex" /></p>
<p>In other words: the chance that the next orchid we find is of a brand new species should be close to the fraction of orchids that are singletons now.</p>
<p>Of course it would be nice to turn these &#8216;shoulds&#8217; into precise theorems!  Theorem 1 in this paper does that:</p>
<p>&bull; David McAllester and Robert E. Schapire, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7209&amp;rep=rep1&amp;type=pdf" rel="nofollow">On the convergence rate of Good&#8211;Turing estimators</a>, February 17, 2000.</p>
<p>By the way: the only difference between the formulas for <img src="https://s0.wp.com/latex.php?latex=n_1%2FN&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1/N" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> is that the first contains the exponent <img src="https://s0.wp.com/latex.php?latex=N-1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N-1," class="latex" /> while the second contains the exponent <img src="https://s0.wp.com/latex.php?latex=N.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N." class="latex" />  So, Lou Jost&#8217;s argument is a version of <a href="https://johncarlosbaez.wordpress.com/2012/06/21/the-mathematics-of-biodiversity-part-1/#comment-15963">Boris Borcic&#8217;s &#8216;time-reversal&#8217; idea</a>:</p>
<blockquote><p>
Goods estimate is what you immediately obtain if you time-reverse your sampling procedure, e.g., if you ask for the probability that there is a change in the number of species in your sample when you randomly remove a specimen from it.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/24/the-mathematics-of-biodiversity-part-2/#comments">7 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/24/the-mathematics-of-biodiversity-part-2/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10237 post type-post status-publish format-standard hentry category-biodiversity category-probability" id="post-10237">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/21/the-mathematics-of-biodiversity-part-1/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;1)</a></h2>
				<small>21 June, 2012</small><br />


				<div class="entry">
					<p>I&#8217;m in Barcelona now, and I want to blog about this:</p>
<p>&bull; <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Research-Program-on-Mathematics-of-Biodiversity.aspx">Research Program on the Mathematics of Biodiversity</a>, June-July 2012, Centre de Recerca Matemtica, Barcelona, Spain.  Organized by Ben Allen, Silvia Cuadrado, Tom Leinster, Richard Reeve and John Woolliams.</p>
<p>We&#8217;re having daily informal talks and there&#8217;s no way I can blog about all of them, talk to people here, and still get enough work done.  So, I&#8217;ll just mention a few things that strike me!  For example, this morning Lou Jost told me about an interesting paper by <a href="http://en.wikipedia.org/wiki/I._J._Good">I. J. Good</a>.  </p>
<p>I&#8217;d known of I. J. Good as one of the guys who came up with the concept of a &#8216;technological singularity&#8217;.  In 1963 he wrote:</p>
<blockquote><p>
Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an &#8216;intelligence explosion,&#8217; and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.
</p></blockquote>
<p>He was a British mathematician who worked as a cryptologist at Bletchley Park with Alan Turing.  After World War II, he continued to work with Turing on the design of computers and Bayesian statistics at the University of Manchester.  Later he moved to the US.  In 1968, thanks to his interest in artificial intelligence, he served as consultant for Stanley Kubrick&#8217;s film <i>2001: A Space Odyssey</i>.  He died in 2009.</p>
<p>Good was also a big chess enthusiast, and worked on writing programs to play chess. He&#8217;s the guy in front here:</p>
<div align="center"><a href="http://chessprogramming.wikispaces.com/Jack+Good?responseToken=088059b641f2f9d6dc6fe57e92d26c489"><img width="400" src="https://i0.wp.com/spec.lib.vt.edu/imagebase/vtarchive/screen/VTA0006.jpg" /></a></div>
<p>If you want to learn more about his work on chess, click on this photo!</p>
<p>But the paper Lou Jost mentioned is on a rather different subject:</p>
<p>&bull; Irving John Good, <a href="http://www.ling.upenn.edu/courses/cogs502/GoodTuring1953.pdf">The population frequency of species and the estimation of population parameters</a>, <i>Biometrika</i> <b>40</b> (1953), 237&#8211;264.</p>
<p>Let me just state one result, sloppily, without any details or precise hypotheses!  </p>
<p><b>Puzzle:</b> Suppose you go into the jungles of Ecuador and start collecting orchids.  You count the number of orchids of each different species that you find.  You get a list of numbers, something like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=14%2C+10%2C+8%2C+6%2C+2%2C+1%2C+1%2C+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="14, 10, 8, 6, 2, 1, 1, 1" class="latex" /></p>
<p>What is the chance that the next orchid you find will belong to a new species?  </p>
<p>Good gives a rule of thumb for solving problems of this type:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bn_1%7D%7BN%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{n_1}{N} }" class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> is the total number of orchid you collected, and <img src="https://s0.wp.com/latex.php?latex=n_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_i" class="latex" /> is the number of species for which you found exactly <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> orchids of that species.  In our example, </p>
<p><img src="https://s0.wp.com/latex.php?latex=n_1+%3D+3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_1 = 3" class="latex" /></p>
<p>since we found just one orchid of three different species: those are the three 1&#8217;s at the end of our list.  Furthermore,</p>
<p><img src="https://s0.wp.com/latex.php?latex=N+%3D+14+%2B+10+%2B+8+%2B+6+%2B+2+%2B+1+%2B+1+%3D+42&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N = 14 + 10 + 8 + 6 + 2 + 1 + 1 = 42" class="latex" /></p>
<p>So here is Good&#8217;s estimate the chance that the next orchid you collect will be of a new species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bn_1%7D%7BN%7D+%3D+%5Cfrac%7B3%7D%7B42%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{n_1}{N} = &#92;frac{3}{42} } " class="latex" /></p>
<p>Good&#8217;s argument is nontrivial&#8212;and of course it depends on some assumptions on the nature of the distribution of populations of different species!  Since he doesn&#8217;t state these assumptions succinctly and I haven&#8217;t read the paper carefully yet, I&#8217;m afraid you&#8217;ll have to read the paper to find out what they are.</p>
<p>Of course the math works for samples of <i>anything</i> that comes in distinct types, not just species of organisms!  Good considers four examples: </p>
<p>&bull; moths captured in a light-trap at Rothamsted, England,</p>
<p>&bull; words in American newspapers,</p>
<p>&bull; nouns in Macaulay&#8217;s essay on Bacon, </p>
<p>&bull; chess openings in games published by the <i>British Chess Magazine</i> in 1951.</p>
<p>By comparing a small sample to a bigger one, he studies how well his rule works in practice, and apparently it does okay.  </p>
<p>In his paper, I. J. Good thanks Alan Turing for coming up with the basic idea.   In fact he says Turing gave an &#8216;intuitive demonstration&#8217; of it&#8212;but he doesn&#8217;t give this intuitive demonstration, and according to Lou Jost he actually admits somewhere that he forgot it.   </p>
<p>You can read more about the idea here:</p>
<p>&bull; <a href="http://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation">Good&#8211;Turing frequency estimation</a>.</p>
<p>By the way, <a href="http://www.loujost.com/">Lou Jost</a> is not only an expert on <a href="http://www.loujost.com/Statistics%20and%20Physics/Diversity%20and%20Similarity/JostEntropy%20AndDiversity.pdf">biodiversity and its relation to entropy</a>!  He lives in the jungles of Ecuador and has discovered over 60 new species of orchids, including <a href="http://en.wikinews.org/wiki/American_botanist_Lou_Jost_discovers_world%27s_smallest_orchid">the world&#8217;s smallest:</a></p>
<div align="center"><a href="http://en.wikinews.org/wiki/American_botanist_Lou_Jost_discovers_world%27s_smallest_orchid"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Platystele_P5313ruler2.jpg/220px-Platystele_P5313ruler2.jpg" /></a></div>
<p>He found it in Ecuador, and the petals are just a few cells thick!  (Typically, the news reports say he found it in Bolivia and the petals are just one cell thick.)</p>
<p>He said:</p>
<blockquote><p>
I found it among the roots of another plant that I had collected, another small orchid which I took back to grow in my greenhouse to get it to flower. A few months later I saw that down among the roots was a tiny little plant that I realised was more interesting than the bigger orchid. Looking at the flower is often the best way to be able to identify which species of orchid you&#8217;ve got hold of  and can tell you whether you&#8217;re looking at an unknown species or not.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/21/the-mathematics-of-biodiversity-part-1/#comments">25 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/21/the-mathematics-of-biodiversity-part-1/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10114 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-probability" id="post-10114">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/" rel="bookmark">Information Geometry (Part&nbsp;11)</a></h2>
				<small>7 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/">Last time</a> we saw that given a bunch of different species of self-replicating entities, the entropy of their population distribution can go either up or down as time passes.  This is true even in the pathetically simple case where all the replicators have constant fitness&mdash;so they don&#8217;t interact with each other, and don&#8217;t run into any &#8216;limits to growth&#8217;.</p>
<p>This is a bit of a bummer, since it would be nice to use entropy to explain how replicators are always extracting information from their environment, thanks to natural selection.</p>
<p>Luckily, a slight variant of entropy, called &#8216;relative entropy&#8217;, behaves better.  When our replicators have an &#8216;evolutionary stable state&#8217;, the relative entropy is <i>guaranteed to always change in the same direction</i> as time passes!</p>
<p>Thanks to Einstein, we&#8217;ve all heard that times and distances are relative.  But how is entropy relative?</p>
<p>It&#8217;s easy to understand if you think of entropy as lack of information.  Say I have a coin hidden under my hand.  I tell you it&#8217;s heads-up.  How much information did I just give you?  Maybe 1 bit?  That&#8217;s true if you know it&#8217;s a fair coin and I flipped it fairly before covering it up with my hand.  But what if you put the coin down there yourself a minute ago, heads up, and I just put my hand over it?  Then I&#8217;ve given you no information at all.  The difference is the choice of &#8216;prior&#8217;: that is, what probability distribution you attributed to the coin <i>before</i> I gave you my message.</p>
<p>My love affair with relative entropy began in college when my friend Bruce Smith and I read Hugh Everett&#8217;s thesis, <a href="http://www.pbs.org/wgbh/nova/manyworlds/pdf/dissertation.pdf"><i>The Relative State Formulation of Quantum Mechanics</i></a>.  This was the origin of what&#8217;s now often called the &#8216;many-worlds interpretation&#8217; of quantum mechanics.  But it also has a great introduction to relative entropy.  Instead of talking about &#8216;many worlds&#8217;, I wish people would say that Everett explained some of the mysteries of quantum mechanics using the fact that entropy is relative.</p>
<p>Anyway, it&#8217;s nice to see relative entropy showing up in biology.</p>
<h3> Relative Entropy </h3>
<div align="center">
<img src="http://math.ucr.edu/home/baez/mathematical/bertrand's_paradox.png" />
</div>
<p>Inscribe an equilateral triangle in a circle.  Randomly choose a line segment joining two points of this circle.  What is the probability that this segment is longer than a side of the triangle?</p>
<p>This puzzle is called <a href="http://en.wikipedia.org/wiki/Bertrand_paradox_%28probability%29">Bertrand&#8217;s paradox</a>, because different ways of solving it give different answers.   To crack the paradox, you need to realize that it&#8217;s meaningless to say you&#8217;ll &#8220;randomly&#8221; choose something until you say more about how you&#8217;re going to do it.</p>
<p>In other words, you can&#8217;t compute the probability of an event until you pick a recipe for computing probabilities.  Such a recipe is called a <a href="http://en.wikipedia.org/wiki/Probability_measure">probability measure</a>.</p>
<p>This applies to computing entropy, too!   The formula for entropy clearly involves a <a href="http://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a>, even when our set of events is finite:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Csum_i+p_i+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;sum_i p_i &#92;ln(p_i) " class="latex" /></p>
<p>But this formula conceals a fact that becomes obvious when our set of events is infinite.  Now the sum becomes an integral:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Cint_X+p%28x%29+%5Cln%28p%28x%29%29+%5C%2C+d+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;int_X p(x) &#92;ln(p(x)) &#92;, d x" class="latex" /></p>
<p>And now it&#8217;s clear that this formula makes no sense until we choose the <a href="http://en.wikipedia.org/wiki/Measure_%28mathematics%29">measure</a> <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />   On a finite set we have a god-given choice of measure, called <a href="http://en.wikipedia.org/wiki/Counting_measure">counting measure</a>.  Integrals with respect to this are just sums.   But in general we don&#8217;t have such a god-given choice.  And even for finite sets, working with counting measure is a <i>choice</i>: we are <i>choosing</i> to believe that in the absence of further evidence, all options are equally likely.</p>
<p>Taking this fact into account, it seems like we need two things to compute entropy: a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x)" class="latex" />, and a measure <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />   That&#8217;s on the right track.  But an even better way to think of it is this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Cint_X++%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D+%5Cln+%5Cleft%28%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D%5Cright%29+%5C%2C+dx+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;int_X  &#92;frac{p(x) dx}{dx} &#92;ln &#92;left(&#92;frac{p(x) dx}{dx}&#92;right) &#92;, dx }" class="latex" /></p>
<p>Now we see the entropy depends <i>two</i> measures: the probability measure <img src="https://s0.wp.com/latex.php?latex=p%28x%29++dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x)  dx " class="latex" /> we care about, but also the measure <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />  Their ratio is important, but that&#8217;s not enough: we also need one of these measures to do the integral.  Above I used the measure <img src="https://s0.wp.com/latex.php?latex=dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dx" class="latex" /> to do the integral, but we can also use <img src="https://s0.wp.com/latex.php?latex=p%28x%29+dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x) dx" class="latex" /> if we write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Cint_X+%5Cln+%5Cleft%28%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D%5Cright%29+p%28x%29+dx+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;int_X &#92;ln &#92;left(&#92;frac{p(x) dx}{dx}&#92;right) p(x) dx } " class="latex" /></p>
<p>Either way, we are computing the entropy of one measure <i>relative to another</i>.  So we might as well admit it, and talk about <b>relative entropy</b>.</p>
<p>The entropy of the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu" class="latex" /> <b>relative to</b> the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;nu" class="latex" /> is defined by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+S%28d+%5Cmu%2C+d+%5Cnu%29+%26%3D%26+%5Cdisplaystyle%7B+-+%5Cint_X+%5Cfrac%7Bd+%5Cmu%28x%29+%7D%7Bd+%5Cnu%28x%29%7D+%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29++d%5Cnu%28x%29+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cint_X++%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29+d%5Cmu%28x%29+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} S(d &#92;mu, d &#92;nu) &amp;=&amp; &#92;displaystyle{ - &#92;int_X &#92;frac{d &#92;mu(x) }{d &#92;nu(x)} &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right)  d&#92;nu(x) } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;int_X  &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right) d&#92;mu(x) } &#92;end{array} " class="latex" /></p>
<p>The second formula is simpler, but the first looks more like summing <img src="https://s0.wp.com/latex.php?latex=-p+%5Cln%28p%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-p &#92;ln(p)," class="latex" /> so they&#8217;re both useful.</p>
<p>Since we&#8217;re taking entropy to be lack of information, we can also get rid of the minus sign and define <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"><b>relative information</b></a> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+I%28d+%5Cmu%2C+d+%5Cnu%29+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_X+%5Cfrac%7Bd+%5Cmu%28x%29+%7D%7Bd+%5Cnu%28x%29%7D+%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29++d%5Cnu%28x%29+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B++%5Cint_X++%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29+d%5Cmu%28x%29+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} I(d &#92;mu, d &#92;nu) &amp;=&amp; &#92;displaystyle{ &#92;int_X &#92;frac{d &#92;mu(x) }{d &#92;nu(x)} &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right)  d&#92;nu(x) } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{  &#92;int_X  &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right) d&#92;mu(x) } &#92;end{array} " class="latex" /></p>
<p>If you thought something was randomly distributed according to the probability measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cnu%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;nu," class="latex" /> but then you you discover it&#8217;s randomly distributed according to the probability measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu," class="latex" /> how much information have you gained?  The answer is <img src="https://s0.wp.com/latex.php?latex=I%28d%5Cmu%2Cd%5Cnu%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(d&#92;mu,d&#92;nu)." class="latex" /></p>
<p>For more on relative entropy, read <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">Part 6</a> of this series.  I gave some examples illustrating how it works.  Those should convince you that it&#8217;s a useful concept.</p>
<p>Okay: now let&#8217;s switch back to a more lowbrow approach.  In the case of a finite set, we can revert to thinking of our two measures as probability distributions, and write the information gain as</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Cdisplaystyle%7B++%5Csum_i++%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i+%7D%5Cright%29+q_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;displaystyle{  &#92;sum_i  &#92;ln &#92;left(&#92;frac{q_i}{p_i }&#92;right) q_i} " class="latex" /></p>
<p>If you want to sound like a Bayesian, call <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Prior_probability">prior probability distribution</a> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution">posterior probability distribution</a>.  Whatever you call them, <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> is the amount of information you get if you thought <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and someone tells you &#8220;no, <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />!</p>
<p>We&#8217;ll use this idea to think about how a population gains information about its environment as time goes by, thanks to natural selection.  The rest of this post will be an exposition of Theorem 1 in this paper:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>Harper says versions of this theorem ave previously appeared in work by Ethan Akin, and independently in work by Josef Hofbauer and Karl Sigmund.  He also credits others <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15724">here</a>.  An idea this good is rarely noticed by just one person.</p>
<h3> The change in relative information </h3>
<p>So: consider <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different species of replicators.   Let <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> be the population of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, and assume these populations change according to the <b><a href="http://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) &#92;, P_i } " class="latex" /></p>
<p>where each function <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> depends smoothly on all the populations.  And as usual, we let</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_j+P_j%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_j P_j} } " class="latex" /></p>
<p>be the fraction of replicators in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.</p>
<p>Let&#8217;s study the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is some fixed probability distribution.   We&#8217;ll see something great happens when <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a stable equilibrium solution of the replicator equation.  In this case, the relative information can never increase!  It can only decrease or stay constant.</p>
<p>We&#8217;ll think about what all this <i>means</i> later.  First, let&#8217;s see that it&#8217;s true!  Remember,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+I%28q%2Cp%29+%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i++%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7B+p_i+%7D%5Cright%29+q_i+%7D++%5C%5C+%5C%5C+%26%3D%26++%5Cdisplaystyle%7B+%5Csum_i++%5CBig%28%5Cln%28q_i%29+-+%5Cln%28p_i%29+%5CBig%29+q_i+%7D+%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} I(q,p) &amp;=&amp; &#92;displaystyle{ &#92;sum_i  &#92;ln &#92;left(&#92;frac{q_i}{ p_i }&#92;right) q_i }  &#92;&#92; &#92;&#92; &amp;=&amp;  &#92;displaystyle{ &#92;sum_i  &#92;Big(&#92;ln(q_i) - &#92;ln(p_i) &#92;Big) q_i } &#92;end{array}" class="latex" /></p>
<p>and only <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> depends on time, not <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" />, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29%7D++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cfrac%7Bd%7D%7Bdt%7D+%5Csum_i+%5Cln%28p_i%29++q_i+%7D%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Csum_i+%5Cfrac%7B%5Cdot%7Bp%7D_i%7D%7Bp_i%7D+%5C%2C+q_i+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;frac{d}{dt} I(q,p)}  &amp;=&amp; &#92;displaystyle{ - &#92;frac{d}{dt} &#92;sum_i &#92;ln(p_i)  q_i }&#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;sum_i &#92;frac{&#92;dot{p}_i}{p_i} &#92;, q_i } &#92;end{array} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i" class="latex" /> is the rate of change of the probability <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /> We saw a nice formula for this in <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/">Part 9</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, p_i }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_i%28P%29+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P) = f_i(P_1, &#92;dots, P_n) " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_i+f_i%28P%29+p_i++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_i f_i(P) p_i  } " class="latex" /></p>
<p>is the <b>mean fitness</b> of the species.  So, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%7D+%3D+%5Cdisplaystyle%7B+-+%5Csum_i+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+q_i+%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) } = &#92;displaystyle{ - &#92;sum_i &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, q_i }  " class="latex" /></p>
<p>Nice, but we can fiddle with this expression to get something more enlightening.  Remember, the numbers <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> sum to one.  So:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%7D+%26%3D%26++%5Cdisplaystyle%7B++%5Clangle+f%28P%29+%5Crangle+-+%5Csum_i+f_i%28P%29+q_i++%7D+%5C%5C++%5C%5C+%26%3D%26+%5Cdisplaystyle%7B++%5Csum_i+f_i%28P%29+%28p_i+-+q_i%29++%7D++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;frac{d}{dt} I(q,p) } &amp;=&amp;  &#92;displaystyle{  &#92;langle f(P) &#92;rangle - &#92;sum_i f_i(P) q_i  } &#92;&#92;  &#92;&#92; &amp;=&amp; &#92;displaystyle{  &#92;sum_i f_i(P) (p_i - q_i)  }  &#92;end{array} " class="latex" /></p>
<p>where in the last step I used the definition of the mean fitness.  This result looks even cuter if we treat the numbers <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)" class="latex" /> as the components of a vector <img src="https://s0.wp.com/latex.php?latex=f%28P%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)," class="latex" /> and similarly for the numbers <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />  Then we can use the dot product of vectors to say</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%3D+f%28P%29+%5Ccdot+%28p+-+q%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) = f(P) &#92;cdot (p - q) }" class="latex" /></p>
<p>So, the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> will always decrease if</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+%5Ccdot+%28p+-+q%29+%5Cle+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) &#92;cdot (p - q) &#92;le 0" class="latex" /></p>
<p>for all choices of the population <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" /></p>
<p>And now something really nice happens: this is also the condition for <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to be an <a href="http://en.wikipedia.org/wiki/Evolutionarily_stable_state">evolutionarily stable state</a>.  This concept goes back to <a href="http://en.wikipedia.org/wiki/John_Maynard_Smith">John Maynard Smith</a>, the founder of evolutionary game theory.  In 1982 he wrote:</p>
<blockquote><p>
A population is said to be in an evolutionarily stable state if its genetic composition is restored by selection after a disturbance, provided the disturbance is not too large.
</p></blockquote>
<p>I will explain the math next time&#8212;I need to straighten out some things in my mind first.  But the basic idea is compelling: an evolutionarily stable state is like a situation where our replicators &#8216;know all there is to know&#8217; about the environment and each other.  In any other state, the population has &#8216;something left to learn&#8217;&#8212;and the amount left to learn is the relative information we&#8217;ve been talking about!  But as time goes on, the information still left to learn <i>decreases!</i></p>
<p>Note: in the real world, nature has never found an evolutionarily stable state&#8230; except sometimes approximately, on sufficiently short time scales, in sufficiently small regions.   So we are still talking about an idealization of reality!   But that&#8217;s okay, as long as we know it.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/#comments">58 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;11)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/category/probability/page/8/" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/category/probability/page/6/" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the probability category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see whats on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkins environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/probability/page/7/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

				<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = JSON.parse( decodeURIComponent( '%7B%22settings%22%3A%7B%22id%22%3A%22content%22%2C%22ajaxurl%22%3A%22https%3A%5C%2F%5C%2Fjohncarlosbaez.wordpress.com%5C%2F%3Finfinity%3Dscrolling%22%2C%22type%22%3A%22scroll%22%2C%22wrapper%22%3Atrue%2C%22wrapper_class%22%3A%22infinite-wrap%22%2C%22footer%22%3Atrue%2C%22click_handle%22%3A%221%22%2C%22text%22%3A%22Older%20posts%22%2C%22totop%22%3A%22Scroll%20back%20to%20top%22%2C%22currentday%22%3A%2207.06.12%22%2C%22order%22%3A%22DESC%22%2C%22scripts%22%3A%5B%5D%2C%22styles%22%3A%5B%5D%2C%22google_analytics%22%3Afalse%2C%22offset%22%3A7%2C%22history%22%3A%7B%22host%22%3A%22johncarlosbaez.wordpress.com%22%2C%22path%22%3A%22%5C%2Fcategory%5C%2Fprobability%5C%2Fpage%5C%2F%25d%5C%2F%22%2C%22use_trailing_slashes%22%3Atrue%2C%22parameters%22%3A%22%22%7D%2C%22query_args%22%3A%7B%22paged%22%3A7%2C%22category_name%22%3A%22probability%22%2C%22error%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A0%2C%22post_parent%22%3A%22%22%2C%22subpost%22%3A%22%22%2C%22subpost_id%22%3A%22%22%2C%22attachment%22%3A%22%22%2C%22attachment_id%22%3A0%2C%22name%22%3A%22%22%2C%22pagename%22%3A%22%22%2C%22page_id%22%3A0%2C%22second%22%3A%22%22%2C%22minute%22%3A%22%22%2C%22hour%22%3A%22%22%2C%22day%22%3A0%2C%22monthnum%22%3A0%2C%22year%22%3A0%2C%22w%22%3A0%2C%22tag%22%3A%22%22%2C%22cat%22%3A10451%2C%22tag_id%22%3A%22%22%2C%22author%22%3A%22%22%2C%22author_name%22%3A%22%22%2C%22feed%22%3A%22%22%2C%22tb%22%3A%22%22%2C%22meta_key%22%3A%22%22%2C%22meta_value%22%3A%22%22%2C%22preview%22%3A%22%22%2C%22s%22%3A%22%22%2C%22sentence%22%3A%22%22%2C%22title%22%3A%22%22%2C%22fields%22%3A%22%22%2C%22menu_order%22%3A%22%22%2C%22embed%22%3A%22%22%2C%22category__in%22%3A%5B%5D%2C%22category__not_in%22%3A%5B%5D%2C%22category__and%22%3A%5B%5D%2C%22post__in%22%3A%5B%5D%2C%22post__not_in%22%3A%5B%5D%2C%22post_name__in%22%3A%5B%5D%2C%22tag__in%22%3A%5B%5D%2C%22tag__not_in%22%3A%5B%5D%2C%22tag__and%22%3A%5B%5D%2C%22tag_slug__in%22%3A%5B%5D%2C%22tag_slug__and%22%3A%5B%5D%2C%22post_parent__in%22%3A%5B%5D%2C%22post_parent__not_in%22%3A%5B%5D%2C%22author__in%22%3A%5B%5D%2C%22author__not_in%22%3A%5B%5D%2C%22lazy_load_term_meta%22%3Afalse%2C%22posts_per_page%22%3A10%2C%22ignore_sticky_posts%22%3Afalse%2C%22suppress_filters%22%3Afalse%2C%22cache_results%22%3Afalse%2C%22update_post_term_cache%22%3Atrue%2C%22update_post_meta_cache%22%3Atrue%2C%22post_type%22%3A%22%22%2C%22nopaging%22%3Afalse%2C%22comments_per_page%22%3A%22100%22%2C%22no_found_rows%22%3Afalse%2C%22order%22%3A%22DESC%22%7D%2C%22query_before%22%3A%222021-09-26%2017%3A25%3A23%22%2C%22last_post_date%22%3A%222012-06-07%2009%3A59%3A46%22%2C%22body_class%22%3A%22infinite-scroll%20neverending%22%2C%22loading_text%22%3A%22Loading%20new%20page%22%2C%22stats%22%3A%22blog%3D12777403%26v%3Dwpcom%26tz%3D0%26user_id%3D0%26subd%3Djohncarlosbaez%26x_pagetype%3Dinfinite%22%7D%7D' ) );
		//]]>
		</script>
		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>
		<div id="infinite-footer">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://johncarlosbaez.wordpress.com/" rel="home">
						Azimuth					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a> 				</div>
			</div>
		</div><!-- #infinite-footer -->
		
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/_static/??-eJzTLy/QTc7PK0nNK9EvyClNz8wr1i+uzCtJrMjITM/IAeKS1CJMEWP94uSizIISoOIM5/yiVL2sYh19yo1yKioFmldQADTOPtfW0MzQ1MTczNLYKAsAjy0/rA=='></script>
<script type='text/javascript'>
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "https://s1.wp.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.head.appendChild( corecss );
		var themecssurl = "https://s2.wp.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?m=1363304414h&amp;ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		document.head.appendChild( themecss );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();

	// Infinite scroll support
	if ( typeof( jQuery ) !== 'undefined' ) {
		jQuery( function( $ ) {
			$( document.body ).on( 'post-load', function() {
				SyntaxHighlighter.highlight();
			} );
		} );
	}
</script>
<link rel='stylesheet' id='all-css-0-2' href='https://s1.wp.com/_static/??-eJyFy00OQDAQQOELGUP8xUKcpWoiZVTTadO4vVhY2LB8L/kwOdCHDWQD7hEcx8VYQa38EYUYJRlHHqZoZ6Zci2T4I1YKTukNnvGFgmGaYVHM5M933Wzch7Ktir5p+rpbL+lvP34=?cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F06%2F07%2Finformation-geometry-part-11%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s0.wp.com/_static/??-eJyNkN1uwyAMhV9oDkouKvVi2rMQcCNT/oZN07z9yNRMbTZFu8LH8J2DreYMJkXBKMqxsngjg/neOX5TT1ehQvZ1osiK4oUiyfJTHLzVNlCEURcVNAuWVoEUba68h1q2+6xYlsfRzdmkALmk+wIFW49lYygaXy3yCjWJYUTbtaCDj8xkJxRWXEc2hbJQivyXH8+XNDo0sjfbnG5kMSnNvNq5h84F+ddIT/FGl1QZvXIouY0PW+OAEfJoYdLer1t5Uf9Igu/97WTjPsJ7fxrO534YTr37Ai2cwB8='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			(function() {
				var extend = function(out) {
					out = out || {};

					for (var i = 1; i < arguments.length; i++) {
						if (!arguments[i])
						continue;

						for (var key in arguments[i]) {
						if (arguments[i].hasOwnProperty(key))
							out[key] = arguments[i][key];
						}
					}

					return out;
				};
				extend( window.infiniteScroll.settings.scripts, ["postmessage","mobile-useragent-info","rlt-proxy","jquery-core","jquery-migrate","jquery","wpcom-actionbar-placeholder","grofiles-cards","wpgroho","syntaxhighlighter-core","syntaxhighlighter-brush-cpp","devicepx","the-neverending-homepage","wpcom-masterbar-tracks-js","jquery.wpcom-proxy-request","wp-embed","jetpack-subscriptions-js","swfobject","videopress","jetpack-carousel","tiled-gallery","carousel-wpcom"] );
				extend( window.infiniteScroll.settings.styles, ["the-neverending-homepage","infinity-contempt","wp-block-library","mediaelement","wp-mediaelement","jetpack-layout-grid","jetpack-ratings","coblocks-frontend","wpcom-core-compat-playlist-styles","wpcom-text-widget-styles","wpcom-bbpress2-staff-css","contempt","geo-location-flair","reblogging","a8c-global-print","h4-global","global-styles","jetpack-global-styles-frontend-style","jetpack-carousel-swiper-css","jetpack-carousel","tiled-gallery"] );
			})();
		</script>
				<span id="infinite-aria" aria-live="polite"></span>
		<script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZ6JWlFZCwwMmp3b2ElTTlJckFGLywwRmpGRm5FRXhGU3ZFQ0YwRVtuS3VfbyxuLF82VmdpaD0yUT0tT1J8KzEzd0dhc0wyM255UFNuaFNsd1pCN3BbcUt5NFFlLURPfHJfP3FmMEl1NlM9eWs5OCVvRXAuNkVbP3ZXTUIzfG12c1pJS29DL2lSaDVVcy42M2N1TFhqTXhxVnxGM3xzL101bFIrYkdiRkVubVRTOC49Q1gmYmdzQiwsJiVPQl05M0tEbHd2LFQ='}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>