<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>information and entropy | Azimuth | Page 9</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; information and entropy Category Feed" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IlqVOdbxwfBYOEVOBZSBYeXuz1e6sVrveMDl9yU9AHrOwUyJIJGMTOTSPqUpMz5iQ+mIMttYbuVFMLxChytyMPJXFTBfcGfKNXQPFc6aAfFNq2A9KmobBSRMm+yoCmqJLl5V6gKURJhua4zGHKiM41BB46qxo5eSgOxQRwGvbh4hpG+fc2v8G/S3+pJSbAWU9S9Z9aiR8QfdD9r9bFE2YfN3A7fSF7QY18t4cVlqC4nd29WTzzjkes768+BXsiM4DMV7PtiB4v45kHiOMyQVqFXxGbFF8/pSZe4qP6u5W3T/sx3F3+ACgX/YH?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F03%2F02%2Finformation-geometry-part-7%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/information-and-entropy\/page\/9\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Finformation-and-entropy%2Fpage%2F9%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F03%2F02%2Finformation-geometry-part-7%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="information and entropy &#8211; Page 9 &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about information and entropy written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive paged category category-information-and-entropy category-23375499 paged-9 category-paged-9 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-2576 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2576">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/" rel="bookmark">Information Geometry (Part&nbsp;7)</a></h2>
				<small>2 March, 2011</small><br />


				<div class="entry">
					<p>Today, I want to describe how the <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Fisher information metric</a> is related to <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  I&#8217;ve explained both these concepts separately (click the links for details); now I want to put them together. </p>
<p>But first, let me explain what this whole series of blog posts is about.  Information geometry, obviously!  But what&#8217;s that?</p>
<p>Information geometry is the geometry of &#8216;statistical manifolds&#8217;.   Let me explain that concept twice: first vaguely, and then precisely.  </p>
<p>Vaguely speaking, a statistical manifold is a <a href="http://en.wikipedia.org/wiki/Manifold">manifold</a> whose points are hypotheses about some situation.   For example, suppose you have a coin.  You could have various hypotheses about what happens when you flip it.  For example: you could hypothesize that the coin will land heads up with probability <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is any number between 0 and 1.  This makes the interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,1]" class="latex" /> into a statistical manifold.  Technically this is a manifold <i>with boundary</i>, but that&#8217;s okay.</p>
<p>Or, you could have various hypotheses about the IQ&#8217;s of American politicians.  For example: you could hypothesize that they&#8217;re distributed according to a Gaussian probability distribution with mean <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and standard deviation <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />.  This makes the space of pairs <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x,y)" class="latex" /> into a statistical manifold.  Of course we require <img src="https://s0.wp.com/latex.php?latex=y+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;ge 0" class="latex" />, which gives us a manifold with boundary. We might also want to assume <img src="https://s0.wp.com/latex.php?latex=x+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;ge 0" class="latex" />, which would give us a manifold <i>with corners</i>, but that&#8217;s okay too.  We&#8217;re going to be pretty relaxed about what counts as a &#8216;manifold&#8217; here.</p>
<p>If we have a manifold whose points are hypotheses about some situation, we say the manifold &#8216;parametrizes&#8217; these hypotheses.  So, the concept of statistical manifold is fundamental to the subject known as <a href="http://en.wikipedia.org/wiki/Parametric_statistics">parametric statistics</a>.  </p>
<p>Parametric statistics is a huge subject!   You could say that information geometry is the application of geometry to this subject.</p>
<p>But now let me go ahead and make the idea of &#8216;statistical manifold&#8217; more precise.  There&#8217;s a classical and a quantum version of this idea.  I&#8217;m working at the <a href="http://www.quantumlah.org/">Centre of Quantum Technologies</a>, so I&#8217;m being paid to be quantum&mdash;but today I&#8217;m in a classical mood, so I&#8217;ll only describe the classical version.  Let&#8217;s say a <b>classical statistical manifold</b> is a smooth function <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> from a manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the space of probability distributions on some measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  </p>
<p>We should think of <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> as a space of <b>events</b>.  In our first example, it&#8217;s just <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%2C+T%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{H, T&#92;}" class="latex" />: we flip a coin and it lands either heads up or tails up.   In our second it&#8217;s <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" />: we measure the IQ of an American politician and get some real number.</p>
<p>We should think of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> as a space of <b>hypotheses</b>.  For each point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />, we have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  This is hypothesis about the events in question: for example &#8220;when I flip the coin, there&#8217;s 55% chance that it will land heads up&#8221;, or &#8220;when I measure the IQ of an American politician, the answer will be distributed according to a Gaussian with mean 0 and standard deviation 100.&#8221;</p>
<p>Now, suppose someone hands you a classical statistical manifold <img src="https://s0.wp.com/latex.php?latex=%28M%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(M,p)" class="latex" />.  Each point in <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> is a hypothesis.  Apparently some hypotheses are more similar than others.  It would be nice to make this precise.  So, you might like to define a metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> that says how &#8216;far apart&#8217; two hypotheses are.  People know lots of ways to do this; the challenge is to find ways that have clear meanings.</p>
<p>Last time I explained the concept of <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  Suppose we have two probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />. Then the <b>entropy of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /></b> is the amount of information you gain when you start with the hypothesis <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> but then discover that you should switch to the new improved hypothesis <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  It equals:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp%7D%7Bq%7D+%5C%3B+%5Cln%28%5Cfrac%7Bp%7D%7Bq%7D%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; &#92;frac{p}{q} &#92;; &#92;ln(&#92;frac{p}{q}) &#92;; q d &#92;omega " class="latex" /></p>
<p>You could try to use this to define a distance between points <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> in our statistical manifold, like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29+%3D++%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp_x%7D%7Bp_y%7D+%5C%3B+%5Cln%28%5Cfrac%7Bp_x%7D%7Bp_y%7D%29+%5C%3B+p_y+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y) =  &#92;int_&#92;Omega &#92;; &#92;frac{p_x}{p_y} &#92;; &#92;ln(&#92;frac{p_x}{p_y}) &#92;; p_y d &#92;omega " class="latex" /></p>
<p>This is definitely an important function.  Unfortunately, as I explained <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">last time</a>, it doesn&#8217;t obey the axioms that a distance function should!   Worst of all, it doesn&#8217;t obey the triangle inequality.</p>
<p>Can we &#8216;fix&#8217; it?  Yes, we can!   And when we do, we get the Fisher information metric, which is actually a <i>Riemannian</i> metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  Suppose we put local coordinates on some patch of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> containing the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  Then the <b>Fisher information metric</b> is given by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D%28x%29+%3D+%5Cint_%5COmega++%5Cpartial_i+%28%5Cln+p_x%29+%5C%3B+%5Cpartial_j+%28%5Cln+p_x%29+%5C%3B+p_x+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}(x) = &#92;int_&#92;Omega  &#92;partial_i (&#92;ln p_x) &#92;; &#92;partial_j (&#92;ln p_x) &#92;; p_x d &#92;omega" class="latex" /></p>
<p>You can think of my whole series of articles so far as an attempt to understand this funny-looking formula.   I&#8217;ve shown how to get it from a few different starting-points, most recently back in <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Part 3</a>.  But now let&#8217;s get it starting from relative entropy!</p>
<p>Fix any point in our statistical manifold and choose local coordinates for which this point is the origin, <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" />.  The amount of information we gain if move to some other point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is the relative entropy <img src="https://s0.wp.com/latex.php?latex=S%28x%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,0)" class="latex" />.   But what&#8217;s this like when <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is really close to <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" />?  We can imagine doing a Taylor series expansion of <img src="https://s0.wp.com/latex.php?latex=S%28x%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,0)" class="latex" /> to answer this question.</p>
<p>Surprisingly, to first order the answer is always zero!  Mathematically:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>In plain English: if you change your mind slightly, you  learn a negligible amount &mdash; <i>not</i> an amount proportional to how much you changed your mind.</p>
<p>This must have some profound significance.  I wish I knew what.  Could it mean that people are reluctant to change their minds except in big jumps?</p>
<p>Anyway, if you think about it, this fact makes it obvious that <img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y)" class="latex" /> can&#8217;t obey the triangle inequality.  <img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y)" class="latex" /> could be pretty big, but if we draw a curve from <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" />, and mark <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> closely spaced points <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> on this curve, then <img src="https://s0.wp.com/latex.php?latex=S%28x_%7Bi%2B1%7D%2C+x_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x_{i+1}, x_i)" class="latex" /> is zero to first order, so it must be of order <img src="https://s0.wp.com/latex.php?latex=1%2Fn%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/n^2" class="latex" />, so if the triangle inequality were true we&#8217;d have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28x%2Cy%29+%5Cle+%5Csum_i+S%28x_%7Bi%2Bi%7D%2Cx_i%29+%5Cle+%5Cmathrm%7Bconst%7D+%5C%2C+n+%5Ccdot+%5Cfrac%7B1%7D%7Bn%5E2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(x,y) &#92;le &#92;sum_i S(x_{i+i},x_i) &#92;le &#92;mathrm{const} &#92;, n &#92;cdot &#92;frac{1}{n^2}" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />, which is a contradiction.</p>
<p>In plain English: if you change your mind in one big jump, the amount of information you gain is more than the sum of the amounts you&#8217;d gain if you change your mind in lots of little steps!  This seems pretty darn strange, but the paper I mentioned in <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">part 1</a> helps:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>.</p>
<p>You&#8217;ll see he takes a curve and chops it into lots of little pieces as I just did, and explains what&#8217;s going on.</p>
<p>Okay, so what about second order?  What&#8217;s</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} ?" class="latex" /></p>
<p>Well, this is the punchline of this blog post: <i>it&#8217;s the Fisher information metric:</i></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>And since the Fisher information metric is a <a href="http://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian metric</a>, we can then apply <a href="http://en.wikipedia.org/wiki/Riemannian_manifold#Riemannian_manifolds_as_metric_spaces">the usual recipe</a> and define distances in a way that obeys the triangle inequality.  Crooks calls this distance <b>thermodynamic length</b> in the special case that he considers, and he explains its physical meaning.</p>
<p>Now let me prove that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>This can be somewhat tedious if you do it by straighforwardly grinding it out&mdash;I know, I did it.  So let me show you a better way, which requires more conceptual acrobatics but less brute force.</p>
<p>The trick is to work with the <b>universal</b> statistical manifold for the measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  Namely, we take <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to be the space of <i>all</i> probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />!  This is typically an <i>infinite-dimensional</i> manifold, but that&#8217;s okay: we&#8217;re being relaxed about what counts as a manifold here.  In this case, we don&#8217;t need to write <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> for the probability distribution corresponding to the point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />.  In this case, a point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> just <i>is</i> a probability distribution on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, so we&#8217;ll just call it <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  </p>
<p>If we can prove the formulas for this universal example, they&#8217;ll automatically follow for every other example, by abstract nonsense.  Why?  Because <i>any</i> statistical manifold with measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> is the same as a manifold with a smooth map to the <i>universal</i> statistical manifold!  So, geometrical structures on the universal one <a href="http://en.wikipedia.org/wiki/Pullback_%28differential_geometry%29">&#8216;pull back&#8217;</a> to give structures on all the rest.  The Fisher information metric and the function <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> can be defined as pullbacks in this way!   So, to study them, we can just study the universal example.  </p>
<p>(If you&#8217;re familiar with &#8216;classifying spaces for bundles&#8217; or other sorts of &#8216;classifying spaces&#8217;, all this should seem awfully familiar.  It&#8217;s a standard math trick.)</p>
<p>So, let&#8217;s prove that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i S(x,0)|_{x = 0} = 0" class="latex" /></p>
<p>by proving it in the universal example. Given any probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, and taking a nearby probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bp%7D%7Bq%7D+%3D+1+%2B+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{p}{q} = 1 + f " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is some small function.   We only need to show that <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> is zero to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  And this is pretty easy.  By definition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+%5Cfrac%7Bp%7D%7Bq%7D+%5C%2C+%5Cln%28%5Cfrac%7Bp%7D%7Bq%7D%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; &#92;frac{p}{q} &#92;, &#92;ln(&#92;frac{p}{q}) &#92;; q d &#92;omega " class="latex" /></p>
<p>or in other words,</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+%281+%2B+f%29+%5C%2C+%5Cln%281+%2B+f%29+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; (1 + f) &#92;, &#92;ln(1 + f) &#92;; q d &#92;omega " class="latex" /></p>
<p>We can calculate this to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and show we get zero.   But let&#8217;s actually work it out to second order, since we&#8217;ll need that later:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cln+%281+%2B+f%29+%3D+f+-+%5Cfrac%7B1%7D%7B2%7D+f%5E2+%2B+%5Ccdots+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (1 + f) = f - &#92;frac{1}{2} f^2 + &#92;cdots " class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%281+%2B+f%29+%5C%2C+%5Cln+%281%2B+f%29+%3D+f+%2B+%5Cfrac%7B1%7D%7B2%7D+f%5E2+%2B+%5Ccdots+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1 + f) &#92;, &#92;ln (1+ f) = f + &#92;frac{1}{2} f^2 + &#92;cdots " class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+S%28p%2Cq%29+%26%3D%26+%5Cint_%5COmega+%5C%3B+%281+%2B+f%29+%5C%3B+%5Cln%281+%2B+f%29+%5C%3B+q+d+%5Comega+%5C%5C+%26%3D%26+%5Cint_%5COmega+f+%5C%2C+q+d+%5Comega+%2B+%5Cfrac%7B1%7D%7B2%7D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+%2B+%5Ccdots+%5Cend%7Baligned%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{aligned} S(p,q) &amp;=&amp; &#92;int_&#92;Omega &#92;; (1 + f) &#92;; &#92;ln(1 + f) &#92;; q d &#92;omega &#92;&#92; &amp;=&amp; &#92;int_&#92;Omega f &#92;, q d &#92;omega + &#92;frac{1}{2} &#92;int_&#92;Omega f^2&#92;, q d &#92;omega + &#92;cdots &#92;end{aligned} " class="latex" /></p>
<p>Why does this vanish to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />?  It&#8217;s because <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> are both probability distributions and  <img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" />, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%281+%2B+f%29+%5C%2C+q+d%5Comega+%3D+%5Cint_%5COmega+p+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega (1 + f) &#92;, q d&#92;omega = &#92;int_&#92;Omega p d&#92;omega = 1" class="latex" /></p>
<p>but also</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+q+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega q d&#92;omega = 1" class="latex" /></p>
<p>so subtracting we see</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+f+%5C%2C+q+d%5Comega+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega f &#92;, q d&#92;omega = 0" class="latex" /></p>
<p>So, <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> vanishes to first order in <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  <i>Voil&agrave;!</i></p>
<p>Next let&#8217;s prove the more interesting formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cpartial_j+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;partial_j S(x,0)|_{x = 0} = g_{ij}" class="latex" /></p>
<p>which relates relative entropy to the Fisher information metric. Since both sides are symmetric matrices, it suffices to show their diagonal entries agree in any coordinate system:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bii%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = g_{ii}" class="latex" /></p>
<p>Devoted followers of this series of posts will note that I keep using this trick, which takes advantage of the <a href="http://en.wikipedia.org/wiki/Polarization_identity">polarization identity</a>.  </p>
<p>To prove </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+g_%7Bii%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = g_{ii}" class="latex" /></p>
<p>it&#8217;s enough to consider the universal example.  We take the origin to be some probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and take <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> to be a nearby probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> which is pushed a tiny bit in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate direction.  As before we write <img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" />.  We look at the second-order term in our formula for <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2} &#92;int_&#92;Omega f^2&#92;, q d &#92;omega " class="latex" /></p>
<p>Using the usual second-order Taylor&#8217;s formula, which has a <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> built into it, we can say</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial%5E2_i+S%28x%2C0%29%7C_%7Bx+%3D+0%7D+%3D+%5Cint_%5COmega+f%5E2%5C%2C+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial^2_i S(x,0)|_{x = 0} = &#92;int_&#92;Omega f^2&#92;, q d &#92;omega " class="latex" /> </p>
<p>On the other hand, our formula for the Fisher information metric gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D+%5Cleft.+%5Cint_%5COmega++%5Cpartial_i+%5Cln+p+%5C%3B+%5Cpartial_i+%5Cln+p+%5C%3B+q+d+%5Comega+%5Cright%7C_%7Bp%3Dq%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} = &#92;left. &#92;int_&#92;Omega  &#92;partial_i &#92;ln p &#92;; &#92;partial_i &#92;ln p &#92;; q d &#92;omega &#92;right|_{p=q} " class="latex" /></p>
<p>The right hand sides of the last two formulas look awfully similar!  And indeed they agree, because we can show that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cpartial_i+%5Cln+p+%5Cright%7C_%7Bp+%3D+q%7D+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;partial_i &#92;ln p &#92;right|_{p = q} = f" class="latex" /></p>
<p>How?  Well, we assumed that <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is what we get by taking <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and pushing it a little bit in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate direction; we have also written that little change as</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%2Fq+%3D+1+%2B+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q = 1 + f" class="latex" /></p>
<p>for some small function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />.  So, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%28p%2Fq%29+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i (p/q) = f" class="latex" /></p>
<p>and thus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+p+%3D+f+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i p = f q" class="latex" /></p>
<p>and thus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i+%5Cln+p+%3D+%5Cfrac%7B%5Cpartial_i+p%7D%7Bp%7D+%3D+%5Cfrac%7Bfq%7D%7Bp%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i &#92;ln p = &#92;frac{&#92;partial_i p}{p} = &#92;frac{fq}{p}" class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cpartial_i+%5Cln+p+%5Cright%7C_%7Bp%3Dq%7D+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;partial_i &#92;ln p &#92;right|_{p=q} = f" class="latex" /></p>
<p>as desired.</p>
<p>This argument may seem a little hand-wavy and nonrigorous, with words like &#8216;a little bit&#8217;.  If you&#8217;re used to taking arguments involving infinitesimal changes and translating them into calculus (or differential geometry), it should make sense.  If it doesn&#8217;t, I apologize.  It&#8217;s easy to make it more rigorous, but only at the cost of more annoying notation, which doesn&#8217;t seem good in a blog post.</p>
<h4>Boring technicalities</h4>
<p>If you&#8217;re actually the kind of person who reads a section called &#8216;boring technicalities&#8217;, I&#8217;ll admit to you that my calculations don&#8217;t make sense if the integrals diverge, or we&#8217;re dividing by zero in the ratio <img src="https://s0.wp.com/latex.php?latex=p%2Fq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q" class="latex" />.  To avoid these problems, here&#8217;s what we should do.  Fix a <a href="http://en.wikipedia.org/wiki/%CE%A3-finite_measure"><img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma" class="latex" />-finite</a> measure space <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega, d&#92;omega)" class="latex" />.  Then, define the <b>universal statistical manifold</b> to be the space <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> consisting of all probability measures that are <a href="http://en.wikipedia.org/wiki/Equivalence_%28measure_theory%29">equivalent</a> to <img src="https://s0.wp.com/latex.php?latex=d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;omega" class="latex" />, in the usual sense of measure theory.  By <a href="http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem">Radon-Nikodym</a>, we can write any such measure as <img src="https://s0.wp.com/latex.php?latex=q+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q d &#92;omega" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in L^1(&#92;Omega, d&#92;omega)" class="latex" />.  Moreover, given two of these guys, say <img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q d&#92;omega" class="latex" />, they are <a href="http://en.wikipedia.org/wiki/Absolute_continuity#Absolute_continuity_of_measures">absolutely continuous</a> with respect to each other, so we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega+%3D+%5Cfrac%7Bp%7D%7Bq%7D+%5C%3B+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega = &#92;frac{p}{q} &#92;; q d &#92;omega " class="latex" /></p>
<p>where the ratio <img src="https://s0.wp.com/latex.php?latex=p%2Fq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p/q" class="latex" /> is well-defined almost everywhere and lies in <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+q+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, q d&#92;omega)" class="latex" />.  This is enough to guarantee that we&#8217;re never dividing by zero, and I think it&#8217;s enough to make sure all my integrals converge.</p>
<p>We do still need to make <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> into some sort of infinite-dimensional manifold, to justify all the derivatives.   There are various ways to approach this issue, all of which start from the fact that <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, d&#92;omega)" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Banach_space">Banach space</a>, which is about the nicest sort of infinite-dimensional manifold one could imagine.  Sitting in <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega, d&#92;omega)" class="latex" /> is the hyperplane consisting of functions <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> with </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+q+d%5Comega+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega q d&#92;omega = 1" class="latex" /></p>
<p>and this is a <a href="http://en.wikipedia.org/wiki/Banach_manifold">Banach manifold</a>.  To get <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> we need to take a subspace of that hyperplane.  If this subspace were open then <img src="https://s0.wp.com/latex.php?latex=P%28%5COmega%2Cd+%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(&#92;Omega,d &#92;omega)" class="latex" /> would be a Banach manifold in its own right.  I haven&#8217;t checked this yet, for various reasons.  </p>
<p>For one thing, there&#8217;s a nice theory of <a href="http://en.wikipedia.org/wiki/Diffeology">&#8216;diffeological spaces&#8217;</a>, which generalize manifolds.  Every Banach manifold is a diffeological space, and every subset of a diffeological space is again a diffeological space.  For many purposes we don&#8217;t need our &#8216;statistical manifolds&#8217; to be manifolds: diffeological spaces will do just fine.  This is one reason why I&#8217;m being pretty relaxed here about what counts as a &#8216;manifold&#8217;.</p>
<p>For another, I know that people have worked out a lot of this stuff, so I can just look things up when I need to.  And so can you!  This book is a good place to start:</p>
<p>&bull; Paolo Gibilisco, Eva Riccomagno, Maria Piera Rogantin and Henry P. Wynn, <i>Algebraic and Geometric Methods in Statistics</i>, Cambridge U. Press, Cambridge, 2009.</p>
<p>I find the chapters by Raymond Streater especially congenial.  For the technical issue I&#8217;m talking about now it&#8217;s worth reading section 14.2, &#8220;Manifolds modelled by Orlicz spaces&#8221;, which tackles the problem of constructing a universal statistical manifold in a more sophisticated way than I&#8217;ve just done.  And in chapter 15, &#8220;The Banach manifold of quantum states&#8221;, he tackles the quantum version!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/#comments">25 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;7)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2467 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-2467">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" rel="bookmark">R&eacute;nyi Entropy and Free&nbsp;Energy</a></h2>
				<small>10 February, 2011</small><br />


				<div class="entry">
					<p>I want to keep telling you about information geometry&#8230; but I got sidetracked into thinking about something slightly different, thanks to some fascinating discussions here at the CQT.  </p>
<p>There are a lot of people interested in entropy here, so some of us &mdash; <a href="http://arxiv.org/find/all/1/all:+AND+oscar+dahlsten/0/1/0/all/0/1">Oscar Dahlsten</a>, <a href="http://arxiv.org/find/all/1/all:+AND+mile+gu/0/1/0/all/0/1">Mile Gu</a>, <a href="http://arxiv.org/find/all/1/all:+AND+elisabeth+rieper/0/1/0/all/0/1">Elisabeth Rieper</a>, <a href="http://arxiv.org/find/all/1/all:+AND+wonmin+son/0/1/0/all/0/1">Wonmin Son</a> and me &mdash; decided to start meeting more or less regularly.  I call it the Entropy Club.  I&#8217;m learning a lot of wonderful things, and I hope to tell you about them someday.  But for now, here&#8217;s a little idea I came up with, triggered by our conversations:</p>
<p>&bull; John Baez, <a href="http://arxiv.org/abs/1102.2098">R&eacute;nyi entropy and free energy</a>.</p>
<p>In 1960, Alfred R&eacute;nyi defined a generalization of the usual Shannon entropy that depends on a parameter.  If <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a probability distribution on a finite set, its <b><a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a></b> of order <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is defined to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Cbeta+%3D+%5Cfrac%7B1%7D%7B1+-+%5Cbeta%7D+%5Cln+%5Csum_i+p_i%5E%5Cbeta+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;beta = &#92;frac{1}{1 - &#92;beta} &#92;ln &#92;sum_i p_i^&#92;beta } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+%5Cbeta+%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le &#92;beta &lt; &#92;infty" class="latex" />.   This looks pretty weird at first, and we need <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cne+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;ne 1" class="latex" /> to avoid dividing by zero, but you can show that the R&eacute;nyi entropy approaches the <b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a></b> as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> approaches<br />
1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7B%5Cbeta+%5Cto+1%7D+H_%5Cbeta+%3D+-%5Csum_%7Bi%7D+p_i+%5Cln+p_i+.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lim_{&#92;beta &#92;to 1} H_&#92;beta = -&#92;sum_{i} p_i &#92;ln p_i . " class="latex" /></p>
<p>(A fun puzzle, which I leave to you.)  So, it&#8217;s customary to define <img src="https://s0.wp.com/latex.php?latex=H_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_1" class="latex" /> to be the Shannon entropy&#8230; and then the R&eacute;nyi entropy generalizes the Shannon entropy by allowing an adjustable parameter <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />. </p>
<p>But what does it <i>mean?</i></p>
<p>If you ask people what&#8217;s good about the R&eacute;nyi entropy, they&#8217;ll usually say: it&#8217;s additive!  In other words, when you combine two independent probability distributions into a single one, their R&eacute;nyi entropies add.  And that&#8217;s true &mdash; but there are other quantities that have the same property.  So I wanted a better way to think about R&eacute;nyi entropy, and here&#8217;s what I&#8217;ve come up with so far.</p>
<p>Any probability distribution can be seen as the state of thermal equilibrium for some Hamiltonian at some fixed temperature, say <img src="https://s0.wp.com/latex.php?latex=T+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = 1" class="latex" />.  And that Hamiltonian is unique.  Starting with that Hamiltonian, we can then compute the free energy <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> at any temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />, and up to a certain factor this free energy turns out to be the R&eacute;nyi entropy <img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" />.  More precisely:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%281+-+T%29+H_%5Cbeta.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = (1 - T) H_&#92;beta. " class="latex" /></p>
<p>So, up to the fudge factor <img src="https://s0.wp.com/latex.php?latex=1+-+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 - T" class="latex" />, R&eacute;nyi entropy is the same as free energy. It seems like a good thing to know &mdash; but I haven&#039;t seen anyone say it anywhere!  Have you?</p>
<p>Let me show you why it&#8217;s true &mdash; the proof is pathetically simple.  We start with our probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />.  We can always write</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+e%5E%7B-+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = e^{- E_i} " class="latex" /></p>
<p>for some real numbers <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" />.   Let&#8217;s think of these numbers <img src="https://s0.wp.com/latex.php?latex=E_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_i" class="latex" /> as energies.  Then the state of thermal equilibrium, also known as the <b><a href="http://en.wikipedia.org/wiki/Canonical_ensemble">canonical ensemble</a></b> or <b><a href="http://en.wikipedia.org/wiki/Gibbs_state">Gibbs state</a></b> at inverse temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Be%5E%7B-+%5Cbeta+E_i%7D%7D%7BZ%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{e^{- &#92;beta E_i}}{Z} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is the <b><a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29#Definition">partition function</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_i+e%5E%7B-%5Cbeta+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_i e^{-&#92;beta E_i} " class="latex" /></p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=Z+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = 1" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />, the Gibbs state reduces to our original probability distribution at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />.</p>
<p>Now in thermodynamics, the quantity</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-+%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = - &#92;frac{1}{&#92;beta} &#92;ln Z " class="latex" /></p>
<p>is called the <b><a href="http://en.wikipedia.org/wiki/Helmholtz_free_energy#Relation_to_the_partition_function">free energy</a></b>.  It&#8217;s important, because it equals the total expected energy of our system, minus the energy in the form of heat.   Roughly speaking, it&#8217;s the energy that you can use.</p>
<p>Let&#8217;s see how the R&eacute;nyi entropy is related to the free energy.  The proof is a trivial calculation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+%5Cbeta+F+%3D+%5Cln+Z+%3D+%5Cln+%5Csum_%7Bi+%5Cin+X%7D+e%5E%7B-%5Cbeta+E_i%7D+%3D+%5Cln+%5Csum_%7Bi+%5Cin+X%7D+p_i%5E%5Cbeta+%3D+%281+-+%5Cbeta%29+H_%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;beta F = &#92;ln Z = &#92;ln &#92;sum_{i &#92;in X} e^{-&#92;beta E_i} = &#92;ln &#92;sum_{i &#92;in X} p_i^&#92;beta = (1 - &#92;beta) H_&#92;beta " class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%5Cbeta+%3D+-++%5Cfrac%7B%5Cbeta%7D%7B1+-+%5Cbeta%7D+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_&#92;beta = -  &#92;frac{&#92;beta}{1 - &#92;beta} F" class="latex" /></p>
<p>at least for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cne+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;ne 1" class="latex" />.  But you can also check that both sides of this equation have well-defined limits as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to 1" class="latex" />.  </p>
<p>The relation between free energy and R&eacute;nyi entropy looks even neater if we solve for <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> and write the answer using <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%281+-+T%29H_%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = (1 - T)H_&#92;beta " class="latex" /></p>
<p>So, what&#8217;s this fact good for?  I&#8217;m not sure yet!  In my paper, I combine it with this equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+E+%5Crangle+-+T+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle E &#92;rangle - T S " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle" class="latex" /> is the expected energy in the Gibbs state at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+E+%5Crangle+%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Csum_i+E_i+%5C%2C+e%5E%7B-%5Cbeta+E_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle E &#92;rangle = &#92;frac{1}{Z} &#92;sum_i E_i &#92;, e^{-&#92;beta E_i} " class="latex" /></p>
<p>while <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the usual Shannon entropy of this Gibbs state.  I also show that all this stuff works quantum-mechanically as well as classically.  But so far, it seems the main benefit is that R&eacute;nyi entropy has become a lot less mysterious.  It&#8217;s not a mutant version of Shannon entropy: it&#8217;s just a familiar friend in disguise.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/#comments">138 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" rel="bookmark" title="Permanent Link to R&eacute;nyi Entropy and Free&nbsp;Energy">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2298 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2298">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="bookmark">Information Geometry (Part&nbsp;6)</a></h2>
				<small>21 January, 2011</small><br />


				<div class="entry">
					<p>So far, my thread on information geometry hasn&#8217;t said much about <i>information</i>.  It&#8217;s time to remedy that.</p>
<p>I&#8217;ve been telling you about the Fisher information metric.  In statistics this is nice a way to define a &#8216;distance&#8217; between two probability distributions.  But it also has a quantum version.  </p>
<p>So far I&#8217;ve showed you how to define the Fisher information metric in three equivalent ways.  I also showed that in the quantum case, the Fisher information metric is the real part of a complex-valued thing.  The imaginary part is related to the uncertainty principle.</p>
<p>You can see it all here:</p>
<p> <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">Part 1</a> &nbsp; &nbsp;  <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/">Part 2</a>  &nbsp; &nbsp;  <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/">Part 3</a>  &nbsp; &nbsp;  <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">Part 4</a> &nbsp; &nbsp;  <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/">Part 5</a></p>
<p>But there&#8217;s yet another way to define the Fisher information metric, which really involves <i>information</i>.   </p>
<p>To explain this, I need to start with the idea of &#8216;information gain&#8217;, or &#8216;relative entropy&#8217;.   And it looks like I should do a whole post on this.</p>
<p>So: </p>
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Measure_%28mathematics%29#Definition">measure space</a> &mdash; that is, a space you can do integrals over.  By a <b>probability distribution</b> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, I&#8217;ll mean a nonnegative function </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3A+%5COmega+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : &#92;Omega &#92;to &#92;mathbb{R}" class="latex" /> </p>
<p>whose integral is 1.  Here <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" /> is my name for the measure on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  Physicists might call <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> the &#8216;phase space&#8217; of some classical system, but probability theorists might call it a space of &#8216;events&#8217;.  Today I&#8217;ll use the probability theorist&#8217;s language.  The idea here is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_A+%5C%3B+p%28%5Comega%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_A &#92;; p(&#92;omega) &#92;; d &#92;omega " class="latex" /></p>
<p>gives the probability that when an event happens, it&#8217;ll be one in the subset <img src="https://s0.wp.com/latex.php?latex=A+%5Csubseteq+%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;subseteq &#92;Omega" class="latex" />.  That&#8217;s why we want </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ge 0" class="latex" /></p>
<p>Probabilities are supposed to be nonnegative.  And that&#8217;s also why we want</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5C%3B+d+%5Comega+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;; d &#92;omega = 1 " class="latex" /></p>
<p>This says that the probability of <i>some</i> event happening is 1.</p>
<p>Now, suppose we have two probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  The <b><a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">information gain</a></b> as we go from <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>We also call this the entropy of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> <b>relative to</b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  It says how much information you learn if you discover that the probability distribution of an event is <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, if before you had thought it was <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.</p>
<p>I like relative entropy because it&#8217;s related to the <a href="http://en.wikipedia.org/wiki/Bayesian_probability">Bayesian interpretation of probability</a>.  The idea here is that you can&#8217;t really &#8216;observe&#8217; probabilities as frequencies of events, except in some unattainable limit where you repeat an experiment over and over infinitely many times.  Instead, you start with some hypothesis about how likely things are: a probability distribution called the <a href="http://en.wikipedia.org/wiki/Prior_probability"><b>prior</b></a>.  Then you update this using <a href="http://yudkowsky.net/rational/bayes">Bayes&#8217; rule</a> when you gain new information.  The updated probability distribution &mdash; your new improved hypothesis &mdash; is called the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution"><b>posterior</b></a>.  </p>
<p>And if you don&#8217;t do the updating right, you need a swift kick in the posterior!</p>
<p>So, we can think of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as the prior probability distribution, and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as the posterior.  Then <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> measures the <i>amount of information</i> that caused you to change your views.</p>
<p>For example, suppose you&#8217;re flipping a coin, so your set of events is just </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5COmega+%3D+%5C%7B+%5Cmathrm%7Bheads%7D%2C+%5Cmathrm%7Btails%7D+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega = &#92;{ &#92;mathrm{heads}, &#92;mathrm{tails} &#92;}" class="latex" /></p>
<p>In this case all the integrals are just sums with two terms.  Suppose your prior assumption is that the coin is fair.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=q%28%5Cmathrm%7Bheads%7D%29+%3D+1%2F2%2C+%5C%3B+q%28%5Cmathrm%7Btails%7D%29+%3D+1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;mathrm{heads}) = 1/2, &#92;; q(&#92;mathrm{tails}) = 1/2" class="latex" /></p>
<p>But then suppose someone you trust comes up and says &#8220;Sorry, that&#8217;s a trick coin: it always comes up heads!&#8221;  So you update our probability distribution and get this posterior:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathrm%7Bheads%7D%29+%3D+1%2C+%5C%3B+p%28%5Cmathrm%7Btails%7D%29+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;mathrm{heads}) = 1, &#92;; p(&#92;mathrm{tails}) = 0 " class="latex" /></p>
<p>How much information have you gained?  Or in other words, what&#8217;s the relative entropy?  It&#8217;s this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+%3D+1+%5Ccdot+%5Clog%28%5Cfrac%7B1%7D%7B1%2F2%7D%29+%2B+0+%5Ccdot+%5Clog%28%5Cfrac%7B0%7D%7B1%2F2%7D%29+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega = 1 &#92;cdot &#92;log(&#92;frac{1}{1/2}) + 0 &#92;cdot &#92;log(&#92;frac{0}{1/2}) = 1 " class="latex" /></p>
<p>Here I&#8217;m doing the logarithm in base 2, and you&#8217;re supposed to know that in this game <img src="https://s0.wp.com/latex.php?latex=0+%5Clog+0+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;log 0 = 0" class="latex" />.  </p>
<p>So: you&#8217;ve learned <i>one bit of information!</i></p>
<p>That&#8217;s supposed to make perfect sense.  On the other hand, the reverse scenario takes a bit more thought. </p>
<p>You start out feeling sure that the coin always lands heads up.  Then someone you trust says &#8220;No, that&#8217;s a perfectly fair coin.&#8221;   If you work out the amount of information you learned this time, you&#8217;ll see it&#8217;s <i>infinite</i>.  </p>
<p>Why is that?</p>
<p>The reason is that something that you thought was impossible &mdash; the coin landing tails up &mdash; turned out to be possible.  In this game, it counts as infinitely shocking to learn something like that, so the information gain is infinite.   If you hadn&#8217;t been so darn sure of yourself &mdash; if you had just believed that the coin <i>almost always</i> landed heads up &mdash; your information gain would be large but finite.</p>
<p>The Bayesian philosophy is built into the concept of information gain, because information gain depends on two things: the prior and the posterior.  And that&#8217;s just as it should be: <i>you can only say how much you learned if you know what you believed beforehand!</i></p>
<p>You might say that information gain depends on <i>three</i> things: <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" />.  And you&#8217;d be right!    Unfortunately, the notation <img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q)" class="latex" /> is a bit misleading.   Information gain really <i>does</i> depend on just two things, but these things are not <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />: they&#8217;re <img src="https://s0.wp.com/latex.php?latex=p%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;omega) d&#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;omega) d&#92;omega" class="latex" />.   These are called <a href="http://en.wikipedia.org/wiki/Probability_measure">probability measures</a>, and they&#8217;re ultimately more important than the probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />. </p>
<p>To see this, take our information gain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>and juggle it ever so slightly to get this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B++%5Clog%28%5Cfrac%7Bp%28%5Comega%29+d%5Comega%7D%7Bq%28%5Comega%29d+%5Comega%7D%29+%5C%3B+p%28%5Comega%29+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;;  &#92;log(&#92;frac{p(&#92;omega) d&#92;omega}{q(&#92;omega)d &#92;omega}) &#92;; p(&#92;omega) d &#92;omega " class="latex" /></p>
<p>Clearly this depends only on <img src="https://s0.wp.com/latex.php?latex=p%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(&#92;omega) d&#92;omega" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%28%5Comega%29+d%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q(&#92;omega) d&#92;omega" class="latex" />.  Indeed, it&#8217;s good to work directly with these probability measures and give them short names, like</p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cmu+%3D+p%28%5Comega%29+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;mu = p(&#92;omega) d &#92;omega " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=d%5Cnu+%3D+q%28%5Comega%29+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;nu = q(&#92;omega) d &#92;omega" class="latex" /></p>
<p>Then the formula for information gain looks more slick:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%5COmega+%5C%3B+%5Clog%28%5Cfrac%7Bd%5Cmu%7D%7Bd%5Cnu%7D%29+%5C%3B+d%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_&#92;Omega &#92;; &#92;log(&#92;frac{d&#92;mu}{d&#92;nu}) &#92;; d&#92;mu " class="latex" /></p>
<p>And by the way, in case you&#8217;re wondering, the <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d" class="latex" /> here doesn&#8217;t actually mean much: we&#8217;re just so brainwashed into wanting a <img src="https://s0.wp.com/latex.php?latex=d+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x" class="latex" /> in our integrals that people often use <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu" class="latex" /> for a measure even though the simpler notation <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> might be more logical.  So, the function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Cmu%7D%7Bd%5Cnu%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{d&#92;mu}{d&#92;nu} " class="latex" /></p>
<p>is really just a ratio of probability measures, but people call it a <a href="http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem">Radon-Nikodym derivative</a>, because it looks like a derivative (and in some important examples it actually is).   So, if I were talking to myself, I could have shortened this blog entry immensely by working with directly probability measures, leaving out the <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d" class="latex" />&#8216;s, and saying:</p>
<blockquote><p>
Suppose <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" /> are probability measures; then the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" />, or information gain, is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Cmu%2C+%5Cnu%29+%3D++%5Cint_%5COmega+%5C%3B+%5Clog%28%5Cfrac%7B%5Cmu%7D%7B%5Cnu%7D%29+%5C%3B+%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(&#92;mu, &#92;nu) =  &#92;int_&#92;Omega &#92;; &#92;log(&#92;frac{&#92;mu}{&#92;nu}) &#92;; &#92;mu " class="latex" /></p></blockquote>
<p>But I&#8217;m under the impression that people are actually reading this stuff, and that most of you are happier with functions than measures.  So, I decided to start with</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>and then gradually work my way up to the more sophisticated way to think about relative entropy!  But having gotten that off my chest, now I&#8217;ll revert to the original naive way.</p>
<p>As a warmup for next time, let me pose a question.  How much is this quantity</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D++%5Cint_%5COmega+%5C%3B+p%28%5Comega%29+%5Clog%28%5Cfrac%7Bp%28%5Comega%29%7D%7Bq%28%5Comega%29%7D%29+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) =  &#92;int_&#92;Omega &#92;; p(&#92;omega) &#92;log(&#92;frac{p(&#92;omega)}{q(&#92;omega)}) &#92;; d &#92;omega " class="latex" /></p>
<p>like a <i>distance</i> between probability distributions?  A distance function, or <a href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29">metric</a>, is supposed to satisfy some axioms.  Alas, relative entropy satisfies some of these, but not the most interesting one!</p>
<p>&bull; If you&#8217;ve got a metric, the distance between points should always be nonnegative. Indeed, this holds:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) &#92;ge 0" class="latex" /></p>
<p>So, we never learn a negative amount when we update our prior, at least according to this definition.   It&#8217;s a fun exercise to prove this inequality, at least if you know some tricks involving inequalities and convex functions &mdash; otherwise it might be hard.  </p>
<p>&bull;  If you&#8217;ve got a metric, the distance between two points should only be zero if they&#8217;re really the same point.  In fact, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) = 0" class="latex" /> </p>
<p>if and only if </p>
<p><img src="https://s0.wp.com/latex.php?latex=p+d%5Comega+%3D+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d&#92;omega = q d &#92;omega " class="latex" /></p>
<p>It&#8217;s possible to have <img src="https://s0.wp.com/latex.php?latex=p+d%5Comega+%3D+q+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d&#92;omega = q d &#92;omega " class="latex" /> even if <img src="https://s0.wp.com/latex.php?latex=p+%5Cne+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;ne q" class="latex" />, because <img src="https://s0.wp.com/latex.php?latex=d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;omega" class="latex" /> can be zero somewhere.  But this is just more evidence that we should really be talking about the probability measure <img src="https://s0.wp.com/latex.php?latex=p+d+%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p d &#92;omega" class="latex" /> instead of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  If we do that, we&#8217;re okay so far!</p>
<p>&bull; If you&#8217;ve got a metric, the distance from your first point to your second point is the same as the distance from the second to the first.  Alas, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%2Cq%29+%5Cne+S%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p,q) &#92;ne S(q,p)" class="latex" /></p>
<p>in general.  We already saw this in our example of the flipped coin.  This is a slight bummer, but I could live with it, since <a href="http://www.tac.mta.ca/tac/reprints/articles/1/tr1.pdf">Lawvere has already shown</a> that it&#8217;s wise to generalize the concept of metric by dropping this axiom.</p>
<p>&bull; If you&#8217;ve got a metric, it obeys the <a href="http://en.wikipedia.org/wiki/Triangle_inequality">triangle inequality</a>.  This is the really interesting axiom, and alas, this too fails.  Later we&#8217;ll see why.</p>
<p>So, relative entropy does a fairly miserable job of acting like a distance function.  People call it a <a href="http://en.wikipedia.org/wiki/Statistical_distance">divergence</a>.  In fact, they often call it the <b>Kullback-Leibler divergence</b>.  I don&#8217;t like that, because &#8216;the Kullback-Leibler divergence&#8217;  doesn&#8217;t really explain the idea: it sounds more like the title of a bad spy novel.  &#8216;Relative entropy&#8217;, on the other hand, makes a lot of sense if you understand entropy.  And &#8216;information gain&#8217; makes sense if you understand information.</p>
<p>Anyway: how can we save this miserable attempt to get a distance function on the space of probability distributions?  <i>Simple: take its matrix of second derivatives and use that to define a Riemannian metric</i> <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.  This Riemannian metric in turn defines a metric of the more elementary sort we&#8217;ve been discussing today.</p>
<p>And this Riemannian metric is the Fisher information metric I&#8217;ve been talking about all along!</p>
<p>More details later, I hope.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/#comments">21 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;6)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2197 post type-post status-publish format-standard hentry category-information-and-entropy category-quantum-technologies" id="post-2197">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/01/11/quantum-information-processing-2011/" rel="bookmark">Quantum Information Processing 2011 (Day&nbsp;1)</a></h2>
				<small>11 January, 2011</small><br />


				<div class="entry">
					<p>This year&#8217;s session of the big conference on quantum computation, quantum cryptography, and so on is being held in Singapore this year: </p>
<p>&bull; <a href="http://qip2011.quantumlah.org/">QIP 2011</a>, the 14th Workshop on Quantum Information Processing, 8-14 January 2011, Singapore.</p>
<p>Because the battery on my laptop is old and not very energetic, and I can&#8217;t find any sockets in the huge ballroom where the talks are being delivered, I can&#8217;t live-blog the talks.  So, my reports here will be quite incomplete.</p>
<p>Here are microscopic summaries of <i>just three</i> talks from Monday&#8217;s session.  You can see arXiv references, slides, and videos of the talks <a href="http://qip2011.quantumlah.org/scientificprogramme/">here</a>.   I&#8217;ll just give links to the slides.  </p>
<p><a href="http://qip2011.quantumlah.org/scientificprogramme/abstract/1011.0105p.pdf">Christian Kurtsiefer</a> gave a nice talk on how to exploit the physics of photodetectors to attack quantum key distribution systems!  By cutting the optical fiber and shining a lot of light down both directions, evil Eve can &#8216;blind&#8217; Alice and Bob&#8217;s photodetectors.  Then, by shining a quick even brighter pulse of light, she can fool one of their photodetectors into thinking it&#8217;s seen a single photon.   She can even fool them into thinking they&#8217;ve seen a violation of Bell&#8217;s inequality, by purely classical means, thanks to the fact that only a small percentage of photons make it down the cable in the first place.  Christian and his collaborators have actually done this trick in an experiment here at the CQT!</p>
<p><a href="http://qip2011.quantumlah.org/scientificprogramme/abstract/1009.2840p.pdf">Tzu-Chieh Wei and Akimasa Miyake</a> gave a two-part joint talk on how the AKLT ground state is universal for measurement-based quantum computation.  The AKLT ground state works like this: you&#8217;ve got a hexagonal lattice with three spin-1/2 particles at each vertex.  Think of each particle as attached to one of the three edges coming out of that vertex.  In the ground state, you start by putting the pair of particles at the ends of each edge in the spin-0 (also known as &#8220;singlet&#8221;, or antisymmetrized) state, and then you project the three particles at each vertex down to the spin-3/2 (completely symmetrized) state.  This is indeed the ground state of a cleverly chosen antiferromagnetic Hamiltonian.  But has anyone ever prepared this sort of system in the lab?</p>
<p><a href="http://qip2011.quantumlah.org/scientificprogramme/abstract/158p.pdf">David Poulin</a> gave a talk on how to efficiently compute time evolution given a time-dependent quantum Hamiltonian.  The trickiness arises from Hamiltonians that change very rapidly with time.  In a naive evenly spaced discretization of the time-ordered exponential, this would require you to use lots of tiny time steps to get a good approximation.  But using a clever <i>randomly chosen</i> discretization you can avoid this problem, at least for uniformly bounded Hamiltonians, those obeying:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7C+H%28t%29+%5C%7C+%5Cle+K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;| H(t) &#92;| &#92;le K" class="latex" /></p>
<p>for all times <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.  The key is that the high-frequency part of a time-dependent Hamiltonian only couples faraway energy levels, but a uniformly bounded Hamiltonian <i>doesn&#8217;t have</i> faraway energy levels.</p>
<p>A couple more things &mdash; really just notes to myself: </p>
<p><a href="http://www.perimeterinstitute.ca/personal/sflammia/">Steve Flammia</a> told me about this paper relating the Cramer-Rao bound (which involves Fisher information) to the time-energy uncertainty principle:</p>
<p>&bull; Sergio Boixo, Steven T. Flammia, Carlton M. Caves, and J.M. Geremia, <a href="http://arxiv.org/abs/quant-ph/0609179">Generalized limits for single-parameter quantum estimation</a>.</p>
<p><a href="http://www.perimeterinstitute.ca/index.php?option=com_content&amp;task=view&amp;id=30&amp;Itemid=72&amp;pi=Markus_Mueller">Markus M&uuml;ller</a> told me about a paper mentioning relations between Maxwell&#8217;s demon and algorithmic entropy.  I need to get some references on this work &mdash; it might help me make progress on <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/">algorithmic thermodynamics</a>.  It&#8217;s probably one of these:</p>
<p>&bull; Markus M&uuml;ller, <a href="http://arxiv.org/abs/0712.4377">Quantum Kolmogorov complexity and the quantum Turing machine</a> (PhD thesis).</p>
<p>&bull; Markus M&uuml;ller, <a href="http://www.arxiv.org/abs/0707.2924">On the quantum Kolmogorov complexity of classical strings</a>, <i>Int. J. Quant. Inf.</i> <b>7</b> (2009), 701-711.</p>
<p>Hmm &mdash; the first one says:</p>
<blockquote><p>
A concrete proposal for an application of quantum Kolmogorov complexity is to analyze a quantum version of the thought experiment of Maxwells demon. In one of the versions of this thought experiment, some microscopic device tries to decrease the entropy of some gas in a box, without the expense of energy, by intelligently opening or closing some little door that separates both halves of the box. It is clear that a device like this cannot work as described, since its existence would violate the second law of thermodynamics. But then, the question is what prevents such a little device (or demon) from operating.</p>
<p>Roughly, the answer is that the demon has to make observations to decide whether to close or open the door, and these observations accumulate information. From time to time, the demon must erase this additional information, which is only possible at the expense of energy, due to Landauers principle. In Li and Vitanyi&#8217;s book <i>An Introduction to Kolmogorov Complexity and Its Applications</i>, this cost of energy is analyzed under very weak assumptions with the help of Kolmogorov complexity. Basically, the energy that the demon can extract from the gas is limited by the difference of the entropy of the gas, plus the difference of the Kolmogorov complexity of the demons memory before and after the demons actions. The power of this analysis is that it even encloses the case that the demon has a computer to do clever calculations, e.g. to compress the accumulated information before erasing it.</p>
</blockquote>
<p>So, I guess I need to reread Li and Vitanyi&#8217;s book!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/11/quantum-information-processing-2011/#comments">21 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/" rel="category tag">quantum technologies</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/11/quantum-information-processing-2011/" rel="bookmark" title="Permanent Link to Quantum Information Processing 2011 (Day&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-2142 post type-post status-publish format-standard hentry category-information-and-entropy" id="post-2142">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/" rel="bookmark">Algorithmic Thermodynamics (Part&nbsp;2)</a></h2>
				<small>6 January, 2011</small><br />


				<div class="entry">
					<p>Here are some notes for a talk tomorrow at the <a href="http://www.quantumlah.org/">Centre for Quantum Technologies</a>.  You can think of this as a kind of followup to <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">my first post on this subject</a>.  </p>
<h4>Introduction</h4>
<p>The idea of <i>entropy</i> arose in thermodynamics, and its connection to probability theory is fundamental to statistical mechanics.  Its connection to <i>information</i> was developed later by Shannon.  We now think of entropy and information as two sides of the same coin.  </p>
<p>But there&#8217;s another concept, called <i>algorithmic information</i>, which was developed in work on logic and computer science.  This concept is related to Shannon&#8217;s earlier notion of information, but it also seems rather different.  </p>
<p>For example, Shannon&#8217;s ideas let us compute the information of a <i>probability distribution</i> on bit strings.  So if we detect a radio signal from an extraterrestrial life form, that sends us a string of a dozen 0&#8217;s and 1&#8217;s each day, and it seems the string is chosen randomly according to some probability distribution, we can calculate the information per message.  But if I just show you a <i>single</i> bit string, like this:</p>
<div align="center">
011101100001
</div>
<p>it makes no sense to ask what its information is.  On the other hand, we <i>can</i> define its algorithmic information.</p>
<p>Nonetheless, my goal here is to show you that algorithmic information can really be seen as a <i>special case</i> of the old probabilistic concept of information.  This lets us take all the familiar tools from statistical mechanics and thermodynamics, and apply them to algorithmic information theory.  Mike Stay and I started to do this here:</p>
<p>&bull; John Baez and Mike Stay, <a href="http://arxiv.org/abs/1010.2067">Algorithmic thermodynamics</a>.</p>
<p>Unfortunately, I&#8217;ll only have time to sketch a bit of what we did!  </p>
<h4>Algorithmic information &#8211; first definition</h4>
<p>Here&#8217;s a definition of algorithmic information.  Later we&#8217;ll see a better one that&#8217;s almost equivalent.  </p>
<p>Fix a programming language where a program is a finite bit string and its output, if it halts, is again a finite bit string.  Then the <b>algorithmic information</b> of a finite bit string is the length of the shortest program that has that bit string as output.  </p>
<p>So, for example, the algorithmic information of a string of a trillion 0&#8217;s is low, because you can write a short program that prints this out.  On the other hand, the algorithmic information of a long &#8220;random&#8221; string of bits will be high, because the shortest program to print it out will be a program that says &#8220;print out this string: 01011100101100&#8230;&#8221; &mdash; so the program is approximately as long as the string itself: slightly longer, but only by a fixed amount.   </p>
<p>Of course you may ask what &#8220;random&#8221; means here.  I haven&#8217;t defined it!   But the point is, people have used these ideas to <i>give a definition</i> what it means for a string of bits to be random.  There are different ways to do it.  For example: a bit string is <b>&epsilon;-random</b> if the shortest program that prints it out is at least &epsilon; times as long as the string itself.  </p>
<p>You may also wonder: &#8220;Doesn&#8217;t the definition of algorithmic information depend on the details of our programming language?&#8221;  And the answer is, &#8220;Yes, but not very much.&#8221;  More precisely, for any two universal programming languages, there&#8217;s a constant <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> such that for any finite bit string, the algorithmic information of that string will differ by at most <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> depending on which language we use.  </p>
<h4>Ordinary information</h4>
<p>Let&#8217;s see how algorithmic information compares to the usual concept of information introduced by Shannon.  The usual concept works like this:</p>
<p>If an event of probability <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> occurs, we define the <b>information</b> gained by learning this event has occurred to be <img src="https://s0.wp.com/latex.php?latex=-+%5Clog+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- &#92;log p" class="latex" />.  We take the logarithm because probabilities of independent events multiply, and we want information to add.  The minus sign makes the information positive.   We can use any base we want for the logarithm: physicists like <img src="https://s0.wp.com/latex.php?latex=e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e" class="latex" />, while computer scientists favor 2.</p>
<p>If there&#8217;s a set <img src="https://s0.wp.com/latex.php?latex=X+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X " class="latex" /> of possible outcomes, where the outcome <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> occurs with probability <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" />, then the <i>average</i> or <i>expected</i> amount of information we gain by learning the outcome is </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bx+%5Cin+X%7D+p_x+%5Clog+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{x &#92;in X} p_x &#92;log p_x " class="latex" /></p>
<p>We call this the <b>entropy</b> of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  </p>
<p>Now, ordinary information doesn&#8217;t look very much like algorithmic information!  But there&#8217;s a second definition of algorithmic information, almost equivalent to the first one, that makes the relation clearer.</p>
<h4>Algorithmic information &#8211; second definition</h4>
<p>First, a minor shift in viewpoint.  Before we defined algorithmic information for a finite bit string.  Now let&#8217;s define it for a natural number.  This change in viewpoint is no big deal, since we can set up a one-to-one correspondence between natural numbers and finite bit strings that&#8217;s easy to compute.  </p>
<p>We&#8217;ll still think of our programs as finite bit strings.  Not every such string gives a program that halts.   We also may demand that bit strings have special features to count as programs.  For example, we may demand that programs end in the string 11111, just like a Fortran program must end with the word END.  </p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be the set of programs that halt.  Without loss of generality, we can assume that <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is <b>prefix-free</b>.  This means that if <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />, no other bit string starting with the string <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is also in <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  For example, if the string 11111 marks the end of a program, and is only allowed to show up at the end of the program, <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> will be prefix-free.  </p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=V%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x)" class="latex" /> be the length of the program <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />.  It&#8217;s easy to see that if <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is prefix-free, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+X%7D+2%5E%7B-V%28x%29%7D+%5Cle+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in X} 2^{-V(x)} &#92;le 1 " class="latex" /></p>
<p>Making this sum finite is the reason we want the prefix-free condition &mdash; you&#8217;ll see exactly why in a minute.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=N%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N(x)" class="latex" /> be the output of the program <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" />.  Remember, we now think of the output as a natural number.  So, we have functions</p>
<p><img src="https://s0.wp.com/latex.php?latex=V%2C+N+%3A+X+%5Cto+%5Cmathbb%7BN%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V, N : X &#92;to &#92;mathbb{N}" class="latex" /></p>
<p>We define the <b>algorithmic information</b> of a number <img src="https://s0.wp.com/latex.php?latex=n+%5Cin+%5Cmathbb%7BN%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;in &#92;mathbb{N}" class="latex" /> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+-+%5Clog+%5Csum_%7Bx+%5Cin+X%2C+N%28x%29+%3D+n%7D+2%5E%7B-V%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = - &#92;log &#92;sum_{x &#92;in X, N(x) = n} 2^{-V(x)} " class="latex" /></p>
<p>So: we do a sum over all programs <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> having the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> as output.  We sum up <img src="https://s0.wp.com/latex.php?latex=2%5E%7B-V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2^{-V(x)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=V%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x)" class="latex" /> is the length of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.   The sum converges because </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+X%7D+2%5E%7B-V%28x%29%7D+%5Cle+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in X} 2^{-V(x)} &#92;le 1 " class="latex" /></p>
<p>That&#8217;s where the prefix-free condition comes into play.<br />
Finally, we take minus the logarithm of this sum.</p>
<p>This seems a bit complicated!  But suppose there&#8217;s just <i>one</i> program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> having the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> as output.  Then the sum consists of one term, the logarithm cancels out the exponential, and the minus signs cancel too, so we get </p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+V%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = V(x)" class="latex" /></p>
<p>This matches our first definition: the algorithmic information of a number is the length of the shortest program having that number as output. </p>
<p>Of course in this case the shortest program is the <i>only</i> program having that output.  If we have more than one program with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> as output, the second definition of algorithmic entropy gives a smaller answer than the first definition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S_%7B%5Cmathrm%7Bsecond%7D%7D%28n%29+%5Cle+S_%7B%5Cmathrm%7Bfirst%7D%7D%28n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_{&#92;mathrm{second}}(n) &#92;le S_{&#92;mathrm{first}}(n) " class="latex" /></p>
<p>However, there&#8217;s a famous theorem, proved by Leonid Levin in 1974, that says the new definition is not very different from the old one.  The difference is bounded by a constant.  More precisely: for any universal prefix-free programming language, there&#8217;s a constant <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> such that for every <img src="https://s0.wp.com/latex.php?latex=n+%5Cin+%5Cmathbb%7BN%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;in &#92;mathbb{N}" class="latex" />, </p>
<p><img src="https://s0.wp.com/latex.php?latex=S_%7B%5Cmathrm%7Bsecond%7D%7D%28n%29+%5Cge+S_%7B%5Cmathrm%7Bfirst%7D%7D%28n%29+-+K+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_{&#92;mathrm{second}}(n) &#92;ge S_{&#92;mathrm{first}}(n) - K " class="latex" /></p>
<p>Since the algorithmic information depends on our programming language, it was only well-defined within an additive constant in the first place.  So, switching from the first definition to the second one doesn&#8217;t significantly change the concept.  But, it makes the relation to ordinary information a lot easier to see! </p>
<p>From now on we&#8217;ll use the second definition.</p>
<h4>Algorithmic information versus ordinary information</h4>
<p>To relate algorithmic information to ordinary information, we need to get logarithms and probabilities into the game.  We see a logarithm here:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+-+%5Clog+%5Csum_%7Bx+%5Cin+X%2C+N%28x%29+%3D+n%7D+2%5E%7B-V%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = - &#92;log &#92;sum_{x &#92;in X, N(x) = n} 2^{-V(x)} " class="latex" /></p>
<p>but it&#8217;s in a funny place, outside the sum &mdash; and where are the probabilities?  </p>
<p>To solve both these puzzles, let&#8217;s define a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on the set of natural numbers by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_n+%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Csum_%7Bx+%5Cin+X%2C+N%28x%29+%3D+n%7D+2%5E%7B-V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_n = &#92;frac{1}{Z} &#92;sum_{x &#92;in X, N(x) = n} 2^{-V(x)}" class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalizing constant, to make the probabilities sum to 1.   Taking</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+2%5E%7B-V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} 2^{-V(x)}" class="latex" /></p>
<p>ensures that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bn+%5Cin+%5Cmathbb%7BN%7D%7D+p_n+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{n &#92;in &#92;mathbb{N}} p_n = 1 " class="latex" /></p>
<p>Now, notice that</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28n%29+%3D+-+%5Clog+p_n+%5C%3B+%2B+%5C%3B+%5Clog+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(n) = - &#92;log p_n &#92;; + &#92;; &#92;log Z " class="latex" /></p>
<p>So, up to an additive constant, the algorithmic entropy of the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=-%5Clog+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;log p_n" class="latex" />.  But <img src="https://s0.wp.com/latex.php?latex=-%5Clog+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;log p_n" class="latex" /> is just the information gained upon learning the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />, if we started out knowing only that <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> was chosen randomly according to the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.</p>
<p>What&#8217;s the meaning of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />?   Simple: <img src="https://s0.wp.com/latex.php?latex=p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_n" class="latex" /> is the probability that the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is the output of a randomly chosen program&#8230; where &#8216;randomly chosen&#8217; means chosen according to a certain specific rule.  The rule is that increasing the length of a program by 1 makes it half as probable.  In other words, the probability <img src="https://s0.wp.com/latex.php?latex=q_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x" class="latex" /> of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is </p>
<p><img src="https://s0.wp.com/latex.php?latex=q_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+2%5E%7B-V%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x = &#92;frac{1}{Z} 2^{-V(x)} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is the normalization factor chosen to make these probabilities sum to 1. </p>
<p>So, the relation between algorithmic information and ordinary information is this:</p>
<blockquote><p>The algorithmic information of a number is the information gained by learning that number, if beforehand we only knew that it was the output of a randomly chosen program.</p></blockquote>
<h4>Algorithmic thermodynamics</h4>
<p>Why should the probability of the program <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> be defined as</p>
<p><img src="https://s0.wp.com/latex.php?latex=q_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+2%5E%7B-V%28x%29%7D+%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x = &#92;frac{1}{Z} 2^{-V(x)} ?" class="latex" /></p>
<p>There is no reason we <i>need</i> to use this probability distribution.  However, there&#8217;s something good about it.  It&#8217;s an example of a <b>Gibbs state</b>, meaning a probability distribution that maximizes entropy subject to a constraint on the expected value of some observable.  Nature likes to maximize entropy, so Gibbs states are fundamental to statistical mechanics.  So is the quantity <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />: it&#8217;s called the <b>partition function</b>.</p>
<p>The idea works like this: suppose we want to maximize the entropy of a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on the set of programs, subject to a constraint on the expected value of the length of the program.  Then the answer is</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cgamma+V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = &#92;frac{1}{Z} e^{-&#92;gamma V(x)}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> is some number, and the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> ensures that the probabilities sum to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-+%5Cgamma+V%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{- &#92;gamma V(x)}" class="latex" /></p>
<p>So, <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> equals our previous probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> when </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%5Cln+2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma = &#92;ln 2 " class="latex" /></p>
<p>However, it is also interesting to consider other values of <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" />, and in fact Tadaki has already done so:</p>
<p>&bull; K. Tadaki, <a href="http://arxiv.org/abs/nlin.CD/0212001">A generalization of Chaitins halting probability and halting self-similar sets</a>, <i>Hokkaido Math. J.</i> <b>31</b> (2002), 219253. </p>
<p>The partition function converges for <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%5Cge+2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma &#92;ge 2" class="latex" /> but diverges otherwise.  </p>
<p>More generally, we can look for the probability distribution that maximizes entropy subject to constraints on the expected value of <i>several</i> observables.  For example, let:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E(x) = " class="latex" /> the logarithm of the runtime of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V%28x%29+%3D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x) = " class="latex" /> the length of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=N%28x%29+%3D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N(x) = " class="latex" /> the output of the program <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>Then the probability distribution that maximizes entropy subject to constraints on the expected values of these three quantities is</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_x+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta+E%28x%29+-+%5Cgamma+V%28x%29+-+%5Cdelta+N%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = &#92;frac{1}{Z} e^{-&#92;beta E(x) - &#92;gamma V(x) - &#92;delta N(x)} " class="latex" /></p>
<p>where now the partition function is</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-%5Cbeta+E%28x%29+-+%5Cgamma+V%28x%29+-+%5Cdelta+N%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{-&#92;beta E(x) - &#92;gamma V(x) - &#92;delta N(x)}" class="latex" /></p>
<p>We&#8217;ve chosen our notation to remind experts on statistical mechanics that they&#8217;ve seen formulas like this before. The exact same formulas describe a piston full of gas in thermal equilibrium!     From a formal, purely mathematical viewpoint:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> is analogous to the <b>internal energy</b> of the gas, and <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is analogous to <img src="https://s0.wp.com/latex.php?latex=1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/T" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the <b>temperature</b> in units where Boltzmann&#8217;s constant is 1.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> is analogous to the <b>volume</b> of the gas, and <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> is analogous to <img src="https://s0.wp.com/latex.php?latex=P%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P/T" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> is the <b>pressure</b>.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> is analogous to the <b>number of molecules</b> in the gas, and <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta" class="latex" /> is analogous to <img src="https://s0.wp.com/latex.php?latex=-%5Cmu%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;mu/T" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> is the <b>chemical potential</b>.</p>
<p>The analogy here is quite arbitrary.  I&#8217;m not saying that the length of a program is profoundly similar to the volume of a cylinder of gas; we could have chosen the letter <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> to stand for the length of a program and everything would still work.  </p>
<p>But the analogy works.  In other words: now we can take a lot of familiar facts about thermodynamics and instantly translate them into analogous facts about algorithmic information theory!  For example, define the entropy of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in the usual way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+%5Csum_%7Bx+%5Cin+X%7D+p_x+%5Cln+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = &#92;sum_{x &#92;in X} p_x &#92;ln p_x " class="latex" /></p>
<p>We can think of this as a function of either <img src="https://s0.wp.com/latex.php?latex=T%2C+P%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T, P," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> or the expected values of <img src="https://s0.wp.com/latex.php?latex=E%2C+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, V" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" />.  In thermodynamics we learn lots of equations like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+E%7D+%5Cright%7C_%7BV%2CN%7D+%3D+%5Cfrac%7B1%7D%7BT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;frac{&#92;partial S}{&#92;partial E} &#92;right|_{V,N} = &#92;frac{1}{T}" class="latex" /></p>
<p>where by standard abuse of notation we&#8217;re using <img src="https://s0.wp.com/latex.php?latex=E%2C+V%2C+N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, V, N" class="latex" /> to stand for their expected values.  And, <i>all these equations remain true in algorithmic thermodynamics!</i>  </p>
<p>The interesting part is figuring out what all these equations really <i>mean</i> in the context of algorithmic thermodynamics.  For a start on that, try <a href="http://arxiv.org/abs/1010.2067">our paper</a>.</p>
<p>For example, we call <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> <b>algorithmic temperature</b>.  If you allow programs to run longer, more of them will halt and give an answer. Thanks to the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+E%7D+%5Cright%7C_%7BV%2CN%7D+%3D+%5Cfrac%7B1%7D%7BT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left. &#92;frac{&#92;partial S}{&#92;partial E} &#92;right|_{V,N} = &#92;frac{1}{T}" class="latex" /></p>
<p>the algorithmic temperature is roughly the number of times you have to double the runtime in order to double the number of programs that satisfy the constraints on length and output.</p>
<p>We also consider the algorithmic analogues of thermodynamic cycles, familiar from old work on steam engines.  Charles Babbage described a computer powered by a steam engine; we describe a heat engine powered by programs!  The significance of this line of thinking remains fairly mysterious. However, I&#8217;m hoping that it will help point the way toward a further synthesis of algorithmic information theory and thermodynamics.</p>
<div align="center">
<img src="https://i2.wp.com/math.ucr.edu/home/baez/piston_tiny.jpg" />
</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/#comments">43 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/01/06/algorithmic-thermodynamics-part-2/" rel="bookmark" title="Permanent Link to Algorithmic Thermodynamics (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1573 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1573">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/" rel="bookmark">Information Geometry (Part&nbsp;5)</a></h2>
				<small>2 November, 2010</small><br />


				<div class="entry">
					<p>I&#8217;m trying to understand the Fisher information metric and how it&#8217;s related to <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#Ottinger">&Ouml;ttinger&#8217;s formalism</a> for &#8216;dissipative mechanics&#8217; &mdash; that is, mechanics including friction. They involve similar physics, and they involve similar math, but it&#8217;s not quite clear how they fit together.</p>
<p>I think it will help to do an example.  The harmonic oscillator is a trusty workhorse throughout physics, so let&#8217;s do that.</p>
<p>So: suppose you have a rock hanging on a spring, and it can bounce up and down.  Suppose it&#8217;s in thermal equilibrium with its environment.  It will wiggle up and down ever so slightly, thanks to thermal fluctuations.   The hotter it is, the more it wiggles.  These vibrations are random, so its position and momentum at any given moment can be treated as random variables.  </p>
<p>If we take quantum mechanics into account, there&#8217;s an extra source of randomness: <i>quantum</i> fluctuations.  Now there will be fluctuations even at zero temperature.   Ultimately this is due to the uncertainty principle.  Indeed, if you know the position for sure, you can&#8217;t know the momentum at all!  </p>
<p>Let&#8217;s see how the position, momentum and energy of our rock will fluctuate given that we know all three of these quantities <i>on average</i>.  The fluctuations will form a little fuzzy blob, roughly ellipsoidal in shape, in the 3-dimensional space whose coordinates are position, momentum and energy:</p>
<div align="center">
<img src="https://i0.wp.com/www.math.dartmouth.edu/archive/m22x06/public_html/ellipsoid.jpg" />
</div>
<p>Yeah, I know you&#8217;re sick of this picture, but this time it&#8217;s for real: I want to calculate what this ellipsoid actually looks like!  I&#8217;m not promising I&#8217;ll do it &mdash; I may get stuck, or bored &mdash; but at least I&#8217;ll <i>try</i>.</p>
<p>Before I start the calculation, let&#8217;s guess the answer. A harmonic oscillator has a position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />, and its energy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+%2B+p%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2}(q^2 + p^2)" class="latex" /></p>
<p>Here I&#8217;m working in units where lots of things equal 1, to keep things simple.</p>
<p>You&#8217;ll notice that this energy has rotational symmetry in the position-momentum plane.  This is ultimately what makes the harmonic oscillator such a beloved physical system.   So, we might naively guess that our little ellipsoid will have rotational symmetry as well, like this:</p>
<div align="center">
<img width="120" src="http://upload.wikimedia.org/wikipedia/commons/8/88/ProlateSpheroid.png" />
</div>
<p>or this:</p>
<div align="center">
<img width="200" src="http://upload.wikimedia.org/wikipedia/commons/b/b5/OblateSpheroid.PNG" />
</div>
<p>Here I&#8217;m using the <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> coordinates for position and momentum, while the <img src="https://s0.wp.com/latex.php?latex=z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z" class="latex" /> coordinate stands for energy.  So in these examples the position and momentum fluctuations are the same size, while the energy fluctuations, drawn in the vertical direction, might be bigger or smaller.</p>
<p>Unfortunately, this guess really is naive.  After all, there are <i>lots</i> of these ellipsoids, one centered at each point in position-momentum-energy space.  Remember the rules of the game!  You give me any point in this space.  I take the coordinates of this point as the <i>mean</i> values of position, momentum and energy, and I find the maximum-entropy state with these mean values.  Then I work out the fluctuations in this state, and draw them as an ellipsoid. </p>
<p>If you pick a point where position and momentum have mean value zero, you haven&#8217;t broken the rotational symmetry of the problem.  So, my ellipsoid must be rotationally symmetric.  But if you pick some other mean value for position and momentum, all bets are off!</p>
<p>Fortunately, this naive guess is actually right: <i>all</i> the ellipsoids are rotationally symmetric &mdash; even the ones centered at nonzero values of position and momentum!  We&#8217;ll see why soon.  And if you&#8217;ve been following this series of posts, you&#8217;ll know what this implies: the &#8220;Fisher information metric&#8221; <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on position-momentum-energy space has rotational symmetry about any vertical axis.  (Again, I&#8217;m using the vertical direction for energy.)  So, if we slice this space with any horizontal plane, the metric on this plane must be the plane&#8217;s usual metric times a constant:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq%5E2+%2B+dp%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} &#92;, (dq^2 + dp^2) " class="latex" /></p>
<p>Why?  Because only the usual metric on the plane, or any multiple of it, has ordinary rotations around every point as symmetries.  </p>
<p>So, roughly speaking, we&#8217;re recovering the &#8216;obvious&#8217; geometry of the position-momentum plane from the Fisher information metric.  <i>We&#8217;re recovering &#8216;ordinary&#8217; geometry from information geometry!</i>  </p>
<p>But this should not be terribly surprising, since we used the harmonic oscillator Hamiltonian</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+%2B+p%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2}(q^2 + p^2)" class="latex" /></p>
<p>as an input to our game.  It&#8217;s mainly just a confirmation that things are working as we&#8217;d hope.  </p>
<p>There&#8217;s more, though.  <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">Last time</a> I realized that because observables in quantum mechanics don&#8217;t commute, the Fisher information metric has a curious skew-symmetric partner called <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />.   So, we should also study this in our example.  And when we do, we&#8217;ll see that restricted to any horizontal plane in position-momentum-energy space, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq+%5C%2C+dp+-+dp+%5C%2C+dq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = &#92;mathrm{constant} &#92;, (dq &#92;, dp - dp &#92;, dq)" class="latex" /></p>
<p>This looks like a mutant version of the Fisher information metric</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq%5E2+%2B+dp%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} &#92;, (dq^2 + dp^2) " class="latex" /></p>
<p>and if you know your geometry, you&#8217;ll know it&#8217;s the usual <a href="http://en.wikipedia.org/wiki/Symplectic_manifold">&#8216;symplectic structure&#8217;</a> on the position-energy plane &mdash; at least, times some constant.   </p>
<p>All this is very reminiscent of &Ouml;ttinger&#8217;s work on dissipative mechanics.  But we&#8217;ll also see something else: while the constant in <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> depends on the energy &mdash; that is, on which horizontal plane we take &mdash; the constant in <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> does not!</p>
<p>Why?  It&#8217;s perfectly sensible.  The metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on our horizontal plane keeps track of fluctuations in position and momentum.  Thermal fluctuations get bigger when it&#8217;s hotter &mdash; and to boost the average energy of our oscillator, we must heat it up.   So, as we increase the energy, moving our horizontal plane further up in position-momentum-energy space, the metric on the plane gets bigger!   In other words, our ellipsoids get a fat cross-section at high energies.</p>
<p>On the other hand, the symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> arises from the fact that position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> don&#8217;t commute in quantum mechanics.  They obey Heisenberg&#8217;s <a href="http://en.wikipedia.org/wiki/Canonical_commutation_relation">&#8216;canonical commutation relation&#8217;</a>:  </p>
<p><img src="https://s0.wp.com/latex.php?latex=q+p+-+p+q+%3D+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q p - p q = i " class="latex" /></p>
<p>This relation doesn&#8217;t involve energy, so <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> will be the same on every horizontal plane.   And it turns out this relation implies</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega++%3D+%5Cmathrm%7Bconstant%7D+%5C%2C+%28dq+%5C%2C+dp+-+dp+%5C%2C+dq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega  = &#92;mathrm{constant} &#92;, (dq &#92;, dp - dp &#92;, dq)" class="latex" /></p>
<p>for some constant we&#8217;ll compute later.  </p>
<p>Okay, that&#8217;s the basic idea.  Now let&#8217;s actually do some computations.  For starters, let&#8217;s see why all our ellipsoids have rotational symmetry!</p>
<p>To do this, we need to understand a bit about the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that maximizes entropy given certain mean values of position, momentum and energy.   So, let&#8217;s choose the numbers we want for these mean values  (also known as &#8216;expected values&#8217; or &#8216;expectation values&#8217;):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = E " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+q+%5Crangle+%3D+q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle q &#92;rangle = q_0" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+p+%5Crangle+%3D+p_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle p &#92;rangle = p_0" class="latex" /></p>
<p>I hope this isn&#8217;t too confusing: <img src="https://s0.wp.com/latex.php?latex=H%2C+p%2C+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H, p, q" class="latex" /> are our observables which are operators, while <img src="https://s0.wp.com/latex.php?latex=E%2C+p_0%2C+q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, p_0, q_0" class="latex" /> are the mean values we have chosen for them.  The state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> depends on <img src="https://s0.wp.com/latex.php?latex=E%2C+p_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E, p_0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0" class="latex" />.</p>
<p>We&#8217;re doing quantum mechanics, so position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> are both self-adjoint operators on the Hilbert space <img src="https://s0.wp.com/latex.php?latex=L%5E2%28%5Cmathbb%7BR%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^2(&#92;mathbb{R})" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28q%5Cpsi%29%28x%29+%3D+x+%5Cpsi%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q&#92;psi)(x) = x &#92;psi(x) " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p%5Cpsi%29%28x%29+%3D+-+i+%5Cfrac%7Bd+%5Cpsi%7D%7Bdx%7D%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p&#92;psi)(x) = - i &#92;frac{d &#92;psi}{dx}(x)" class="latex" /></p>
<p>Indeed all our observables, including the Hamiltonian </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Cfrac%7B1%7D%7B2%7D+%28p%5E2+%2B+q%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;frac{1}{2} (p^2 + q^2) " class="latex" /></p>
<p>are self-adjoint operators on this Hilbert space, and the state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is a <a href="http://en.wikipedia.org/wiki/Density_matrix">density matrix</a> on this space, meaning a positive self-adjoint operator with trace 1.</p>
<p>Now: how do we compute <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />?   It&#8217;s a <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">Lagrange multiplier</a> problem: maximizing some function given some constraints.  And it&#8217;s well-known that when you solve this problem, you get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1%2C+%5Clambda%5E2%2C+%5Clambda%5E3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1, &#92;lambda^2, &#92;lambda^3" class="latex" /> are three numbers we yet have to find, and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalizing factor called the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} )" class="latex" /></p>
<p>Now let&#8217;s look at a special case.  If we choose <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1+%3D+%5Clambda%5E2+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1 = &#92;lambda^2 = 0" class="latex" />, we&#8217;re back a simpler and more famous problem, namely maximizing entropy subject to a constraint only on energy!  The solution is then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta+H%7D+%2C+%5Cqquad+Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-+%5Cbeta+H%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;beta H} , &#92;qquad Z = &#92;mathrm{tr} (e^{- &#92;beta H} )" class="latex" /></p>
<p>Here I&#8217;m using the letter <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> instead of <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^3" class="latex" /> because this is traditional.  This quantity has an important physical meaning!  It&#8217;s the <i>reciprocal of temperature</i> in units where Boltzmann&#8217;s constant is 1.   </p>
<p>Anyway, back to our special case!  In this special case it&#8217;s easy to explicitly calculate <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  Indeed, people have known how ever since <a href="http://en.wikipedia.org/wiki/Planck%27s_law#Derivation">Planck</a> put the &#8216;quantum&#8217; in quantum mechanics!   He figured out how black-body radiation works.  A box of hot radiation is just a big bunch of harmonic oscillators in thermal equilibrium.  You can work out its partition function by multiplying the partition function of each one.  </p>
<p>So, it would be great to reduce our general problem to this special case.  To do this, let&#8217;s rewrite</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%28%5Clambda%5E1+q+%2B+%5Clambda%5E2+p+%2B+%5Clambda%5E3+H%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-(&#92;lambda^1 q + &#92;lambda^2 p + &#92;lambda^3 H)} )" class="latex" /></p>
<p>in terms of some new variables, like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;beta(H - f q - g p)} " class="latex" /></p>
<p>where now </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;beta(H - f q - g p)} )" class="latex" /></p>
<p>Think about it!  Now our problem is just like an oscillator with a modified Hamiltonian</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%27+%3D+H+-+f+q+-+g+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039; = H - f q - g p" class="latex" /></p>
<p>What does this mean, physically?  Well, if you push on something with a force <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />, its potential energy will pick up a term <img src="https://s0.wp.com/latex.php?latex=-+f+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- f q" class="latex" />.  So, the first two terms are just the Hamiltonian for a harmonic oscillator <i>with an extra force pushing on it!</i>  </p>
<p>I don&#8217;t know a nice interpretation for the <img src="https://s0.wp.com/latex.php?latex=-+g+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- g p" class="latex" /> term.  We could say that besides the extra force equal to <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" />, we also have an extra &#8216;gorce&#8217; equal to <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />.  I don&#8217;t know what that means.  Luckily, I don&#8217;t need to!   Mathematically, our whole problem is invariant under rotations in the position-momentum plane, so whatever works for <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> must also work for <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.   </p>
<p>Now here&#8217;s the cool part.  We can complete the square:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H%27+%26+%3D+%5Cfrac%7B1%7D%7B2%7D+%28q%5E2+%2B+p%5E2%29+-++f+q+-+g+p+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%28q%5E2+-+2+q+f+%2B+f%5E2%29+%2B+%5Cfrac%7B1%7D%7B2%7D%28p%5E2+-+2+q+g+%2B+g%5E2%29+-+%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29++%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%28%28q+-+f%29%5E2+%2B+%28p+-+g%29%5E2%29++-+%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29++%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{aligned} H&#039; &amp; = &#92;frac{1}{2} (q^2 + p^2) -  f q - g p &#92;&#92; &amp;= &#92;frac{1}{2}(q^2 - 2 q f + f^2) + &#92;frac{1}{2}(p^2 - 2 q g + g^2) - &#92;frac{1}{2}(g^2 + f^2)  &#92;&#92; &amp;= &#92;frac{1}{2}((q - f)^2 + (p - g)^2)  - &#92;frac{1}{2}(g^2 + f^2)  &#92;end{aligned}" class="latex" /></p>
<p>so if we define &#8216;translated&#8217; position and momentum operators:</p>
<p><img src="https://s0.wp.com/latex.php?latex=q%27+%3D+q+-+f%2C+%5Cqquad+p%27+%3D+p+-+g+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q&#039; = q - f, &#92;qquad p&#039; = p - g " class="latex" /></p>
<p>we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=H%27+%3D+%5Cfrac%7B1%7D%7B2%7D%28%7Bq%27%7D%5E2+%2B+%7Bp%27%7D%5E2%29+-++%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039; = &#92;frac{1}{2}({q&#039;}^2 + {p&#039;}^2) -  &#92;frac{1}{2}(g^2 + f^2) " class="latex" /></p>
<p>So: apart from a constant, <img src="https://s0.wp.com/latex.php?latex=H%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039;" class="latex" /> is just the harmonic oscillator Hamiltonian in terms of &#8216;translated&#8217; position and momentum operators!</p>
<p>In other words: we&#8217;re studying a strange variant of the harmonic oscillator, where we are pushing on it with an extra force and also an extra  &#8216;gorce&#8217;.  But this strange variant is <i>exactly the same as the usual harmonic oscillator</i>, except that we&#8217;re working in translated coordinates on position-momentum space, and subtracting a constant from the Hamiltonian.</p>
<p>These are pretty minor differences.  So, we&#8217;ve succeeded in reducing our problem to the problem of a harmonic oscillator in thermal equilibrium at some temperature!</p>
<p>This makes it easy to calculate</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta%28H+-+f+q+-+g+p%29%7D+%29+%3D+%5Cmathrm%7Btr%7D%28e%5E%7B-%5Cbeta+H%27%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;beta(H - f q - g p)} ) = &#92;mathrm{tr}(e^{-&#92;beta H&#039;})" class="latex" /></p>
<p>By our formula for <img src="https://s0.wp.com/latex.php?latex=H%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H&#039;" class="latex" />, this is just</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cfrac%7B1%7D%7B2%7D%28%7Bq%27%7D%5E2+%2B+%7Bp%27%7D%5E2%29%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;mathrm{tr} (e^{-&#92;frac{1}{2}({q&#039;}^2 + {p&#039;}^2)})" class="latex" /></p>
<p>And the second factor here equals the partition function for the good old harmonic oscillator:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Cbeta+H%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;mathrm{tr} (e^{-&#92;beta H})" class="latex" /></p>
<p>So now we&#8217;re back to a textbook problem.  The eigenvalues of the <a href="http://en.wikipedia.org/wiki/Harmonic_oscillator_%28quantum%29#Hamiltonian_and_energy_eigenstates">harmonic oscillator Hamiltonian</a> are </p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%2B+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n + &#92;frac{1}{2}" class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=n+%3D+0%2C1%2C2%2C3%2C+%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 0,1,2,3, &#92;dots" class="latex" /></p>
<p>So, the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-%5Cbeta+H%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-&#92;beta H}" class="latex" /> are are just</p>
<p><img src="https://s0.wp.com/latex.php?latex=e%5E%7B-%5Cbeta%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-&#92;beta(n + &#92;frac{1}{2})} " class="latex" /></p>
<p>and to take the trace of this operator, we sum up these eigenvalues:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28e%5E%7B-%5Cbeta+H%7D%29+%3D+%5Csum_%7Bn+%3D+0%7D%5E%5Cinfty+e%5E%7B-%5Cbeta+%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7D+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta%2F2%7D%7D%7B1+-+e%5E%7B-%5Cbeta%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(e^{-&#92;beta H}) = &#92;sum_{n = 0}^&#92;infty e^{-&#92;beta (n + &#92;frac{1}{2})} = &#92;frac{e^{-&#92;beta/2}}{1 - e^{-&#92;beta}} " class="latex" /></p>
<p>So:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+e%5E%7B%5Cfrac%7B1%7D%7B2%7D%28g%5E2+%2B+f%5E2%29%7D+%5C%3B+%5Cfrac%7Be%5E%7B-%5Cbeta%2F2%7D%7D%7B1+-+e%5E%7B-%5Cbeta%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = e^{&#92;frac{1}{2}(g^2 + f^2)} &#92;; &#92;frac{e^{-&#92;beta/2}}{1 - e^{-&#92;beta}} " class="latex" /></p>
<p>We can now compute the Fisher information metric using this formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda%5Ei+%5Cpartial+%5Clambda%5Ej%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda^i &#92;partial &#92;lambda^j} &#92;ln Z" class="latex" /></p>
<p>if we remember how our new variables are related to the <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1+%3D+%5Cbeta+f+%2C+%5Cqquad+%5Clambda%5E2+%3D+%5Cbeta+g%2C+%5Cqquad+%5Clambda%5E3+%3D+%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1 = &#92;beta f , &#92;qquad &#92;lambda^2 = &#92;beta g, &#92;qquad &#92;lambda^3 = &#92;beta" class="latex" /></p>
<p>It&#8217;s just calculus!  But I&#8217;m feeling a bit tired, so I&#8217;ll leave this pleasure to you.  </p>
<p>For now, I&#8217;d rather go back to our basic intuition about how the Fisher information metric describes fluctuations of observables.  Mathematically, this means it&#8217;s the real part of the covariance matrix</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>where for us</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_1+%3D+q%2C+%5Cqquad+X_2+%3D+p%2C+%5Cqquad+X_3+%3D+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1 = q, &#92;qquad X_2 = p, &#92;qquad X_3 = E " class="latex" /></p>
<p>Here we are taking expected values using the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  We&#8217;ve seen this mixed state is just like the maximum-entropy state of a harmonic oscillator at fixed temperature &mdash; except for two caveats: we&#8217;re working in translated coordinates on position-momentum space, and subtracting a constant from the Hamiltonian.  But neither of these two caveats affects the fluctuations <img src="https://s0.wp.com/latex.php?latex=%28X_i+-+%5Clangle+X_i+%5Crangle%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X_i - &#92;langle X_i &#92;rangle)" class="latex" /> or the covariance matrix.  </p>
<p>So, as indeed we&#8217;ve already seen, <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> has rotational symmetry in the 1-2 plane.  Thus, we&#8217;ll completely know it once we know <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D+%3D+g_%7B22%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11} = g_{22}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g_%7B33%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{33}" class="latex" />; the other components are zero for symmetry reasons.  <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11}" class="latex" /> will equal the variance of position for a harmonic oscillator at a given temperature, while <img src="https://s0.wp.com/latex.php?latex=g_%7B33%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{33}" class="latex" /> will equal the variance of its energy.   We can work these out or look them up.  </p>
<p>I won&#8217;t do that now: I&#8217;m after insight, not formulas.  For physical reasons, it&#8217;s obvious that <img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11}" class="latex" /> must diminish with diminishing energy &mdash; but not go to zero.   Why? Well, as the temperature approaches zero, a harmonic oscillator in thermal equilibrium approaches its state of least energy: the so-called <a href="http://en.wikipedia.org/wiki/Ground_state">&#8216;ground state&#8217;</a>.  In its ground state, the standard deviations of position and momentum are as small as allowed by the Heisenberg uncertainty principle:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+p+%5CDelta+q++%5Cge+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta p &#92;Delta q  &#92;ge &#92;frac{1}{2}" class="latex" /></p>
<p>and they&#8217;re equal, so </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7B11%7D+%3D+%28%5CDelta+q%29%5E2+%3D+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{11} = (&#92;Delta q)^2 = &#92;frac{1}{2}" class="latex" />.</p>
<p>That&#8217;s enough about the metric.  Now, what about the metric&#8217;s skew-symmetric partner?  This is: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cmathrm%7BIm%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;mathrm{Im} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>Last time we saw that <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> is all about expected values of commutators:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cfrac%7B1%7D%7B2i%7D+%5Clangle+%5BX_i%2C+X_j%5D+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;frac{1}{2i} &#92;langle [X_i, X_j] &#92;rangle" class="latex" /></p>
<p>and this makes it easy to compute.  For example, </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5BX_1%2C+X_2%5D+%3D+q+p+-+p+q+%3D+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[X_1, X_2] = q p - p q = i" class="latex" /></p>
<p>so </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B12%7D+%3D+%5Cfrac%7B1%7D%7B2%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{12} = &#92;frac{1}{2} " class="latex" /></p>
<p>Of course </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B11%7D+%3D+%5Comega_%7B22%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{11} = &#92;omega_{22} = 0" class="latex" /></p>
<p>by skew-symmetry, so we know the restriction of <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> to any horizontal plane.  We can also work out other components, like <img src="https://s0.wp.com/latex.php?latex=%5Comega_%7B13%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{13}" class="latex" />, but I don&#8217;t want to.  I&#8217;d rather just state this:</p>
<blockquote><p>
<b>Summary:</b> Restricted to any horizontal plane in the position-momentum-energy space, the Fisher information metric for the harmonic oscillator is</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+%5Cmathrm%7Bconstant%7D+%28dq_0%5E2+%2B+dp_0%5E2%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = &#92;mathrm{constant} (dq_0^2 + dp_0^2) " class="latex" /></p>
<p>with a constant depending on the temperature, equalling <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> in the zero-temperature limit, and increasing as the temperature rises.  Restricted to the same plane, the Fisher information metric&#8217;s skew-symmetric partner is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+%5Cfrac%7B1%7D%7B2%7D+dq_0+%5Cwedge+dp_0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = &#92;frac{1}{2} dq_0 &#92;wedge dp_0 " class="latex" />
</p></blockquote>
<p>(Remember, the mean values <img src="https://s0.wp.com/latex.php?latex=q_0%2C+p_0%2C+E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0, p_0, E_0" class="latex" /> are the coordinates on position-momentum-energy space.  We could also use coordinates <img src="https://s0.wp.com/latex.php?latex=f%2C+g%2C+%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f, g, &#92;beta" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=f%2C+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f, g" class="latex" /> and temperature.  In the chatty intro to this article you saw formulas like those above but without the subscripts; that&#8217;s before I got serious about using <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to mean <i>operators</i>.)</p>
<p>And now for the moral.  Actually I have two: a physics moral and a math moral.    </p>
<p>First, what is the physical meaning of <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> when restricted to a plane of constant <img src="https://s0.wp.com/latex.php?latex=E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_0" class="latex" />, or if you prefer, a plane of constant temperature?</p>
<blockquote><p>
<b>Physics Moral:</b> Restricted to a constant-temperature plane, <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> is the covariance matrix for our observables.  It is temperature-dependent.  In the zero-temperature limit, the thermal fluctuations go away and <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> depends only on quantum fluctuations in the ground state.  On the other hand, <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> restricted to a constant-temperature plane describes Heisenberg uncertainty relations for noncommuting observables.  In our example, it is temperature-independent.
</p></blockquote>
<p>Second, what does this have to do with <a href="http://en.wikipedia.org/wiki/K%C3%A4hler_manifold">K&auml;hler geometry?</a>  Remember, the complex plane has a <i>complex-valued</i> metric on it, called a K&auml;hler structure.  Its real part is a <a href="http://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian metric</a>, and its imaginary part is a <a href="http://en.wikipedia.org/wiki/Symplectic_manifold">symplectic structure</a>.  We can think of the the complex plane as the position-momentum plane for a point particle.  Then the symplectic structure is the basic ingredient needed for <a href="http://en.wikipedia.org/wiki/Hamiltonian_mechanics">Hamiltonian mechanics</a>, while the Riemannian structure is the basic ingredient needed for the harmonic oscillator Hamiltonian.  </p>
<blockquote><p>
<b>Math Moral:</b> In the example we considered, <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> restricted to a constant-temperature plane is equal to <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> the usual symplectic structure on the complex plane.  On the other hand, <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> restricted to a constant-temperature plane is a multiple of the usual Riemannian metric on the complex plane &mdash; but this multiple is <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{2}" class="latex" /> <i>only when the temperature is zero!</i>  So, only at temperature zero are <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> the real and imaginary parts of a K&auml;hler structure.
</p></blockquote>
<p>It will be interesting to see how much of this stuff is true more generally.  The harmonic oscillator is much nicer than your average physical system, so it can be misleading, but I think <i>some</i> of the morals we&#8217;ve seen here can be generalized.</p>
<p>Some other time I may so more about how all this is<br />
related to <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#Ottinger">&Ouml;ttinger&#8217;s formalism</a>, but the quick point is that he too has mixed states, and a symmetric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />, and a skew-symmetric <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />.  So it&#8217;s nice to see if they match up in an example.</p>
<p>Finally, two footnotes on terminology:</p>
<p><b>&beta;:</b>   In fact, this quantity <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FkT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/kT" class="latex" /> is so important it deserves a better name than &#8216;reciprocal of temperature&#8217;.   How about &#8216;coolness&#8217;?  An important lesson from statistical mechanics is that coolness is more fundamental than temperature.  This makes some facts more plausible.  For example, if you say &#8220;you can never reach absolute zero,&#8221; it sounds very odd, since you can get as close as you like, and it&#8217;s even possible to get <a href="http://en.wikipedia.org/wiki/Negative_temperature"><i>negative</i> temperatures</a> &mdash; but temperature zero remains tantalizingly out of reach.   But &#8220;you can never attain infinite coolness&#8221; &mdash; now that makes sense.</p>
<p><b>Gorce:</b>  I apologize to Richard Feynman for stealing the word <a href="http://student.fizika.org/~jsisko/Knjige/Opca%20Fizika/Feynman%20Lectures%20on%20Physics/Vol%201%20Ch%2012%20-%20Characteristics%20of%20Force.pdf">&#8216;gorce&#8217;</a> and using it a different way.  Does anyone have a good intuition for what&#8217;s going on when you apply my sort of &#8216;gorce&#8217; to a point particle?  You need to think about velocity-dependent potentials, of that I&#8217;m sure.  In the presence of a velocity-dependent potential, momentum is <i>not</i> just mass times velocity.  Which is good: if it were, we could never have a system where the mean value of both <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> stayed constant over time!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/#comments">53 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/11/02/information-geometry-part-5/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1504 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1504">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/" rel="bookmark">Information Geometry (Part&nbsp;4)</a></h2>
				<small>29 October, 2010</small><br />


				<div class="entry">
					<p>Before moving on, I&#8217;d like to clear up a mistake I&#8217;d been making in all my previous posts on this subject.</p>
<p>(By now I&#8217;ve tried to fix those posts, because people often get information from the web in a hasty way, and I don&#8217;t want my mistake to spread.  But you&#8217;ll still see traces of my mistake infecting the <i>comments</i> on those posts.)</p>
<p>So what&#8217;s the mistake?  It&#8217;s embarrassingly simple, but also simple to fix.  A <a href="http://en.wikipedia.org/wiki/Metric_tensor">Riemannian metric</a> must be symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+g_%7Bji%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = g_{ji} " class="latex" />  </p>
<p>Now, I had defined the Fisher information metric to be the so-called <a href="http://en.wikipedia.org/wiki/Covariance_matrix">&#8216;covariance matrix&#8217;</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%3B%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;langle (X_i - &#92;langle X_i &#92;rangle) &#92;;(X_j- &#92;langle X_j &#92;rangle)&#92;rangle" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> are some observable-valued functions on a manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, and the angle brackets mean &#8220;expectation value&#8221;, computed using a mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that also depends on the point in <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.</p>
<p>The covariance matrix is symmetric in classical mechanics, since then observables commute, so:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB+%5Crangle+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB &#92;rangle = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>But it&#8217;s not symmetric is quantum mechanics!  After all, suppose <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is the position operator for a particle, and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is the momentum operator.  Then according to Heisenberg</p>
<p><img src="https://s0.wp.com/latex.php?latex=qp+%3D+pq+%2B+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="qp = pq + i " class="latex" /></p>
<p>in units where Planck&#8217;s constant is 1.  Taking expectation values, we get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+qp+%5Crangle+%3D+%5Clangle+pq+%5Crangle+%2B+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle qp &#92;rangle = &#92;langle pq &#92;rangle + i" class="latex" /></p>
<p>and in particular:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+qp+%5Crangle+%5Cne+%5Clangle+pq+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle qp &#92;rangle &#92;ne &#92;langle pq &#92;rangle " class="latex" /></p>
<p>We can use this to get examples where <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> is not symmetric.    </p>
<p>However, it turns out that the <i>real part</i> of the covariance matrix is symmetric, even in quantum mechanics &mdash; and that&#8217;s what we should use as our Fisher information metric.</p>
<p>Why is the real part of the covariance matrix symmetric, even in quantum mechanics?  Well, suppose <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is any density matrix, and <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> are any observables.  Then by definition</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB+%5Crangle+%3D+%5Cmathrm%7Btr%7D+%28%5Crho+AB%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB &#92;rangle = &#92;mathrm{tr} (&#92;rho AB)" class="latex" /></p>
<p>so taking the complex conjugate of both sides</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A++%3D+%5Cmathrm%7Btr%7D%28%5Crho+AB%29%5E%2A+%3D+%5Cmathrm%7Btr%7D%28%28%5Crho+A+B%29%5E%2A%29+%3D+%5Cmathrm%7Btr%7D%28B%5E%2A+A%5E%2A+%5Crho%5E%2A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^*  = &#92;mathrm{tr}(&#92;rho AB)^* = &#92;mathrm{tr}((&#92;rho A B)^*) = &#92;mathrm{tr}(B^* A^* &#92;rho^*)" class="latex" /></p>
<p>where I&#8217;m using an asterisk both for the complex conjugate of a number and the adjoint of an operator.  But our observables are self-adjoint, and so is our density matrix, so we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28B%5E%2A+A%5E%2A+%5Crho%5E%2A%29+%3D+%5Cmathrm%7Btr%7D%28B+A+%5Crho%29+%3D+%5Cmathrm%7Btr%7D%28%5Crho+B+A%29+%3D+%5Clangle+B+A+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(B^* A^* &#92;rho^*) = &#92;mathrm{tr}(B A &#92;rho) = &#92;mathrm{tr}(&#92;rho B A) = &#92;langle B A &#92;rangle " class="latex" /></p>
<p>where in the second step we used the cyclic property of the trace.  In short:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^* = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>If we take real parts, we get something symmetric:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB%5Crangle+%3D++%5Cmathrm%7BRe%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB&#92;rangle =  &#92;mathrm{Re} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>So, if we redefine the Fisher information metric to be the <i>real part</i> of the covariance matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%3B+%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle) &#92;; (X_j- &#92;langle X_j &#92;rangle)&#92;rangle " class="latex" /></p>
<p>then it&#8217;s symmetric, as it should be. </p>
<p>Last time I mentioned a general setup using von Neumann algebras, that handles the classical and quantum situations simultaneously.  That applies here!   Taking the real part has no effect in classical mechanics, so we don&#8217;t need it there &mdash; but it doesn&#8217;t hurt, either.</p>
<p>Taking the real part never has any effect when <img src="https://s0.wp.com/latex.php?latex=i+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = j" class="latex" />, either, since the expected value of the <i>square</i> of an observable is a nonnegative number:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle &#92;ge 0" class="latex" /></p>
<p>This has two nice consequences.  </p>
<p>First, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle++%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} = &#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle  &#92;ge 0 " class="latex" /></p>
<p>and since this is true in <i>any</i> coordinate system, our would-be metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> is indeed nonnegative.  It&#8217;ll be an honest Riemannian metric whenever it&#8217;s positive definite.   </p>
<p>Second, suppose we&#8217;re working in the special case discussed in <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/">Part 2</a>, where our manifold is an open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7B%5Crho%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{&#92;rho}" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;mathbb{R}^n" class="latex" /> is the Gibbs state with <img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i" class="latex" />.  Then all the usual rules of statistical mechanics apply.  So, we can compute the variance of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> using the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%5E2+%5Crangle+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i%5E2%7D+%5Cln+Z++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)^2 &#92;rangle = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i^2} &#92;ln Z  " class="latex" /></p>
<p>In other words, </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bii%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i%5E2%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ii} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i^2} &#92;ln Z " class="latex" /></p>
<p>But since this is true in <i>any</i> coordinate system, we must have</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;ln Z " class="latex" /></p>
<p>(Here I&#8217;m using a little math trick: two symmetric bilinear forms whose diagonal entries agree in <i>any</i> basis must be equal.  We&#8217;ve already seen that the left side is symmetric, and the right side is symmetric by a famous fact about mixed partial derivatives.)</p>
<p>However, I&#8217;m pretty sure this cute formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;ln Z " class="latex" /></p>
<p>only holds in the special case I&#8217;m talking about now, where points in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> are parametrizing Gibbs states in the obvious way.   In general we must use</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29%28X_j-+%5Clangle+X_j+%5Crangle%29%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)(X_j- &#92;langle X_j &#92;rangle)&#92;rangle " class="latex" /></p>
<p>or equivalently, </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>Okay.  So much for cleaning up Last Week&#8217;s Mess.   Here&#8217;s something new.  We&#8217;ve seen that whenever <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> are observables (that is, self-adjoint),</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+AB%5Crangle%5E%2A+%3D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle AB&#92;rangle^* = &#92;langle BA &#92;rangle " class="latex" /></p>
<p>We got something symmetric by taking the real part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB%5Crangle+%3D++%5Cmathrm%7BRe%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB&#92;rangle =  &#92;mathrm{Re} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>Indeed,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BRe%7D+%5Clangle+AB+%5Crangle+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Clangle+AB+%2B+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Re} &#92;langle AB &#92;rangle = &#92;frac{1}{2} &#92;langle AB + BA &#92;rangle " class="latex" /></p>
<p>But by the same reasoning, we get something <i>antisymmetric</i> by taking the <i>imaginary</i> part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BIm%7D+%5Clangle+AB%5Crangle+%3D++-+%5Cmathrm%7BIm%7D+%5Clangle+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Im} &#92;langle AB&#92;rangle =  - &#92;mathrm{Im} &#92;langle BA &#92;rangle " class="latex" /></p>
<p>and indeed,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BIm%7D+%5Clangle+AB+%5Crangle+%3D+%5Cfrac%7B1%7D%7B2i%7D+%5Clangle+AB+-+BA+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Im} &#92;langle AB &#92;rangle = &#92;frac{1}{2i} &#92;langle AB - BA &#92;rangle " class="latex" /></p>
<p><a href="http://en.wikipedia.org/wiki/Commutator">Commutators</a> like <img src="https://s0.wp.com/latex.php?latex=AB-BA&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="AB-BA" class="latex" /> are important in quantum mechanics, so maybe we shouldn&#8217;t just throw out the imaginary part of the covariance matrix in our desperate search for a Riemannian metric!  Besides the symmetric tensor on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>we can also define a skew-symmetric tensor:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega_%7Bij%7D+%3D+%5Cmathrm%7BIm%7D+%5C%2C++%5Cmathrm%7Btr%7D+%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cfrac%7B%5Cpartial+%5Cln+%5Crho%7D%7B%5Cpartial+%5Clambda_j%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega_{ij} = &#92;mathrm{Im} &#92;,  &#92;mathrm{tr} (&#92;rho &#92;; &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_i} &#92;frac{&#92;partial &#92;ln &#92;rho}{&#92;partial &#92;lambda_j})" class="latex" /></p>
<p>This will vanish in the classical case, but not in the quantum case!</p>
<p>If you&#8217;ve studied enough geometry, you should now be reminded of things like &#8216;K&auml;hler manifolds&#8217; and &#8216;almost  K&auml;hler manifolds&#8217;.  A <a href="http://en.wikipedia.org/wiki/K%C3%A4hler_manifold">K&auml;hler manifold</a> is a manifold that&#8217;s equipped with a symmetric tensor <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and a skew-symmetric tensor <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> which fit together in the best possible way.  An <a href="http://en.wikipedia.org/wiki/Almost_K%C3%A4hler_manifold#K.C3.A4hler_manifolds">almost K&auml;hler manifold</a> is something similar, but not quite as nice.   We should probably see examples of these arising in information geometry!  And that could be pretty interesting.</p>
<p>But in general, if we start with any old manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> together with a function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> taking values in mixed states, we seem to be making <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> into something even less nice.  It gets a symmetric bilinear form <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> on each tangent space, and a skew-symmetric bilinear form <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />, and they vary smoothly from point to point&#8230; but they might be degenerate, and I don&#8217;t see any reason for them to &#8216;fit together&#8217; in the nice way we need for a K&auml;hler or almost K&auml;hler manifold.</p>
<p>However, I still think something interesting might be going on here.  For one thing, there are <i>other</i> situations in physics where a space of states is equipped with a symmetric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> and a skew-symmetric <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />. They show up in &#8216;dissipative mechanics&#8217; &mdash; the study of systems whose entropy increases.</p>
<p><a name="Ottinger">To conclude,</a> let me remind you of some things I said in <a href="http://math.ucr.edu/home/baez/week295.html">week295</a> of This Week&#8217;s Finds.  This is a huge digression from information geometry, but I&#8217;d like to lay out the the puzzle pieces in public view, in case it helps anyone get some good ideas.</p>
<p>I wrote:</p>
<blockquote><p>
&bull;  Hans Christian &Ouml;ttinger, <i>Beyond Equilibrium Thermodynamics</i>, Wiley, 2005.</p>
<p>I thank Arnold Neumaier for pointing out this book!  It considers a fascinating generalization of Hamiltonian mechanics that applies to  systems with dissipation: for example, electrical circuits with resistors, or mechanical systems with friction. </p>
<p>In ordinary Hamiltonian mechanics the space of states is a manifold and time evolution is a flow on this manifold determined by a smooth function called the Hamiltonian, which describes the <i>energy</i> of any state.  In this generalization the space of states is still a manifold, but now time evolution is determined by two smooth functions: the energy and the <i>entropy!</i> In ordinary Hamiltonian mechanics, energy is automatically conserved.  In this generalization that&#8217;s also true, but energy can go into the form of heat&#8230; and entropy automatically <i>increases!</i></p>
<p>Mathematically, the idea goes like this.  We start with a Poisson manifold, but in addition to the skew-symmetric Poisson bracket {F,G} of smooth functions on some manifold, we also have a symmetric bilinear bracket [F,G] obeying the Leibniz law</p>
<p>[F,GH] = [F,G]H + G[F,H]</p>
<p>and this positivity condition:</p>
<p>[F,F] &ge; 0</p>
<p>The time evolution of any function is given by a generalization of Hamilton&#8217;s equations:</p>
<p>dF/dt = {H,F} + [S,F]</p>
<p>where H is a function called the &quot;energy&quot; or &quot;Hamiltonian&quot;, and S is a function called the &quot;entropy&quot;.   The first term on the right is the usual one. The new second term describes dissipation: as we shall see, it pushes the state towards increasing entropy.</p>
<p>If we require that</p>
<p>[H,F] = {S,F} = 0</p>
<p>for every function F, then we get conservation of energy, as usual in Hamiltonian mechanics:</p>
<p>dH/dt = {H,H} + [S,H] = 0</p>
<p>But we also get the second law of thermodynamics:</p>
<p>dS/dt = {H,S} + [S,S] &ge; 0</p>
<p>Entropy always increases!</p>
<p>&Ouml;ttinger calls this framework &#8220;GENERIC&#8221; &#8211; an annoying acronym for &#8220;General Equation for the NonEquilibrium Reversible-Irreversible Coupling&#8221;.  There are lots of papers about it.  But I&#8217;m wondering if any geometers have looked into it!  </p>
<p>If we didn&#8217;t need the equations [H,F] = {S,F} = 0, we could easily get the necessary brackets starting with a K&auml;hler manifold.  The  imaginary part of the K&auml;hler structure is a symplectic structure, say &omega;, so we can define</p>
<p>{F,G} = &omega;(dF,dG)</p>
<p>as usual to get Poisson brackets.  The real part of the K&auml;hler structure is a Riemannian structure, say g, so we can define</p>
<p>[F,G] = g(dF,dG)</p>
<p>This satisfies</p>
<p>[F,GH] = [F,G]H + G[F,H]</p>
<p>and </p>
<p>[F,F] &ge; 0</p>
<p>Don&#8217;t be fooled: this stuff is not rocket science.  In particular, the inequality above has a simple meaning: when we move in the direction of the gradient of F, the function F increases.  So adding the second term to Hamilton&#8217;s equations has the effect of pushing the system towards increasing entropy.</p>
<p>Note that I&#8217;m being a tad unorthodox by letting &omega; and g eat cotangent vectors instead of tangent vectors &#8211; but that&#8217;s no big deal. The big deal is this: if we start with a K&auml;hler manifold and define brackets this way, we don&#8217;t get [H,F] = 0 or {S,F} = 0 for all functions F unless H and S are constant!  That&#8217;s no good for applications to physics.  To get around this problem, we would need to consider some sort of <i>degenerate</i> K&auml;hler structure &#8211; one where &omega; and g are degenerate bilinear forms on the cotangent space.</p>
<p>Has anyone thought about such things?  They remind me a little of &quot;Dirac structures&quot; and &quot;generalized complex geometry&quot; &#8211; but I don&#8217;t know enough about those subjects to know if they&#8217;re relevant here.</p>
<p>This GENERIC framework suggests that energy and entropy should be viewed as two parts of a single entity &#8211; maybe even its real and imaginary parts!  And that in turn reminds me of other strange  things, like the idea of using complex-valued Hamiltonians to describe dissipative systems, or the idea of &#8220;inverse temperature  as imaginary time&#8221;.  I can&#8217;t tell yet if there&#8217;s a big idea lurking here, or just a mess&#8230;.</p>
</blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/#comments">36 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1408 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1408">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/" rel="bookmark">Information Geometry (Part&nbsp;3)</a></h2>
				<small>25 October, 2010</small><br />


				<div class="entry">
					<p>So far in this series of posts I&#8217;ve been explaining a paper by Gavin Crooks. Now I want to go ahead and explain a little research of my own. </p>
<p>I&#8217;m not claiming my results are new &mdash; indeed I have no idea whether they are, and I&#8217;d like to hear from any experts who might know.  I&#8217;m just claiming that this is some work I did last weekend.</p>
<p>People sometimes worry that if they explain their ideas before publishing them, someone will &#8216;steal&#8217; them.  But I think this overestimates the value of ideas, at least in esoteric fields like mathematical physics.  The problem is not people stealing your ideas: the hard part is <i>giving them away</i>.  And let&#8217;s face it, people in love with math and physics will do research unless you actively stop them.  I&#8217;m reminded of this scene from the <a href="http://www.marx-brothers.org/whyaduck/info/movies/scenes/ravelli.htm">Marx Brothers movie</a> where Harpo and Chico, playing wandering musicians, walk into a hotel and offer to play:</p>
<blockquote><p>
Groucho: What do you fellows get an hour?</p>
<p>Chico: Oh, for playing we getta ten dollars an hour.</p>
<p>Groucho: I see&#8230;What do you get for not playing?</p>
<p>Chico: Twelve dollars an hour.</p>
<p>Groucho: Well, clip me off a piece of that.</p>
<p>Chico: Now, for rehearsing we make special rate. Thatsa fifteen dollars an hour.</p>
<p>Groucho: That&#8217;s for rehearsing?</p>
<p>Chico: Thatsa for rehearsing.</p>
<p>Groucho: And what do you get for not rehearsing?</p>
<p>Chico: You couldn&#8217;t afford it.
</p></blockquote>
<p>So, I&#8217;m just rehearsing in public here &mdash; but I of course I hope to write a paper about this stuff someday, once I get enough material.</p>
<p>Remember where we were.  We had considered a manifold &mdash; let&#8217;s finally give it a name, say <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> &mdash; that parametrizes Gibbs states of some physical system.  By <b><a href="http://en.wikipedia.org/wiki/Canonical_ensemble">Gibbs state</a></b>, I mean a state that maximizes entropy subject to constraints on the expected values of some observables.  And we had seen that in favorable cases, we get a Riemannian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />!  It looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> are our observables, and the angle bracket means &#8216;expected value&#8217;. </p>
<p>All this applies to both classical or quantum mechanics.  Crooks wrote down a beautiful formula for this metric in the classical case.  But since I&#8217;m at the Centre for <i>Quantum</i> Technologies, not the Centre for Classical Technologies, I redid his calculation in the quantum case.  The big difference is that in quantum mechanics, observables don&#8217;t commute!  But in the calculations I did, that didn&#8217;t seem to matter much &mdash; mainly because I took a lot of traces, which imposes a kind of commutativity:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28AB%29+%3D+%5Cmathrm%7Btr%7D%28BA%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(AB) = &#92;mathrm{tr}(BA) " class="latex" /></p>
<p>In fact, if I&#8217;d wanted to show off, I could have done the classical and quantum cases simultaneously by replacing all operators by elements of any <a href="http://en.wikipedia.org/wiki/Von_Neumann_algebra">von Neumann algebra</a> equipped with a <a href="http://en.wikipedia.org/wiki/Von_Neumann_algebra#Weights.2C_states.2C_and_traces">trace</a>.  Don&#8217;t worry about this much: it&#8217;s just a general formalism for treating classical and quantum mechanics on an equal footing.  One example is the algebra of bounded operators on a Hilbert space, with the usual concept of trace.  Then we&#8217;re doing quantum mechanics as usual.  But another example is the algebra of suitably nice functions on a suitably nice space, where taking the trace of a function means <i>integrating</i> it.  And then we&#8217;re doing classical mechanics!   </p>
<p>For example, I showed you how to derive a beautiful formula for the metric I wrote down a minute ago: </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} ) " class="latex" /></p>
<p>But if we want to do the classical version, we can say <i>Hey, presto!</i> and write it down like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cint_%5COmega+p%28%5Comega%29+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;int_&#92;Omega p(&#92;omega) &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^j} &#92;; d &#92;omega " class="latex" /></p>
<p>What did I do just now?  I changed the trace to an integral over some space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />.  I rewrote <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to make you think &#8216;probability distribution&#8217;.  And I don&#8217;t need to take the real part anymore, since is everything already real when we&#8217;re doing classical mechanics.    Now this metric is the <b><a href="http://en.wikipedia.org/wiki/Fisher_information_metric">Fisher information metric</a></b> that statisticians know and love!</p>
<p>In what follows, I&#8217;ll keep talking about the quantum case, but in the back of my mind I&#8217;ll be using von Neumann algebras, so everything will apply to the classical case too.</p>
<p>So what am I going to do?  I&#8217;m going to fix a big problem with the story I&#8217;ve told so far.</p>
<p>Here&#8217;s the problem: so far we&#8217;ve only studied a special case of the Fisher information metric.  We&#8217;ve been assuming our states are Gibbs states, parametrized by the expectation values of some observables <img src="https://s0.wp.com/latex.php?latex=X_1%2C+%5Cdots%2C+X_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1, &#92;dots, X_n" class="latex" />.   Our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> was really just some open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />: a point in here was a list of expectation values.</p>
<p>But people like to work a lot more generally.  We could look at <i>any</i> smooth function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> from a smooth manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the set of density matrices for some quantum system.   We can still write down the metric</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} )  " class="latex" /></p>
<p>in this more general situation.  Nobody can stop us!  But it would be better if we could <i>derive</i> this formula, as before, starting from a formula like the one we had before:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5C%2C+%28X_i+-+%5Clangle+X_i+%5Crangle%29+%5C%2C+%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5C%2C+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;, (X_i - &#92;langle X_i &#92;rangle) &#92;, (X_j - &#92;langle X_j &#92;rangle)  &#92;, &#92;rangle " class="latex" /></p>
<p>The challenge is that now we don&#8217;t have observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> to start with.  All we have is a smooth function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> from some manifold to some set of states.   How can we pull observables out of thin air?</p>
<p>Well, you may remember that last time we had</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Clambda%5Ei+X_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;lambda^i X_i}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> were some functions on our manifold and </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D%28e%5E%7B-%5Clambda%5Ei+X_i%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr}(e^{-&#92;lambda^i X_i})" class="latex" /></p>
<p>was the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a>.  Let&#8217;s copy this idea.  </p>
<p>So, we&#8217;ll start with our density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, but then write it as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-A}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is some self-adjoint operator and </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-A%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-A})" class="latex" /></p>
<p>(Note that <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, like <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, is really an operator-valued function on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  So, I should write something like <img src="https://s0.wp.com/latex.php?latex=A%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A(x)" class="latex" /> to denote its value at a particular point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" />, but I won&#8217;t usually do that.  As usual, I expect some intelligence on your part!)</p>
<p>Now we can repeat some calculations I did last time.  As before, let&#8217;s take the logarithm of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D+%5C%2C+%5Crho+%3D+-A+-+%5Cmathrm%7Bln%7D%5C%2C++Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln} &#92;, &#92;rho = -A - &#92;mathrm{ln}&#92;,  Z" class="latex" /></p>
<p>and then differentiate it.  Suppose <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> are local coordinates near some point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D%5C%2C+%5Crho+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A+-+%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln}&#92;, &#92;rho = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A - &#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z" class="latex" /></p>
<p>Last time we had nice formulas for both terms on the right-hand side above.  To get similar formulas now, let&#8217;s define operators</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i = &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A" class="latex" /></p>
<p>This gives a nice name to the first term on the right-hand side above.  What about the second term?  We can calculate it out:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z+%3D+%5Cfrac%7B1%7D%7BZ%7D++%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Btr%7D%28e%5E%7B-A%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%7D++%5Cmathrm%7Btr%7D%28%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+e%5E%7B-A%7D%29+%3D+-+%5Cfrac%7B1%7D%7BZ%7D++%5Cmathrm%7Btr%7D%28e%5E%7B-A%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z = &#92;frac{1}{Z}  &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} &#92;mathrm{tr}(e^{-A}) = &#92;frac{1}{Z}  &#92;mathrm{tr}(&#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} e^{-A}) = - &#92;frac{1}{Z}  &#92;mathrm{tr}(e^{-A} &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A)" class="latex" /></p>
<p>where in the last step we use the chain rule.   Next, use the definition of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, and get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+Z+%3D+-+%5Cmathrm%7Btr%7D%28%5Crho+X_i%29+%3D+-+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{1}{Z} &#92;frac{&#92;partial }{&#92;partial &#92;lambda^i} Z = - &#92;mathrm{tr}(&#92;rho X_i) = - &#92;langle X_i &#92;rangle" class="latex" /></p>
<p>This is just what we got last time!  Ain&#8217;t it fun to calculate when it all works out so nicely?</p>
<p>So, putting both terms together, we see </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho+%3D+-+X_i+%2B+%5Clangle+X_i+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho = - X_i + &#92;langle X_i &#92;rangle " class="latex" /></p>
<p>or better:</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho" class="latex" /></p>
<p>This is a nice formula for the &#8216;fluctuation&#8217; of the observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, meaning how much they differ from their expected values.  And it looks exactly like the formula we had last time!  The difference is that last time we <i>started out</i> assuming we had a bunch of observables, <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, and defined <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> to be the state maximizing the entropy subject to constraints on the expectation values of all these observables.<br />
Now we&#8217;re starting with <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> and working backwards.</p>
<p>From here on out, it&#8217;s easy.   As before, we can define <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> to be the real part of the covariance matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>Using the formula </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;mathrm{ln} &#92;rho" class="latex" /></p>
<p>we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^j} &#92;rangle " class="latex" /></p>
<p>or </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D%5C%2C%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho%7D%7B%5Cpartial+%5Clambda%5Ej%7D%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re}&#92;,&#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho}{&#92;partial &#92;lambda^j}) " class="latex" /></p>
<p><i>Voil&agrave;!</i>  </p>
<p>When this matrix is positive definite at every point, we get a Riemanian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.   Last time I said this is what people call the <a href="http://en.wikipedia.org/wiki/Bures_metric">&#8216;Bures metric&#8217;</a> &mdash; though frankly, now that I examine the formulas, I&#8217;m not so sure.  But in the classical case, it&#8217;s called the Fisher information metric.   </p>
<p>Differential geometers like to use <img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i" class="latex" /> as a shorthand for <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial}{&#92;partial_i}" class="latex" />, so they&#8217;d write down our metric in a prettier way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cpartial_i+%28%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+%5C%3B+%5Cpartial_j+%28%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;partial_i (&#92;mathrm{ln} &#92;, &#92;rho) &#92;; &#92;partial_j (&#92;mathrm{ln} &#92;, &#92;rho) )" class="latex" /></p>
<p>Differential geometers like coordinate-free formulas, so let&#8217;s also give a coordinate-free formula for our metric.  Suppose <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in M" class="latex" /> is a point in our manifold, and suppose <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> are tangent vectors to this point.  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28v%2Cw%29+%3D+%5Cmathrm%7BRe%7D+%5C%2C+%5Clangle+v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29+%5C%3B+w%28%5Cmathrm%7Bln%7D+%5C%2C%5Crho%29+%5Crangle+%5C%3B+%3D+%5C%3B+%5Cmathrm%7BRe%7D+%5C%2C%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29+%5C%3B+w%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(v,w) = &#92;mathrm{Re} &#92;, &#92;langle v(&#92;mathrm{ln}&#92;, &#92;rho) &#92;; w(&#92;mathrm{ln} &#92;,&#92;rho) &#92;rangle &#92;; = &#92;; &#92;mathrm{Re} &#92;,&#92;mathrm{tr}(&#92;rho &#92;; v(&#92;mathrm{ln}&#92;, &#92;rho) &#92;; w(&#92;mathrm{ln}&#92;, &#92;rho))  " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, &#92;rho" class="latex" /> is a smooth operator-valued function on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=v%28%5Cmathrm%7Bln%7D%5C%2C+%5Crho%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v(&#92;mathrm{ln}&#92;, &#92;rho)" class="latex" /> means the derivative of this function in the <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> direction at the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.</p>
<p>So, this is all very nice.  To conclude, two more points: a technical one, and a more important philosophical one.</p>
<p>First, the technical point.  When I said <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> could be <i>any</i> smooth function from a smooth manifold to some set of states, I was actually lying.  That&#8217;s an important pedagogical technique: the brazen lie.</p>
<p>We can&#8217;t really take the logarithm of <i>every</i> density matrix.  Remember, we take the log of a density matrix by taking the log of all its eigenvalues.  These eigenvalues are &ge; 0, but if one of them is zero, we&#8217;re in trouble!  The logarithm of zero is undefined.</p>
<p>On the other hand, there&#8217;s no problem taking the logarithm of our density-matrix-valued function <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> when it&#8217;s <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite</a> at each point of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />.  You see, a density matrix is positive definite iff its eigenvalues are all &gt; 0.   In this case it has a unique self-adjoint logarithm.</p>
<p>So, we must assume <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is positive definite.   But what&#8217;s the physical significance of this &#8216;positive definiteness&#8217; condition?  Well, any density matrix can be diagonalized using some orthonormal basis.  It can then be seen as probabilistic mixture &mdash; not a quantum superposition! &mdash; of pure states taken from this basis. Its eigenvalues are the probabilities of finding the mixed state to be in one of these pure states.  So, saying that all its eigenvalues are all &gt; 0 amounts to saying that all the pure states in this orthonormal basis show up with <i>nonzero</i> probability! Intuitively, this means our mixed state is &#8216;really mixed&#8217;.  For example, it can&#8217;t be a pure state.  In math jargon, it means our mixed state is in the <i>interior</i> of the convex set of mixed states.</p>
<p>Second, the philosophical point.  Instead of starting with the density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, I took <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> as fundamental.   But different choices of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> give the same <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  After all,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-A}" class="latex" /></p>
<p>where we cleverly divide by the normalization factor</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-A%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-A})" class="latex" /></p>
<p>to get <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D+%5C%2C+%5Crho+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr} &#92;, &#92;rho = 1" class="latex" />.  So, if we multiply <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-A%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e^{-A}" class="latex" /> by any positive constant, or indeed any positive <i>function</i> on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> will remain unchanged!  </p>
<p>So we have added a little extra information when switching from <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />.   You can think of this as &#8216;gauge freedom&#8217;, because I&#8217;m saying we can do any transformation like</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%5Cmapsto+A+%2B+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;mapsto A + f " class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=f%3A+M+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: M &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is a smooth function.   This doesn&#8217;t change <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />, so arguably it doesn&#8217;t change the &#8216;physics&#8217; of what I&#8217;m doing.  It <i>does</i> change <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  It also changes the observables </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i = &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} A" class="latex" /></p>
<p>But it doesn&#8217;t change their &#8216;fluctuations&#8217; </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle" class="latex" /></p>
<p>so it doesn&#8217;t change the metric <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.</p>
<p>This gauge freedom is interesting, and I want to understand it better. It&#8217;s related to something very simple yet mysterious.  In statistical mechanics the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> begins life as &#8216;just a normalizing factor&#8217;.   If you change the physics so that <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> gets multiplied by some number, the Gibbs state doesn&#8217;t change.  But then the partition function takes on an incredibly significant role as something whose logarithm you differentiate to get lots of physically interesting information!   So in some sense the partition function doesn&#8217;t matter much&#8230; but <i>changes</i> in the partition function matter a lot.</p>
<p>This is just like the split personality of phases in quantum mechanics.  On the one hand they &#8216;don&#8217;t matter&#8217;: you can multiply a unit vector by any phase and the pure state it defines doesn&#8217;t change.  But on the other hand, <i>changes</i> in phase matter a lot.  </p>
<p>Indeed the analogy here is quite deep: it&#8217;s the analogy between probabilities in statistical mechanics and amplitudes in quantum mechanics, the analogy between <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bexp%7D%28-%5Cbeta+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{exp}(-&#92;beta H)" class="latex" /> in statistical mechanics and <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bexp%7D%28-i+t+H+%2F+%5Chbar%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{exp}(-i t H / &#92;hbar)" class="latex" /> in quantum mechanics, and so on.  This is part of a bigger story about &#8216;rigs&#8217; which I told back in the <a href="http://math.ucr.edu/home/baez/qg-winter2007/qg-winter2007.html#quantization">Winter 2007 quantum gravity seminar</a>, especially in <a href="http://math.ucr.edu/home/baez/qg-winter2007/w07week05a.pdf">week13</a>.  So, it&#8217;s fun to see it showing up yet again&#8230; even though I don&#8217;t completely understand it here.</p>
<p>[Note: in the original version of this post, I omitted the real part in my definition <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" />, giving a &#8216;Riemannian metric&#8217; that was <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">neither real nor symmetric</a> in the quantum case.  Most of the comments below are based on that original version, not the new fixed one.]</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/#comments">58 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/25/information-geometry-part-3/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1372 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1372">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/" rel="bookmark">Information Geometry (Part&nbsp;2)</a></h2>
				<small>23 October, 2010</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">Last time</a> I provided some background to this paper:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>.</p>
<p>Now I&#8217;ll tell you a bit about what it actually says!</p>
<p>Remember the story so far: we&#8217;ve got a physical system that&#8217;s in a state of maximum entropy.  I didn&#8217;t emphasize this yet, but that happens whenever our system is in <a href="http://en.wikipedia.org/wiki/Thermodynamic_equilibrium">thermodynamic equilibrium</a>.  An example would be a box of gas inside a piston.  Suppose you choose any number for the energy of the gas and any number for its volume.   Then there&#8217;s a unique state of the gas that maximizes its entropy, given the constraint that <i>on average</i>, its energy and volume have the values you&#8217;ve chosen.  And this describes what the gas will be like in equilibrium!</p>
<p>Remember, by &#8216;state&#8217; I mean <i>mixed</i> state: it&#8217;s a probabilistic description.  And I say the energy and volume have chosen values <i>on average</i> because there will be random fluctuations.  Indeed, if you look carefully at the head of the piston, you&#8217;ll see it quivering: the volume of the gas only equals the volume you&#8217;ve specified <i>on average</i>.  Same for the energy.</p>
<p>More generally: imagine picking any list of numbers, and finding the maximum entropy state where some chosen observables have these numbers as their average values.  Then there will be fluctuations in the values of these observables &mdash; thermal fluctuations, but also possibly quantum fluctuations.  So, you&#8217;ll get a probability distribution on the space of possible values of your chosen observables.  You should visualize this probability distribution as a little fuzzy cloud centered at the average value!  </p>
<div align="center">
<img width="400" src="https://i0.wp.com/www.math.dartmouth.edu/archive/m22x06/public_html/ellipsoid.jpg" alt="" />
</div>
<p>To a first approximation, this cloud will be shaped like a little ellipsoid.  And if you can pick the average value of your observables to be whatever you&#8217;ll like, you&#8217;ll get lots of little ellipsoids this way, one centered at each point.  And the cool idea is to imagine the space of possible values of your observables as having a weirdly warped geometry, such that <i>relative to this geometry, these ellipsoids are actually spheres</i>.  </p>
<p>This weirdly warped geometry is an example of an <a href="http://en.wikipedia.org/wiki/Information_geometry">&#8216;information geometry&#8217;</a>: a geometry that&#8217;s defined using the concept of information.  This shouldn&#8217;t be surprising: after all, we&#8217;re talking about maximum entropy, and <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">entropy is related to information</a>.  But I want to gradually make this idea more precise.</p>
<p>Bring on the math!</p>
<p>We&#8217;ve got a bunch of observables <img src="https://s0.wp.com/latex.php?latex=X_1%2C+%5Cdots+%2C+X_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1, &#92;dots , X_n" class="latex" />, and we&#8217;re assuming that for any list of numbers <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cdots+%2C+x_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1, &#92;dots , x_n" class="latex" />, the system has a unique maximal-entropy state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> for which the expected value of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>This state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is called the <b>Gibbs state</b> and I told you how to find it when it exists.   In fact it may not exist for <i>every</i> list of numbers <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cdots+%2C+x_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1, &#92;dots , x_n" class="latex" />, but we&#8217;ll be perfectly happy if it does for all choices of </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%28x_1%2C+%5Cdots%2C+x_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = (x_1, &#92;dots, x_n) " class="latex" /></p>
<p>lying in some open subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />  </p>
<p>By the way, I should really call this Gibbs state <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho(x)" class="latex" /> or something to indicate how it depends on <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />, but I won&#8217;t usually do that.  I expect some intelligence on your part!</p>
<p>Now at each point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> we can define a <a href="http://en.wikipedia.org/wiki/Covariance">covariance matrix</a></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>If we take its real part, we get a symmetric matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>It&#8217;s also nonnegative &mdash; that&#8217;s easy to see, since the variance of a probability distribution can&#8217;t be negative.  When we&#8217;re lucky this matrix will be <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite</a>.  When we&#8217;re even luckier, it will depend smoothly on <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  In this case, <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> will define a <a href="http://en.wikipedia.org/wiki/Riemannian_metric#Riemannian_metrics">Riemannian metric</a> on our open set.   </p>
<p>So far this is all review of <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/">last time</a>.  Sorry: I seem to have reached the age where I can&#8217;t say anything interesting without warming up for about 15 minutes first.  It&#8217;s like when my mom tells me about an exciting event that happened to her: she starts by saying &#8220;Well, I woke up, and it was cloudy out&#8230;&#8221;  </p>
<p>But now I want to give you an explicit formula for the metric <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />, and then <i>rewrite it</i> in a way that&#8217;ll work even when the state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is <i>not</i> a maximal-entropy state.  And this formula will then be the general definition of the <a href="http://en.wikipedia.org/wiki/Fisher_information_metric">&#8216;Fisher information metric&#8217;</a> (if we&#8217;re doing classical mechanics), or a quantum version thereof (if we&#8217;re doing quantum mechanics).</p>
<p>Crooks does the classical case &mdash; so let&#8217;s do the quantum case, okay?  Last time I claimed that in the quantum case, our maximum-entropy state is the <b>Gibbs state</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Clambda%5Ei+X_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;lambda^i X_i}" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> are the &#8216;conjugate variables&#8217; of the observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />, we&#8217;re using the <a href="http://en.wikipedia.org/wiki/Einstein_notation">Einstein summation convention</a> to sum over repeated indices that show up once upstairs and once downstairs, and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is the <b>partition function</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D+%28e%5E%7B-%5Clambda%5Ei+X_i%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr} (e^{-&#92;lambda^i X_i})" class="latex" /></p>
<p>(To be honest: last time I wrote the indices on the conjugate variables <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> as subscripts rather than superscripts, because I didn&#8217;t want some poor schlep out there to think that <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E1%2C+%5Cdots+%2C+%5Clambda%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^1, &#92;dots , &#92;lambda^n" class="latex" /> were the powers of some number <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" />.  But now I&#8217;m assuming you&#8217;re all grown up and ready to juggle indices!  We&#8217;re doing Riemannian geometry, after all.)</p>
<p>Also last time I claimed that it&#8217;s tremendously fun and enlightening to take the derivative of the logarithm of <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />.  The reason is that it gives you the mean values of your observables:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda^i} &#92;ln Z " class="latex" /></p>
<p>But now let&#8217;s take the derivative of the logarithm of <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" />.  Remember, <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is an operator &mdash; in fact a <a href="http://en.wikipedia.org/wiki/Density_matrix">density matrix</a>.  But we can take its logarithm as explained last time, and the usual rules apply, so starting from</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-%5Clambda%5Ei+X_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} e^{-&#92;lambda^i X_i}" class="latex" /></p>
<p>we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+%5Crho+%3D+-+%5Clambda%5Ei+X_i+-+%5Cmathrm%7Bln%7D+%5C%2CZ+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, &#92;rho = - &#92;lambda^i X_i - &#92;mathrm{ln} &#92;,Z " class="latex" /></p>
<p>Next, let&#8217;s differentiate both sides with respect to <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" />.  Why?  Well, from what I just said, you should be itching to differentiate <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, Z" class="latex" />.   So let&#8217;s give in to that temptation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial++%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5Cmathrm%7Bln%7D++%5Crho+%3D+-X_i+%2B+%5Clangle+X_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial  }{&#92;partial &#92;lambda^i} &#92;mathrm{ln}  &#92;rho = -X_i + &#92;langle X_i &#92;rangle" class="latex" /> </p>
<p>Hey!  Now we&#8217;ve got a formula for the &#8216;fluctuation&#8217; of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> &mdash; that is, how much it differs from its mean value:</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = - &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i}" class="latex" /></p>
<p>This is incredibly cool!  I should have learned this formula decades ago, but somehow I just bumped into it now.  I knew of course that <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D+%5C%2C+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln} &#92;, &#92;rho" class="latex" /> shows up in the formula for the <b>entropy</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Crho%29+%3D+%5Cmathrm%7Btr%7D+%28+%5Crho+%5C%3B+%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(&#92;rho) = &#92;mathrm{tr} ( &#92;rho &#92;; &#92;mathrm{ln} &#92;, &#92;rho) " class="latex" /></p>
<p>But I never had the brains to think about <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, &#92;rho" class="latex" /> all by itself.  So I&#8217;m really excited to discover that it&#8217;s an interesting entity in its own right &mdash; and fun to differentiate, just like <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bln%7D%5C%2C+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{ln}&#92;, Z" class="latex" />.  </p>
<p>Now we get our cool formula for <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" />.  Remember, it&#8217;s defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>But now that we know </p>
<p><img src="https://s0.wp.com/latex.php?latex=X_i+-+%5Clangle+X_i+%5Crangle+%3D+-%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i - &#92;langle X_i &#92;rangle = -&#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i}" class="latex" /></p>
<p>we get the formula we were looking for:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j}  &#92;rangle " class="latex" /></p>
<p>Beautiful, eh?  And of course the expected value of any observable <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> in the state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Cmathrm%7Btr%7D%28%5Crho+A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;mathrm{tr}(&#92;rho A)" class="latex" /></p>
<p>so we can also write the covariance matrix like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D%5C%2C+%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+%5Crho+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re}&#92;, &#92;mathrm{tr}(&#92;rho &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} &#92;rho }{&#92;partial &#92;lambda^j} ) " class="latex" /></p>
<p>Lo and behold!  This formula makes sense whenever <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is <i>any</i> density matrix depending smoothly on some parameters <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" />.  We don&#8217;t need it to be a Gibbs state!   So, we can work more generally.</p>
<p>Indeed, whenever we have <i>any</i> smooth function from a manifold to the space of density matrices for some Hilbert space, we can define <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> by the above formula!  And when it&#8217;s positive definite, we get a Riemannian metric on our manifold: the <b>Bures information metric</b>.</p>
<p>The classical analogue is the somewhat more well-known &#8216;Fisher information metric&#8217;.  When we go from quantum to classical, operators become functions and traces become integrals.  There&#8217;s nothing complex anymore, so taking the real part becomes unnecessary.  So the Fisher information metric looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cint_%5COmega+p%28%5Comega%29+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ei%7D+%5C%3B+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bln%7D+p%28%5Comega%29+%7D%7B%5Cpartial+%5Clambda%5Ej%7D+%5C%3B+d+%5Comega+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;int_&#92;Omega p(&#92;omega) &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^i} &#92;; &#92;frac{&#92;partial &#92;mathrm{ln} p(&#92;omega) }{&#92;partial &#92;lambda^j} &#92;; d &#92;omega " class="latex" /></p>
<p>Here I&#8217;m assuming we&#8217;ve got a smooth function <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> from some manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" /> to the space of probability distributions on some measure space <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C+d%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega, d&#92;omega)" class="latex" />.    Working in local coordinates <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda^i" class="latex" /> on our manifold <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, the above formula defines a Riemannian metric on <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="M" class="latex" />, at least when <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij}" class="latex" /> is positive definite.  And that&#8217;s the <b>Fisher information metric</b>!</p>
<p>Crooks says more: he describes an experiment that would let you measure the length of a path with respect to the Fisher information metric &mdash; at least in the case where the state <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho(x)" class="latex" /> is the Gibbs state with <img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i" class="latex" />.   And that explains why he calls it &#8216;thermodynamic length&#8217;.</p>
<p>There&#8217;s a lot more to say about this, and also about another question: <i>What use is the Fisher information metric in the general case where the states <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho(x)" class="latex" /> aren&#8217;t Gibbs states?</i></p>
<p>But it&#8217;s dinnertime, so I&#8217;ll stop here.</p>
<p>[Note: in the original version of this post, I omitted the real part in my definition <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" />, giving a &#8216;Riemannian metric&#8217; that was <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">neither real nor symmetric</a> in the quantum case.  Most of the comments below are based on that original version, not the new fixed one.]</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/#comments">22 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/23/information-geometry-part-2/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-1343 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-1343">
				<h2><a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/" rel="bookmark">Information Geometry (Part&nbsp;1)</a></h2>
				<small>22 October, 2010</small><br />


				<div class="entry">
					<p>I&#8217;d like to provide a bit of background to this interesting paper:</p>
<p>&bull; Gavin E. Crooks, <a href="http://arxiv.org/abs/0706.0559" rel="nofollow">Measuring thermodynamic length</a>.</p>
<p>which was pointed out by <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/#comment-2092">John F</a> in our discussion of <a href="https://johncarlosbaez.wordpress.com/2010/10/19/entropy-and-uncertainty/">entropy and uncertainty</a>.  </p>
<p>The idea here should work for either classical or quantum statistical mechanics.  The paper describes the classical version, so just for a change of pace let me describe the quantum version.</p>
<p>First a lightning review of <a href="http://en.wikipedia.org/wiki/Quantum_statistical_mechanics">quantum statistical mechanics</a>.  Suppose you have a quantum system with some Hilbert space.  When you know as much as possible about your system, then you describe it by a unit vector in this Hilbert space, and you say your system is in a <a href="http://en.wikipedia.org/wiki/Quantum_state#Pure_states_as_rays_in_a_Hilbert_space"><b>pure state</b></a>.  Sometimes people just call a pure state a &#8216;state&#8217;.  But that can be confusing, because in statistical mechanics you also need more general &#8216;mixed states&#8217; where you <i>don&#8217;t</i> know as much as possible.   A mixed state is described by a <a href="http://en.wikipedia.org/wiki/Density_matrix"><b>density matrix</b></a>, meaning a <a href="http://en.wikipedia.org/wiki/Positive_element">positive operator</a> <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> with <a href="http://en.wikipedia.org/wiki/Trace_%28linear_algebra%29">trace</a> equal to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Btr%7D%28%5Crho%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{tr}(&#92;rho) = 1" class="latex" /></p>
<p>The idea is that any observable is described by a self-adjoint operator <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, and the expected value of this observable in the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Cmathrm%7Btr%7D%28%5Crho+A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;mathrm{tr}(&#92;rho A)" class="latex" /></p>
<p>The <b>entropy</b> of a mixed state is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Crho%29+%3D+-%5Cmathrm%7Btr%7D%28%5Crho+%5C%3B+%5Cmathrm%7Bln%7D+%5C%2C+%5Crho%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(&#92;rho) = -&#92;mathrm{tr}(&#92;rho &#92;; &#92;mathrm{ln} &#92;, &#92;rho)" class="latex" /></p>
<p>where we take the logarithm of the density matrix just by taking the log of each of its eigenvalues, while keeping the same eigenvectors.   This formula for entropy should remind you of the one that Gibbs and Shannon used &mdash; the one I explained <a href="https://johncarlosbaez.wordpress.com/2010/10/12/algorithmic-thermodynamics/">a while back</a>.  </p>
<p>Back then I told you about the &#8216;Gibbs ensemble&#8217;: the mixed state that maximizes entropy subject to the constraint that some observable have a given value.  We can do the same thing in quantum mechanics, and we can even do it for a bunch of observables at once.  Suppose we have some observables <img src="https://s0.wp.com/latex.php?latex=X_1%2C+%5Cdots%2C+X_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_1, &#92;dots, X_n" class="latex" /> and we want to find the mixed state <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> that maximizes entropy subject to these constraints:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>for some numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />.  Then a little exercise in <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">Lagrange multipliers</a> shows that the answer is the <b>Gibbs state</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Cmathrm%7Bexp%7D%28-%5Clambda_1+X_1+%2B+%5Ccdots+%2B+%5Clambda_n+X_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho = &#92;frac{1}{Z} &#92;mathrm{exp}(-&#92;lambda_1 X_1 + &#92;cdots + &#92;lambda_n X_n) " class="latex" /></p>
<p>Huh?  <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/sm_confused.gif" alt="" /></p>
<p>This answer needs some explanation.  First of all, the numbers <img src="https://s0.wp.com/latex.php?latex=%5Clambda_1%2C+%5Cdots+%5Clambda_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda_1, &#92;dots &#92;lambda_n" class="latex" /> are called Lagrange multipliers.  You have to choose them right to get </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>So, in favorable cases, they will be functions of the numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />.  And when you&#8217;re really lucky, you can solve for the numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> in terms of the numbers <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda_i" class="latex" />.  We call <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda_i" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Conjugate_variables_%28thermodynamics%29"><b>conjugate variable</b></a> of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />.  For example, the conjugate variable of energy is inverse temperature! </p>
<p>Second of all, we take the exponential of a self-adjoint operator just as we took the logarithm of a density matrix: just take the exponential of each eigenvalue.  </p>
<p>(At least this works when our self-adjoint operator has only eigenvalues in its spectrum, not any <a>continuous spectrum</a>.  Otherwise we need to get serious and use the <a href="http://en.wikipedia.org/wiki/Functional_calculus">functional calculus</a>.  Luckily, if your system&#8217;s Hilbert space is finite-dimensional, you can ignore this parenthetical remark!)</p>
<p>But third: what&#8217;s that number <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />?  It begins life as a humble normalizing factor.  Its job is to make sure <img src="https://s0.wp.com/latex.php?latex=%5Crho&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;rho" class="latex" /> has trace equal to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cmathrm%7Btr%7D%28%5Cmathrm%7Bexp%7D%28-%5Clambda_1+X_1+%2B+%5Ccdots+%2B+%5Clambda_n+X_n%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;mathrm{tr}(&#92;mathrm{exp}(-&#92;lambda_1 X_1 + &#92;cdots + &#92;lambda_n X_n)) " class="latex" /></p>
<p>However, once you get going, it becomes incredibly important!  It&#8217;s called the <a href="http://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29"><b>partition function</b></a> of your system.</p>
<p>As an example of what it&#8217;s good for, it turns out you can compute the numbers <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=x_i+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cmathrm%7Bln%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda_i} &#92;mathrm{ln} Z" class="latex" /></p>
<p>In other words, you can compute the expected values of the observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> by differentiating the log of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Clambda_i%7D+%5Cmathrm%7Bln%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = - &#92;frac{&#92;partial}{&#92;partial &#92;lambda_i} &#92;mathrm{ln} Z" class="latex" /></p>
<p>Or in still other words,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+-+%5Cfrac%7B1%7D%7BZ%7D+%5C%3B+%5Cfrac%7B%5Cpartial+Z%7D%7B%5Cpartial+%5Clambda_i%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = - &#92;frac{1}{Z} &#92;; &#92;frac{&#92;partial Z}{&#92;partial &#92;lambda_i}" class="latex" /> </p>
<p>To believe this you just have to take the equations I&#8217;ve given you so far and mess around &mdash; there&#8217;s really no substitute for doing it yourself.  I&#8217;ve done it fifty times, and every time I feel smarter.</p>
<p>But we can go further: after the <a href="http://en.wikipedia.org/wiki/Expected_value">&#8216;expected value&#8217;</a> or &#8216;mean&#8217; of an observable comes its <a href="http://en.wikipedia.org/wiki/Variance">variance</a>, which is the square of its standard deviation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5CDelta+A%29%5E2+%3D+%5Clangle+A%5E2+%5Crangle+-+%5Clangle+A+%5Crangle%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Delta A)^2 = &#92;langle A^2 &#92;rangle - &#92;langle A &#92;rangle^2 " class="latex" /></p>
<p>This measures the size of fluctuations around the mean.  And in the Gibbs state, we can compute the variance of the observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> as the <i>second</i> derivative of the log of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i%5E2+%5Crangle+-+%5Clangle+X_i+%5Crangle%5E2+%3D++%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial%5E2+%5Clambda_i%7D+%5Cmathrm%7Bln%7D+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i^2 &#92;rangle - &#92;langle X_i &#92;rangle^2 =  &#92;frac{&#92;partial^2}{&#92;partial^2 &#92;lambda_i} &#92;mathrm{ln} Z" class="latex" /></p>
<p>Again: calculate and see.</p>
<p>But when we&#8217;ve got lots of observables, there&#8217;s something better than the variance of each one.  There&#8217;s the <a href="http://en.wikipedia.org/wiki/Covariance">covariance matrix</a> of the whole lot of them!  Each observable <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> fluctuates around its mean value <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" />&#8230; but these fluctuations are not independent!  They&#8217;re <i>correlated</i>, and the covariance matrix says how.  </p>
<p>All this is very visual, at least for me.  If you imagine the fluctuations as forming a blurry patch near the point <img src="https://s0.wp.com/latex.php?latex=%28x_1%2C+%5Cdots%2C+x_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x_1, &#92;dots, x_n)" class="latex" />, this patch will be ellipsoidal in shape, at least when all our random fluctuations are Gaussian.  And then the <i>shape</i> of this ellipsoid is precisely captured by the covariance matrix!  In particular, the eigenvectors of the covariance matrix will point along the principal axes of this ellipsoid, and the eigenvalues will say how stretched out the ellipsoid is in each direction!</p>
<div align="center">
<img width="400" src="https://i0.wp.com/www.math.dartmouth.edu/archive/m22x06/public_html/ellipsoid.jpg" alt="" />
</div>
<p>To understand the covariance matrix, it may help to start by rewriting the variance of a single observable <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5CDelta+A%29%5E2+%3D+%5Clangle+%28A+-+%5Clangle+A+%5Crangle%29%5E2+%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Delta A)^2 = &#92;langle (A - &#92;langle A &#92;rangle)^2 &#92;rangle " class="latex" /></p>
<p>That&#8217;s a lot of angle brackets, but the meaning should be clear. First we look at the difference between our observable and its mean value, namely </p>
<p><img src="https://s0.wp.com/latex.php?latex=A+-+%5Clangle+A+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A - &#92;langle A &#92;rangle" class="latex" /></p>
<p>Then we square this, to get something that&#8217;s big and positive whenever our observable is far from its mean.  Then we take the mean value of the <i>that</i>, to get an idea of how far our observable is from the mean <i>on average</i>.</p>
<p>We can use the same trick to define the covariance of a bunch of observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" />.   We get an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix called the <b>covariance matrix</b>, whose entry in the <i>i</i>th row and <i>j</i>th column is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>If you think about it, you can see that this will measure correlations in the fluctuations of your observables.  </p>
<p>An interesting difference between classical and quantum mechanics shows up here.  In classical mechanics the covariance matrix is always symmetric &mdash; but not in quantum mechanics!  You see, in classical mechanics, whenever we have two observables <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+B+%5Crangle+%3D+%5Clangle+B+A+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A B &#92;rangle = &#92;langle B A &#92;rangle" class="latex" /></p>
<p>since observables commute.  But in quantum mechanics this is not true!  For example, consider the position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> of a particle.  We have</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+p+%3D+p+q+%2B+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q p = p q + i " class="latex" /></p>
<p>so taking expectation values we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+q+p+%5Crangle+%3D+%5Clangle+p+q+%5Crangle+%2B+i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle q p &#92;rangle = &#92;langle p q &#92;rangle + i " class="latex" /></p>
<p>So, it&#8217;s easy to get a non-symmetric covariance matrix when our observables <img src="https://s0.wp.com/latex.php?latex=X_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X_i" class="latex" /> don&#8217;t commute.  However, the <i>real part</i> of the covariance matrix is symmetric, even in quantum mechanics.   So let&#8217;s define </p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cmathrm%7BRe%7D++%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;mathrm{Re}  &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" /></p>
<p>You can check that the matrix entries here are the second derivatives of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cmathrm%7Bln%7D+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;mathrm{ln} Z " class="latex" /></p>
<p>And now for the cool part: this is where information geometry comes in!  Suppose that for any choice of values <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i" class="latex" /> we have a Gibbs state with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+X_i+%5Crangle+%3D+x_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle X_i &#92;rangle = x_i " class="latex" /></p>
<p>Then for each point </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%28x_1%2C+%5Cdots+%2C+x_n%29+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = (x_1, &#92;dots , x_n) &#92;in &#92;mathbb{R}^n" class="latex" /> </p>
<p>we have a matrix</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D++%5Cmathrm%7BRe%7D++%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+%3D+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Clambda_i+%5Cpartial+%5Clambda_j%7D+%5Cmathrm%7Bln%7D+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} =  &#92;mathrm{Re}  &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle = &#92;frac{&#92;partial^2}{&#92;partial &#92;lambda_i &#92;partial &#92;lambda_j} &#92;mathrm{ln} Z " class="latex" /></p>
<p>And this matrix is not only symmetric, it&#8217;s also <a href="http://en.wikipedia.org/wiki/Positive_operator">positive</a>.  And when it&#8217;s <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite</a> we can think of it as an inner product on the <a href="http://en.wikipedia.org/wiki/Tangent_space">tangent space</a> of the point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  In other words, we get a <a href="http://en.wikipedia.org/wiki/Riemannian_metric#Riemannian_metrics">Riemannian metric</a> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" />.  This is called the <a href="http://en.wikipedia.org/wiki/Fisher_information_metric"><b>Fisher information metric</b></a>.</p>
<p>I hope you can see through the jargon to the simple idea.  We&#8217;ve got a space. Each point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> in this space describes the maximum-entropy state of a quantum system for which our observables have specified mean values.  But in each of these states, the observables are random variables.  They don&#8217;t just sit at their mean value, they fluctuate!  You can picture these fluctuations as forming a little smeared-out blob in our space.  To a first approximation, this blob is an ellipsoid.  And if we think of this ellipsoid as a &#8216;unit ball&#8217;, it gives us a standard for measuring the <i>length</i> of any little vector sticking out of our point.  In other words, we&#8217;ve got a Riemannian metric: <i>the Fisher information metric!</i></p>
<p>Now if you look at the Wikipedia article you&#8217;ll see a more general but to me somewhat <a href="http://en.wikipedia.org/wiki/Fisher_information_metric">scarier definition</a> of the Fisher information metric.  This applies whenever we&#8217;ve got a manifold whose points label <i>arbitrary</i> mixed states of a system.  But Crooks shows this definition reduces to his &mdash; the one I just described &mdash; when our manifold is <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> and it&#8217;s parametrizing Gibbs states in the way we&#8217;ve just seen.  </p>
<p>More precisely: both Crooks and the Wikipedia article describe the classical story, but it parallels the quantum story I&#8217;ve been telling&#8230; and I think the quantum version is well-known.  I believe the quantum version of the Fisher information metric is sometimes called the <a href="http://en.wikipedia.org/wiki/Bures_metric">Bures metric</a>, though I&#8217;m a bit confused about what the Bures metric actually is.</p>
<p>[Note: in the original version of this post, I omitted the real part in my definition <img src="https://s0.wp.com/latex.php?latex=g_%7Bij%7D+%3D+%5Cmathrm%7BRe%7D+%5Clangle+%28X_i+-+%5Clangle+X_i+%5Crangle%29++%28X_j+-+%5Clangle+X_j+%5Crangle%29++%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{ij} = &#92;mathrm{Re} &#92;langle (X_i - &#92;langle X_i &#92;rangle)  (X_j - &#92;langle X_j &#92;rangle)  &#92;rangle " class="latex" />, giving a &#8216;Riemannian metric&#8217; that was <a href="https://johncarlosbaez.wordpress.com/2010/10/29/information-geometry-part-4/">neither real nor symmetric</a> in the quantum case.  Most of the comments below are based on that original version, not the new fixed one.]</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/#comments">27 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2010/10/22/information-geometry/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/10/" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/8/" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the information and entropy category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see whats on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkins environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/9/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

				<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = JSON.parse( decodeURIComponent( '%7B%22settings%22%3A%7B%22id%22%3A%22content%22%2C%22ajaxurl%22%3A%22https%3A%5C%2F%5C%2Fjohncarlosbaez.wordpress.com%5C%2F%3Finfinity%3Dscrolling%22%2C%22type%22%3A%22scroll%22%2C%22wrapper%22%3Atrue%2C%22wrapper_class%22%3A%22infinite-wrap%22%2C%22footer%22%3Atrue%2C%22click_handle%22%3A%221%22%2C%22text%22%3A%22Older%20posts%22%2C%22totop%22%3A%22Scroll%20back%20to%20top%22%2C%22currentday%22%3A%2222.10.10%22%2C%22order%22%3A%22DESC%22%2C%22scripts%22%3A%5B%5D%2C%22styles%22%3A%5B%5D%2C%22google_analytics%22%3Afalse%2C%22offset%22%3A9%2C%22history%22%3A%7B%22host%22%3A%22johncarlosbaez.wordpress.com%22%2C%22path%22%3A%22%5C%2Fcategory%5C%2Finformation-and-entropy%5C%2Fpage%5C%2F%25d%5C%2F%22%2C%22use_trailing_slashes%22%3Atrue%2C%22parameters%22%3A%22%22%7D%2C%22query_args%22%3A%7B%22paged%22%3A9%2C%22category_name%22%3A%22information-and-entropy%22%2C%22error%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A0%2C%22post_parent%22%3A%22%22%2C%22subpost%22%3A%22%22%2C%22subpost_id%22%3A%22%22%2C%22attachment%22%3A%22%22%2C%22attachment_id%22%3A0%2C%22name%22%3A%22%22%2C%22pagename%22%3A%22%22%2C%22page_id%22%3A0%2C%22second%22%3A%22%22%2C%22minute%22%3A%22%22%2C%22hour%22%3A%22%22%2C%22day%22%3A0%2C%22monthnum%22%3A0%2C%22year%22%3A0%2C%22w%22%3A0%2C%22tag%22%3A%22%22%2C%22cat%22%3A23375499%2C%22tag_id%22%3A%22%22%2C%22author%22%3A%22%22%2C%22author_name%22%3A%22%22%2C%22feed%22%3A%22%22%2C%22tb%22%3A%22%22%2C%22meta_key%22%3A%22%22%2C%22meta_value%22%3A%22%22%2C%22preview%22%3A%22%22%2C%22s%22%3A%22%22%2C%22sentence%22%3A%22%22%2C%22title%22%3A%22%22%2C%22fields%22%3A%22%22%2C%22menu_order%22%3A%22%22%2C%22embed%22%3A%22%22%2C%22category__in%22%3A%5B%5D%2C%22category__not_in%22%3A%5B%5D%2C%22category__and%22%3A%5B%5D%2C%22post__in%22%3A%5B%5D%2C%22post__not_in%22%3A%5B%5D%2C%22post_name__in%22%3A%5B%5D%2C%22tag__in%22%3A%5B%5D%2C%22tag__not_in%22%3A%5B%5D%2C%22tag__and%22%3A%5B%5D%2C%22tag_slug__in%22%3A%5B%5D%2C%22tag_slug__and%22%3A%5B%5D%2C%22post_parent__in%22%3A%5B%5D%2C%22post_parent__not_in%22%3A%5B%5D%2C%22author__in%22%3A%5B%5D%2C%22author__not_in%22%3A%5B%5D%2C%22lazy_load_term_meta%22%3Afalse%2C%22posts_per_page%22%3A10%2C%22ignore_sticky_posts%22%3Afalse%2C%22suppress_filters%22%3Afalse%2C%22cache_results%22%3Afalse%2C%22update_post_term_cache%22%3Atrue%2C%22update_post_meta_cache%22%3Atrue%2C%22post_type%22%3A%22%22%2C%22nopaging%22%3Afalse%2C%22comments_per_page%22%3A%22100%22%2C%22no_found_rows%22%3Afalse%2C%22order%22%3A%22DESC%22%7D%2C%22query_before%22%3A%222021-09-26%2017%3A28%3A38%22%2C%22last_post_date%22%3A%222010-10-22%2010%3A37%3A05%22%2C%22body_class%22%3A%22infinite-scroll%20neverending%22%2C%22loading_text%22%3A%22Loading%20new%20page%22%2C%22stats%22%3A%22blog%3D12777403%26v%3Dwpcom%26tz%3D0%26user_id%3D0%26subd%3Djohncarlosbaez%26x_pagetype%3Dinfinite%22%7D%7D' ) );
		//]]>
		</script>
		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>
		<div id="infinite-footer">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://johncarlosbaez.wordpress.com/" rel="home">
						Azimuth					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a> 				</div>
			</div>
		</div><!-- #infinite-footer -->
		
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s1.wp.com/_static/??-eJyFy00OQDAQQOELGUP8xUKcpWoiZVTTadO4vVhY2LB8L/kwOdCHDWQD7hEcx8VYQa38EYUYJRlHHqZoZ6Zci2T4I1YKTukNnvGFgmGaYVHM5M933Wzch7Ktir5p+rpbL+lvP34=?cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2010%2F10%2F22%2Finformation-geometry%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s0.wp.com/_static/??-eJyNkN1uwyAMhV9oDkouKvVi2rMQcCNT/oZN07z9yNRMbTZFu8LH8J2DreYMJkXBKMqxsngjg/neOX5TT1ehQvZ1osiK4oUiyfJTHLzVNlCEURcVNAuWVoEUba68h1q2+6xYlsfRzdmkALmk+wIFW49lYygaXy3yCjWJYUTbtaCDj8xkJxRWXEc2hbJQivyXH8+XNDo0sjfbnG5kMSnNvNq5h84F+ddIT/FGl1QZvXIouY0PW+OAEfJoYdLer1t5Uf9Igu/97WTjPsJ7fxrO534YTr37Ai2cwB8='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			(function() {
				var extend = function(out) {
					out = out || {};

					for (var i = 1; i < arguments.length; i++) {
						if (!arguments[i])
						continue;

						for (var key in arguments[i]) {
						if (arguments[i].hasOwnProperty(key))
							out[key] = arguments[i][key];
						}
					}

					return out;
				};
				extend( window.infiniteScroll.settings.scripts, ["postmessage","mobile-useragent-info","rlt-proxy","jquery-core","jquery-migrate","jquery","wpcom-actionbar-placeholder","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","wpcom-masterbar-tracks-js","jquery.wpcom-proxy-request","wp-embed","jetpack-subscriptions-js","swfobject","videopress","jetpack-carousel","tiled-gallery","carousel-wpcom"] );
				extend( window.infiniteScroll.settings.styles, ["the-neverending-homepage","infinity-contempt","wp-block-library","mediaelement","wp-mediaelement","jetpack-layout-grid","jetpack-ratings","coblocks-frontend","wpcom-core-compat-playlist-styles","wpcom-text-widget-styles","wpcom-bbpress2-staff-css","contempt","geo-location-flair","reblogging","a8c-global-print","h4-global","global-styles","jetpack-global-styles-frontend-style","jetpack-carousel-swiper-css","jetpack-carousel","tiled-gallery"] );
			})();
		</script>
				<span id="infinite-aria" aria-live="polite"></span>
		<script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTY5YndjYjhMa0V4K2ozLVZHM3JwVms9UC5Fbk81SmYuWk1GeWcydXJlJmVOczJ0NkMyTVBUMHczN1tCcHpHTWwycnl0K2ZmTEo/SHZlLC1DZnM5UVB+YnlYPWlPK1pXSyVOUklbeGxwW1EyYkFCTGhRT25IflAsX1NPLTdocS8vbVZ5Jl1xU1FDW2FDK2ozTDVnNnw1Vix6Qk5IMFh2N29IWE5pbGRLUHBhVERENEUwMV9VJnAtdmswbH4uaTlTY0lDZWRwNDA='}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>