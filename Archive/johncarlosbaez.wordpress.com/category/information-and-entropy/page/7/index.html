<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>information and entropy | Azimuth | Page 7</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; information and entropy Category Feed" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IlqVOdbxwfBYOEVOBZSBYeXuz1e6sVrveMDl9yU9AHrOwUyJIJGMTOTSPqUpMz5iQ+mIMttYbuVFMLxChytyMPJXFTBfcGfKNXQPFc6aAfFNq2A9KmobBSRMm+yoCmqJLl5V6gKURJhua4zGHKiM41BB46qxo5eSgOxQRwGvbh4hpG+fc2v8G/S3+pJSbAWU9S9Z9aiR8QfdD9r9bFE2YfN3A7fSF7QY18t4cVlqC4nd29WTzzjkes768+BXsiM4DMV7PtiB4v45kHiOMyQVqFXxGbFF8/pSZe4qP6u5W3T/sx3F3+ACgX/YH?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F08%2F29%2Fan-entropy-challenge%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/information-and-entropy\/page\/7\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Finformation-and-entropy%2Fpage%2F7%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F08%2F29%2Fan-entropy-challenge%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="information and entropy &#8211; Page 7 &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about information and entropy written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive paged category category-information-and-entropy category-23375499 paged-7 category-paged-7 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-11843 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-probability category-puzzles" id="post-11843">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/" rel="bookmark">An Entropy Challenge</a></h2>
				<small>29 August, 2012</small><br />


				<div class="entry">
					<p>If you like computer calculations, here&#8217;s a little challenge for you.  <a href="http://www.quantumlah.org/people/Dahlsten">Oscar Dahlsten</a> may have solved it, but we&#8217;d love for you to check his work.   It&#8217;s pretty important for the foundations of thermodynamics, but you don&#8217;t need to know any physics or even anything beyond a little algebra to tackle it!  First I&#8217;ll explain it in really simple terms, then I&#8217;ll remind you a bit of why it matters.</p>
<p>We&#8217;re looking for two lists of nonnegative numbers, of the same length, listed in decreasing order:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_1+%5Cge+p_2+%5Cge+%5Ccdots+%5Cge+p_n+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 &#92;ge p_2 &#92;ge &#92;cdots &#92;ge p_n &#92;ge 0 " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=q_1+%5Cge+q_2+%5Cge+%5Ccdots+%5Cge+q_n+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1 &#92;ge q_2 &#92;ge &#92;cdots &#92;ge q_n &#92;ge 0 " class="latex" /></p>
<p>that sum to 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_1+%2B+%5Ccdots+%2B+p_n+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 + &#92;cdots + p_n = 1" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=q_1+%2B+%5Ccdots+%2B+q_n+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1 + &#92;cdots + q_n = 1" class="latex" /></p>
<p>and that obey this inequality:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7B1+-+%5Cbeta%7D+%5Cln+%5Csum_%7Bi%3D1%7D%5En+p_i%5E%5Cbeta++%5Cle++%5Cfrac%7B1%7D%7B1+-+%5Cbeta%7D+%5Cln+%5Csum_%7Bi%3D1%7D%5En+q_i%5E%5Cbeta+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{1}{1 - &#92;beta} &#92;ln &#92;sum_{i=1}^n p_i^&#92;beta  &#92;le  &#92;frac{1}{1 - &#92;beta} &#92;ln &#92;sum_{i=1}^n q_i^&#92;beta } " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Cbeta+%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &lt; &#92;beta &lt; &#92;infty" class="latex" /> (ignoring <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" />), yet do <i>not</i> obey these inequalities:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_1+%2B+%5Ccdots+%2B+p_k+%5Cge+q_1+%2B+%5Ccdots+%2B+q_k+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 + &#92;cdots + p_k &#92;ge q_1 + &#92;cdots + q_k " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+k+%5Cle+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;le k &#92;le n." class="latex" /></p>
<p>Oscar&#8217;s proposed solution is this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%280.4%2C+0.29%2C+0.29%2C+0.02%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (0.4, 0.29, 0.29, 0.02)" class="latex" /> </p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%3D+%280.39%2C+0.31%2C+0.2%2C+0.1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = (0.39, 0.31, 0.2, 0.1)" class="latex" /></p>
<p>Can you see if this works?  Is there a simpler example, like one with lists of just 3 numbers?</p>
<p>This question came up near the end of my post <a href="https://johncarlosbaez.wordpress.com/2012/08/24/more-second-laws-of-thermodynamics/">More Second Laws of Thermodynamics</a>.  I phrased the question with a bit more jargon, and said a lot more about its significance.  Suppose we have two probability distributions on a finite set, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  We say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> <b>majorizes</b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> if</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_1+%2B+%5Ccdots+%2B+p_k+%5Cge+q_1+%2B+%5Ccdots+%2B+q_k+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 + &#92;cdots + p_k &#92;ge q_1 + &#92;cdots + q_k " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+k+%5Cle+n%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;le k &#92;le n," class="latex" /> when we write both lists of numbers in decreasing order.  This means <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is &#8216;less flat&#8217; than <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, so it should have less entropy.  And indeed it does: not just for ordinary entropy, but also for R&eacute;nyi entropy!  The <b>R&eacute;nyi entropy</b> of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Cbeta%28p%29+%3D+%5Cfrac%7B1%7D%7B1+-+%5Cbeta%7D+%5Cln+%5Csum_%7Bi%3D1%7D%5En+p_i%5E%5Cbeta++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;beta(p) = &#92;frac{1}{1 - &#92;beta} &#92;ln &#92;sum_{i=1}^n p_i^&#92;beta  } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Cbeta+%3C+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &lt; &#92;beta &lt; 1" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=1+%3C+%5Cbeta+%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &lt; &#92;beta &lt; &#92;infty" class="latex" />.  We can also define R&eacute;nyi entropy for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+0%2C+1%2C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 0, 1, &#92;infty" class="latex" /> by taking a limit, and at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" /> we get the ordinary entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_1%28p%29+%3D+-+%5Csum_%7Bi+%3D+1%7D%5En+p_i+%5Cln+%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_1(p) = - &#92;sum_{i = 1}^n p_i &#92;ln (p_i) } " class="latex" /></p>
<p>The question is whether majorization is more powerful than R&eacute;nyi entropy as a tool to to tell when one probability distribution is less flat than another.  I know that if <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> its R&eacute;nyi entropy is less than than that of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+%5Cbeta+%5Cle+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le &#92;beta &#92;le &#92;infty." class="latex" />  Your mission, should you choose to accept it, is to show the converse is not true.  </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comments">30 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>, <a href="https://johncarlosbaez.wordpress.com/category/puzzles/" rel="category tag">puzzles</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/" rel="bookmark" title="Permanent Link to An Entropy Challenge">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-11756 post type-post status-publish format-standard hentry category-information-and-entropy category-physics category-probability" id="post-11756">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/08/24/more-second-laws-of-thermodynamics/" rel="bookmark">More Second Laws of&nbsp;Thermodynamics</a></h2>
				<small>24 August, 2012</small><br />


				<div class="entry">
					<p><a href="http://www.quantumlah.org/people/Dahlsten">Oscar Dahlsten</a> is visiting the Centre for Quantum Technologies, so we&#8217;re continuing some conversations about entropy that we started last year, back when the <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">Entropy Club</a> was active.  But now <a href="http://www.cs.ox.ac.uk/people/jamie.vicary/">Jamie Vicary</a> and <a href="http://www.azimuthproject.org/azimuth/show/Brendan+Fong">Brendan Fong</a> are involved in the conversations.</p>
<p>I was surprised when Oscar told me that for a large class of random processes, the usual second law of thermodynamics is just one of infinitely many laws saying that various kinds of disorder increase.  I&#8217;m annoyed that nobody ever told me about this before! It&#8217;s as if they told me about conservation of <i>energy</i> but not conservation of <i>schmenergy</i>, and <i>phlenergy</i>, and <i>zenergy</i>&#8230;</p>
<p>So I need to tell you about this.  You may not understand it, but at least I can say I tried.  I don&#8217;t want you blaming <i>me</i> for concealing all these extra second laws of thermodynamics!</p>
<p>Here&#8217;s the basic idea.   Not all random processes are guaranteed to make entropy increase.  But a bunch of them always make probability distributions flatter in a certain precise sense.  This makes the entropy of the probability distribution increase.  But when you make a probability distribution flatter in this sense, a bunch of other quantities increase too!  For example, besides the usual entropy, there are infinitely many other kinds of entropy, called &#8216;R&eacute;nyi entropies&#8217;, one for each number between 0 and &infin;.  And a doubly stochastic operator makes <i>all</i> the R&eacute;nyi entropies increase!  This fact is a special case of Theorem 10 here:</p>
<p>&bull; Tim van Erven and Peter Harremoës, <a href="http://arxiv.org/abs/1001.4448">Rényi divergence and majorization</a>.  </p>
<p>Let me state this fact precisely, and then say a word about how this is related to quantum theory and &#8216;the collapse of the wavefunction&#8217;.</p>
<p>To keep things simple let&#8217;s talk about probability distributions on a finite set, though Erven and Harremoës generalize it all to a measure space.  </p>
<p>How do we make precise the concept that one probability distribution is flatter than another?  You know it when you see it, at least some of the time.  For example, suppose I have some system in thermal equilibrium at some temperature, and the probabilities of it being in various states look like this:</p>
<div align="center"><img width="250" src="https://i0.wp.com/math.ucr.edu/home/baez/biodiversity/probabilities_T=1.png" alt="" /></div>
<p>Then say I triple the temperature.   The probabilities flatten out:</p>
<div align="center"><img width="250" src="https://i2.wp.com/math.ucr.edu/home/baez/biodiversity/probabilities_T=3.png" alt="" /></div>
<p>But how can we make this concept precise in a completely general way?  We can do it using the concept of &#8216;majorization&#8217;.  If one probability distribution is <i>less</i> flat than another, people say it &#8216;majorizes&#8217; that other one. </p>
<p>Here&#8217;s the definition.  Say we have two probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on the same set.  For each one, list the probabilities in decreasing order:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_1+%5Cge+p_2+%5Cge+%5Ccdots+%5Cge+p_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 &#92;ge p_2 &#92;ge &#92;cdots &#92;ge p_n " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=q_1+%5Cge+q_2+%5Cge+%5Ccdots+%5Cge+q_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1 &#92;ge q_2 &#92;ge &#92;cdots &#92;ge q_n " class="latex" /></p>
<p>Then we say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> <b> <a href="http://en.wikipedia.org/wiki/Majorization">majorizes</a></b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> if </p>
<p><img src="https://s0.wp.com/latex.php?latex=p_1+%2B+%5Ccdots+%2B+p_k+%5Cge+q_1+%2B+%5Ccdots+%2B+q_k+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1 + &#92;cdots + p_k &#92;ge q_1 + &#92;cdots + q_k " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+k+%5Cle+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;le k &#92;le n." class="latex" />  So, the idea is that the biggest probabilities in the distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> add up to more than the corresponding biggest ones in <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>In 1960, Alfred R&eacute;nyi defined a generalization of the usual Shannon entropy that depends on a parameter <img src="https://s0.wp.com/latex.php?latex=%5Cbeta.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta." class="latex" />  If <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a probability distribution on a finite set, its <b><a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">R&eacute;nyi entropy</a></b> of order <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is defined to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Cbeta%28p%29+%3D+%5Cfrac%7B1%7D%7B1+-+%5Cbeta%7D+%5Cln+%5Csum_i+p_i%5E%5Cbeta+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;beta(p) = &#92;frac{1}{1 - &#92;beta} &#92;ln &#92;sum_i p_i^&#92;beta } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+%5Cbeta+%3C+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le &#92;beta &lt; &#92;infty." class="latex" />   Well, to be honest: if <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is 0, 1, or <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;infty" class="latex" /> we have to define this by taking a limit where we let <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> creep up to that value.  But the limit exists, and when <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1" class="latex" /> we get the usual Shannon entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_1%28p%29+%3D+-+%5Csum_i+p_i+%5Cln%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_1(p) = - &#92;sum_i p_i &#92;ln(p_i) } " class="latex" /></p>
<p>As I explained a while ago, R&eacute;nyi entropies are important ways of measuring <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">biodiversity</a>.  But here&#8217;s what I learned just now, from the paper by Erven and Harremoës:</p>
<p><b>Theorem 1.</b>  If a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes a probability distribution <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> its R&eacute;nyi entropies are smaller:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Cbeta%28p%29+%5Cle+H_%5Cbeta%28q%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;beta(p) &#92;le H_&#92;beta(q) } " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+%5Cbeta+%3C+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le &#92;beta &lt; &#92;infty." class="latex" /></p>
<p>And here&#8217;s what makes this fact so nice.  If you do something to a classical system in a way that might involve some randomness, we can describe your action using a stochastic matrix.   An <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is called <b><a href="http://en.wikipedia.org/wiki/Stochastic_matrix">stochastic</a></b> if whenever <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in &#92;mathbb{R}^n" class="latex" /> is a probability distribution, so is <img src="https://s0.wp.com/latex.php?latex=T+p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T p." class="latex" /> This is equivalent to saying:</p>
<p>&bull;  the matrix entries of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> are all <img src="https://s0.wp.com/latex.php?latex=%5Cge+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ge 0," class="latex" /> and</p>
<p>&bull;  each column of <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> sums to 1.</p>
<p>If <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is stochastic, it&#8217;s not necessarily true that the entropy of <img src="https://s0.wp.com/latex.php?latex=T+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T p" class="latex" /> is greater than or equal to that of <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> not even for the Shannon entropy.  </p>
<p><b>Puzzle 1.</b> Find a counterexample.</p>
<p>However, entropy does increase if we use specially nice stochastic matrices called &#8216;doubly stochastic&#8217; matrices.   People say a matrix <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> <b><a href="http://en.wikipedia.org/wiki/Doubly_stochastic_matrix">doubly stochastic</a></b> if it&#8217;s stochastic and it maps the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_0+%3D+%28%5Cfrac%7B1%7D%7Bn%7D%2C+%5Cdots%2C+%5Cfrac%7B1%7D%7Bn%7D%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_0 = (&#92;frac{1}{n}, &#92;dots, &#92;frac{1}{n}) } " class="latex" /></p>
<p>to itself.  This is the most spread-out probability distribution of all: every other probability distribution majorizes this one.</p>
<p>Why do they call such matrices &#8216;doubly&#8217; stochastic?  Well, if you&#8217;ve got a stochastic matrix, each <i>column</i> sums to one.  But a stochastic operator is doubly stochastic if and only if each <i>row</i> sums to 1 as well.</p>
<p>Here&#8217;s a really cool fact:</p>
<p><b>Theorem 2.</b>  If <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is doubly stochastic, <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=T+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T p" class="latex" /> for any probability distribution <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in &#92;mathbb{R}^n." class="latex" />  Conversely, if a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes a probability distribution <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=q+%3D+T+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = T p" class="latex" /> for some doubly stochastic matrix <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" />.</p>
<p>Taken together, Theorems 1 and 2 say that doubly stochastic transformations increase entropy&#8230; but not just Shannon entropy!  They increase all the different R&eacute;nyi entropies, as well.  So if time evolution is described by a doubly stochastic matrix, we get <i>lots</i> of &#8216;second laws of thermodynamics&#8217;, saying that all these different kinds of entropy increase!</p>
<p>Finally, what does all this have to do with quantum mechanics, and collapsing the wavefunction?  There are different things to say, but this is the simplest:</p>
<p><b>Theorem 3.</b>  Given two probability distributions <img src="https://s0.wp.com/latex.php?latex=p%2C+q+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q &#92;in &#92;mathbb{R}^n" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> if and only there exists a self-adjoint matrix <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> with eigenvalues <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and diagonal entries <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" /></p>
<p>The matrix <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> will be a <a href="http://en.wikipedia.org/wiki/Density_matrix"><b>density matrix</b></a>: a self-adjoint matrix with positive eigenvalues and trace equal to 1.  We use such matrices to describe mixed states in quantum mechanics.  </p>
<p>Theorem 3 gives a precise sense in which preparing a quantum system in some state, letting time evolve, and then measuring it &#8216;increases randomness&#8217;.  </p>
<p>How?  Well, suppose we have a quantum system whose Hilbert space is <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{C}^n." class="latex" />  If we prepare the system in a mixture of the standard basis states with probabilities <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> we can describe it with a diagonal density matrix <img src="https://s0.wp.com/latex.php?latex=D_0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_0." class="latex" />  Then suppose we wait a while and some unitary time evolution occurs.  The system is now described by a new density matrix </p>
<p><img src="https://s0.wp.com/latex.php?latex=D+%3D+U+D_0+%5C%2C+U%5E%7B-1%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D = U D_0 &#92;, U^{-1} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is some unitary operator.  If we then do a measurement to see which of the standard basis states our system now lies in, we&#8217;ll get the different possible results with probabilities <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> the diagonal entries of <img src="https://s0.wp.com/latex.php?latex=D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D." class="latex" />  But the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" /> will still be the numbers <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />  So, by the theorem, <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />!</p>
<p>So, not only Shannon entropy but also all the R&eacute;nyi entropies will increase!</p>
<p>Of course, there are some big physics questions lurking here.  Like: <i>what about the real world?</i>   In the real world, do lots of different kinds of entropy tend to increase, or just some?  </p>
<p>Of course, there&#8217;s a huge famous old problem about how reversible time evolution can be compatible with <i>any</i> sort of law saying that entropy must always increase!   Still, there are some arguments, going back to Boltzmann&#8217;s H-theorem, which show entropy increases under some extra conditions.  So then we can ask if other kinds of entropy, like R&eacute;nyi entropy, increase as well.  This will be true whenever we can argue that time evolution is described by doubly stochastic matrices.  Theorem 3 gives a partial answer, but there&#8217;s probably much more to say. </p>
<p>I don&#8217;t have much more to say right now, though. I&#8217;ll just point out that while doubly stochastic matrices map the &#8216;maximally smeared-out&#8217; probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_0+%3D+%28%5Cfrac%7B1%7D%7Bn%7D%2C+%5Cdots%2C+%5Cfrac%7B1%7D%7Bn%7D%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_0 = (&#92;frac{1}{n}, &#92;dots, &#92;frac{1}{n}) } " class="latex" /></p>
<p>to itself, a lot of this theory generalizes to stochastic matrices that map exactly one <i>other</i> probability distribution to itself.  We need to work with <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#R.C3.A9nyi_divergence">relative R&eacute;nyi entropy</a> instead of R&eacute;nyi entropy, and so on, but I don&#8217;t think these adjustments are really a big deal.  And there are nice theorems that let you know when a stochastic matrix maps exactly one probability distribution to itself, based on the <a href="https://johncarlosbaez.wordpress.com/2012/08/06/network-theory-part-20/">Perron&ndash;Frobenius theorem</a>.</p>
<h3> References </h3>
<p>I already gave you a reference for Theorem 1, namely the paper by Erven and Harremoës, though I don&#8217;t think they were the first to prove this particular result: they generalize it quite a lot.</p>
<p>What about Theorem 2?  It goes back at least to here:</p>
<p>&bull; Barry C. Arnold, <i>Majorization and the Lorenz Order: A Brief Introduction</i>, Springer Lecture Notes in Statistics <b>43</b>, Springer, Berlin, 1987.</p>
<p>The partial order on probability distributions given by majorization is also called the &#8216;Lorenz order&#8217;, but mainly when we consider probability distributions on infinite sets.  This name  presumably comes from the <a href="http://en.wikipedia.org/wiki/Lorenz_curve">Lorenz curve</a>, a measure of income inequality.  This curve shows for the bottom x% of households, what percentage y% of the total income they have:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/Lorenz_curve"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Economics_Gini_coefficient2.svg/400px-Economics_Gini_coefficient2.svg.png" /></a></div>
<p><b>Puzzle 2.</b> If you&#8217;ve got two different probability distributions of incomes, and one majorizes the other, how are their Lorenz curves related?</p>
<p>When we generalize majorization by letting some other probability distribution take the place of</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_0+%3D+%28%5Cfrac%7B1%7D%7Bn%7D%2C+%5Cdots%2C+%5Cfrac%7B1%7D%7Bn%7D%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_0 = (&#92;frac{1}{n}, &#92;dots, &#92;frac{1}{n}) } " class="latex" /></p>
<p>it seems people call it the &#8216;Markov order&#8217;.   Here&#8217;s a really fascinating paper on that, which I&#8217;m just barely beginning to understand:</p>
<p>&bull; A. N. Gorban, P. A. Gorban and G. Judge, <a href="http://arxiv.org/abs/1003.1377">Entropy: the Markov ordering approach</a>, <i><a href="http://www.mdpi.com/1099-4300/12/5/1145">Entropy</a></i> <b>12</b> (2010), 1145&#8211;1193. </p>
<p>What about Theorem 3?  Apparently it goes back to here:</p>
<p>&bull; A. Uhlmann, <i>Wiss. Z. Karl-Marx-Univ. Leipzig</i> <b>20</b> (1971), 633.</p>
<p>though I only know this thanks to a more recent paper:</p>
<p>&bull; Michael A. Nielsen, <a href="http://arxiv.org/abs/quant-ph/9811053">Conditions for a class of entanglement transformations</a>, <i>Phys. Rev. Lett.</i> <b>83</b> (1999), 436&#8211;439.</p>
<p>By the way, Nielsen&#8217;s paper contains another very nice result about majorization!  Suppose you have states <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> of a 2-part quantum system.  You can trace out one part and get density matrices describing mixed states of the other part, say <img src="https://s0.wp.com/latex.php?latex=D_%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_&#92;psi" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D_%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_&#92;phi" class="latex" />.  Then Nielsen shows you can get from <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> using &#8216;local operations and classical communication&#8217; if and only if <img src="https://s0.wp.com/latex.php?latex=D_%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_&#92;phi" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=D_%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_&#92;psi" class="latex" />.  Note that things are going backwards here compared to how they&#8217;ve been going in the rest of this post: if we can get from <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" />, then all forms of entropy go <i>down</i> when we go from <img src="https://s0.wp.com/latex.php?latex=D_%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_&#92;psi" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=D_%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D_&#92;phi" class="latex" />!  This &#8216;anti-second-law&#8217; behavior is confusing at first, but <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/">familiar to me by now</a>.</p>
<p>When I first learned all this stuff, I naturally thought of the following question&#8212;maybe you did too, just now.  If <img src="https://s0.wp.com/latex.php?latex=p%2C+q+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q &#92;in &#92;mathbb{R}^n" class="latex" /> are probability distributions and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Cbeta%28p%29+%5Cle+H_%5Cbeta%28q%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;beta(p) &#92;le H_&#92;beta(q) } " class="latex" /></p>
<p>for all  <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+%5Cbeta+%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le &#92;beta &lt; &#92;infty" class="latex" />, is it true that <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />?</p>
<p>Apparently the answer must be <i>no</i>, because Klimesh has gone to quite a bit of work to obtain a weaker conclusion: not that <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, but that <img src="https://s0.wp.com/latex.php?latex=p+%5Cotimes+r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;otimes r" class="latex" /> majorizes <img src="https://s0.wp.com/latex.php?latex=q+%5Cotimes+r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;otimes r" class="latex" /> for some probability distribution <img src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BR%7D%5Em.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r &#92;in &#92;mathbb{R}^m." class="latex" />  He calls this <b>catalytic majorization</b>, with <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="r" class="latex" /> serving as a &#8216;catalyst&#8217;:</p>
<p>&bull; Matthew Klimesh, <a href="http://arxiv.org/abs/0709.3680">Inequalities that collectively completely characterizes the catalytic majorization relation</a>.</p>
<p>I thank <a href="http://www.vlatkovedral.org/">Vlatko Vedral</a> here at the CQT for pointing this out!</p>
<p>Finally, here is a good general introduction to majorization, pointed out by Vasileios Anagnostopoulos:</p>
<p>&bull; T. Ando, Majorization, doubly stochastic matrices, and comparison of eigenvalues, <i><a href="http://www.sciencedirect.com/science/article/pii/0024379589905806#">Linear Algebra and its Applications</a></i> <b>118</b> (1989), 163-–248.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/24/more-second-laws-of-thermodynamics/#comments">47 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/08/24/more-second-laws-of-thermodynamics/" rel="bookmark" title="Permanent Link to More Second Laws of&nbsp;Thermodynamics">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-11087 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-probability" id="post-11087">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/" rel="bookmark">The Noisy Channel Coding&nbsp;Theorem</a></h2>
				<small>28 July, 2012</small><br />


				<div class="entry">
					<div align="center"><a href="http://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf;jsessionid=639715502BCB1045260D9FA914505CCE?sequence=1"><img src="https://lh5.googleusercontent.com/-v-pRcD4u6yc/T8v91X48mII/AAAAAAAAK-8/mxthvAIQMTs/w497-h373/shannon.jpg" /></a></div>
<p>Here&#8217;s a charming, easily readable tale of Claude Shannon and how he came up with information theory:</p>
<p>• Erico Guizzo, <i><a href="http://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf;jsessionid=639715502BCB1045260D9FA914505CCE?sequence=1">The Essential Message: Claude Shannon and the Making of Information Theory</a></i>.</p>
<p>I hadn&#8217;t known his PhD thesis was on genetics!  His master&#8217;s thesis introduced Boolean logic to circuit design.  And as a kid,  he once set up a telegraph line to a friend&#8217;s house half a mile away.</p>
<p>So, he was perfectly placed to turn <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">information</a> into a mathematical quantity, deeply related to entropy, and prove some now-famous theorems about it.</p>
<p>These theorems set limits on how much information we can transmit through a noisy channel.   More excitingly, they say we can cook up coding schemes that let us come <i>as close as we want</i> to this limit, with an arbitrarily low probability of error.</p>
<p>As Erico Guizzo points out, these results are fundamental to the &#8216;information age&#8217; we live in today:</p>
<blockquote><p>
Can we transmit, say, a high-resolution picture over a telephone line? How long will that take? Is there a best way to do it?</p>
<p>Before Shannon, engineers had no clear answers to these questions. At that time, a wild zoo of technologies was in operation, each with a life of its own&#8212;telephone, telegraph, radio, television, radar, and a number of other systems developed during the war. Shannon came up with a unifying, general  theory of communication. It didn&#8217;t matter whether you transmitted signals using a copper wire, an optical fiber, or a parabolic dish. It didn&#8217;t matter if you were transmitting text, voice, or images. Shannon envisioned communication in abstract, mathematical terms; he defined what the once fuzzy concept of &#8220;information&#8221; meant for communication engineers and proposed a precise way to quantify it. According to him, the information content of any kind of message could be measured in binary digits, or just <b>bits</b>&#8212;a name suggested by a colleague at Bell Labs. Shannon took the bit as the fundamental unit in information theory. It was the first time that the term appeared in print.
</p></blockquote>
<p>So, I want to understand Shannon&#8217;s theorems and their proofs&#8212;especially because they clarify the relation between <i>information</i> and <i>entropy</i>, two concepts I&#8217;d like to be an expert on.  It&#8217;s sort of embarrassing that I don&#8217;t already know this stuff!  But I thought I&#8217;d post some preliminary remarks anyway, in case you too are trying to learn this stuff, or in case you can help me.</p>
<p>There are various different theorems I should learn.  For example:</p>
<p>• The <a href="http://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">source coding theorem</a> says it&#8217;s impossible to compress a stream of data to make the average number of bits per symbol in the compressed data less than the <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon information</a> of the source, without some of the data almost certainly getting lost.  However, you can make the number of bits per symbols arbitrarily close to the Shannon entropy with a probability of error as small as you like.</p>
<p>• the <a href="http://en.wikipedia.org/wiki/Noisy_channel_coding_theorem">noisy channel coding theorem</a> is a generalization to data sent over a noisy channel.</p>
<p>The proof of the noisy channel coding theorem seems not so bad&#8212;there&#8217;s a sketch of a proof in <a href="http://en.wikipedia.org/wiki/Noisy_channel_coding_theorem">the Wikipedia article on this theorem</a>.   But many theorems have a hard lemma at their heart, and for this one it&#8217;s a result in probability theory called the <a href="http://en.wikipedia.org/wiki/Asymptotic_equipartition_property">asymptotic equipartition property</a>.</p>
<p>You should not try to <i>dodge</i> the hard lemma at the heart of the theorem you&#8217;re trying to understand: there&#8217;s a reason it&#8217;s there.    So what&#8217;s the asymptotic equipartition property?</p>
<p>Here&#8217;s a somewhat watered-down statement that gets the basic idea across.   Suppose you have a method of randomly generating letters&#8212;for example, a probability distribution on the set of letters.  Suppose you randomly generate a string of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> letters, and compute <img src="https://s0.wp.com/latex.php?latex=-%281%2Fn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-(1/n)" class="latex" /> times the logarithm of the probability that you got that string.   Then as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty" class="latex" /> this number &#8216;almost surely&#8217; approaches some number <img src="https://s0.wp.com/latex.php?latex=S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S." class="latex" />  What&#8217;s this number <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />?  It&#8217;s the <i>entropy</i> of the probability distribution you used to generate those letters!</p>
<p>(<b><a href="http://en.wikipedia.org/wiki/Almost_surely">Almost surely</a></b> is probability jargon for &#8216;with probability 100%&#8217;, which is not the same as &#8216;always&#8217;.)</p>
<p>This result is really cool&#8212;definitely worth understanding in its own right!  It says that while many strings are possible, the ones you&#8217;re most likely to see lie in a certain &#8216;typical set&#8217;.  The &#8216;typical&#8217; strings are the ones where when you compute <img src="https://s0.wp.com/latex.php?latex=-%281%2Fn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-(1/n)" class="latex" /> times the log of their probability, the result is close to <img src="https://s0.wp.com/latex.php?latex=S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S." class="latex" />  How close?  Well, you get to pick that.</p>
<p>The typical strings are not individually the most probable strings!  But if you randomly generate a string, it&#8217;s very probable that it lies in the typical set.  That sounds a bit paradoxical, but if you think about it, you&#8217;ll see it&#8217;s not.  Think of repeatedly flipping a coin that has a 90% chance of landing heads up.    The most probable single outcome is that it lands heads up every time.  But the <i>typical</i> outcome is that it lands up close to 90% of the time.  And, there are lots of ways this can happen.   So, if you flip the coin a bunch of times, there&#8217;s a very high chance that the outcome is typical.</p>
<p>It&#8217;s easy to see how this result is the key to the noisy channel coding theorem.  In general, the &#8216;typical set&#8217; has few elements compared to the whole set of strings with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> letters.  So, you can make short codes for the strings in this set, and compress your message that way, and this works almost all the time.  How much you can compress your message depends on the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />.</p>
<p>(I said &#8216;in general&#8217; because there&#8217;s one exception: when every <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />-letter string is equally probable, every string is in the typical set.  In this very special case, no compression is possible.)</p>
<p>So, we&#8217;re seeing the link between information and entropy!</p>
<p>The actual coding schemes that people use are a lot trickier than the simple scheme I&#8217;m hinting at here.  When you read about them, you see scary things like this:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/Turbo_code"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Turbo_decoder.svg/300px-Turbo_decoder.svg.png" /></a></div>
<p>But presumably they&#8217;re faster to implement, hence more practical.</p>
<p>The first coding schemes that come really close to the Shannon limit are the <a href="http://en.wikipedia.org/wiki/Turbo_code">turbo codes.</a>   Surprisingly, these codes were developed only in 1993!  They&#8217;re used in 3G mobile communications and deep space satellite communications.</p>
<p>One key trick is to use, not <i>one</i> decoder, but <i>two</i>.   These two decoders keep communicating with each other and improving their guesses about the signal they&#8217;re received, until they agree:</p>
<blockquote><p>
This iterative process continues until the two decoders come up with the same hypothesis for the m-bit pattern of the payload, typically in 15 to 18 cycles. An analogy can be drawn between this process and that of solving cross-reference puzzles like crossword or sudoku. Consider a partially completed, possibly garbled crossword puzzle. Two puzzle solvers (decoders) are trying to solve it: one possessing only the &#8220;down&#8221; clues (parity bits), and the other possessing only the &#8220;across&#8221; clues. To start, both solvers guess the answers (hypotheses) to their own clues, noting down how confident they are in each letter (payload bit). Then, they compare notes, by exchanging answers and confidence ratings with each other, noticing where and how they differ. Based on this new knowledge, they both come up with updated answers and confidence ratings, repeating the whole process until they converge to the same solution.
</p></blockquote>
<p>This can be seen as &#8220;an instance of loopy belief propagation in Bayesian networks.&#8221;</p>
<p>By the way, the picture I showed you above is a flowchart of the decoding scheme for a simple turbo code.  You can see the two decoders, and maybe the loop where data gets fed back to the decoders.</p>
<p>While I said this picture is &#8220;scary&#8221;, I actually like it because it&#8217;s an example of <a href="http://math.ucr.edu/home/baez/networks/">network theory</a> applied to real-life problems.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comments">18 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/" rel="bookmark" title="Permanent Link to The Noisy Channel Coding&nbsp;Theorem">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10663 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy" id="post-10663">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/14/the-mathematics-of-biodiversity-part-8/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;8)</a></h2>
				<small>14 July, 2012</small><br />


				<div class="entry">
					<p>Last time I mentioned that estimating entropy from real-world data is important not just for measuring biodiversity, but also for another area of biology: <i>neurobiology!</i></p>
<p>When you look at something, neurons in your eye start firing.  But how, exactly, is their firing related to what you see?  Questions like this are hard!  Answering them&#8212; <a href="http://en.wikipedia.org/wiki/Neural_coding">&#8216;cracking the neural code&#8217;</a>&#8212;is a big challenge.  To make progress, neuroscientists are using information theory.    But as I explained <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/">last time</a>, estimating information from experimental data is tricky.</p>
<p><a href="http://www.anc.upmc.fr/rbrasselet.html">Romain Brasselet</a>, now a postdoc at the Max Planck Institute for Biological Cybernetics at Tübingen, is working on these topics.  He sent me a nice email explaining this area.</p>
<p>This is a bit of a digression, but the <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Research-Program-on-Mathematics-of-Biodiversity.aspx">Mathematics of Biodiversity program</a> in Barcelona has been extraordinarily multidisciplinary, with category theorists rubbing shoulders with ecologists, immunologists and geneticists.  One of the common themes is entropy and its role in biology, so I think it&#8217;s worth posting Romain&#8217;s comments here.  This is what he has to say&#8230;</p>
<h3> Information in neurobiology</h3>
<p>I will try to explain why neurobiologists are today very interested in reliable estimates of entropy/information and what are the techniques we use to obtain them.</p>
<p>The activity of sensory as well as more central neurons is known to be modulated by external stimulations. In 1926, in a seminal paper, Adrian observed that neurons in the sciatic nerve of the frog fire action potentials (or spikes) when some muscle in the hindlimb is stretched. In addition, he observed that the frequency of the spikes increases with the amplitude of the stretching.</p>
<p>• E.D. Adrian, <a href="http://jp.physoc.org/content/61/1/49.full.pdf"> The impulses produced by sensory nerve endings.</a> (1926).</p>
<p>For another very nice example, in 1962, Hubel and Wiesel found <a href="http://en.wikipedia.org/wiki/Simple_cell">neurons</a> in the cat visual cortex whose activity depends on the orientation of a visual stimulus, a simple black line over white background: some neurons fire preferentially for one orientation of the line (Hubel and Wiesel were awarded the 1981 Nobel Prize in Physiology for their work). This incidentally led to the concept of &#8220;receptive field&#8221; which is of tremendous importance in neurobiology&#8212;but though it&#8217;s fascinating, it&#8217;s a different topic.</p>
<p>Good, we are now able to define what makes a neuron tick. The problem is that neural activity is often very &#8220;noisy&#8221;: when the exact same stimulus is presented many times, the responses appear to be very different from trial to trial. Even careful observation cannot necessarily reveal correlations between the stimulations and the neural activity. So we would like a measure capable of capturing the statistical dependencies between the stimulation and the response of the neuron to know if we can say something about the stimulation just by observing the response of a neuron, which is essentially the task of the brain. In particular, we want a fundamental measure that does not rely on any assumption about the functioning of the brain. Information theory provides the tools to do this, that is why we like to use it: we often try to measure the mutual information between stimuli and responses.</p>
<p>To my knowledge, the first paper using information theory in neuroscience was by MacKay and McCulloch in 1952:</p>
<p>• Donald M. Mackay and Warren S. McCulloch, <a href="http://www.weizmann.ac.il/complex/tlusty/courses/InfoInBio/Papers/MacKayMcCulloch1952.pdf"> The limiting information capacity of a neuronal link</a>, <i>Bulletin of Mathematical Biophysics</i> <b>14</b> (1952), 127&#8211;135.</p>
<p>But information theory was not used in neuroscience much until the early 90&#8217;s. It started again with a paper by Bialek <i>et al.</i> in 1991:</p>
<p>• W. Bialek, F. Rieke, R. R. de Ruyter van Steveninck and D. Warland, Reading a neural code, <a href="http://www.sciencemag.org/content/252/5014/1854.short"> <i>Science</i></a> <b>252</b> (1991), 1854&#8211;1857.</p>
<p>However, when applying information-theoretic methods to biological data, we often have a limited sampling of the neural response, we are usually very happy when we have 50 trials for a given stimulus. Why is this limited sample a problem?</p>
<p>During the major part of the 20th century, following Adrian&#8217;s finding, the paradigm for the neural code was the frequency of the spikes or, equivalently, the number of spikes in a window of time. But in the early 90&#8217;s, it was observed that the exact timing of spikes is (in some cases) reliable across trials. So instead of considering the neural response as a single number (the number of spikes), the temporal patterns of spikes started to be taken into account.  But time is continuous, so to be able to do actual computations, time was discretized and a neural response became a binary string.</p>
<p>Now, if you consider relevant time-scales, say, a 100 millisecond time window with a 1 millisecond bin with a firing frequency of about 50 per second, then your response space is huge and the estimates of information with only 50 trials are not reliable anymore. That&#8217;s why a lot of efforts have been carried out to overcome the limited sampling bias.</p>
<p>Now, getting at the techniques developed in this field, John already mentioned the work by Liam Paninski, but here are other very interesting references:</p>
<p>• Stefano Panzeri and Alessandro Treves, <a href="http://digitallibrary.sissa.it/handle/1963/935"> Analytical estimates of limited sampling biases in different information measures</a>, <i><a href="http://www.ingentaconnect.com/content/apl/network/1996/00000007/00000001/art00006"> Network: Computation in Neural Systems</a></i> <b>7</b> (1996), 87&#8211;107.</p>
<p>They computed the first-order bias of the information (related to the <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/">Miller&#8211;Madow correction</a>) and then used a Bayesian technique to estimate the number of responses not included in the sample but that would be in an infinite sample (a goal similar to that of <a href="https://johncarlosbaez.wordpress.com/2012/06/21/the-mathematics-of-biodiversity-part-1/">Good&#8217;s rule of thumb</a>).</p>
<p>• S.P. Strong, R. Koberle, R.R. de Ruyter van Steveninck, and W. Bialek, Entropy and information in neural spike trains, <i><a href="http://prl.aps.org/abstract/PRL/v80/i1/p197_1"> Phys. Rev. Lett.</a></i> <b>80</b> (1998), 197&#8211;200.</p>
<p>The entropy (or if you prefer, information) estimate can be expanded in a power series in <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> (the sample size) around the true value. By computing the estimate for various values of <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> and fitting it with a parabola, it is possible to estimate the value of the entropy as <img src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N &#92;rightarrow &#92;infty." class="latex" /></p>
<p>These approaches are also well-known:</p>
<p>• Ilya Nemenman, Fariel Shafee and William Bialek, <a href="http://arxiv.org/abs/physics/0108025"> Entropy and inference, revisited</a>, 2002.</p>
<p>• Alexander Kraskov, Harald Stögbauer and Peter Grassberger, Estimating mutual information, <a href="http://pre.aps.org/abstract/PRE/v69/i6/e066138"> <i>Phys. Rev. E.</i> </a> <b>69</b> (2004), 066138.</p>
<p>Actually, Stefano Panzeri has quite a few impressive papers about this problem, and recently with colleagues he has made public a free Matlab toolbox for information theory (<a href="http://www.ibtb.org/">www.ibtb.org</a>) implementing various correction methods.</p>
<p>Finally, the work by Jonathan Victor is worth mentioning, since he provided (to my knowledge again) the first estimate of mutual information using geometry. This is of particular interest with respect to the work by <a href="http://www.maths.gla.ac.uk/~tl/mdiss.pdf">Christina Cobbold and Tom Leinster</a> on measures of biodiversity that take the distance between species into account:</p>
<p>• J. D. Victor and K. P. Purpura, Nature and precision of temporal coding in visual cortex: a metric-space analysis, <a href="http://jn.physiology.org/content/76/2/1310.short"><i>Journal of Neural Physiology</i></a> <b>76</b> (1996), 1310&#8211;1326.</p>
<p>He introduced a distance between sequences of spikes and from this, derived a lower bound on mutual information.</p>
<p>• Jonathan D. Victor, <a href="http://www-users.med.cornell.edu/~jdvicto/pdfs/vict03.pdf">Binless strategies for estimation of information from neural data</a>, <a href="http://pre.aps.org/abstract/PRE/v66/i5/e051903"><i>Phys. Rev. E.</i></a> <b>66</b> (2002), 051903.</p>
<p>Taking inspiration from work by Kozachenko and Leonenko, he obtained an estimate of the information based on the distances between the closest responses.</p>
<p>Without getting too technical, that&#8217;s what we do in neuroscience about the limited sampling bias. The incentive is that obtaining reliable estimates is crucial to understand the <a href="http://en.wikipedia.org/wiki/Neural_coding">&#8216;neural code&#8217;</a>, the holy grail of computational neuroscientists.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/14/the-mathematics-of-biodiversity-part-8/#comments">9 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/14/the-mathematics-of-biodiversity-part-8/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;8)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10580 post type-post status-publish format-standard hentry category-biodiversity category-information-and-entropy category-probability" id="post-10580">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;7)</a></h2>
				<small>12 July, 2012</small><br />


				<div class="entry">
					<p>How ignorant are you?  </p>
<p>Do you know?  </p>
<p><i>Do you know how much don&#8217;t you know?</i></p>
<p>It seems hard to accurately estimate your lack of knowledge.  It even seems hard to say precisely <i>how hard</i> it is.  But the cool thing is, we can actually extract an interesting math question from this problem.  And one answer to this question leads to the following conclusion:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate how ignorant you are.<br />
</b></p></blockquote>
<p>But the devil is in the details.  So let&#8217;s see the details!</p>
<p>The <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a> of a probability distribution is a way of measuring how ignorant we are when this probability distribution describes our knowledge. </p>
<p>For example, suppose all we care about is whether this ancient Roman coin will land heads up or tails up:</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/File:PupienusSest.jpg"><img src="http://upload.wikimedia.org/wikipedia/commons/6/63/PupienusSest.jpg" /></a></div>
<p>If we know there&#8217;s a 50% chance of it landing heads up, that&#8217;s a Shannon entropy of 1 bit: we&#8217;re missing one bit of information.  </p>
<p>But suppose for some reason we know for sure it&#8217;s going to land heads up.  For example, suppose we know the guy on this coin is the emperor Pupienus Maximus, a egomaniac who had lead put on the back of all coins bearing his likeness, so his face would never hit the dirt!  Then the Shannon entropy is 0: we know what&#8217;s going to happen when we toss this coin.</p>
<p>Or suppose we know there&#8217;s a 90% it will land heads up, and a 10% chance it lands tails up.  Then the Shannon entropy is somewhere in between.  We can calculate it like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+0.9+%5Clog_2+%280.9%29+-+0.1+%5Clog_2+%280.1%29+%3D+0.46899...+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- 0.9 &#92;log_2 (0.9) - 0.1 &#92;log_2 (0.1) = 0.46899... " class="latex" /></p>
<p>so that&#8217;s how many bits of information we&#8217;re missing.</p>
<p>But now suppose we have no idea.  Suppose we just start flipping the coin over and over, and seeing what happens.  Can we <i>estimate</i> the Shannon entropy?</p>
<p>Here&#8217;s a naive way to do it.  First, use your experimental data to estimate the probability that that the coin lands heads-up.  Then, stick that probability into the formula for Shannon entropy.  For example, say we flip the coin 3 times and it lands head-up once.  Then we can <i>estimate</i> the probability of it landing heads-up as 1/3, and tails-up as 2/3.  So we can estimate that the Shannon entropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Cfrac%7B1%7D%7B3%7D+%5Clog_2+%28%5Cfrac%7B1%7D%7B3%7D%29+-%5Cfrac%7B2%7D%7B3%7D+%5Clog_2+%28%5Cfrac%7B2%7D%7B3%7D%29+%3D+0.918...+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;frac{1}{3} &#92;log_2 (&#92;frac{1}{3}) -&#92;frac{2}{3} &#92;log_2 (&#92;frac{2}{3}) = 0.918... } " class="latex" /></p>
<p>But it turns out that this approach systematically <i>underestimates</i> the Shannon entropy!  </p>
<p>Say we have a coin that lands up a certain fraction of the time, say <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />   And say we play this game: we flip our coin <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> times, see what we get, and estimate the Shannon entropy using the simple recipe I just illustrated.  </p>
<p>Of course, our estimate will depend on the luck of the game.  But on average, it will be <i>less</i> than the <i>actual</i> Shannon entropy, which is </p>
<p><img src="https://s0.wp.com/latex.php?latex=-+p+%5Clog_2+%28p%29+-+%281-p%29+%5Clog_2+%281-p%29++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- p &#92;log_2 (p) - (1-p) &#92;log_2 (1-p)  " class="latex" /></p>
<p>We can prove this mathematically.  But it shouldn&#8217;t be surprising.  After all, if <img src="https://s0.wp.com/latex.php?latex=n+%3D+1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 1," class="latex" /> we&#8217;re playing a game where we flip the coin just <i>once</i>.  And with this game, our naive estimate of the Shannon entropy will always be <i>zero!</i>  Each time we play the game, the coin will either land heads up 100% of the time, or tails up 100% of the time!  </p>
<p>If we play the game with more coin flips, the error gets less severe.  In fact it approaches zero as the number of coin flips gets ever larger, so that <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty." class="latex" />  The case where you flip the coin just once is an extreme case&#8212;but extreme cases can be good to think about, because they can indicate what may happen in less extreme cases.</p>
<p>One moral here is that naively generalizing on the basis of limited data can make you feel more sure you know what&#8217;s going on than you actually are.  </p>
<p>I hope you knew <i>that</i> already!</p>
<p>But we can also say, in a more technical way, that the naive way of estimating Shannon entropy is a <a href="http://en.wikipedia.org/wiki/Bias_of_an_estimator"><b>biased estimator</b></a>: the average value of the estimator is different from the value of the quantity being estimated.   </p>
<p>Here&#8217;s an example of an unbiased estimator.  Say you&#8217;re trying to estimate the probability that the coin will land heads up.  You flip it <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> times and see that it lands up <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m" class="latex" /> times.  You estimate that the probability is <img src="https://s0.wp.com/latex.php?latex=m%2Fn.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m/n." class="latex" />  That&#8217;s the obvious thing to do, and it turns out to be unbiased.  </p>
<p>Statisticians like to think about <a href="http://en.wikipedia.org/wiki/Estimator">estimators</a>, and being unbiased is one way an estimator can be &#8216;good&#8217;.  Beware: it&#8217;s not the only way!  There are estimators that are unbiased, but whose standard deviation is so huge that they&#8217;re almost useless.  It can be better to have an estimate of something that&#8217;s more accurate, even though on average it&#8217;s a bit too low.  So sometimes, a biased estimator can be more useful than an unbiased estimator.  </p>
<p>Nonetheless, my ears perked up when Lou Jost mentioned that there is no unbiased estimator for Shannon entropy.  In rough terms, the moral is that:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate how ignorant you are.</b>
</p></blockquote>
<p>I think this is important.  For example, it&#8217;s important because Shannon entropy is also used as a measure of <i>biodiversity</i>.  Instead of flipping a coin repeatedly and seeing which side lands up, now we go out and collect plants or animals, and see which species we find.  The relative abundance of different species defines a  probability distribution on the set of species.  In this language, the moral is:</p>
<blockquote><p>
<b>There&#8217;s no unbiased way to estimate biodiversity.<br />
</b></p></blockquote>
<p>But of course, this doesn&#8217;t mean we should give up.  We may just have to settle for an estimator that&#8217;s a bit biased!  And people have spent a bunch of time looking for estimators that are less biased than the naive one I just described.  </p>
<p>By the way, equating &#8216;biodiversity&#8217; with &#8216;Shannon entropy&#8217; is sloppy: there are <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">many measures of biodiversity</a>.  The Shannon entropy is just a special case of the <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a>, which depends on a parameter <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />: we get Shannon entropy when <img src="https://s0.wp.com/latex.php?latex=q+%3D+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 1." class="latex" />  </p>
<p>As <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gets smaller, the R&eacute;nyi entropy gets more and more sensitive to rare species&#8212;or shifting back to the language of probability theory, rare events.  It&#8217;s the rare events that make Shannon entropy hard to estimate, so I imagine there should be theorems about estimators for R&eacute;nyi entropy, which say it gets harder to estimate as <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gets smaller.  Do you know such theorems?</p>
<p>Also, I should add that biodiversity is better captured by the &#8216;Hill numbers&#8217;, which are functions of the R&eacute;nyi entropy, than by the R&eacute;nyi entropy itself.  (See <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">here</a> for the formulas.)  Since these functions are nonlinear, the lack of an unbiased estimator for R&eacute;nyi entropy doesn&#8217;t instantly imply the same for the Hill numbers.  So there are also some obvious questions about unbiased estimators for Hill numbers.  Do you know answers to those?</p>
<p>Here are some papers on estimators for entropy.  Most of these focus on estimating the Shannon entropy of a probability distribution on a finite set.  </p>
<p>This old classic has a proof that the &#8216;naive&#8217; estimator of Shannon entropy is biased, and estimates on the bias:</p>
<p>&bull; Bernard Harris, <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a020217.pdf">The statistical estimation of entropy in the non-parametric case</a>, Army Research Office, 1975.</p>
<p>He shows the bias goes to zero as we increase the number of samples: the number I was calling <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> in my coin flip example.  In fact he shows the bias goes to zero like <img src="https://s0.wp.com/latex.php?latex=O%281%2Fn%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(1/n)." class="latex" />  This is big <a href="http://en.wikipedia.org/wiki/Big_O_notation">big O notation</a> which means that as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%2B%5Cinfty%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to +&#92;infty," class="latex" /> the bias is bounded by some constant times <img src="https://s0.wp.com/latex.php?latex=1%2Fn.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/n." class="latex" /> This constant depends on the size of our finite set&#8212;or, if you want to do better, the <b>class number</b>, which is the number of elements on which our probability distribution is nonzero. </p>
<p>Using this idea, he shows that you can find a less biased estimator if you have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> on a finite set and you know that exactly <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> of these probabilities are nonzero.   To do this, just take the &#8216;naive&#8217; estimator I described earlier and add <img src="https://s0.wp.com/latex.php?latex=%28k-1%29%2F2n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(k-1)/2n." class="latex" />  This is called the <b>Miller&#8211;Madow bias correction</b>.  The bias of this improved estimator goes to zero like <img src="https://s0.wp.com/latex.php?latex=O%281%2Fn%5E2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="O(1/n^2)." class="latex" /></p>
<p>The problem is that in practice you don&#8217;t know ahead of time how many probabilities are nonzero!  In applications to biodiversity this would amount to knowing ahead of time how many species exist, before you go out looking for them. </p>
<p>But what about the theorem that there&#8217;s no unbiased estimator for Shannon entropy?  The best reference I&#8217;ve found is this:</p>
<p>&bull; Liam Paninski, <a href="http://www.stat.columbia.edu/~liam/research/abstracts/info_est-nc-abs.html">Estimation of entropy and mutual information</a>, <i>Neural Computation</i> <b>15</b> (2003) 1191-1254. </p>
<p>In Proposition 8 of Appendix A, Paninski gives a quick proof that there is no unbiased estimator of Shannon entropy for probability distributions on a finite set.  But his paper goes far beyond this.  Indeed, it seems like a pretty definitive modern discussion of the whole subject of estimating entropy.  Interestingly, this subject is dominated by neurobiologists studying entropy of signals in the brain!  So, lots of his examples involve brain signals.</p>
<p>Another overview, with tons of references, is this:</p>
<p>&bull; J. Beirlant, E. J. Dudewicz, L. Gy&ouml;rfi, and E. C. van der Meulen, <a href="http://www.its.caltech.edu/~jimbeck/summerlectures/references/Entropy%20estimation.pdf">Nonparametric entropy estimation: an overview</a>.  </p>
<p>This paper focuses on the situation where don&#8217;t know ahead of time how many probabilities are nonzero:</p>
<p>&bull; Anne Chao and T.-J. Shen, <a href="http://wayback.archive.org/web/20110715000000*/http://chao.stat.nthu.edu.tw/paper/2003_eest_10_p429.pdf">Nonparametric estimation of Shannon&#8217;s index of diversity when there are unseen species in sample</a>, <i><a href="http://www.springerlink.com/content/j23110l474087421/">Environmental and Ecological Statistics</a></i> <b>10</b> (2003), 429&amp;&#8211;443.</p>
<p>In 2003 there was a conference on the problem of estimating entropy, whose webpage has useful information.  As you can see, it was dominated by neurobiologists:</p>
<p>&bull; <a href="http://menem.com/~ilya/pages/NIPS03/">Estimation of entropy and information of undersampled probability distributions: theory, algorithms, and applications to the neural code</a>, Whistler, British Columbia, Canada, 12 December 2003.</p>
<p>By the way, I was very confused for a while, because these guys claim to have found an unbiased estimator of Shannon entropy:</p>
<p>&bull; Stephen Montgomery Smith and Thomas Sch&uuml;rmann, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.6882&amp;rep=rep1&amp;type=pdf">Unbiased estimators for entropy and class number</a>.</p>
<p>However, their way of estimating entropy has a funny property: in the language of biodiversity, it&#8217;s only well-defined if our samples include at least one species of each organism.   So, we cannot compute this estimate for an <i>arbitary</i> list of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> samples.  This means it&#8217;s not <a href="http://en.wikipedia.org/wiki/Estimator">estimator</a> in the usual sense&#8212;the sense that Paninski is using!  So it doesn&#8217;t really contradict Paninski&#8217;s result.</p>
<p>To wrap up, let me state Paninski&#8217;s result in a mathematically precise way. Suppose <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is a probability distribution on a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" />.  Suppose <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is any number we can compute from <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />: that is, any real-valued function on the set of probability distributions.   We&#8217;ll be interested in the case where <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the <b>Shannon entropy</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-%5Csum_%7Bx+%5Cin+X%7D+p%28x%29+%5C%2C+%5Clog+p%28x%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = -&#92;sum_{x &#92;in X} p(x) &#92;, &#92;log p(x) }" class="latex" /></p>
<p>Here we can use whatever base for the logarithm we like: earlier I was using base 2, but that&#8217;s not sacred.  Define an <b>estimator</b> to be any function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D%3A+X%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}: X^n &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>The idea is that given <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> <b>samples</b> from the set <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> meaning points <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cdots%2C+x_n+%5Cin+X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_1, &#92;dots, x_n &#92;in X," class="latex" /> the estimator gives a number <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D%28x_1%2C+%5Cdots%2C+x_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}(x_1, &#92;dots, x_n)" class="latex" />.   This number is supposed to estimate some feature of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />: for example, its entropy.   </p>
<p>If the samples are independent and distributed according to the distribution <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> the <b>sample mean of the estimator</b> will be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Chat%7BS%7D+%5Crangle+%3D+%5Csum_%7Bx_1%2C+%5Cdots%2C+x_n+%5Cin+X%7D+%5Chat%7BS%7D%28x_1%2C+%5Cdots%2C+x_n%29+%5C%2C+p%28x_1%29+%5Ccdots+p%28x_n%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;hat{S} &#92;rangle = &#92;sum_{x_1, &#92;dots, x_n &#92;in X} &#92;hat{S}(x_1, &#92;dots, x_n) &#92;, p(x_1) &#92;cdots p(x_n) } " class="latex" /></p>
<p>The <b>bias</b> of the estimator is the difference between the sample mean of the estimator and actual value of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" />: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Chat%7BS%7D+%5Crangle+-+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;hat{S} &#92;rangle - S " class="latex" /></p>
<p>The estimator <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BS%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hat{S}" class="latex" /> is <b>unbiased</b> if this bias is zero for all <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>Proposition 8 of Paninski&#8217;s paper says there exists no unbiased estimator for entropy!  The proof is very short&#8230; </p>
<p>Okay, that&#8217;s all for today.</p>
<p>I&#8217;m back in Singapore now; I learned so much at the <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx">Mathematics of Biodiversity</a> conference that there&#8217;s no way I&#8217;ll be able to tell you all that information.   I&#8217;ll try to write a few more blog posts, but please be aware that my posts so far give a hopelessly biased and idiosyncratic view of the conference, which would be almost unrecognizable to most of the participants.  There are a lot of important themes I haven&#8217;t touched on at all&#8230; while this business of entropy estimation barely came up: I just find it interesting!</p>
<p>If more of you blogged more, we wouldn&#8217;t have this problem. </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/#comments">26 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/12/the-mathematics-of-biodiversity-part-7/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;7)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10541 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics category-physics category-probability" id="post-10541">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;5)</a></h2>
				<small>3 July, 2012</small><br />


				<div class="entry">
					<p>I&#8217;d be happy to get your feedback on these slides of the talk I&#8217;m giving the day after tomorrow:</p>
<p>&bull; John Baez, <a href="http://math.ucr.edu/home/baez/biodiversity/">Diversity, entropy and thermodynamics</a>, 6 July 2012, Exploratory Conference on the Mathematics of Biodiversity, Centre de Recerca Matemàtica, Barcelona.</p>
<blockquote><p>
<b>Abstract:</b> As is well known, some popular measures of biodiversity are formally identical to measures of entropy developed by Shannon, Rényi and others. This fact is part of a larger analogy between thermodynamics and the mathematics of biodiversity, which we explore here. Any probability distribution can be extended to a 1-parameter family of probability distributions where the parameter has the physical meaning of &#8216;temperature&#8217;. This allows us to introduce thermodynamic concepts such as energy, entropy, free energy and the partition function in any situation where a probability distribution is present&#8212;for example, the probability distribution describing the relative abundances of different species in an ecosystem. The Rényi entropy of this probability distribution is closely related to the change in free energy with temperature. We give one application of thermodynamic ideas to population dynamics, coming from the work of Marc Harper: as a population approaches an &#8216;evolutionary optimum&#8217;, the amount of Shannon information it has &#8216;left to learn&#8217; is nonincreasing. This fact is closely related to the Second Law of Thermodynamics.
</p></blockquote>
<p>This talk is rather different than the one I&#8217;d envisaged giving!  There was a lot of interest in my work on R&eacute;nyi entropy and thermodynamics, because R&eacute;nyi entropies&#8212;and their exponentials, called the Hill numbers&#8212;are an <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/">important measure of biodiversity</a>.  So, I decided to spend a lot of time talking about that.</p>
<div align="center"><a href="http://math.ucr.edu/home/baez/biodiversity/"><img src="https://i2.wp.com/math.ucr.edu/home/baez/biodiversity/408px-Forest_fruits_from_Barro_Colorado.jpg" /></a></div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/#comments">12 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/03/the-mathematics-of-biodiversity-part-5/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10481 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics" id="post-10481">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/" rel="bookmark">The Mathematics of Biodiversity (Part&nbsp;4)</a></h2>
				<small>2 July, 2012</small><br />


				<div class="entry">
					<div align="center">
<a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/barcelona_biodiversity_poster.jpg" /><br />
</a>
</div>
<p>Today the conference part of this program is starting:</p>
<p>&bull; <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Research-Program-on-Mathematics-of-Biodiversity.aspx">Research Program on the Mathematics of Biodiversity</a>, June-July 2012, Centre de Recerca Matemàtica, Barcelona, Spain.  Organized by Ben Allen, Silvia Cuadrado, Tom Leinster, Richard Reeve and John Woolliams.</p>
<p>Lou Jost kicked off the proceedings with an impassioned call to think harder about fundamental concepts:</p>
<p>&bull; Lou Jost,  <a href="http://math.ucr.edu/home/baez/biodiversity/JostBarcelonaPublicGeneticsEcology.pdf">Why biologists should care about the mathematics of biodiversity</a>. </p>
<p>Then Tom Leinster gave an introduction to some of these concepts, and Lou explained how they show up in ecology, genetics, economics and physics.  </p>
<p>Suppose we have <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different species on an island.  Suppose a fraction <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> of the organisms belong to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  So,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i+%3D+1%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i = 1} " class="latex" /></p>
<p>and mathematically we can treat these numbers as probabilities.</p>
<p>People have many ways to compute the &#8216;biodiversity&#8217; from these numbers.   Some of these can be wildly misleading when applied incorrectly, and this has led to shocking errors.  For example, in genetics, a commonly used formula for determining when plants or animals on a bunch of islands will split into separate species is completely wrong.</p>
<p>In fact, if we&#8217;re not careful, some measures of biodiversity can fool us into thinking we&#8217;re <i>saving</i> most of the biodiversity when we&#8217;re actually <i>losing</i> almost all of it!   </p>
<p>One good example involves measures of similarity between tropical butterflies in the canopy (the top of the forest) and the understory (the bottom).  According to Lou Just, some published studies say the similarity is about 95%.  That sounds like the two communities are almost the same.  However, <i>almost no</i> butterflies living in the canopy live in the understory, and vice versa!  The problem is that mathematics is being used inappropriately.</p>
<p>Here are four famous measures of biodiversity:</p>
<p>&bull; <b>Species richness</b>.  This is just the number of species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /></p>
<p>&bull; <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Shannon_index">Shannon entropy</a></b>.  This is the expected amount of information you gain when someone tells you which species an organism belongs to:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Csum_%7Bi%3D1%7D%5En+p_i+%5Cln%28p_i%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;sum_{i=1}^n p_i &#92;ln(p_i) }" class="latex" /></p>
<p>&bull; The <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Inverse_Simpson_index">inverse Simpson index</a></b>.  This is the reciprocal of the probability that two randomly chosen organisms belong to the same species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+1+%5Cbig%2F+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ 1 &#92;big/ &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>The probability that two organisms belong to the same species is called the <a href="http://en.wikipedia.org/wiki/Diversity_index#Simpson_index"><b>Simpson index</b></a>:  </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>This is used in economics as a measure of the concentration of wealth, where <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the fraction of wealth owned by the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th individual.  Be careful: there&#8217;s a lot of different jargon in different fields, so it&#8217;s easy to get confused at first!  For example, the probability that two organisms belong to <i>different</i> species is often called the <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Gini-Simpson_index">Gini&#8211;Simpson index</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+1+-+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ 1 - &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>It was introduced by the statistician <a href="http://en.wikipedia.org/wiki/Corrado_Gini">Corrado Gini</a> a century ago, in 1912 and the ecologist <a href="http://en.wikipedia.org/wiki/Edward_H._Simpson">Edward H. Simpson</a> in 1949.  It&#8217;s also called the <b><a href="http://www.uwyo.edu/dbmcd/molmark/lect04/lect4.html">heterozygosity</a></b> in genetics.</p>
<p>&bull; The <b><a href="http://en.wikipedia.org/wiki/Diversity_index#Berger-Parker_index">Berger&#8211;Parker index</a></b>.   This is the fraction of organisms that belong to the most common species:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bmax%7D+%5C%2C+p_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{max} &#92;, p_i " class="latex" /></p>
<p>So, unlike the other main ones I&#8217;ve listed, this quantity tends to go <i>down</i> when biodiversity goes up.  To fix this we could take its reciprocal, as we did with the Simpson index.</p>
<p>What a mess, eh?  But here&#8217;s some good news: all these quantities are functions of a single quantity, the <b><a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_q%28p%29+%3D+%5Cfrac%7B1%7D%7B1+-q%7D+%5Cln+%5Csum_%7Bi%3D1%7D%5En+p_i%5Eq++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_q(p) = &#92;frac{1}{1 -q} &#92;ln &#92;sum_{i=1}^n p_i^q  } " class="latex" /></p>
<p>for various values of the parameter <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  </p>
<p>I&#8217;ve written about the R&eacute;nyi entropies and their role in thermodynamics before <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">on this blog</a>.  I&#8217;ll also talk about it later in this conference, and I&#8217;ll show you my slides.  So, I won&#8217;t repeat that story here.  Suffice it to say that R&eacute;nyi entropies are fascinating but still a bit mysterious to me.</p>
<p>But one of Lou Jost&#8217;s main points is that we can make bad mistakes if we work with R&eacute;nyi entropies when we should be working with their exponentials, which are called <b>Hill numbers</b> and denoted by a <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D" class="latex" />, for &#8216;diversity&#8217;:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%7B%7D%5EqD%28p%29+%3D+e%5E%7BH_q%28p%29%7D+%3D+++%5Cleft%28%5Csum_%7Bi%3D1%7D%5En+p_i%5Eq+%5Cright%29%5E%7B%5Cfrac%7B1%7D%7B1-q%7D%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ {}^qD(p) = e^{H_q(p)} =   &#92;left(&#92;sum_{i=1}^n p_i^q &#92;right)^{&#92;frac{1}{1-q}}  } " class="latex" /></p>
<p>These were introduced by <a href="http://intranet.catie.ac.cr/intranet/posgrado/Agrof-Cult-AyP/Curso%20SAF%20A%20y%20P%202011/Propedeutico%20Agroforestal/Lecturas%20optativas/Diversity%20and%20evenness%20a%20unifying%20notation%20and%20consequences.pdf">M. O. Hill</a> in 1973.  One reason they&#8217;re good is that they are <b>effective numbers</b>.  This means that if all the species are equally common, the Hill number equals the number of species, regardless of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5C%3B+%5CLongrightarrow+%5C%3B+%7B%7D%5EqD%28p%29+%3D+n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;frac{1}{n} &#92;; &#92;Longrightarrow &#92;; {}^qD(p) = n " class="latex" /></p>
<p>So, they&#8217;re a way of measuring an &#8216;effective&#8217; number of species in situations where species are <i>not</i> all equally common.  </p>
<p>A closely related fact is that the Hill numbers obey the <b>replication principle</b>.  This means that if we have probability distributions on two finite sets, each with Hill number <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> for some choice of <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> and we combine them with equal weights to get a probability distribution on the disjoint union of those sets, the resulting distribution has Hill number <img src="https://s0.wp.com/latex.php?latex=2X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2X." class="latex" /></p>
<p>Another good fact is that the Hill numbers are as large as possible when all the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are equal.  They&#8217;re as small as possible, namely 1, when one of the <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> equals 1 and the rest are zero.</p>
<p>Let&#8217;s see how all the measures of biodiversity I listed are either Hill numbers or can easily be converted to Hill numbers.  We&#8217;ll also see that at <img src="https://s0.wp.com/latex.php?latex=q+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 0," class="latex" /> the Hill number treats all species that are present in an equal way, regardless of their abundance.  As <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> increases, it counts more abundant species more heavily, since we&#8217;re raising the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> to a bigger power.  And when <img src="https://s0.wp.com/latex.php?latex=q+%3D+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = &#92;infty" class="latex" />, we only care about the <i>most</i> abundant species: none of the others matter at all!</p>
<p>Here goes:</p>
<p>&bull; The species richness is the limit of the Hill numbers as <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to 0" class="latex" /> from above:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clim_%7Bq+%5Cto+0%5E%2B%7D+%7B%7D%5EqD+%28p%29+%3D+n+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;lim_{q &#92;to 0^+} {}^qD (p) = n } " class="latex" /></p>
<p>So, we can just call this <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E0D%28p%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^0D(p)." class="latex" /></p>
<p>&bull; The exponential of the Shannon entropy is the limit of the Hill numbers as <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to 1" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clim_%7Bq+%5Cto+1%7D+%7B%7D%5EqD%28p%29+%3D+%5Cexp%5Cleft%28-+%5Csum_%7Bi%3D1%7D%5En+p_i+%5Cln%28p_i%29%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;lim_{q &#92;to 1} {}^qD(p) = &#92;exp&#92;left(- &#92;sum_{i=1}^n p_i &#92;ln(p_i)&#92;right) } " class="latex" /></p>
<p>So, we can just call this <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E1D%28p%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^1D(p)." class="latex" /></p>
<p>&bull; The inverse Simpson index is the Hill number at <img src="https://s0.wp.com/latex.php?latex=q+%3D+2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 2" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%7B%7D%5E2D%28p%29+%3D++1+%5Cbig%2F+%5Csum_%7Bi%3D1%7D%5En+p_i%5E2+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  {}^2D(p) =  1 &#92;big/ &#92;sum_{i=1}^n p_i^2 } " class="latex" /></p>
<p>&bull;  The reciprocal of the Berger&#8211;Parker index is the limit of Hill numbers as <img src="https://s0.wp.com/latex.php?latex=q+%5Cto+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;to +&#92;infty" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clim_%7Bq+%5Cto+%2B%5Cinfty%7D+%7B%7D%5EqD%28p%29+%3D+1+%5Cbig%2F+%5Cmathrm%7Bmax%7D+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;lim_{q &#92;to +&#92;infty} {}^qD(p) = 1 &#92;big/ &#92;mathrm{max} &#92;, p_i } " class="latex" /></p>
<p>so we can call this quantity <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E%5Cinfty+D%28p%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^&#92;infty D(p)." class="latex" /></p>
<p>These facts mean that understanding Hill numbers will help us understand lots of measures of biodiversity!  And the good properties of Hill numbers will help us avoid dangerous mistakes.</p>
<p>For mathematicians, a good challenge is to find theorems uniquely characterizing the Hill numbers&#8230;. preferably with assumptions that biologists will accept as plausible facts about &#8216;diversity&#8217;.  Some theorems like this already exist for specific choices of <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> but it will be better to characterize the function <img src="https://s0.wp.com/latex.php?latex=%7B%7D%5Eq+D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="{}^q D" class="latex" /> for all values of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in one blow.  Tom Leinster is working on such a theorem now.</p>
<p>Another important task is to generalize Hill numbers to take into account things like:</p>
<p>&bull; &#8216;distances&#8217; between species, measured either genetically, phylogenetically or functionally,</p>
<p>&bull; &#8216;values&#8217; for species, measured either economically or<br />
any other way.</p>
<p>There&#8217;s a lot of work on this, and many of the talks here conference will discuss these generalizations.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/#comments">13 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/07/02/the-mathematics-of-biodiversity-part-4/" rel="bookmark" title="Permanent Link to The Mathematics of Biodiversity (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10340 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics category-probability" id="post-10340">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/" rel="bookmark">Information Geometry (Part&nbsp;13)</a></h2>
				<small>26 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/">Last time</a> I gave a sketchy overview of evolutionary game theory.  Now let&#8217;s get serious.</p>
<p>I&#8217;ll start by explaining &#8216;Nash equilibria&#8217; for 2-person games.  These are situations where neither player can profit by changing what they&#8217;re doing.  Then I&#8217;ll introduce &#8216;mixed strategies&#8217;, where the players can choose among several strategies with different probabilities.  Then I&#8217;ll introduce evolutionary game theory, where we think of each strategy as a <i>species</i>, and its probability as <i>the fraction of organisms that belong to that species</i>.</p>
<p>Back in <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/">Part 9</a>, I told you about the &#8216;replicator equation&#8217;, which says how these fractions change with time thanks to natural selection.   Now we&#8217;ll see how this leads to the idea of an &#8216;evolutionarily stable strategy&#8217;.   And finally, we&#8217;ll see that when evolution takes us toward such a stable strategy, the amount of information the organisms have &#8216;left to learn&#8217; keeps decreasing!</p>
<h3> Nash equilibria </h3>
<p>We can describe a certain kind of two-person game using a <b>payoff matrix</b>, which is an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;times n" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> of real numbers.  We think of <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> as the payoff that either player gets if they choose strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and their opponent chooses strategy <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" /></p>
<p>Note that in this kind of game, there&#8217;s no significant difference between the &#8216;first player&#8217; and the &#8216;second player&#8217;: <i>either</i> player wins an amount <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> if they choose strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> and their opponent chooses strategy <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />  So, this kind of game is called <b>symmetric</b> even though the matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> may not be symmetric.  Indeed, it&#8217;s common for this matrix to be antisymmetric, meaning <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D+%3D+-+A_%7Bji%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij} = - A_{ji}," class="latex" /> since in this case what one player wins, the other loses.  Games with this extra property are called <b>zero-sum games</b>.  But we won&#8217;t limit ourselves to those!</p>
<p>We say a strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> is a <b>symmetric Nash equilibrium</b> if</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7Bii%7D+%5Cge+A_%7Bji%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ii} &#92;ge A_{ji} " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />  This means that if both players use strategy <img src="https://s0.wp.com/latex.php?latex=i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i," class="latex" /> neither gains anything by switching to another strategy.</p>
<p>For example, suppose our matrix is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%28+%5Cbegin%7Barray%7D%7Brr%7D+-1+%26+-12+%5C%5C++0+%26+-3+%5Cend%7Barray%7D+%5Cright%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left( &#92;begin{array}{rr} -1 &amp; -12 &#92;&#92;  0 &amp; -3 &#92;end{array} &#92;right) " class="latex" /></p>
<p>Then we&#8217;ve got the Prisoner&#8217;s Dilemma exactly as described last time!  Here strategy 1 is <b>cooperate</b> and strategy 2 is <b>defect</b>.  If a player cooperates and so does his opponent, he wins</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B11%7D+%3D+-1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{11} = -1 " class="latex" /></p>
<p>meaning he gets one month in jail.   We include a minus sign because &#8216;winning a month in jail&#8217; is not a good thing.   If the player cooperates but his opponent defects, he gets a whole year in jail:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B12%7D+%3D+-12&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{12} = -12" class="latex" /></p>
<p>If he defects but his opponent cooperates, he doesn&#8217;t go to jail at all:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B21%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{21} = 0" class="latex" /></p>
<p>And if they both defect, they both get three months in jail:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B22%7D+%3D+-3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{22} = -3" class="latex" /></p>
<p>You can see that defecting is a Nash equilibrium, since</p>
<p><img src="https://s0.wp.com/latex.php?latex=A_%7B22%7D+%5Cge+A_%7B12%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{22} &#92;ge A_{12}" class="latex" /></p>
<p>So, oddly, if our prisoners know game theory and believe Nash equilibria are best, they&#8217;ll both be worse off than if they cooperate and don&#8217;t betray each other.</p>
<div align="center">
<img width="170" src="http://math.ucr.edu/home/baez/prisoner's_dilemma_left.jpg" /> &nbsp;&nbsp;&nbsp;&nbsp; <img width="170" src="http://math.ucr.edu/home/baez/prisoner's_dilemma_right.jpg" /></div>
<h3> Nash equilibria  for mixed strategies </h3>
<p>So far we&#8217;ve been assuming that with 100% certainty, each player chooses one strategy <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C2%2C3%2C%5Cdots%2C+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,2,3,&#92;dots, n." class="latex" />  Since we&#8217;ll be considering more general strategies in a minute, let&#8217;s call these <b>pure strategies</b>.</p>
<p>Now let&#8217;s throw some probability theory into the stew!  Let&#8217;s allow the players to pick different pure strategies with different probabilities.  So, we define a <b>mixed strategy</b> to be a probability distribution on the set of pure strategies.  In other words, it&#8217;s a list of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> nonnegative numbers</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%5Cge+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;ge 0 " class="latex" /></p>
<p>that sum to one:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i = 1 } " class="latex" /></p>
<p>Say I choose the mixed strategy <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> while you, my opponent, choose the mixed strategy <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  Say our choices are made independently.  Then the probability that I choose the pure strategy <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> while you chose <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+q_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i q_j" class="latex" /></p>
<p>so the expected value of my winnings is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2Cj+%3D+1%7D%5En+p_i+A_%7Bij%7D+q_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i,j = 1}^n p_i A_{ij} q_j }" class="latex" /></p>
<p>or using vector notation</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%5Ccdot+A+q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;cdot A q " class="latex" /></p>
<p>where the dot is the usual dot product on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n." class="latex" /></p>
<p>We can easily adapt the concept of Nash equilibrium to mixed strategies.  A mixed strategy <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a <b>symmetric  Nash equilibrium</b> if for any other mixed strategy <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge++p+%5Ccdot+A+q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge  p &#92;cdot A q " class="latex" /></p>
<p>This means that if both you and I are playing the mixed strategy <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> I can&#8217;t improve my expected winnings by unilaterally switching to the mixed strategy <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />  And neither can you, because the game is symmetric!</p>
<p>If this were a course on game theory, I would now do some examples.  But it&#8217;s not, so I&#8217;ll just send you to page 6 of <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">Sandholm&#8217;s paper</a>: he looks at some famous games like &#8216;hawks and doves&#8217; and &#8216;rock paper scissors&#8217;.</p>
<h3> Evolutionarily stable strategies </h3>
<p>We&#8217;re finally ready to discuss evolutionarily stable strategies.  To do this, let&#8217;s reinterpret the &#8216;pure strategies&#8217; <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C2%2C3%2C+%5Cdots+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,2,3, &#92;dots n" class="latex" /> as <b>species</b>.  Here I don&#8217;t necessarily mean species in the classic biological sense: I just mean different kinds of self-replicating entities, or <b>replicators</b>.  For example, they could be different <a href="http://en.wikipedia.org/wiki/Allele">alleles</a> of the same gene.</p>
<p>Similarly, we&#8217;ll reinterpret the &#8216;mixed strategy&#8217; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as describing a mixed population of replicators, where the fraction of replicators belonging to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />  These numbers are still probabilities: <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the probability that a randomly chosen replicator will belong to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.</p>
<p>We&#8217;ll reinterpret the payoff matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bij%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{ij}" class="latex" /> as a <b>fitness matrix</b>.  In our earlier discussion of the replicator equation, we assumed that the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species grew according to the replicator equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+P_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) P_i }" class="latex" /></p>
<p>where the <b>fitness function</b> <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is any smooth function of the populations of each kind of replicator.</p>
<p>But in evolutionary game theory it&#8217;s common to start by looking at a simple special case where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7Bf_i%28P_1%2C+%5Cdots%2C+P_n%29+++%3D+%5Csum_%7Bj%3D1%7D%5En+A_%7Bij%7D+p_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{f_i(P_1, &#92;dots, P_n)   = &#92;sum_{j=1}^n A_{ij} p_j }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_j+%3D+%5Cfrac%7BP_j%7D%7B%5Csum_k+P_k%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_j = &#92;frac{P_j}{&#92;sum_k P_k} }" class="latex" /></p>
<p>is the fraction of replicators who belong to the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th species.</p>
<p>What does this mean?  The idea is that we have a well-mixed population of game players&#8212;or replicators.   Each one has its own pure strategy&#8212;or species.   Each one randomly roams around and &#8216;plays games&#8217; with each other replicator it meets.  It gets to reproduce at a rate proportional to its expected winnings.</p>
<p>This is unrealistic in all sorts of ways, but it&#8217;s mathematically cute, and it&#8217;s been studied a lot, so it&#8217;s good to know about.  Today I&#8217;ll explain evolutionarily stable strategies only in this special case.  Later I&#8217;ll go back to the general case.</p>
<p>Suppose that we select a sample of replicators from the overall population.  What is the mean fitness of the replicators in this sample?  For this, we need to know the probability that a replicator from this sample belongs to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  Say it&#8217;s <img src="https://s0.wp.com/latex.php?latex=q_j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_j." class="latex" />  Then the mean fitness of our sample is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%2Cj%3D1%7D%5En+q_i+A_%7Bij%7D+p_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i,j=1}^n q_i A_{ij} p_j }" class="latex" /></p>
<p>This is just a weighted average of the fitnesses in our earlier formula.   But using the magic of vectors, we can write this sum as</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+p+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A p " class="latex" /></p>
<p>We already saw this type of expression in the last section!  It&#8217;s my expected winnings if I play the mixed strategy <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and you play the mixed strategy <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>John Maynard Smith defined <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to be <b>evolutionarily stable strategy</b> if when we add a small population of &#8216;invaders&#8217; distributed according to any other probability distribution <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> the original population is more fit than the invaders.</p>
<p>In simple terms: a small &#8216;invading&#8217; population will do worse than the population as a whole.</p>
<p>Mathematically, this means:</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+%28%281-%5Cepsilon%29q+%2B+%5Cepsilon+p%29+%3E++p+%5Ccdot+A+%28%281-%5Cepsilon%29q+%2B+%5Cepsilon+p%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A ((1-&#92;epsilon)q + &#92;epsilon p) &gt;  p &#92;cdot A ((1-&#92;epsilon)q + &#92;epsilon p) " class="latex" /></p>
<p>for all mixed strategies <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and all sufficiently small <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Cge+0+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon &#92;ge 0 ." class="latex" />   Here</p>
<p><img src="https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29q+%2B+%5Cepsilon+p+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1-&#92;epsilon)q + &#92;epsilon p " class="latex" /></p>
<p>is the population we get by replacing an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon" class="latex" />-sized portion of our original population by invaders.</p>
<p><b>Puzzle:</b>  Show that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an evolutionarily stable strategy if and only these two conditions hold for all mixed stategies <img src="https://s0.wp.com/latex.php?latex=p%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p:" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge+p+%5Ccdot+A+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge p &#92;cdot A q" class="latex" /></p>
<p>and also, for all <img src="https://s0.wp.com/latex.php?latex=q+%5Cne+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;ne p" class="latex" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%3D+p+%5Ccdot+A+q+%5C%3B+%5Cimplies+%5C%3B+q+%5Ccdot+A+p+%3E+p+%5Ccdot+A+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q = p &#92;cdot A q &#92;; &#92;implies &#92;; q &#92;cdot A p &gt; p &#92;cdot A p" class="latex" /></p>
<p>The first condition says that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a symmetric Nash equilibrium.  In other words, the invaders can&#8217;t on average be <i>better</i> playing against the original population than members of the original population are.    The second says that if the invaders are <i>just as good</i> at playing against the original population, they must be worse at playing against each other!  The combination of these conditions means the invaders won&#8217;t take over.</p>
<p>Again, I should do some examples&#8230; but instead I&#8217;ll refer you to page 9 of <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">Sandholm&#8217;s paper</a>, and also these course notes:</p>
<p>&bull; Samuel Alizon and Daniel Cownden, <a href="http://dcownden.files.wordpress.com/2009/01/notes6.pdf">Evolutionary games and evolutionarily stable strategies</a>.</p>
<p>&bull; Samuel Alizon and Daniel Cownden, <a href="http://dcownden.files.wordpress.com/2009/01/notes8.pdf">Replicator dynamics</a>.</p>
<h3> The decrease of relative information </h3>
<p>Now comes the punchline&#8230; but with a slight surprise twist at the end.  <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/">Last time</a> we let</p>
<p><img src="https://s0.wp.com/latex.php?latex=P+%3D+%28P_1%2C+%5Cdots+%2C+P_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P = (P_1, &#92;dots , P_n)" class="latex" /></p>
<p>be a population that evolves with time according to the replicator equation, and we let <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> be the corresponding probability distribution.  We supposed <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> was some fixed probability distribution.   We saw that the relative information</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Cdisplaystyle%7B+%5Csum_i+%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7B+p_i+%7D%5Cright%29+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;displaystyle{ &#92;sum_i &#92;ln &#92;left(&#92;frac{q_i}{ p_i }&#92;right) q_i } " class="latex" /></p>
<p>obeys</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%3D++%28p+-+q%29+%7D+%5Ccdot+f%28P%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) =  (p - q) } &#92;cdot f(P) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" /> is the vector of fitness functions.  So, this relative information can never increase if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p+-+q%29+%5Ccdot+f%28P%29+%5Cle+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p - q) &#92;cdot f(P) &#92;le 0 " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" />.</p>
<p>We can adapt this to the special case we&#8217;re looking at now.  Remember, right now we&#8217;re assuming</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7Bf_i%28P_1%2C+%5Cdots%2C+P_n%29+++%3D+%5Csum_%7Bj%3D1%7D%5En+A_%7Bij%7D+p_j+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{f_i(P_1, &#92;dots, P_n)   = &#92;sum_{j=1}^n A_{ij} p_j }" class="latex" /></p>
<p>so</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+%3D+A+p+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) = A p " class="latex" /></p>
<p>Thus, the relative information will never increase if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28p+-+q%29+%5Ccdot+A+p+%5Cle+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(p - q) &#92;cdot A p &#92;le 0" class="latex" /></p>
<p>or in other words,</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+p+%5Cge+p+%5Ccdot+A+p++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad++%281%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A p &#92;ge p &#92;cdot A p  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad  (1) " class="latex" /></p>
<p>Now, this looks very similar to the conditions for an evolutionary stable strategy as stated in the Puzzle above.  <i>But it&#8217;s not the same!</i>  That&#8217;s the surprise twist.</p>
<p>Remember, the Puzzle says that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an evolutionarily stable state if for all mixed strategies <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge+p+%5Ccdot+A+q++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge p &#92;cdot A q  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad (2)" class="latex" /></p>
<p>and also</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%3D+p+%5Ccdot+A+q+%5C%3B+%5Cimplies+%5C%3B+q+%5Ccdot+A+p+%3E+p+%5Ccdot+A+p++%5Cqquad+%5C%3B+%283%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q = p &#92;cdot A q &#92;; &#92;implies &#92;; q &#92;cdot A p &gt; p &#92;cdot A p  &#92;qquad &#92;; (3)" class="latex" /></p>
<p>Note that condition (1), the one we want, is <i>neither</i> condition (2) <i>nor</i> condition (3)!  This drove me crazy for almost a day.</p>
<div align="center">
<img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/spiral_eyes.gif" alt="" /></div>
<p>I kept thinking I&#8217;d made a mistake, like mixing up <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> somewhere.  You&#8217;ve got to mind your p&#8217;s and q&#8217;s in this game!</p>
<p>But the solution turned out to be this.  After Maynard Smith came up with his definition of &#8216;evolutionarily stable state&#8217;, another guy came up with a different definition:</p>
<p>&bull; Bernhard Thomas, On evolutionarily stable sets, <i><a href="http://www.springerlink.com/content/g7812u72h6110m26/?MUD=MP">J. Math. Biology</a></i> <b>22</b> (1985), 105&#8211;115.</p>
<p>For him, an <b>evolutionarily stable strategy</b> obeys</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+q+%5Cge+p+%5Ccdot+A+q++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A q &#92;ge p &#92;cdot A q  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad (2)" class="latex" /></p>
<p>and also</p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%5Ccdot+A+p+%5Cge+p+%5Ccdot+A+p++%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad+%5Cqquad++%281%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;cdot A p &#92;ge p &#92;cdot A p  &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad &#92;qquad  (1) " class="latex" /></p>
<p>Condition (1) is stronger than condition (3), so he renamed Maynard Smith&#8217;s evolutionarily stable strategies <b>weakly  evolutionarily stable strategies</b>.  And condition (1) guarantees that the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> can never increase.  So, now we&#8217;re happy.</p>
<p>Except for one thing: why should we switch from Maynard Smith&#8217;s perfectly sensible concept of evolutionarily stable state to this new stronger one?  I don&#8217;t really know, except that</p>
<p>&bull; it&#8217;s not much stronger</p>
<p>and</p>
<p>&bull; it lets us prove the theorem we want!</p>
<p>So, it&#8217;s a small mystery for me to mull over.  If you have any good ideas, let me know.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/#comments">7 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;13)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10254 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-mathematics" id="post-10254">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/" rel="bookmark">Information Geometry (Part&nbsp;12)</a></h2>
				<small>24 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/">Last time</a> we saw that if a population evolves toward an &#8216;evolutionarily stable state&#8217;, then the amount of information our population has &#8216;left to learn&#8217; can never increase!  It must always decrease or stay the same.</p>
<p>This result sounds wonderful: it&#8217;s a lot like the second law of thermodynamics, which says entropy must always increase.  Of course there are some conditions for this wonderful result to hold.  The main condition is that the population evolves according to the <a href="http://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a>.  But the other is the existence of an evolutionarily stable state.   Last time I wrote down the rather odd-looking definition of &#8216;evolutionary stable state&#8217; without justifying it.   I need to do that soon.  But if you&#8217;ve never thought about evolutionary game theory, I think giving you a little background will help.  So today let me try that.</p>
<h3> Evolutionary game theory </h3>
<p>We&#8217;ve been thinking of evolution as similar to <i>inference</i> or <i>learning</i>.  In this analogy, organisms are like &#8216;hypotheses&#8217;, and the population &#8216;does experiments&#8217; to see if these hypotheses make &#8216;correct predictions&#8217; (i.e., can reproduce) or not.  The successful ones are reinforced while the unsuccessful ones are weeded out.  As a result, the population &#8216;learns&#8217;.  And under the conditions of the theorem we discussed last time, the relative information&#8212;the amount &#8216;left to learn&#8217;&#8212;goes down!</p>
<p>While you might object to various points of this analogy, it&#8217;s  useful&#8212;and that&#8217;s really all you can ask of an analogy.  It&#8217;s useful because it lets us steal chunks of math from the subjects of <a href="http://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a> and <a href="http://en.wikipedia.org/wiki/Machine_learning">machine learning</a> and apply them to the study of biodiversity and evolution!  This is what Marc Harper has been doing:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.  </p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>But now let&#8217;s bring in another analogy, also contained in Harper&#8217;s work.  We can also think of evolution as similar to a <i>game</i>.  In this analogy, organisms are like &#8216;strategies&#8217;&#8212;or if you prefer, they have strategies.  The winners get to reproduce, while the losers don&#8217;t.  <a href="http://en.wikipedia.org/wiki/John_Maynard_Smith">John Maynard Smith</a> started developing this analogy in 1973, and eventually wrote a whole book on it:</p>
<p>&bull; John Maynard Smith, <i>Evolution and the Theory of Games</i>, Cambridge University Press, 1982.</p>
<div align="center"><a href="http://en.wikipedia.org/wiki/John_Maynard_Smith"><img src="https://i0.wp.com/math.ucr.edu/home/baez/john_maynard_smith.jpg" alt="" /></a></div>
<p>As far as I can tell, evolutionary game theory has brought almost as many chunks of math <i>to</i> game theory as it has taken from it.  Maybe it&#8217;s just my ignorance showing, but it seems that game theory becomes considerably deeper when we think about games that many players play again and again, with the winners getting to reproduce, while the losers are eliminated.  </p>
<p>According to <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">William Sandholm</a>:</p>
<blockquote><p>
The birth of evolutionary game theory is marked by the publication of a series of papers by mathematical biologist John Maynard Smith. Maynard Smith adapted the methods of traditional game theory, which were created to model the behavior of rational economic agents, to the context of biological natural selection.  He proposed his notion of an evolutionarily stable strategy (ESS) as a way of explaining the existence of ritualized animal conflict.</p>
<p>Maynard Smith’s equilibrium concept was provided with an explicit dynamic foundation through a differential equation model introduced by Taylor and Jonker. Schuster and Sigmund, following Dawkins, dubbed this model the replicator dynamic, and recognized the close links between this game-theoretic dynamic and dynamics studied much earlier in population ecology and population genetics. By the 1980s, evolutionary game theory was a well-developed and firmly established modeling framework in biology.</p>
<p>Towards the end of this period, economists realized the value of the evolutionary approach to game theory in social science contexts, both as a method of providing foundations for the equilibrium concepts of traditional game theory, and as a tool for selecting among equilibria in games that admit more than one. Especially in its early stages, work by economists in evolutionary game theory hewed closely to the interpretation set out by biologists, with the notion of ESS and the replicator dynamic understood as modeling natural selection in populations of agents genetically programmed to behave in specific ways. But it soon became clear that models of essentially the same form could be used to study the behavior of populations of active decision makers.  Indeed, the two approaches sometimes lead to identical models: the replicator dynamic itself can be understood not only as a model of natural selection, but also as one of imitation of successful opponents.  </p>
<p>While the majority of work in evolutionary game theory has been undertaken by biologists and economists, closely related models have been applied to questions in a variety of fields, including transportation science, computer science, and sociology. Some paradigms from evolutionary game theory are close relatives of certain models from physics, and so have attracted the attention of workers in this field. All told, evolutionary game theory provides a common ground for workers from a wide range of disciplines.
</p></blockquote>
<h3> The Prisoner&#8217;s Dilemma </h3>
<p>In game theory, the most famous example is the <a href="http://en.wikipedia.org/wiki/Prisoner%27s_dilemma">Prisoner&#8217;s Dilemma</a>.  In its original form, this &#8216;game&#8217; is played just once:</p>
<blockquote><p>
Two men are arrested, but the police don&#8217;t have enough information to convict them.  So they separate the two men, and offer both the same deal: if one testifies against his partner (or <b>defects</b>), and the other remains silent (and thus <b>cooperates</b> with his partner), the defector goes free and the cooperator goes to jail for 12 months. If both remain silent, both are sentenced to only 1 month in jail for a minor charge. If they both defect, they both receive a 3-month sentence. Each prisoner must choose either to defect or cooperate with his partner in crime; neither gets to hear what the other decides.  What will they do?
</p></blockquote>
<p>Traditional game theory emphasizes the so-called &#8216;Nash equilibrium&#8217; for this game, in which both prisoners defect.  Why don&#8217;t they both cooperate?   They&#8217;d both be better off if they both cooperated.  However, for them to both cooperate is &#8216;unstable&#8217;: either one could shorten their sentence by defecting!  By definition, a <a href="http://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a> has the property that neither player can improve his situation by unilaterally changing his strategy.  </p>
<p>In the Prisoner&#8217;s Dilemma, the Nash equilibrium is not very nice: both parties would be happier if they&#8217;d only cooperate.  That&#8217;s why it&#8217;s called a &#8216;dilemma&#8217;.  Perhaps the most tragic example today is global warming.  Even if all players would be better off if all cooperate to reduce carbon emissions, any <i>one</i> will be better off if everybody <i>except themselves</i> cooperates while they emit more carbon.</p>
<p>For this and many other reasons, people have been interested in &#8216;solving&#8217; the Prisoner&#8217;s Dilemma: that is, finding reasons why cooperation might be favored over defection.  </p>
<p>This book got people really excited in seeing what evolutionary game theory has to say about the Prisoner&#8217;s Dilemma:</p>
<p>&bull; Robert Axelrod, <i>The Evolution of Cooperation</i>, Basic Books, New York, 1984.  (A related article with the same title is <a href="www-personal.umich.edu/~axe/research/Axelrod%20and%20Hamilton%20EC%201981.pdf">available online</a>.)</p>
<p>The idea is that under certain circumstances, strategies that are &#8216;nicer&#8217; than defection will gradually take over.  The most famous of these strategies is &#8216;tit for tat&#8217;, meaning that you cooperate the first time and after that do whatever your opponent just did.  I won&#8217;t go into this further, because it&#8217;s a big digression and I&#8217;m already digressing too far.  I&#8217;ll just mention that from the outlook of evolutionary game theory, the Prisoner&#8217;s Dilemma is still full of surprises.  Just this week, some fascinating new work has been causing a stir:</p>
<p>&bull; William Press and Freeman Dyson, <a href="http://edge.org/conversation/on-iterated-prisoner-dilemma">Iterated Prisoner&#8217;s Dilemma contains strategies that dominate any evolutionary opponent</a>, <i>Edge</i>, 18 June 2012.</p>
<p>I hope I&#8217;ve succeeded in giving you a vague superficial sense of the history of evolutionary game theory and why it&#8217;s interesting.  Next time I&#8217;ll get serious about the task at hand, which is to understand &#8216;evolutionarily stable strategies&#8217;.   If you want to peek ahead, try this nice paper:</p>
<p>&bull; William H. Sandholm, <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">Evolutionary game theory</a>, 12 November 2007.</p>
<p>This is where I got the long quote by Sandholm on the history of evolutionary game theory.  The original quote contained lots of references; if you&#8217;re interested in those, go to page 3 of this paper.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/#comments">10 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;12)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-10114 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-probability" id="post-10114">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/" rel="bookmark">Information Geometry (Part&nbsp;11)</a></h2>
				<small>7 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/">Last time</a> we saw that given a bunch of different species of self-replicating entities, the entropy of their population distribution can go either up or down as time passes.  This is true even in the pathetically simple case where all the replicators have constant fitness&mdash;so they don&#8217;t interact with each other, and don&#8217;t run into any &#8216;limits to growth&#8217;.</p>
<p>This is a bit of a bummer, since it would be nice to use entropy to explain how replicators are always extracting information from their environment, thanks to natural selection.</p>
<p>Luckily, a slight variant of entropy, called &#8216;relative entropy&#8217;, behaves better.  When our replicators have an &#8216;evolutionary stable state&#8217;, the relative entropy is <i>guaranteed to always change in the same direction</i> as time passes!</p>
<p>Thanks to Einstein, we&#8217;ve all heard that times and distances are relative.  But how is entropy relative?</p>
<p>It&#8217;s easy to understand if you think of entropy as lack of information.  Say I have a coin hidden under my hand.  I tell you it&#8217;s heads-up.  How much information did I just give you?  Maybe 1 bit?  That&#8217;s true if you know it&#8217;s a fair coin and I flipped it fairly before covering it up with my hand.  But what if you put the coin down there yourself a minute ago, heads up, and I just put my hand over it?  Then I&#8217;ve given you no information at all.  The difference is the choice of &#8216;prior&#8217;: that is, what probability distribution you attributed to the coin <i>before</i> I gave you my message.</p>
<p>My love affair with relative entropy began in college when my friend Bruce Smith and I read Hugh Everett&#8217;s thesis, <a href="http://www.pbs.org/wgbh/nova/manyworlds/pdf/dissertation.pdf"><i>The Relative State Formulation of Quantum Mechanics</i></a>.  This was the origin of what&#8217;s now often called the &#8216;many-worlds interpretation&#8217; of quantum mechanics.  But it also has a great introduction to relative entropy.  Instead of talking about &#8216;many worlds&#8217;, I wish people would say that Everett explained some of the mysteries of quantum mechanics using the fact that entropy is relative.</p>
<p>Anyway, it&#8217;s nice to see relative entropy showing up in biology.</p>
<h3> Relative Entropy </h3>
<div align="center">
<img src="http://math.ucr.edu/home/baez/mathematical/bertrand's_paradox.png" />
</div>
<p>Inscribe an equilateral triangle in a circle.  Randomly choose a line segment joining two points of this circle.  What is the probability that this segment is longer than a side of the triangle?</p>
<p>This puzzle is called <a href="http://en.wikipedia.org/wiki/Bertrand_paradox_%28probability%29">Bertrand&#8217;s paradox</a>, because different ways of solving it give different answers.   To crack the paradox, you need to realize that it&#8217;s meaningless to say you&#8217;ll &#8220;randomly&#8221; choose something until you say more about how you&#8217;re going to do it.</p>
<p>In other words, you can&#8217;t compute the probability of an event until you pick a recipe for computing probabilities.  Such a recipe is called a <a href="http://en.wikipedia.org/wiki/Probability_measure">probability measure</a>.</p>
<p>This applies to computing entropy, too!   The formula for entropy clearly involves a <a href="http://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a>, even when our set of events is finite:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Csum_i+p_i+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;sum_i p_i &#92;ln(p_i) " class="latex" /></p>
<p>But this formula conceals a fact that becomes obvious when our set of events is infinite.  Now the sum becomes an integral:</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Cint_X+p%28x%29+%5Cln%28p%28x%29%29+%5C%2C+d+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;int_X p(x) &#92;ln(p(x)) &#92;, d x" class="latex" /></p>
<p>And now it&#8217;s clear that this formula makes no sense until we choose the <a href="http://en.wikipedia.org/wiki/Measure_%28mathematics%29">measure</a> <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />   On a finite set we have a god-given choice of measure, called <a href="http://en.wikipedia.org/wiki/Counting_measure">counting measure</a>.  Integrals with respect to this are just sums.   But in general we don&#8217;t have such a god-given choice.  And even for finite sets, working with counting measure is a <i>choice</i>: we are <i>choosing</i> to believe that in the absence of further evidence, all options are equally likely.</p>
<p>Taking this fact into account, it seems like we need two things to compute entropy: a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x)" class="latex" />, and a measure <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />   That&#8217;s on the right track.  But an even better way to think of it is this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Cint_X++%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D+%5Cln+%5Cleft%28%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D%5Cright%29+%5C%2C+dx+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;int_X  &#92;frac{p(x) dx}{dx} &#92;ln &#92;left(&#92;frac{p(x) dx}{dx}&#92;right) &#92;, dx }" class="latex" /></p>
<p>Now we see the entropy depends <i>two</i> measures: the probability measure <img src="https://s0.wp.com/latex.php?latex=p%28x%29++dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x)  dx " class="latex" /> we care about, but also the measure <img src="https://s0.wp.com/latex.php?latex=d+x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d x." class="latex" />  Their ratio is important, but that&#8217;s not enough: we also need one of these measures to do the integral.  Above I used the measure <img src="https://s0.wp.com/latex.php?latex=dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dx" class="latex" /> to do the integral, but we can also use <img src="https://s0.wp.com/latex.php?latex=p%28x%29+dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x) dx" class="latex" /> if we write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Cint_X+%5Cln+%5Cleft%28%5Cfrac%7Bp%28x%29+dx%7D%7Bdx%7D%5Cright%29+p%28x%29+dx+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;int_X &#92;ln &#92;left(&#92;frac{p(x) dx}{dx}&#92;right) p(x) dx } " class="latex" /></p>
<p>Either way, we are computing the entropy of one measure <i>relative to another</i>.  So we might as well admit it, and talk about <b>relative entropy</b>.</p>
<p>The entropy of the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu" class="latex" /> <b>relative to</b> the measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;nu" class="latex" /> is defined by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+S%28d+%5Cmu%2C+d+%5Cnu%29+%26%3D%26+%5Cdisplaystyle%7B+-+%5Cint_X+%5Cfrac%7Bd+%5Cmu%28x%29+%7D%7Bd+%5Cnu%28x%29%7D+%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29++d%5Cnu%28x%29+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cint_X++%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29+d%5Cmu%28x%29+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} S(d &#92;mu, d &#92;nu) &amp;=&amp; &#92;displaystyle{ - &#92;int_X &#92;frac{d &#92;mu(x) }{d &#92;nu(x)} &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right)  d&#92;nu(x) } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;int_X  &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right) d&#92;mu(x) } &#92;end{array} " class="latex" /></p>
<p>The second formula is simpler, but the first looks more like summing <img src="https://s0.wp.com/latex.php?latex=-p+%5Cln%28p%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-p &#92;ln(p)," class="latex" /> so they&#8217;re both useful.</p>
<p>Since we&#8217;re taking entropy to be lack of information, we can also get rid of the minus sign and define <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"><b>relative information</b></a> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+I%28d+%5Cmu%2C+d+%5Cnu%29+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_X+%5Cfrac%7Bd+%5Cmu%28x%29+%7D%7Bd+%5Cnu%28x%29%7D+%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29++d%5Cnu%28x%29+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B++%5Cint_X++%5Cln+%5Cleft%28%5Cfrac%7Bd+%5Cmu%28x%29%7D%7B+d%5Cnu%28x%29+%7D%5Cright%29+d%5Cmu%28x%29+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} I(d &#92;mu, d &#92;nu) &amp;=&amp; &#92;displaystyle{ &#92;int_X &#92;frac{d &#92;mu(x) }{d &#92;nu(x)} &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right)  d&#92;nu(x) } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{  &#92;int_X  &#92;ln &#92;left(&#92;frac{d &#92;mu(x)}{ d&#92;nu(x) }&#92;right) d&#92;mu(x) } &#92;end{array} " class="latex" /></p>
<p>If you thought something was randomly distributed according to the probability measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cnu%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;nu," class="latex" /> but then you you discover it&#8217;s randomly distributed according to the probability measure <img src="https://s0.wp.com/latex.php?latex=d+%5Cmu%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;mu," class="latex" /> how much information have you gained?  The answer is <img src="https://s0.wp.com/latex.php?latex=I%28d%5Cmu%2Cd%5Cnu%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(d&#92;mu,d&#92;nu)." class="latex" /></p>
<p>For more on relative entropy, read <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">Part 6</a> of this series.  I gave some examples illustrating how it works.  Those should convince you that it&#8217;s a useful concept.</p>
<p>Okay: now let&#8217;s switch back to a more lowbrow approach.  In the case of a finite set, we can revert to thinking of our two measures as probability distributions, and write the information gain as</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Cdisplaystyle%7B++%5Csum_i++%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i+%7D%5Cright%29+q_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;displaystyle{  &#92;sum_i  &#92;ln &#92;left(&#92;frac{q_i}{p_i }&#92;right) q_i} " class="latex" /></p>
<p>If you want to sound like a Bayesian, call <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Prior_probability">prior probability distribution</a> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution">posterior probability distribution</a>.  Whatever you call them, <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> is the amount of information you get if you thought <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and someone tells you &#8220;no, <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />!”</p>
<p>We&#8217;ll use this idea to think about how a population gains information about its environment as time goes by, thanks to natural selection.  The rest of this post will be an exposition of Theorem 1 in this paper:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>Harper says versions of this theorem ave previously appeared in work by Ethan Akin, and independently in work by Josef Hofbauer and Karl Sigmund.  He also credits others <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15724">here</a>.  An idea this good is rarely noticed by just one person.</p>
<h3> The change in relative information </h3>
<p>So: consider <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different species of replicators.   Let <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> be the population of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, and assume these populations change according to the <b><a href="http://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) &#92;, P_i } " class="latex" /></p>
<p>where each function <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> depends smoothly on all the populations.  And as usual, we let</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_j+P_j%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_j P_j} } " class="latex" /></p>
<p>be the fraction of replicators in the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.</p>
<p>Let&#8217;s study the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is some fixed probability distribution.   We&#8217;ll see something great happens when <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a stable equilibrium solution of the replicator equation.  In this case, the relative information can never increase!  It can only decrease or stay constant.</p>
<p>We&#8217;ll think about what all this <i>means</i> later.  First, let&#8217;s see that it&#8217;s true!  Remember,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+I%28q%2Cp%29+%26%3D%26+%5Cdisplaystyle%7B+%5Csum_i++%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7B+p_i+%7D%5Cright%29+q_i+%7D++%5C%5C+%5C%5C+%26%3D%26++%5Cdisplaystyle%7B+%5Csum_i++%5CBig%28%5Cln%28q_i%29+-+%5Cln%28p_i%29+%5CBig%29+q_i+%7D+%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} I(q,p) &amp;=&amp; &#92;displaystyle{ &#92;sum_i  &#92;ln &#92;left(&#92;frac{q_i}{ p_i }&#92;right) q_i }  &#92;&#92; &#92;&#92; &amp;=&amp;  &#92;displaystyle{ &#92;sum_i  &#92;Big(&#92;ln(q_i) - &#92;ln(p_i) &#92;Big) q_i } &#92;end{array}" class="latex" /></p>
<p>and only <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> depends on time, not <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" />, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29%7D++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cfrac%7Bd%7D%7Bdt%7D+%5Csum_i+%5Cln%28p_i%29++q_i+%7D%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Csum_i+%5Cfrac%7B%5Cdot%7Bp%7D_i%7D%7Bp_i%7D+%5C%2C+q_i+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;frac{d}{dt} I(q,p)}  &amp;=&amp; &#92;displaystyle{ - &#92;frac{d}{dt} &#92;sum_i &#92;ln(p_i)  q_i }&#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;sum_i &#92;frac{&#92;dot{p}_i}{p_i} &#92;, q_i } &#92;end{array} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i" class="latex" /> is the rate of change of the probability <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /> We saw a nice formula for this in <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/">Part 9</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, p_i }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_i%28P%29+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P) = f_i(P_1, &#92;dots, P_n) " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_i+f_i%28P%29+p_i++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_i f_i(P) p_i  } " class="latex" /></p>
<p>is the <b>mean fitness</b> of the species.  So, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%7D+%3D+%5Cdisplaystyle%7B+-+%5Csum_i+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+q_i+%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) } = &#92;displaystyle{ - &#92;sum_i &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, q_i }  " class="latex" /></p>
<p>Nice, but we can fiddle with this expression to get something more enlightening.  Remember, the numbers <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> sum to one.  So:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%7D+%26%3D%26++%5Cdisplaystyle%7B++%5Clangle+f%28P%29+%5Crangle+-+%5Csum_i+f_i%28P%29+q_i++%7D+%5C%5C++%5C%5C+%26%3D%26+%5Cdisplaystyle%7B++%5Csum_i+f_i%28P%29+%28p_i+-+q_i%29++%7D++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;frac{d}{dt} I(q,p) } &amp;=&amp;  &#92;displaystyle{  &#92;langle f(P) &#92;rangle - &#92;sum_i f_i(P) q_i  } &#92;&#92;  &#92;&#92; &amp;=&amp; &#92;displaystyle{  &#92;sum_i f_i(P) (p_i - q_i)  }  &#92;end{array} " class="latex" /></p>
<p>where in the last step I used the definition of the mean fitness.  This result looks even cuter if we treat the numbers <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)" class="latex" /> as the components of a vector <img src="https://s0.wp.com/latex.php?latex=f%28P%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)," class="latex" /> and similarly for the numbers <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />  Then we can use the dot product of vectors to say</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28q%2Cp%29+%3D+f%28P%29+%5Ccdot+%28p+-+q%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(q,p) = f(P) &#92;cdot (p - q) }" class="latex" /></p>
<p>So, the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> will always decrease if</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+%5Ccdot+%28p+-+q%29+%5Cle+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) &#92;cdot (p - q) &#92;le 0" class="latex" /></p>
<p>for all choices of the population <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" /></p>
<p>And now something really nice happens: this is also the condition for <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> to be an <a href="http://en.wikipedia.org/wiki/Evolutionarily_stable_state">evolutionarily stable state</a>.  This concept goes back to <a href="http://en.wikipedia.org/wiki/John_Maynard_Smith">John Maynard Smith</a>, the founder of evolutionary game theory.  In 1982 he wrote:</p>
<blockquote><p>
A population is said to be in an evolutionarily stable state if its genetic composition is restored by selection after a disturbance, provided the disturbance is not too large.
</p></blockquote>
<p>I will explain the math next time&#8212;I need to straighten out some things in my mind first.  But the basic idea is compelling: an evolutionarily stable state is like a situation where our replicators &#8216;know all there is to know&#8217; about the environment and each other.  In any other state, the population has &#8216;something left to learn&#8217;&#8212;and the amount left to learn is the relative information we&#8217;ve been talking about!  But as time goes on, the information still left to learn <i>decreases!</i></p>
<p>Note: in the real world, nature has never found an evolutionarily stable state&#8230; except sometimes approximately, on sufficiently short time scales, in sufficiently small regions.   So we are still talking about an idealization of reality!   But that&#8217;s okay, as long as we know it.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/#comments">58 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;11)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/8/" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/6/" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the information and entropy category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/7/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

				<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = JSON.parse( decodeURIComponent( '%7B%22settings%22%3A%7B%22id%22%3A%22content%22%2C%22ajaxurl%22%3A%22https%3A%5C%2F%5C%2Fjohncarlosbaez.wordpress.com%5C%2F%3Finfinity%3Dscrolling%22%2C%22type%22%3A%22scroll%22%2C%22wrapper%22%3Atrue%2C%22wrapper_class%22%3A%22infinite-wrap%22%2C%22footer%22%3Atrue%2C%22click_handle%22%3A%221%22%2C%22text%22%3A%22Older%20posts%22%2C%22totop%22%3A%22Scroll%20back%20to%20top%22%2C%22currentday%22%3A%2207.06.12%22%2C%22order%22%3A%22DESC%22%2C%22scripts%22%3A%5B%5D%2C%22styles%22%3A%5B%5D%2C%22google_analytics%22%3Afalse%2C%22offset%22%3A7%2C%22history%22%3A%7B%22host%22%3A%22johncarlosbaez.wordpress.com%22%2C%22path%22%3A%22%5C%2Fcategory%5C%2Finformation-and-entropy%5C%2Fpage%5C%2F%25d%5C%2F%22%2C%22use_trailing_slashes%22%3Atrue%2C%22parameters%22%3A%22%22%7D%2C%22query_args%22%3A%7B%22paged%22%3A7%2C%22category_name%22%3A%22information-and-entropy%22%2C%22error%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A0%2C%22post_parent%22%3A%22%22%2C%22subpost%22%3A%22%22%2C%22subpost_id%22%3A%22%22%2C%22attachment%22%3A%22%22%2C%22attachment_id%22%3A0%2C%22name%22%3A%22%22%2C%22pagename%22%3A%22%22%2C%22page_id%22%3A0%2C%22second%22%3A%22%22%2C%22minute%22%3A%22%22%2C%22hour%22%3A%22%22%2C%22day%22%3A0%2C%22monthnum%22%3A0%2C%22year%22%3A0%2C%22w%22%3A0%2C%22tag%22%3A%22%22%2C%22cat%22%3A23375499%2C%22tag_id%22%3A%22%22%2C%22author%22%3A%22%22%2C%22author_name%22%3A%22%22%2C%22feed%22%3A%22%22%2C%22tb%22%3A%22%22%2C%22meta_key%22%3A%22%22%2C%22meta_value%22%3A%22%22%2C%22preview%22%3A%22%22%2C%22s%22%3A%22%22%2C%22sentence%22%3A%22%22%2C%22title%22%3A%22%22%2C%22fields%22%3A%22%22%2C%22menu_order%22%3A%22%22%2C%22embed%22%3A%22%22%2C%22category__in%22%3A%5B%5D%2C%22category__not_in%22%3A%5B%5D%2C%22category__and%22%3A%5B%5D%2C%22post__in%22%3A%5B%5D%2C%22post__not_in%22%3A%5B%5D%2C%22post_name__in%22%3A%5B%5D%2C%22tag__in%22%3A%5B%5D%2C%22tag__not_in%22%3A%5B%5D%2C%22tag__and%22%3A%5B%5D%2C%22tag_slug__in%22%3A%5B%5D%2C%22tag_slug__and%22%3A%5B%5D%2C%22post_parent__in%22%3A%5B%5D%2C%22post_parent__not_in%22%3A%5B%5D%2C%22author__in%22%3A%5B%5D%2C%22author__not_in%22%3A%5B%5D%2C%22lazy_load_term_meta%22%3Afalse%2C%22posts_per_page%22%3A10%2C%22ignore_sticky_posts%22%3Afalse%2C%22suppress_filters%22%3Afalse%2C%22cache_results%22%3Afalse%2C%22update_post_term_cache%22%3Atrue%2C%22update_post_meta_cache%22%3Atrue%2C%22post_type%22%3A%22%22%2C%22nopaging%22%3Afalse%2C%22comments_per_page%22%3A%22100%22%2C%22no_found_rows%22%3Afalse%2C%22order%22%3A%22DESC%22%7D%2C%22query_before%22%3A%222021-09-26%2017%3A25%3A19%22%2C%22last_post_date%22%3A%222012-06-07%2009%3A59%3A46%22%2C%22body_class%22%3A%22infinite-scroll%20neverending%22%2C%22loading_text%22%3A%22Loading%20new%20page%22%2C%22stats%22%3A%22blog%3D12777403%26v%3Dwpcom%26tz%3D0%26user_id%3D0%26subd%3Djohncarlosbaez%26x_pagetype%3Dinfinite%22%7D%7D' ) );
		//]]>
		</script>
		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>
		<div id="infinite-footer">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://johncarlosbaez.wordpress.com/" rel="home">
						Azimuth					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a> 				</div>
			</div>
		</div><!-- #infinite-footer -->
		
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s1.wp.com/_static/??-eJyFy00OQDAQQOELGUP8xUKcpWoiZVTTadO4vVhY2LB8L/kwOdCHDWQD7hEcx8VYQa38EYUYJRlHHqZoZ6Zci2T4I1YKTukNnvGFgmGaYVHM5M933Wzch7Ktir5p+rpbL+lvP34=?cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F06%2F07%2Finformation-geometry-part-11%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s0.wp.com/_static/??-eJyNkN1uwyAMhV9oDkouKvVi2rMQcCNT/oZN07z9yNRMbTZFu8LH8J2DreYMJkXBKMqxsngjg/neOX5TT1ehQvZ1osiK4oUiyfJTHLzVNlCEURcVNAuWVoEUba68h1q2+6xYlsfRzdmkALmk+wIFW49lYygaXy3yCjWJYUTbtaCDj8xkJxRWXEc2hbJQivyXH8+XNDo0sjfbnG5kMSnNvNq5h84F+ddIT/FGl1QZvXIouY0PW+OAEfJoYdLer1t5Uf9Igu/97WTjPsJ7fxrO534YTr37Ai2cwB8='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			(function() {
				var extend = function(out) {
					out = out || {};

					for (var i = 1; i < arguments.length; i++) {
						if (!arguments[i])
						continue;

						for (var key in arguments[i]) {
						if (arguments[i].hasOwnProperty(key))
							out[key] = arguments[i][key];
						}
					}

					return out;
				};
				extend( window.infiniteScroll.settings.scripts, ["postmessage","mobile-useragent-info","rlt-proxy","jquery-core","jquery-migrate","jquery","wpcom-actionbar-placeholder","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","wpcom-masterbar-tracks-js","jquery.wpcom-proxy-request","wp-embed","jetpack-subscriptions-js","swfobject","videopress","jetpack-carousel","tiled-gallery","carousel-wpcom"] );
				extend( window.infiniteScroll.settings.styles, ["the-neverending-homepage","infinity-contempt","wp-block-library","mediaelement","wp-mediaelement","jetpack-layout-grid","jetpack-ratings","coblocks-frontend","wpcom-core-compat-playlist-styles","wpcom-text-widget-styles","wpcom-bbpress2-staff-css","contempt","geo-location-flair","reblogging","a8c-global-print","h4-global","global-styles","jetpack-global-styles-frontend-style","jetpack-carousel-swiper-css","jetpack-carousel","tiled-gallery"] );
			})();
		</script>
				<span id="infinite-aria" aria-live="polite"></span>
		<script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZnVzVKW2VqN2ZlfC5WbGQ/MTY9dmIxVytZNy5qYUxVXWRxMml5PURScmNEN1BzfnFKV1FEVEpPS2JwRU1GVj9ELFM9JXNjM24tais1N1MlejRVRHl2X3kzSS9RUUZ5T2dDUSUrP0loVT1IZ1l1bENTcTBxb000dnlba1dOL3NfWXYwRFIwd0pvSjhDa0NyRkEsNmZTUndmLXxXUjlqZExMd0Q1VkU9OGs9clhPcVc5bGFYJngrRE9lUC5tRHNGOWIxMGZNLnBl'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>