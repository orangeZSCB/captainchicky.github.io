<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>information and entropy | Azimuth | Page 6</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; information and entropy Category Feed" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IlqVOdbxwfBYOEVOBZSBYeXuz1e6sVrveMDl9yU9AHrOwUyJIJGMTOTSPqUpMz5iQ+mIMttYbuVFMLxChytyMPJXFTBfcGfKNXQPFc6aAfFNq2A9KmobBSRMm+yoCmqJLl5V6gKURJhua4zGHKiM41BB46qxo5eSgOxQRwGvbh4hpG+fc2v8G/S3+pJSbAWU9S9Z9aiR8QfdD9r9bFE2YfN3A7fSF7QY18t4cVlqC4nd29WTzzjkes768+BXsiM4DMV7PtiB4v45kHiOMyQVqFXxGbFF8/pSZe4qP6u5W3T/sx3F3+ACgX/YH?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2014%2F01%2F22%2Frelative-entropy-in-evolutionary-dynamics%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/information-and-entropy\/page\/6\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Finformation-and-entropy%2Fpage%2F6%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2014%2F01%2F22%2Frelative-entropy-in-evolutionary-dynamics%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="information and entropy &#8211; Page 6 &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about information and entropy written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive paged category category-information-and-entropy category-23375499 paged-6 category-paged-6 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-17311 post type-post status-publish format-standard hentry category-biology category-game-theory category-information-and-entropy category-mathematics" id="post-17311">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/" rel="bookmark">Relative Entropy in Evolutionary&nbsp;Dynamics</a></h2>
				<small>22 January, 2014</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://www.marcallenharper.com/">Marc Harper</a></b></i></p>
<p>In John&#8217;s <a href="http://math.ucr.edu/home/baez/information/index.html">information geometry</a> series, he mentioned some of my work in evolutionary dynamics. Today I&#8217;m going to tell you about some exciting extensions!</p>
<h3> The replicator equation </h3>
<p>First a little refresher. For a population of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> replicating types, such as individuals with different eye colors or a gene with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> distinct <a href="http://en.wikipedia.org/wiki/Allele">alleles</a>, the &#8216;replicator equation&#8217; expresses the main idea of natural selection: the relative rate of growth of each type should be proportional to the difference between the fitness of the type and the mean fitness in the population.</p>
<p>To see why this equation should be true, let <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> be the population of individuals of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type, which we allow to be any nonnegative real number.  We can list all these numbers and get a vector:</p>
<p><img src="https://s0.wp.com/latex.php?latex=P+%3D+%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P = (P_1, &#92;dots, P_n) " class="latex" /></p>
<p>The <b>Lotka&ndash;Volterra equation</b> is a very general rule for how these numbers can change with time:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P%29+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P) P_i } " class="latex" /></p>
<p>Each population grows at a rate proportional to itself, where the &#8216;constant of proportionality&#8217;, <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)," class="latex" /> is not necessarily constant: it can be any real-valued function of <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" />  This function is called the <b>fitness</b> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type.  Taken all together, these functions <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> are called the <b>fitness landscape</b>.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> be the fraction of individuals who are of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_%7Bi+%3D1%7D%5En+P_i+%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_{i =1}^n P_i } }" class="latex" /></p>
<p>These numbers <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are between 0 and 1, and they add up to 1.  So, we can also think of them as probabilities: <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the probability that a randomly chosen individual is of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type.  This is how probability theory, and eventually entropy, gets into the game.</p>
<p>Again, we can bundle these numbers into a vector:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28p_1%2C+%5Cdots%2C+p_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (p_1, &#92;dots, p_n) " class="latex" /></p>
<p>which we call the <b>population distribution</b>.  It turns out that the Lotka&ndash;Volterra equation implies the <b>replicator equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left( f_i(P) - &#92;langle f(P) &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_%7Bi+%3D1%7D%5En++f_i%28P%29++p_i++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_{i =1}^n  f_i(P)  p_i  } " class="latex" /></p>
<p>is the <b>mean fitness</b> of all the individuals.  You can see the proof in <a href="http://math.ucr.edu/home/baez/information/information_geometry_9.html">Part 9</a> of the information geometry series.</p>
<p>By the way: if each fitness <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)" class="latex" /> only depends on the fraction of individuals of each type, not the total numbers, we can write the replicator equation in a simpler way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i%28p%29+-+%5Clangle+f%28p%29+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left( f_i(p) - &#92;langle f(p) &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>From now on, when talking about this equation, that&#8217;s what I&#8217;ll do.</p>
<p>Anyway, the take-home message is this: the replicator equation says the fraction of individuals of any type changes at a rate proportional to fitness of that type minus the mean fitness.</p>
<p>Now, it has been known since the late 1970s or early 1980s, thanks to the work of Akin, Bomze, Hofbauer, Shahshahani, and others, that the replicator equation has some very interesting properties.   For one thing, it often makes &#8216;relative entropy&#8217; decrease.   For another, it&#8217;s often an example of &#8216;gradient flow&#8217;.   Let&#8217;s look at both of these in turn, and then talk about some new generalizations of these facts.</p>
<h3> Relative entropy as a Lyapunov function </h3>
<p>I mentioned that we can think of a population distribution as a probability distribution.  This lets us take ideas from probability theory and even information theory and apply them to evolutionary dynamics!  For example, given two population distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> the <b>information</b> of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> <b>relative to</b> <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Cdisplaystyle%7B+%5Csum_i+q_i+%5Cln+%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i+%7D%5Cright%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;displaystyle{ &#92;sum_i q_i &#92;ln &#92;left(&#92;frac{q_i}{p_i }&#92;right)} " class="latex" /></p>
<p>This measures how much information you gain if you have a hypothesis about some state of affairs given by the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> and then someone tells you &#8220;no, the best hypothesis is <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />!‚Äù</p>
<p>It may seem weird to treat a <i>population distribution</i> as a <i>hypothesis</i>, but this turns out to be a good idea.  Evolution can then be seen as a learning process: a process of improving the hypothesis.</p>
<p>We can make this precise by seeing how the relative information changes with the passage of time.  Suppose we have two population distributions <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />   Suppose <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is fixed, while <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> evolves in time according to the replicator equation.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bd+t%7D+I%28q%2Cp%29++%3D++%5Csum_i+f_i%28P%29+%28p_i+-+q_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d}{d t} I(q,p)  =  &#92;sum_i f_i(P) (p_i - q_i) } " class="latex" /></p>
<p>For the proof, see <a href="https://johncarlosbaez.wordpress.com/2012/06/07/information-geometry-part-11/">Part 11</a> of the information geometry series.</p>
<p>So, the information of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> will decrease as <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> evolves according to the replicator equation if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Csum_i+f_i%28P%29+%28p_i+-+q_i%29+%7D+%5Cle+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;sum_i f_i(P) (p_i - q_i) } &#92;le 0 " class="latex" /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> makes this true for all <img src="https://s0.wp.com/latex.php?latex=p%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p," class="latex" /> we say <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an <b>evolutionarily stable state</b>.   For some reasons why, see <a href="https://johncarlosbaez.wordpress.com/2012/06/26/information-geometry-part-13/">Part 13</a>.</p>
<p>What matters now is that when <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is an evolutionarily stable state, <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> says how much information the population has &#8216;left to learn&#8217;&mdash;and we&#8217;re seeing that this always <i>decreases</i>.  Moreover, it turns out that we always have</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) &#92;ge 0" class="latex" /></p>
<p>and <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = 0" class="latex" /> precisely when <img src="https://s0.wp.com/latex.php?latex=p+%3D+q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = q." class="latex" /></p>
<p>People summarize all this by saying that relative information is a &#8216;Lyapunov function&#8217;.  Very roughly, a Lyapunov function is something that decreases with the passage of time, and is zero only at the unique stable state.   To be a bit more precise, suppose we have a differential equation like</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%7D%7Bd+t%7D+x%28t%29+%3D+v%28x%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d}{d t} x(t) = v(x(t)) } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=x%28t%29+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) &#92;in &#92;mathbb{R}^n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> is some smooth vector field on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n." class="latex" />  Then a smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=V+%3A+%5Cmathbb%7BR%7D%5En+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V : &#92;mathbb{R}^n &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>is a <b>Lyapunov function</b> if</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V%28x%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x) &#92;ge 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /></p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=V%28x%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(x) = 0" class="latex" /> iff <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is some particular point <img src="https://s0.wp.com/latex.php?latex=x_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_0" class="latex" /></p>
<p>and</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+V%28x%28t%29%29+%5Cle+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} V(x(t)) &#92;le 0 }" class="latex" /> for every solution of our differential equation.</p>
<p>In this situation, the point <img src="https://s0.wp.com/latex.php?latex=x_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_0" class="latex" /> is a stable equilibrium for our differential equation: this is <b><a href="http://control.ee.ethz.ch/~ifalst/docs/lyapunov.pdf">Lyapunov&#8217;s theorem</a></b>.</p>
<h3> The replicator equation as a gradient flow equation </h3>
<p>The basic idea of Lyapunov&#8217;s theorem is that when a ball likes to roll downhill and the landscape has just one bottom point, that point will be the unique stable equilibrium for the ball.</p>
<p>The idea of gradient flow is similar, but different: sometimes things like to roll downhill <i>as efficiently as possible</i>: they move in the exactly the <i>best direction</i> to make some quantity smaller!  Under certain conditions, the replicator equation is an example of this phenomenon.</p>
<p>Let&#8217;s fill in some details.  For starters, suppose we have some function</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3A+%5Cmathbb%7BR%7D%5En+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F : &#92;mathbb{R}^n &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>Think of <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> as &#8216;height&#8217;.  Then the <b>gradient flow equation</b> says how a point <img src="https://s0.wp.com/latex.php?latex=x%28t%29+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x(t) &#92;in &#92;mathbb{R}^n" class="latex" /> will move if it&#8217;s always trying its very best to go downhill:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+x%28t%29+%3D+-+%5Cnabla+V%28x%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} x(t) = - &#92;nabla V(x(t)) } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla" class="latex" /> is the usual gradient in Euclidean space:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cnabla+V+%3D+%5Cleft%28%5Cpartial_1+V%2C+%5Cdots%2C+%5Cpartial_n+V+%5Cright%29++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;nabla V = &#92;left(&#92;partial_1 V, &#92;dots, &#92;partial_n V &#92;right)  } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cpartial_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;partial_i" class="latex" /> is short for the partial derivative with respect to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th coordinate.</p>
<p>The interesting thing is that under certain conditions, the replicator equation is an example of a gradient flow equation&#8230; but typically <i>not</i> one where <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla" class="latex" /> is the usual gradient in Euclidean space.  Instead, it&#8217;s the gradient on some other space, the space of all population distributions, which has a non-Euclidean geometry!</p>
<p>The space of all population distributions is a <a href="https://en.wikipedia.org/wiki/Simplex">simplex</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7B+p+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+p_i+%5Cge+0%2C+%5C%3B+%5Csum_%7Bi+%3D+1%7D%5En+p_i+%3D+1+%5C%7D+.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{ p &#92;in &#92;mathbb{R}^n : &#92;; p_i &#92;ge 0, &#92;; &#92;sum_{i = 1}^n p_i = 1 &#92;} . " class="latex" /></p>
<p>For example, it&#8217;s an equilateral triangle when <img src="https://s0.wp.com/latex.php?latex=n+%3D+3.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 3." class="latex" />  The equilateral triangle looks flat, but if we measure distances another way it becomes round, exactly like a portion of a sphere, and that&#8217;s the non-Euclidean geometry we need!</p>
<div align="center"><img width="400" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/simplex_eighth-sphere.png" /></div>
<p>In fact this trick works in any dimension.  The idea is to give the simplex a special <a href="https://en.wikipedia.org/wiki/Metric_tensor">Riemannian metric</a>, the &#8216;Fisher information metric&#8217;.  The usual metric on Euclidean space is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7Bi+j%7D+%3D+%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bccl%7D+1+%26+%5Cmathrm%7B+if+%7D+%26+i+%3D+j+%5C%5C+++++++++++++++++++++++++++++++++++++++0+%26%5Cmathrm%7B+if+%7D+%26+i+%5Cne+j+%5Cend%7Barray%7D+%5Cright.+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta_{i j} = &#92;left&#92;{&#92;begin{array}{ccl} 1 &amp; &#92;mathrm{ if } &amp; i = j &#92;&#92;                                       0 &amp;&#92;mathrm{ if } &amp; i &#92;ne j &#92;end{array} &#92;right. " class="latex" /></p>
<p>This simply says that two standard basis vectors like <img src="https://s0.wp.com/latex.php?latex=%280%2C1%2C0%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0,1,0,0)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%280%2C0%2C1%2C0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0,0,1,0)" class="latex" /> have dot product zero if the 1&#8217;s are in different places, and one if they&#8217;re in the same place.  The <b>Fisher information metric</b> is a bit more complicated:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+g_%7Bi+j%7D+%3D+%5Cfrac%7B%5Cdelta_%7Bi+j%7D%7D%7Bp_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ g_{i j} = &#92;frac{&#92;delta_{i j}}{p_i} } " class="latex" /></p>
<p>As before, <img src="https://s0.wp.com/latex.php?latex=g_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{i j}" class="latex" /> is a formula for the dot product of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th and <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th standard basis vectors, but now it depends on where you are in the simplex of population distributions.</p>
<p>We saw how this formula arises from information theory back in <a href="http://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>.  I won&#8217;t repeat the calculation, but the idea is this.  Fix a population distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and consider the information of another one, say <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> relative to this.  We get <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)." class="latex" />  If <img src="https://s0.wp.com/latex.php?latex=q+%3D+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = p" class="latex" /> this is zero:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.+I%28q%2Cp%29%5Cright%7C_%7Bq+%3D+p%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left. I(q,p)&#92;right|_{q = p} = 0 } " class="latex" /></p>
<p>and this point is a local minimum for the relative information. So, the first derivative of <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" /> as we change <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> must be zero:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+I%28q%2Cp%29+%5Cright%7C_%7Bq+%3D+p%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left. &#92;frac{&#92;partial}{&#92;partial q_i} I(q,p) &#92;right|_{q = p} = 0 } " class="latex" /></p>
<p>But the second derivatives are not zero. In fact, since we&#8217;re at a local minimum, it should not be surprising that we get a positive definite <a href="https://en.wikipedia.org/wiki/Hessian_matrix">matrix of second derivatives</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++g_%7Bi+j%7D+%3D+%5Cleft.+%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+q_i+%5Cpartial+q_j%7D+I%28q%2Cp%29+%5Cright%7C_%7Bq+%3D+p%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  g_{i j} = &#92;left. &#92;frac{&#92;partial^2}{&#92;partial q_i &#92;partial q_j} I(q,p) &#92;right|_{q = p} = 0 } " class="latex" /></p>
<p>And, this is the Fisher information metric!  So, the Fisher information metric is a way of taking dot products between vectors in the simplex of population distribution that&#8217;s based on the concept of relative information.</p>
<p>This is not the place to explain <a href="https://en.wikipedia.org/wiki/Riemannian_geometry">Riemannian geometry</a>, but any metric gives a way to measure angles and distances, and thus a way to define the gradient of a function.  After all, the gradient of a function should point at right angles to the level sets of that function, and its length should equal the slope of that function:</p>
<div align="center">
<a href="http://www.math.udel.edu/~driscoll/teaching/243/maple/Gradients.html"><img src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/gradient.gif" /></a>
</div>
<p>So, if we change our way of measuring angles and distances, we get a new concept of gradient!  The <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th component of this new gradient vector field turns out to b</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5Cnabla_g+V%29%5Ei+%3D+g%5E%7Bi+j%7D+%5Cpartial_j+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;nabla_g V)^i = g^{i j} &#92;partial_j V" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=g%5E%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g^{i j}" class="latex" /> is the inverse of the matrix <img src="https://s0.wp.com/latex.php?latex=g_%7Bi+j%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{i j}," class="latex" /> and we sum over the repeated index <img src="https://s0.wp.com/latex.php?latex=j.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j." class="latex" />   As a sanity check, make sure you see why this is the usual Euclidean gradient when <img src="https://s0.wp.com/latex.php?latex=g_%7Bi+j%7D+%3D+%5Cdelta_%7Bi+j%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g_{i j} = &#92;delta_{i j}." class="latex" /></p>
<p>Now suppose the fitness landscape is the good old Euclidean gradient of some function.  Then it turns out that the replicator equation is a special case of gradient flow on the space of population distributions&#8230; but where we use the Fisher information metric to define our concept of gradient!</p>
<p>To get a feel for this, it&#8217;s good to start with the Lotka&ndash;Volterra equation, which describes how the total number of individuals of each type changes.  Suppose the fitness landscape is the Euclidean gradient of some function <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f_i%28P%29+%3D+%5Cfrac%7B%5Cpartial+V%7D%7B%5Cpartial+P_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f_i(P) = &#92;frac{&#92;partial V}{&#92;partial P_i} } " class="latex" /></p>
<p>Then the Lotka&ndash;Volterra equation becomes this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+%5Cfrac%7B%5Cpartial+V%7D%7B%5Cpartial+P_i%7D+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = &#92;frac{&#92;partial V}{&#92;partial P_i} &#92;, P_i } " class="latex" /></p>
<p>This doesn&#8217;t look like the gradient flow equation, thanks to that annoying <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> on the right-hand side!  It certainly ain&#8217;t the gradient flow coming from the function <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> and the usual Euclidean gradient.  However, it <i>is</i> gradient flow coming from <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> and some <i>other</i> metric on the space</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5C%7B+P+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+P_i+%5Cge+0+%5C%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{ P &#92;in &#92;mathbb{R}^n : &#92;; P_i &#92;ge 0 &#92;}  " class="latex" /></p>
<p>For a proof, and the formula for this other metric, see Section 3.7 in this survey:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>Now let&#8217;s turn to the replicator equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i%28p%29++-+%5Clangle+f%28p%29+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left( f_i(p)  - &#92;langle f(p) &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>Again, if the fitness landscape is a Euclidean gradient, we can rewrite the replicator equation as a gradient flow equation&#8230; but again, not with respect to the Euclidean metric.  This time we need to use the Fisher information metric! I sketch a proof in my paper above.</p>
<p>In fact, both these results were first worked out by Shahshahani:</p>
<p>&bull; Siavash Shahshahani, <i>A New Mathematical Framework for the Study of Linkage and Selection</i>, <i>Memoirs of the AMS</i> <b>17</b>, 1979.</p>
<h3> New directions </h3>
<p>All this is just the beginning!  The ideas I just explained are unified in information geometry, where distance-like quantities such as the relative entropy and the Fisher information metric are studied.  From here it&#8217;s a short walk to a very nice version of <a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection">Fisher&#8217;s fundamental theorem of natural selection</a>, which is familiar to researchers both in evolutionary dynamics and in information geometry.</p>
<p>You can see some very nice versions of this story for maximum likelihood estimators and linear programming here:</p>
<p>&bull; Akio Fujiwara and Shun-ichi Amari, Gradient systems in view of information geometry, <a href="http://www.sciencedirect.com/science/article/pii/016727899400175P"><i>Physica D: Nonlinear Phenomena</i></a> <b>80</b> (1995), 317&ndash;327.</p>
<p>Indeed, this seems to be the first paper discussing the similarities between evolutionary game theory and information geometry.</p>
<p>Dash Fryer (at Pomona College) and I have generalized this story in several interesting ways.</p>
<p>First, there are two famous ways to generalize the usual formula for entropy: <a href="http://en.wikipedia.org/wiki/Tsallis_entropy">Tsallis entropy</a> and <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">R&eacute;nyi entropy</a>, both of which involve a parameter <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />  There are Tsallis and R&eacute;nyi versions of relative entropy and the Fisher information metric as well.  Everything I just explained about:</p>
<p>&bull; conditions under which relative entropy is a Lyapunov function for the replicator equation, and</p>
<p>&bull; conditions under which the replicator equation is a special case of gradient flow</p>
<p>generalize to these cases!  However, these generalized entropies give modified versions of the replicator equation.  When we set <img src="https://s0.wp.com/latex.php?latex=q%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q=1" class="latex" /> we get back the usual story.  See</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1764">Escort evolutionary game theory</a>.</p>
<p>My initial interest in these alternate entropies was mostly mathematical&#8212;what is so special about the corresponding geometries?&#8212;but now researchers are starting to find populations that evolve according to these kinds of modified population dynamics!  For example:</p>
<p>&bull; A. Hernando <i>et al</i>, <a href="http://arxiv.org/abs/1201.0905">The workings of the Maximum Entropy Principle in collective human behavior</a>.</p>
<p>There&#8217;s an interesting special case worth some attention. Lots of people fret about the relative entropy not being a distance function obeying the axioms that mathematicians like: for example, it doesn&#8217;t obey the triangle inequality.   Many describe the relative entropy as a <i>distance-like</i> function, and this is often a valid interpretation contextually.  On the other hand, the <img src="https://s0.wp.com/latex.php?latex=q%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q=0" class="latex" /> relative entropy is one-half the Euclidean distance squared!  In this case the modified version of the replicator equation looks like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+f_i%28p%29+-+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bj+%3D+1%7D%5En+f_j%28p%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = f_i(p) - &#92;frac{1}{n} &#92;sum_{j = 1}^n f_j(p) } " class="latex" /></p>
<p>This equation is called the <b>projection dynamic</b>.</p>
<p>Later, I showed that there is a reasonable definition of relative entropy for a much larger family of geometries that satisfies a similar <i>distance minimization</i> property.</p>
<p>In a different direction, Dash showed that you can change the way that selection acts by using a variety of alternative ‚Äòincentives‚Äô, extending the story to some other well-known equations describing evolutionary dynamics. By replacing the terms <img src="https://s0.wp.com/latex.php?latex=x_i+f_i%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i f_i(x)" class="latex" /> in the replicator equation with a variety of other functions, called <b>incentives</b>, we can generate many commonly studied models of evolutionary dynamics. For instance if we exponentiate the fitness landscape (to make it always positive), we get what is commonly known as the <b>logit dynamic</b>.  This amounts to changing the fitness landscape as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f_i+%5Cmapsto+%5Cfrac%7Bx_i+e%5E%7B%5Cbeta+f_i%7D%7D%7B%5Csum_j%7Bx_j+e%5E%7B%5Cbeta+f_j%7D%7D%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f_i &#92;mapsto &#92;frac{x_i e^{&#92;beta f_i}}{&#92;sum_j{x_j e^{&#92;beta f_j}}} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is known as an <b>inverse temperature</b> in statistical thermodynamics and as an <b>intensity of selection</b> in evolutionary dynamics. There are lots of modified versions of the replicator equation, like the best-reply and projection dynamics, more common in economic applications of evolutionary game theory, and they can all be captured in this family. (There are also other ways to simultaneously capture such families, such as Bill Sandholm&#8217;s revision protocols, which were introduced earlier in his exploration of the foundations of game dynamics.)</p>
<p>Dash showed that there is a natural generalization of evolutionarily stable states to &#8216;incentive stable states&#8217;, and that for incentive stable states, the relative entropy is decreasing to zero when the trajectories get near the equilibrium. For the logit and projection dynamics, incentive stable states are simply evolutionarily stable states, and this happens frequently, but not always.</p>
<p>The third generalization is to look at different &#8216;time-scales&#8217;&#8212;that is, different ways of describing time!  We can make up the symbol <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T}" class="latex" /> for a general choice of &#8216;time-scale&#8217;.  So far I&#8217;ve been treating time as a real number, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R} " class="latex" /></p>
<p>But we can also treat time as coming in discrete evenly spaced steps, which amounts to treating time as an integer:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BZ%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{Z} " class="latex" /></p>
<p>More generally, we can make the steps have duration <img src="https://s0.wp.com/latex.php?latex=h%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h," class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=h&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h" class="latex" /> is any positive real number:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z} " class="latex" /></p>
<p>There is a nice way to simultaneously describe the cases <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z}" class="latex" /> using the <a href="http://en.wikipedia.org/wiki/Time-scale_calculus">time-scale calculus</a> and time-scale derivatives. For the time-scale <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R}" class="latex" /> the time-scale derivative is just the ordinary derivative. For the time-scale <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z}," class="latex" /> the time-scale derivative is given by the difference quotient from first year calculus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%5E%7B%5CDelta%7D%28z%29+%3D+%5Cfrac%7Bf%28z%2Bh%29+-+f%28z%29%7D%7Bh%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f^{&#92;Delta}(z) = &#92;frac{f(z+h) - f(z)}{h} } " class="latex" /></p>
<p>and using this as a substitute for the derivative gives difference equations like a discrete-time version of the replicator equation.  There are many other choices of time-scale, such as the <b>quantum time-scale</b> given by <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+q%5E%7B%5Cmathbb%7BZ%7D%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = q^{&#92;mathbb{Z}}," class="latex" /> in which case the time-scale derivative is called the <a href="https://en.wikipedia.org/wiki/Q-derivative"><i>q</i>-derivative</a>, but that&#8217;s a tale for another time.  In any case, the fact that the successive relative entropies are decreasing can be simply state by saying they have negative <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+h%5Cmathbb%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = h&#92;mathbb{Z}" class="latex" /> time-scale derivative.   The continuous case we started with corresponds to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BT%7D+%3D+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{T} = &#92;mathbb{R}." class="latex" /></p>
<p>Remarkably, Dash and I were able to show that you can combine all three of these generalizations into one theorem, and even allow for multiple interacting populations! This produces some really neat population trajectories, such as the following two populations with three types, with fitness functions corresponding to the <a href="https://en.wikipedia.org/wiki/Rock-paper-scissors#Rock-paper-scissors_analogies_in_nature">rock-paper-scissors game</a>. On top we have the replicator equation, which goes along with the Fisher information metric; on the bottom we have the logit dynamic, which goes along with the Euclidean metric on the simplex:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_8_1.png" width="300" /></p>
<p><img src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_8_2.png" width="300" />
</div>
<p>From our theorem, it follows that the relative entropy (ordinary relative entropy on top, the <img src="https://s0.wp.com/latex.php?latex=q+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = 0" class="latex" /> entropy on bottom) converges to zero along the population trajectories.</p>
<p>The final form of the theorem is loosely as follows. Pick a Riemannian geometry given by a metric <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" /> (obeying some mild conditions) and an incentive for each population, as well as a time scale (<img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=h+%5Cmathbb%7BZ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h &#92;mathbb{Z}" class="latex" />) for every population. This gives an evolutionary dynamic with a natural generalization of evolutionarily stable states, and a suitable version of the relative entropy.   Then, if there is an evolutionarily stable state in the interior of the simplex, the time-scale derivative of sum of the relative entropies for each population will decrease as the trajectories converge to the stable state!</p>
<p>When there isn&#8217;t such a stable state, we still get some interesting population dynamics, like the following:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_9_1.png" width="300" /><br />
<img src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/harper/figure_9_2.png" width="300" />
</div>
<p>See this paper for details:</p>
<p>&bull; Marc Harper and Dashiell E. A. Fryer, <a href="http://arxiv.org/abs/1210.5539">Stability of evolutionary dynamics on time scales</a>.</p>
<p>Next time we&#8217;ll see how to make the main idea work in finite populations, without derivatives or deterministic trajectories!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/#comments">30 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/" rel="category tag">game theory</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/" rel="bookmark" title="Permanent Link to Relative Entropy in Evolutionary&nbsp;Dynamics">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-17070 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics" id="post-17070">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/" rel="bookmark">Relative Entropy (Part&nbsp;3)</a></h2>
				<small>25 December, 2013</small><br />


				<div class="entry">
					<p>Holidays are great. There&#8217;s nothing I need to do! Everybody is celebrating! So, I can finally get some real work done.</p>
<p>In the last couple of days I&#8217;ve finished a paper with Jamie Vicary on wormholes and entanglement&#8230; subject to his approval and corrections. More on that later. And now I&#8217;ve returned to working on a paper with Tobias Fritz where we give a Bayesian characterization of the concept of &#8216;relative entropy&#8217;. This summer I wrote two blog articles about this paper.  But then Tobias Fritz noticed a big problem. Our characterization of relative entropy was inspired by this paper:</p>
<p>‚Ä¢ D. Petz, <a href="http://www.renyi.hu/~petz/pdf/52.pdf">Characterization of the relative entropy of states of matrix algebras</a>, <i>Acta Math. Hungar. </i> <b>59</b> (1992), 449&#8211;455.</p>
<p>Here Petz sought to characterize relative entropy both in the &#8216;classical&#8217; case we are concerned with and in the more general &#8216;quantum&#8217; setting. Our original goal was merely to express his results in a more category-theoretic framework! Unfortunately Petz&#8217;s proof contained a significant flaw. Tobias noticed this and spent a lot of time fixing it, with no help from me.</p>
<p>Our paper is now self-contained, and considerably longer. My job now is to polish it up and make it pretty. What follows is the introduction, which should explain the basic ideas.</p>
<h3>A Bayesian characterization of relative entropy</h3>
<p>This paper gives a new characterization of the concept of relative entropy, also known as &#8216;relative information&#8217;, &#8216;information gain&#8217; or &#8216;Kullback-Leibler divergence&#8217;. Whenever we have two probability distributions <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on the same finite set <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> we can define the entropy of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28q%2Cp%29+%3D+%5Csum_%7Bx%5Cin+X%7D+q_x+%5Cln%5Cleft%28+%5Cfrac%7Bq_x%7D%7Bp_x%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(q,p) = &#92;sum_{x&#92;in X} q_x &#92;ln&#92;left( &#92;frac{q_x}{p_x} &#92;right) }" class="latex" /></p>
<p>Here we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_x+%5Cln%5Cleft%28+%5Cfrac%7Bq_x%7D%7Bp_x%7D+%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_x &#92;ln&#92;left( &#92;frac{q_x}{p_x} &#92;right) }" class="latex" /></p>
<p>equal to <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;infty" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=p_x+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = 0," class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=q_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x" class="latex" /> is also zero, in which case we set it equal to 0. Relative entropy thus takes values in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /></p>
<p>Intuitively speaking, <img src="https://s0.wp.com/latex.php?latex=S%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q,p)" class="latex" /> is the expected amount of information gained when we discover the probability distribution is really <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> when we had thought it was <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /> We should think of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as a &#8216;prior&#8217; and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as a &#8216;posterior&#8217;. When we take <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to be the uniform distribution on <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> relative entropy reduces to the ordinary Shannon entropy, up to an additive constant. The advantage of relative entropy is that it makes the role of the prior explicit.</p>
<p>Since Bayesian probability theory emphasizes the role of the prior, relative entropy naturally lends itself to a Bayesian interpretation: it measures how much information we gain <i>given a certain prior</i>. Our goal here is to make this precise in a mathematical characterization of relative entropy. We do this using a category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> where:</p>
<p>‚Ä¢ an object <img src="https://s0.wp.com/latex.php?latex=%28X%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,q)" class="latex" /> consists of a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and a probability distribution <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+q_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;mapsto q_x" class="latex" /> on that set;</p>
<p>‚Ä¢ a morphism <img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /> consists of a measure-preserving function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> together with a probability distribution <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+s_%7Bx+y%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;mapsto s_{x y}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> for each element <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y," class="latex" /> with the property that <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy} = 0" class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y." class="latex" /></p>
<p>We can think of an object of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> as a system with some finite set of <b>states</b> together with a probability distribution on its states. A morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /></p>
<p>then consists of two parts. First, there is a deterministic <b>measurement process</b> <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;to Y" class="latex" /> mapping states of the system being measured, <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> to states of the measurement apparatus, <img src="https://s0.wp.com/latex.php?latex=Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y." class="latex" /> The condition that <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> be measure-preserving says that, after the measurement, the probability that the apparatus be in any state <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y" class="latex" /> is the sum of the probabilities of all states of <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> leading to that outcome:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+r_y+%3D+%5Csum_%7Bx+%5Cin+f%5E%7B-1%7D%28y%29%7D+q_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ r_y = &#92;sum_{x &#92;in f^{-1}(y)} q_x } " class="latex" /></p>
<p>Second, there is a <b>hypothesis</b> <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" />: an assumption about the probability <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy}" class="latex" /> that the system being measured is in the state <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> given any measurement outcome <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y." class="latex" /></p>
<p>Suppose we have any morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /></p>
<p>in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" /> From this we obtain two probability distributions on the states of the system being measured. First, we have the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Csum_%7By+%5Cin+Y%7D+s_%7Bxy%7D+r_y+%7D+%5Cqquad+%5Cqquad+%281%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;sum_{y &#92;in Y} s_{xy} r_y } &#92;qquad &#92;qquad (1) " class="latex" /></p>
<p>This is our <b>prior</b>, given our hypothesis and the probability distribution of measurement results. Second we have the &#8216;true&#8217; probability distribution <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> which would be the <b>posterior</b> if we updated our prior using complete direct knowledge of the system being measured.</p>
<p>It follows that any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> has a relative entropy <img src="https://s0.wp.com/latex.php?latex=S%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q,p)" class="latex" /> associated to it. This is the expected amount of information we gain when we update our prior <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to the posterior <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>In fact, this way of assigning relative entropies to morphisms defines a functor</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_0+%3A+%5Cmathrm%7BFinStat%7D+%5Cto+%5B0%2C%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0 : &#92;mathrm{FinStat} &#92;to [0,&#92;infty] " class="latex" /></p>
<p>where we use <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> to denote the category with one object, the numbers <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+x+%5Cle+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le x &#92;le &#92;infty" class="latex" /> as morphisms, and addition as composition. More precisely, if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /></p>
<p>is any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> we define</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_0%28f%2Cs%29+%3D+S%28q%2Cp%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0(f,s) = S(q,p) " class="latex" /></p>
<p>where the prior <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is defined as in the equation (1).</p>
<p>The fact that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is a functor is nontrivial and rather interesting. It says that given any composable pair of measurement processes:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28X%2Cq%29+%5Cstackrel%7B%28f%2Cs%29%7D%7B%5Clongrightarrow%7D+%28Y%2Cr%29+%5Cstackrel%7B%28g%2Ct%29%7D%7B%5Clongrightarrow%7D+%28Z%2Cu%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,q) &#92;stackrel{(f,s)}{&#92;longrightarrow} (Y,r) &#92;stackrel{(g,t)}{&#92;longrightarrow} (Z,u) " class="latex" /></p>
<p>the relative entropy of their composite is the sum of the relative entropies of the two parts:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_0%28%28g%2Ct%29+%5Ccirc+%28f%2Cs%29%29+%3D+F_0%28g%2Ct%29+%2B+F_0%28f%2Cs%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0((g,t) &#92;circ (f,s)) = F_0(g,t) + F_0(f,s) ." class="latex" /></p>
<p>We prove that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is a functor. However, we go much further: we <i>characterize</i> relative entropy by saying that up to a constant multiple, <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is the <i>unique</i> functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> obeying three reasonable conditions.</p>
<p>The first condition is that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> vanishes on morphisms <img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cq%29+%5Cto+%28Y%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,q) &#92;to (Y,r)" class="latex" /> where the hypothesis <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is <b>optimal</b>. By this, we mean that Equation (1) gives a prior <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> equal to the &#8216;true&#8217; probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on the states of the system being measured.</p>
<p>The second condition is that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is lower semicontinuous. The set <img src="https://s0.wp.com/latex.php?latex=P%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P(X)" class="latex" /> of probability distibutions on a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> naturally has the topology of an <img src="https://s0.wp.com/latex.php?latex=%28n-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(n-1)" class="latex" />-simplex when <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> elements. The set <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]" class="latex" /> has an obvious topology where it&#8217;s homeomorphic to a closed interval. However, with these topologies, the relative entropy does not define a continuous function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+S+%3A+P%28X%29+%5Ctimes+P%28X%29+%26%5Cto%26+%5B0%2C%5Cinfty%5D+%5C%5C+%28q%2Cp%29+%26%5Cmapsto+%26+S%28q%2Cp%29+.++%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{rcl} S : P(X) &#92;times P(X) &amp;&#92;to&amp; [0,&#92;infty] &#92;&#92; (q,p) &amp;&#92;mapsto &amp; S(q,p) .  &#92;end{array}" class="latex" /></p>
<p>The problem is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28q%2Cp%29+%3D+%5Csum_%7Bx%5Cin+X%7D+q_x+%5Cln%5Cleft%28+%5Cfrac%7Bq_x%7D%7Bp_x%7D+%5Cright%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(q,p) = &#92;sum_{x&#92;in X} q_x &#92;ln&#92;left( &#92;frac{q_x}{p_x} &#92;right) } " class="latex" /></p>
<p>and we define <img src="https://s0.wp.com/latex.php?latex=q_x+%5Cln%28q_x%2Fp_x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x &#92;ln(q_x/p_x)" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;infty" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=p_x+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = 0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_x+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_x &gt; 0" class="latex" /> but <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=p_x+%3D+q_x+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x = q_x = 0." class="latex" /> So, it turns out that <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is only <b>lower semicontinuous</b>, meaning that if <img src="https://s0.wp.com/latex.php?latex=p%5Ei+%2C+q%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^i , q^i" class="latex" /> are sequences of probability distributions on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=p%5Ei+%5Cto+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^i &#92;to p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%5Ei+%5Cto+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i &#92;to q" class="latex" /> then</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28q%2Cp%29+%5Cle+%5Climinf_%7Bi+%5Cto+%5Cinfty%7D+S%28q%5Ei%2C+p%5Ei%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q,p) &#92;le &#92;liminf_{i &#92;to &#92;infty} S(q^i, p^i) " class="latex" /></p>
<p>We give the set of morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> its most obvious topology, and show that with this topology, <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> maps morphisms to morphisms in a lower semicontinuous way.</p>
<p>The third condition is that <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is convex linear. We describe how to take convex linear combinations of morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and then the functor <img src="https://s0.wp.com/latex.php?latex=F_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_0" class="latex" /> is convex linear in the sense that it maps any convex linear combination of morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to the corresponding convex linear combination of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /> Intuitively, this means that if we take a coin with probability <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> of landing heads up, and flip it to decide whether to perform one measurement process or another, the expected information gained is <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> times the expected information gain of the first process plus <img src="https://s0.wp.com/latex.php?latex=1-P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1-P" class="latex" /> times the expected information gain of the second process.</p>
<p>Here, then, is our main theorem:</p>
<p><b>Theorem.</b> Any lower semicontinuous, convex-linear functor</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3A+%5Cmathrm%7BFinStat%7D+%5Cto+%5B0%2C%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F : &#92;mathrm{FinStat} &#92;to [0,&#92;infty] " class="latex" /></p>
<p>that vanishes on every morphism with an optimal hypothesis must equal some constant times the relative entropy. In other words, there exists some constant <img src="https://s0.wp.com/latex.php?latex=c+%5Cin+%5B0%2C%5Cinfty%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;in [0,&#92;infty]" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f%2Cs%29+%3D+c+F_0%28f%2Cs%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f,s) = c F_0(f,s) " class="latex" /></p>
<p>for any any morphism <img src="https://s0.wp.com/latex.php?latex=%28f%2Cs%29+%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(f,s) : (X,p) &#92;to (Y,q)" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" /></p>
<h3>Addendum</h3>
<p>If you&#8217;re a maniacally thorough reader of this blog, with a photographic memory, you&#8217;ll recall that our theorem now says &#8216;lower semicontinuous&#8217;, where in <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Part 2</a> of this series I&#8217;d originally said &#8216;continuous&#8217;.</p>
<p>I&#8217;ve fixed that blog article now&#8230; but it was Tobias who noticed this mistake. In the process of fixing our proof to address this issue, he eventually noticed that the proof of Petz&#8217;s theorem, which we&#8217;d been planning to use in our work, was also flawed.</p>
<p>Here&#8217;s the paper we eventually wrote:</p>
<p>‚Ä¢ John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>, <i><a href="http://www.tac.mta.ca/tac/volumes/29/16/29-16abs.html">Theory and Applications of Categories</a></i> <b>29</b> (2014), 421&ndash;456.</p>
<p>And here&#8217;s my whole series of blog articles about it:</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy (Part 1)</a>: how various structures important in probability theory arise naturally when you do linear algebra using only the nonnegative real numbers.</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Relative Entropy (Part 2)</a>: a category related to statistical inference, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and how relative entropy defines a functor on this category.</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Relative Entropy (Part 3)</a>: how to characterize relative entropy as a functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /></p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/">Relative Entropy (Part 4)</a>: wrap-up, and an invitation to read more about the underlying math at the <a href="https://golem.ph.utexas.edu/category/2014/02/relative_entropy.html"><i>n</i>-Category Caf&eacute;</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/#comments">15 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/" rel="bookmark" title="Permanent Link to Relative Entropy (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16858 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-16858">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/11/11/quantropy-part-4/" rel="bookmark">Quantropy (Part 4)</a></h2>
				<small>11 November, 2013</small><br />


				<div class="entry">
					<p>There&#8217;s a new paper on the arXiv:</p>
<p>&bull; John Baez and Blake Pollard, <a href="http://arxiv.org/abs/1311.0813">Quantropy</a>.</p>
<p>Blake is a physics grad student at U. C. Riverside who plans to do his thesis with me.  </p>
<p>If you have carefully read all my previous posts on quantropy (<a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/">Part 1</a>, <a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/">Part 2</a> and <a href="https://johncarlosbaez.wordpress.com/2012/02/18/quantropy-part-3/">Part 3</a>), there&#8217;s only a little new stuff here.  But still, it&#8217;s better organized, and less chatty.</p>
<p>And in fact, Blake came up with a lot of new stuff for this paper!  He studied the quantropy of the harmonic oscillator, and tweaked the analogy between statistical mechanics and quantum mechanics in an interesting way.  Unfortunately, we needed to put a version of this paper on the arXiv by a deadline, and our writeup of this new work wasn&#8217;t quite ready (my fault). So, we&#8217;ll put that other stuff in a new version&#8212;or, I&#8217;m thinking now, a separate paper.</p>
<p>But here are two new things.  </p>
<p>First, putting this paper on the arXiv had the usual good effect of revealing some existing work on the same topic.  Joakim Munkhammar emailed me and pointed out this paper, which is free online:</p>
<p>&bull; Joakim Munkhammar, <a href="http://www.ejtp.com/articles/ejtpv8i25p93.pdf">Canonical relational quantum mechanics from information theory</a>, <i><a href="http://www.ejtp.com/ejtpv8i25">Electronic Journal of Theoretical Physics</a></i> <b>8</b> (2011), 93‚Äì108.</p>
<p>You&#8217;ll see it cites <a href="http://arxiv.org/abs/physics/0605068">Garrett Lisi&#8217;s paper</a> and pushes forward in various directions.  There seems to be a typo where he writes the path integral </p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cdisplaystyle%7B+%5Cint+e%5E%7B-%5Calpha+S%28q%29+%7D+D+q%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;displaystyle{ &#92;int e^{-&#92;alpha S(q) } D q} " class="latex" /></p>
<p>and says </p>
<blockquote><p>
In order to fit the purpose Lisi concluded that the Lagrange multiplier value <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cequiv+1%2Fi+%5Chbar.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;equiv 1/i &#92;hbar." class="latex" />  In similarity with Lisi‚Äôs approach we shall also assume that the arbitrary scaling-part of the constant <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> is in fact <img src="https://s0.wp.com/latex.php?latex=1%2F%5Chbar.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/&#92;hbar." class="latex" />
</p></blockquote>
<p>I&#8217;m pretty sure he means <img src="https://s0.wp.com/latex.php?latex=1%2Fi%5Chbar%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/i&#92;hbar," class="latex" /> given what he writes later. However, he speaks of &#8216;maximizing entropy&#8217;, which is not quite right for a complex-valued quantity; Blake and I prefer to give this new quantity a new name, and speak of &#8216;finding a stationary point of quantropy&#8217;.</p>
<p>But in a way these are small issues; being a mathematician, I&#8217;m more quick to spot tiny technical defects than to absorb significant new ideas, and it will take a while to really understand Munkhammar&#8217;s paper.</p>
<p>Second, while writing our paper, Blake and I noticed another similarity between the partition function of a classical ideal gas and the partition function of a quantum free particle.  Both are given by an integral like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cdisplaystyle%7B%5Cint+e%5E%7B-%5Calpha+S%28q%29+%7D+D+q+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;displaystyle{&#92;int e^{-&#92;alpha S(q) } D q } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is a quadratic function of <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in &#92;mathbb{R}^n." class="latex" />  Here <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is the number of degrees of freedom for the particles in the ideal gas, or the number of time steps for a free particle on a line (where we are discretizing time).  The only big difference is that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1%2FkT+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = 1/kT " class="latex" /></p>
<p>for the ideal gas, but</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1%2Fi+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = 1/i &#92;hbar" class="latex" /></p>
<p>for the free particle.  </p>
<p>In both cases there&#8217;s an ambiguity in the answer!  The reason is that to do this integral, we need to pick a measure <img src="https://s0.wp.com/latex.php?latex=D+q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D q." class="latex" />  The obvious guess is Lebesgue measure</p>
<p><img src="https://s0.wp.com/latex.php?latex=dq+%3D+dq_1+%5Ccdots+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq = dq_1 &#92;cdots dq_n " class="latex" /></p>
<p>on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n." class="latex" />  But this can&#8217;t be right, on physical grounds!</p>
<p>The reason is that the partition function <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> needs to be dimensionless, but <img src="https://s0.wp.com/latex.php?latex=d+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d q" class="latex" /> has units.  To correct this, we need to divide <img src="https://s0.wp.com/latex.php?latex=dq&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq" class="latex" /> by some dimensionful quantity to get <img src="https://s0.wp.com/latex.php?latex=D+q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D q." class="latex" /></p>
<p>In the case of the ideal gas, this dimensionful quantity involves the <a href="http://en.wikipedia.org/wiki/Thermal_de_Broglie_wavelength">&#8216;thermal de Broglie wavelength&#8217;</a> of the particles in the gas.  And this brings Planck&#8217;s constant into the game, <i>even though we&#8217;re not doing quantum mechanics</i>: we&#8217;re studying the statistical mechanics of a <i>classical</i> ideal gas!  </p>
<p>That&#8217;s weird and interesting.  It&#8217;s not the only place where we see that classical statistical mechanics is incomplete or inconsistent, and we need to introduce some ideas from quantum physics to get sensible answers.  The most famous one is the <a href="http://en.wikipedia.org/wiki/Ultraviolet_catastrophe">ultraviolet catastrophe</a>.  What are all rest?</p>
<p>In the case of the free particle, we need to divide by a quantity with dimensions of length<sup><i>n</i></sup> to make</p>
<p><img src="https://s0.wp.com/latex.php?latex=dq+%3D+dq_1+%5Ccdots+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq = dq_1 &#92;cdots dq_n " class="latex" /></p>
<p>dimensionless, since each <img src="https://s0.wp.com/latex.php?latex=dq_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq_i" class="latex" /> has dimensions of length.  The easiest way is to introduce a length scale <img src="https://s0.wp.com/latex.php?latex=%5CDelta+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta x" class="latex" /> and divide each <img src="https://s0.wp.com/latex.php?latex=dq_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dq_i" class="latex" /> by that.  This is commonly done when people study the free particle.  This length scale drops out of the final answer for the questions people usually care about&#8230; but <i>not</i> for the quantropy.  </p>
<p>Similarly, Planck&#8217;s constant drops out of the final answer for some questions about the classical ideal gas, but <i>not</i> for its entropy!</p>
<p>So there&#8217;s an interesting question here, about what this new length scale <img src="https://s0.wp.com/latex.php?latex=%5CDelta+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta x" class="latex" /> means, if anything.  One might argue that quantropy is a bad idea, and the need for this new length scale to make it unambiguous is just proof of that.  However, the mathematical analogy to quantum mechanics is so precise that I think it&#8217;s worth going a bit further out on this limb, and thinking a bit more about what&#8217;s going on.  </p>
<p>Some weird sort of <i>d&eacute;j&agrave; vu</i> phenomenon seems to be going on.  Once upon a time, people tried to calculate the partition functions of classical systems.  They discovered they were infinite or ambiguous until they introduced Planck&#8217;s constant, and eventually quantum mechanics.  Then Feynman introduced the path integral approach to quantum mechanics.  In this approach one is again computing partition functions, but now with a new meaning, and with complex rather than real exponentials.  But these partition functions are again infinite or ambiguous&#8230; <i>for very similar mathematical reasons!</i>  And at least in some cases, we can remove the ambiguity using the same trick as before: introducing a new constant.  But then&#8230; what?</p>
<p>Are we stuck in an infinite loop here?  What, if anything, is the meaning of this &#8216;second Planck&#8217;s constant&#8217;?  Does this have anything to do with <a href="http://en.wikipedia.org/wiki/Second_quantization"> second quantization</a>?  (I don&#8217;t see how, but I can&#8217;t resist asking.)</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/11/quantropy-part-4/#comments">70 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/11/quantropy-part-4/" rel="bookmark" title="Permanent Link to Quantropy (Part 4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16829 post type-post status-publish format-standard hentry category-biology category-conferences category-information-and-entropy category-mathematics" id="post-16829">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/" rel="bookmark">Entropy and Information in Biological Systems (Part&nbsp;1)</a></h2>
				<small>2 November, 2013</small><br />


				<div class="entry">
					<p><a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a> is an ecologist who uses maximum entropy methods to predict the distribution, abundance and energy usage of species.  <a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a> uses information theory in bioinformatics and evolutionary game theory.   Harper, Harte and I are organizing a workshop on entropy and information in biological systems, and I&#8217;m really excited about it!</p>
<p>It&#8217;ll take place at the <a href="http://www.nimbios.org/">National Institute for Mathematical and Biological Synthesis</a> in Knoxville Tennesee.    We are scheduling it for Wednesday-Friday, April 8-10, 2015.  When the date gets confirmed, I&#8217;ll post an advertisement so you can apply to attend.</p>
<p>Writing the proposal was fun, because we got to pull together lots of interesting people who are applying information theory and entropy to biology in quite different ways.   So, here it is!</p>
<div align="center"><a href="http://www.nimbios.org/"><img width="450" src="https://i0.wp.com/www.utk.edu/tntoday/images/nimbios_logo_lg.jpg" /></a></div>
<h3> Proposal </h3>
<p>Ever since Shannon initiated research on information theory in 1948, there have been hopes that the concept of information could serve as a tool to help systematize and unify work in biology.  The link between information and <i>entropy</i> was noted very early on, and it suggested that a full thermodynamic understanding of biology would naturally involve the information processing and storage that are characteristic of living organisms.  However, the subject is full of conceptual pitfalls for the unwary, and progress has been slower than initially expected.  Premature attempts at &#8216;grand syntheses&#8217; have often misfired.  But applications of information theory and entropy to specific highly focused topics in biology have been increasingly successful, such as:</p>
<p>&bull;  the maximum entropy principle in ecology,<br />
&bull;   Shannon and R&eacute;nyi entropies as measures of biodiversity,<br />
&bull;  information theory in evolutionary game theory,<br />
&bull;  information and the thermodynamics of individual cells.</p>
<p>Because they work in diverse fields, researchers in these specific topics have had little opportunity to trade insights and take stock of the progress so far.  The aim of the workshop is to do just this.  </p>
<p>In what follows, participants&#8217; names are in boldface, while the main goals of the workshop are in italics.</p>
<p><b><a href="http://biology.anu.edu.au/roderick_dewar/">Roderick Dewar</a></b> is a key advocate of the principle of Maximum Entropy Production, which says that biological systems&#8212;and indeed all open, non-equilibrium systems&#8212;act to produce entropy at the maximum rate.  Along with others, he has applied this principle to make testable predictions in a wide range of biological systems, from ATP synthesis [DJZ2006] to respiration and photosynthesis of individual plants [D2010] and plant communities.  He has also sought to derive this principle from ideas in statistical mechanics [D2004, D2009], but it remains controversial.  </p>
<p><i>The first goal of this workshop is to study the validity of this principle</i>.</p>
<p>While they may be related, the principle of Maximum Entropy Production should not be confused with the MaxEnt inference procedure, which says that we should choose the probabilistic hypothesis with the highest entropy subject to the constraints provided by our data.  MaxEnt was first explicitly advocated by Jaynes.  He noted that it is already implicit in the procedures of statistical mechanics, but convincingly argued that it can also be applied to situations where entropy is more &#8216;informational&#8217; than &#8216;thermodynamic&#8217; in character.  </p>
<p>Recently <b><a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a></b> has applied MaxEnt in this way to ecology, using it to make specific testable predictions for the distribution, abundance and energy usage of species across spatial scales and across habitats and taxonomic groups [Harte2008, Harte2009, Harte2011].  <b><a href="http://webapps.lsa.umich.edu/eeb/directory/faculty/aostling/">Annette Ostling</a></b> is an expert on other theories that attempt to explain the same data, such as the &#8216;neutral model&#8217; [AOE2008, ODLSG2009, O2005, O2012]. <b><a href="http://biology.anu.edu.au/roderick_dewar/">Dewar</a></b> has also used MaxEnt in ecology [D2008], and he has argued that it underlies the principle of Maximum Entropy Production.    </p>
<p><i>Thus, a second goal of this workshop is to familiarize all the participants with applications of the MaxEnt method to ecology, compare it with competing approaches, and study whether MaxEnt provides a sufficient justification for the principle of Maximum Entropy Production.</i></p>
<p>Entropy is not merely a predictive tool in ecology: it is also widely used as a measure of biodiversity.  Here Shannon&#8217;s original concept of entropy naturally generalizes to &#8216;R&eacute;nyi entropy&#8217;, which depends on a parameter <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;ge 0" class="latex" />.  This equals</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Calpha%28p%29+%3D+%5Cfrac%7B1%7D%7B1-%5Calpha%7D+%5Clog+%5Csum_i+p_i%5E%5Calpha++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;alpha(p) = &#92;frac{1}{1-&#92;alpha} &#92;log &#92;sum_i p_i^&#92;alpha  } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type (which could mean species, some other taxon, etc.).    In the limit <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;to 1" class="latex" /> this reduces to the Shannon entropy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++H%28p%29+%3D+-+%5Csum_i+p_i+%5Clog+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  H(p) = - &#92;sum_i p_i &#92;log p_i } " class="latex" /></p>
<p>As <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> increases, we give less weight to rare types of organisms.  <b><a href="http://www.maths.gla.ac.uk/~cc/">Christina Cobbold</a></b> and <b><a href="http://www.maths.ed.ac.uk/~tl/">Tom Leinster</a></b> have described a systematic and highly flexible system of biodiversity measurement, with R&eacute;nyi entropy at its heart [CL2012].    They consider both the case where all we have are the numbers <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />, and the more subtle case where we take the distance between different types of organisms into account.  </p>
<p><b><a href="http://math.ucr.edu/home/baez/">John Baez</a></b> has explained the role of R&eacute;nyi entropy in thermodynamics [B2011], and together with <b><a href="http://www.maths.ed.ac.uk/~tl/"></a><a>Tom Leinster</a></b> and <b><a href="http://users.icfo.es/Tobias.Fritz/">Tobias Fritz</a></b> he has proved other theorems characterizing entropy which explain its importance for information processing [BFL2011].  However, these ideas have not yet been connected to the widespread use of entropy in biodiversity studies.  More importantly, the use of entropy as a measure of biodiversity has not been clearly connected to MaxEnt methods in ecology.  Does the success of MaxEnt methods imply a tendency for ecosystems to maximize biodiversity subject to the constraints of resource availability?  This seems surprising, but a more nuanced statement along these general lines might be correct.    </p>
<p><i>So, a third goal of this workshop is to clarify relations between known characterizations of entropy, the use of entropy as a measure of biodiversity, and the use of MaxEnt methods in ecology.</i></p>
<p>As the amount of data to analyze in genomics continues to surpass the ability of humans to analyze it, we can expect automated experiment design to become ever more important.   In <b><a href="http://thinking.bioinformatics.ucla.edu/">Chris Lee</a></b> and <b><a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a></b>‚Äôs RoboMendel program [LH2013], a mathematically precise concept of &#8216;potential information&#8217;&#8212;how much information is left to learn&#8212;plays a crucial role in deciding what experiment to do next, given the data obtained so far.  It will be useful for them to interact with <b><a href="http://www.princeton.edu/~wbialek/wbialek.html">William Bialek</a></b>, who has expertise in estimating entropy from empirical data and using it to constrain properties of models [BBS, BNS2001, BNS2002], and <b><a href="http://www2.hawaii.edu/~sstill/">Susanne Still</a></b>, who applies information theory to automated theory building and biology [CES2010, PS2012].</p>
<p>However, there is another link between biology and potential information.  <b><a href="http://people.mbi.ucla.edu/marcharper/">Harper</a></b> has noted that in an ecosystem where the population of each type of organism grows at a rate proportional to its fitness (which may depend on the fraction of organisms of each type), the quantity </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28q%7C%7Cp%29+%3D+%5Csum_i+q_i+%5Cln%28q_i%2Fp_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(q||p) = &#92;sum_i q_i &#92;ln(q_i/p_i) } " class="latex" /></p>
<p>always decreases if there is an evolutionarily stable state [Harper2009].  Here <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype at a given time, while <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> is this fraction in the evolutionarily stable state.  This quantity is often called the Shannon information of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> &#8216;relative to&#8217; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  But in fact, it is precisely the same as <b><a href="http://thinking.bioinformatics.ucla.edu/">Lee</a></b> and <b><a href="http://people.mbi.ucla.edu/marcharper/">Harper</a></b>‚Äôs potential information!  Indeed, there is a precise mathematical analogy between evolutionary games and processes where a probabilistic hypothesis is refined by repeated experiments.  </p>
<p><i>Thus, a fourth goal of this workshop is to develop the concept of evolutionary games as &#8216;learning&#8217; processes in which information is gained over time.</i>  </p>
<p>We shall try to synthesize this with <b><a href="http://octavia.zoology.washington.edu/">Carl Bergstrom</a></b> and <b><a href="http://www.matina.org/">Matina Donaldson-Matasci</a></b>‚Äôs work on the &#8216;fitness value of information&#8217;: a measure of how much increase in fitness a population can obtain per bit of extra information [BL2004, DBL2010, DM2013].   Following <b><a href="http://people.mbi.ucla.edu/marcharper/">Harper</a></b>, we shall consider not only relative Shannon entropy, but also relative R&eacute;nyi entropy, as a measure of information gain [Harper2011].</p>
<p><i>A fifth and final goal of this workshop is to study the interplay between information theory and the thermodynamics of individual cells and organelles.</i></p>
<p><b><a href="http://www2.hawaii.edu/~sstill/">Susanne Still</a></b> has studied the thermodynamics of prediction in biological systems [BCSS2012].   And in a celebrated related piece of work, <b><a href="http://web.mit.edu/physics/people/faculty/england_jeremy.html">Jeremy England</a></b> used thermodynamic arguments to a derive a lower bound for the amount of entropy generated during a process of self-replication of a bacterial cell [England2013].  Interestingly, he showed that <i>E. coli</i> comes within a factor of 3 of this lower bound.   </p>
<p>In short, information theory and entropy methods are becoming powerful tools in biology, from the level of individual cells, to whole ecosystems, to experimental design, model-building, and the measurement of biodiversity. The time is ripe for an investigative workshop that brings together experts from different fields and lets them share insights and methods and begin to tackle some of the big remaining questions.</p>
<h3> Bibliography </h3>
<p>[AOE2008] D. Alonso, A. Ostling and R. Etienne, <a href="http://www-personal.umich.edu/~aostling/papers/alonso2008.pdf">The assumption of symmetry and species abundance distributions</a>, <i>Ecology Letters</i> <b>11</b> (2008), 93&#8211;105.</p>
<p>[TMMABB2012} D. Amodei, W. Bialek, M. J. Berry II, O. Marre, T. Mora, and G. Tkacik, <a href="http://arxiv.org/abs/1207.6319">The simplest maximum entropy model for collective behavior in a neural network</a>, arXiv:1207.6319 (2012).</p>
<p>[B2011] J. Baez, <a href="http://arxiv.org/abs/1102.2098">R&eacute;nyi entropy and free energy</a>, arXiv:1102.2098 (2011).</p>
<p>[BFL2011] J. Baez, T. Fritz and T. Leinster, <a href="http://arxiv.org/abs/1106.1791">A characterization of entropy in terms of information loss</a>, <i>Entropy</i> <b>13</b> (2011), 1945&#8211;1957.</p>
<p>[B2011] J. Baez and M. Stay, <a href="http://arxiv.org/abs/1010.2067">Algorithmic thermodynamics</a>, <i>Math. Struct. Comp. Sci.</i> <b>22</b> (2012), 771&#8211;787.</p>
<p>[BCSS2012] A. J. Bell, G. E. Crooks, S. Still and D. A Sivak, <a href="http://arxiv.org/abs/1203.3271">The thermodynamics of prediction</a>, <i>Phys. Rev. Lett.</i> <b>109</b> (2012), 120604.</p>
<p>[BL2004] C. T. Bergstrom and M. Lachmann, <a href="http://octavia.zoology.washington.edu/publications/BergstromAndLachmann04.pdf">Shannon information and biological fitness</a>, in <i>IEEE Information Theory Workshop 2004</i>, IEEE, 2004, pp. 50-54.</p>
<p>[BBS] M. J. Berry II, W. Bialek and E. Schneidman, <a href="http://arxiv.org/abs/physics/0212114">An information theoretic approach to the functional classification of neurons</a>, in <i>Advances in Neural Information Processing Systems 15</i>, MIT Press, 2005.</p>
<p>[BNS2001] W. Bialek, I. Nemenman and N. Tishby, <a href="http://www.princeton.edu/~wbialek/our_papers/bnt_01a.pdf">Predictability, complexity and learning</a>, <i>Neural Computation</i> <b>13</b> (2001), 2409&#8211;2463.</p>
<p>[BNS2002] W. Bialek, I. Nemenman and F. Shafee, <a href="http://books.nips.cc/papers/files/nips14/LT22.pdf">Entropy and inference, revisited</a>, in <i>Advances in Neural Information Processing Systems 14</i>, MIT Press, 2002.</p>
<p>[CL2012] C. Cobbold and T. Leinster, <a href="http://www.maths.ed.ac.uk/~tl/mdiss.pdf">Measuring diversity: the importance of species similarity</a>, <i>Ecology</i> <b>93</b> (2012), 477&#8211;489.</p>
<p>[CES2010] J. P. Crutchfield, S. Still and C. Ellison, <a href="http://arxiv.org/abs/0708.1580">Optimal causal inference: estimating stored information and approximating causal architecture</a>, <i>Chaos</i> <b>20</b> (2010), 037111.</p>
<p>[D2004] R. C. Dewar, Maximum entropy production and non-equilibrium statistical mechanics, in <i>Non-Equilibrium Thermodynamics and Entropy Production: Life, Earth and Beyond</i>, eds. A. Kleidon and R. Lorenz, Springer, New York, 2004, 41&#8211;55.</p>
<p>[DJZ2006] R. C. Dewar, D. Juret&iacute;c,  P. Zupanov&iacute;c, <a href="http://www.pmfst.hr/~juretic/CPLETT23896.pdf">The functional design of the rotary enzyme ATP synthase is consistent with maximum entropy production</a>, <i>Chem. Phys. Lett.</i> <b>430</b> (2006), 177&#8211;182. </p>
<p>[D2008] R. C. Dewar, A. Port&eacute;, <a href="http://arxiv.org/abs/q-bio/0703061">Statistical mechanics unifies different ecological patterns</a>, <i>J. Theor. Bio.</i> <b>251</b> (2008), 389&#8211;403. </p>
<p>[D2009] R. C. Dewar, <a href="http://www.mdpi.com/1099-4300/11/4/931/pdf">Maximum entropy production as an inference algorithm that translates physical assumptions into macroscopic predictions: don&#8217;t shoot the messenger</a>, <i>Entropy</i> <b>11</b> (2009), 931&#8211;944. </p>
<p>[D2010] R. C. Dewar, <a href="http://rstb.royalsocietypublishing.org/content/365/1545/1429.full">Maximum entropy production and plant optimization theories</a>, <i>Phil. Trans. Roy. Soc. B</i> <b>365</b> (2010) 1429&#8211;1435.</p>
<p>[DBL2010} M. C. Donaldson-Matasci, C. T. Bergstrom, and<br />
M. Lachmann, <a href="http://arxiv.org/abs/q-bio/0510007">The fitness value of information</a>, <i>Oikos</i> <b>119</b> (2010), 219-230.</p>
<p>[DM2013] M. C. Donaldson-Matasci, G. DeGrandi-Hoffman, and A. Dornhaus, Bigger is better: honey bee colonies as distributed information-gathering systems, <i>Animal Behaviour</i> <b>85</b> (2013), 585&#8211;592.</p>
<p>[England2013] J. L. England, <a href="http://arxiv.org/abs/1209.1179">Statistical physics of self-replication</a>, <i>J. Chem. Phys.</i> <b>139</b> (2013), 121923.</p>
<p>[ODLSG2009} J. L. Green,  J. K. Lake, J. P. O‚ÄôDwyer, A. Ostling and V. M. Savage, <a href="http://www-personal.umich.edu/~aostling/papers/ODwyer2009.pdf">An integrative framework for stochastic, size-structured community assembly</a>, <i>PNAS</i> <b>106</b> (2009), 6170&#8211;6175.</p>
<p>[Harper2009] M. Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>, arXiv:0911.1383 (2009).</p>
<p>[Harper2011] M. Harper, <a href="http://arxiv.org/abs/0911.1764">Escort evolutionary game theory</a>, <i>Physica D</i> <b>240</b> (2011), 1411&#8211;1415.</p>
<p>[Harte2008] J. Harte, T. Zillio, E. Conlisk and A. Smith, Maximum entropy and the state-variable approach to macroecology, <i>Ecology</i> <b>89</b> (2008), 2700&#8211;2711.</p>
<p>[Harte2009] J. Harte, A. Smith and D. Storch, Biodiversity scales from plots to biomes with a universal species-area curve, <i>Ecology Letters</i> <b>12</b> (2009), 789‚Äì797.</p>
<p>[Harte2011] J. Harte, <i>Maximum Entropy and Ecology: A Theory of Abundance, Distribution, and Energetics</i>, Oxford U. Press, Oxford, 2011.</p>
<p>[LH2013] M. Harper and C. Lee, <a href="http://arxiv.org/abs/1210.4808">Basic experiment planning via information metrics: the RoboMendel problem</a>, arXiv:1210.4808 (2012).</p>
<p>[O2005] A. Ostling, <a href="http://www-personal.umich.edu/~aostling/papers/O2005.pdf">Neutral theory tested by birds</a>, <i>Nature</i> <b>436</b> (2005), 635.</p>
<p>[O2012] A. Ostling, <a href="http://www-personal.umich.edu/~aostling/papers/O2012fit.pdf">Do fitness-equalizing tradeoffs lead to neutral communities?</a>, <i>Theoretical Ecology</i> <b>5</b> (2012), 181&#8211;194. </p>
<p>[PS2012] D. Precup and S. Still, <a href="http://www2.hawaii.edu/~sstill/StillPrecup2011.pdf">An information-theoretic approach to curiosity-driven reinforcement learning</a>, <i>Theory in Biosciences</i> <b>131</b> (2012), 139&#8211;148.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/#comments">35 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/" rel="bookmark" title="Permanent Link to Entropy and Information in Biological Systems (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16333 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-probability" id="post-16333">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/" rel="bookmark">Relative Entropy (Part&nbsp;2)</a></h2>
				<small>2 July, 2013</small><br />


				<div class="entry">
					<p>In the <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">first part</a> of this mini-series, I describe how various ideas important in probability theory arise naturally when you start doing linear algebra using only the nonnegative real numbers.</p>
<p>But after writing it, I got an email from a rather famous physicist saying he got &#8220;lost at line two&#8221;.  So, you&#8217;ll be happy to hear that the first part is <i>not a prerequisite</i> for the remaining parts!  I wrote it just to intimidate that guy.</p>
<p>Tobias Fritz and I have proved a theorem characterizing the concept of <a href="http://math.ucr.edu/home/baez/information/information_geometry_6.html">relative entropy</a>, which is also known as &#8216;relative information&#8217;, &#8216;information gain&#8217; or&#8212;most terrifying and least helpful of all&#8212;&#8216;Kullback-Leibler divergence&#8217;.   In this second part I&#8217;ll introduce two key players in this theorem.  The first, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> is a category where:</p>
<p>&bull; an object consists of a system with finitely many states, and a probability distribution on those states</p>
<p>and</p>
<p>&bull; a morphism consists of a deterministic &#8216;measurement process&#8217; mapping states of one system to states of another, together with a &#8216;hypothesis&#8217; that lets the observer guess a probability distribution of states of the system being measured, based on what they observe.</p>
<p>The second, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}," class="latex" /> is a subcategory of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" />  It has all the same objects, but only morphisms where the hypothesis is &#8216;optimal&#8217;.  This means that if the observer measures the system many times, and uses the probability distribution of their observations together with their hypothesis to guess the probability distribution of states of the system, they <i>get the correct answer</i> (in the limit of many measurements).</p>
<p>In this part all I will really do is explain precisely what <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> are.  But to whet your appetite, let me explain how we can use them to give a new characterization of relative entropy!</p>
<p>Suppose we have any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}." class="latex" />  In other words: suppose we have a deterministic measurement process, together with a hypothesis that lets the observer guess a probability distribution of states of the system being measured, based on what they observe.</p>
<p>Then we have <i>two</i> probability distributions on the states of the system being measured!  First, the &#8216;true&#8217; probability distribution.  Second, the probability that the observer will guess based on their observations.</p>
<p>Whenever we have two probability distributions on the same set, we can compute the entropy of the first <i>relative to</i> to the second.  This describes how surprised you&#8217;ll be if you discover the probability distribution is really the first, when you thought it was the second.</p>
<p>So: any morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> will have a relative entropy.  It will describe how surprised the observer will be when they discover the true probability distribution, given what they had guessed.</p>
<p>But this amount of surprise will be <i>zero</i> if their hypothesis was &#8216;optimal&#8217; in the sense I described.   So, the relative entropy will vanish on morphisms in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}." class="latex" /></p>
<p>Our theorem says this fact almost characterizes the concept of relative entropy!  More precisely, it says that any convex-linear lower semicontinuous functor</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3A+%5Cmathrm%7BFinStat%7D+%5Cto+%5B0%2C%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F : &#92;mathrm{FinStat} &#92;to [0,&#92;infty] " class="latex" /></p>
<p>that vanishes on the subcategory <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> must equal some constant times the relative entropy.</p>
<p>Don&#8217;t be scared!  This should not make sense to you yet, since I haven&#8217;t said how I&#8217;m thinking of <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%2B%5Cinfty%5D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,+&#92;infty] " class="latex" /> as a category, nor what a &#8216;convex-linear lower semicontinuous functor&#8217; is, nor how relative entropy gives one.  I will explain all that later.  I just want you to get a vague idea of where I&#8217;m going.</p>
<p>Now let me explain the categories <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}." class="latex" />  We need to warm up a bit first.</p>
<h3> FinStoch </h3>
<p>A stochastic map <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;leadsto Y" class="latex" /> is different from an ordinary function, because instead of assigning a unique element of <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> to each element of <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> it assigns a <i>probability distribution on</i> <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> to each element of <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  So you should imagine it as being like a function &#8216;with random noise added&#8217;, so that <img src="https://s0.wp.com/latex.php?latex=f%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)" class="latex" /> is not a specific element of <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> but instead has a probability of taking on different values.  This is why I&#8217;m using a weird wiggly arrow to denote a stochastic map.</p>
<p>More formally:</p>
<p><b>Definition.</b>  Given finite sets <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> a <b>stochastic map</b> <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;leadsto Y" class="latex" /> assigns a real number <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx}" class="latex" /> to each pair <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X%2C+y+%5Cin+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X, y &#92;in Y," class="latex" /> such that fixing any element <img src="https://s0.wp.com/latex.php?latex=x%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x," class="latex" /> the numbers <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx}" class="latex" /> form a probability distribution on <img src="https://s0.wp.com/latex.php?latex=Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y." class="latex" />  We call <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx}" class="latex" /> <b>the probability of <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x." class="latex" /></b></p>
<p>In more detail:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx} &#92;ge 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y." class="latex" /></p>
<p>and</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7By+%5Cin+Y%7D+f_%7Byx%7D+%3D+1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{y &#92;in Y} f_{yx} = 1}" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X." class="latex" /></p>
<p>Note that we can think of <img src="https://s0.wp.com/latex.php?latex=f+%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : X &#92;leadsto Y" class="latex" /> as a <img src="https://s0.wp.com/latex.php?latex=Y+%5Ctimes+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y &#92;times X" class="latex" />-shaped matrix of numbers.  A matrix obeying the two properties above is called <b>stochastic</b>.  This viewpoint is nice because it reduces the problem of composing stochastic maps to matrix multiplication.  It&#8217;s easy to check that multiplying two stochastic matrices gives a stochastic matrix.  So, composing stochastic maps gives a stochastic map.</p>
<p>We thus get a category:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}" class="latex" /> be the category of finite sets and stochastic maps between them.</p>
<p>In case you&#8217;re wondering why I&#8217;m restricting attention to <i>finite</i> sets, it&#8217;s merely because I want to keep things simple.  I don&#8217;t want to worry about whether sums or integrals converge.</p>
<h3> FinProb </h3>
<p>Now take your favorite 1-element set and call it <img src="https://s0.wp.com/latex.php?latex=1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1." class="latex" />  A function <img src="https://s0.wp.com/latex.php?latex=p+%3A+1+%5Cto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : 1 &#92;to X" class="latex" /> is just a point of <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  But a stochastic map <img src="https://s0.wp.com/latex.php?latex=p+%3A+1+%5Cleadsto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : 1 &#92;leadsto X" class="latex" /> is something more interesting: it&#8217;s a probability distribution on <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" /></p>
<p>Why?  Because it gives a probability distribution on <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> for each element of <img src="https://s0.wp.com/latex.php?latex=1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1," class="latex" /> but that set has just one element.</p>
<p>Last time I introduced the rather long-winded phrase <b>finite probability measure space</b> to mean a finite set with a probability distribution on it.  But now we&#8217;ve seen a very quick way to describe such a thing within <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}:" class="latex" /></p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>And this gives a quick way to think about a measure-preserving function between finite probability measure spaces!  It&#8217;s just a commutative triangle like this:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>Note that the horizontal arrow <img src="https://s0.wp.com/latex.php?latex=f%3A++X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f:  X &#92;to Y" class="latex" /> is not wiggly. The straight arrow means it&#8217;s an honest function, not a stochastic map.  But a function is a special case of a stochastic map!  So it makes sense to compose a straight arrow with a wiggly arrow&#8212;and the result is, in general, a wiggly arrow.  So, it makes sense to demand that this triangle commutes, and this says that the function <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;to Y" class="latex" /> is measure-preserving.</p>
<p>Let me work through the details, in case they&#8217;re not clear.</p>
<p>First: how is a function a special case of a stochastic map?  Here&#8217;s how.  If we start with a function <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cto+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;to Y," class="latex" /> we get a matrix of numbers</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_%7Byx%7D+%3D+%5Cdelta_%7By%2Cf%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{yx} = &#92;delta_{y,f(x)} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;delta" class="latex" /> is the Kronecker delta.  So, each element <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> gives a probability distribution that&#8217;s zero except at <img src="https://s0.wp.com/latex.php?latex=f%28x%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)." class="latex" /></p>
<p>Given this, we can work out what this commuting triangle really says:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>If use <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> to stand for the probability distribution that <img src="https://s0.wp.com/latex.php?latex=p%3A+1+%5Cleadsto+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p: 1 &#92;leadsto X" class="latex" /> puts on <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> and similarly for <img src="https://s0.wp.com/latex.php?latex=q_y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_y," class="latex" /> the commuting triangle says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+X%7D+%5Cdelta_%7By%2Cf%28x%29%7D+p_x%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in X} &#92;delta_{y,f(x)} p_x} " class="latex" /></p>
<p>or in other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+X+%3A+f%28x%29+%3D+y%7D+p_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in X : f(x) = y} p_x } " class="latex" /></p>
<p>or if you like:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+f%5E%7B-1%7D%28y%29%7D+p_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in f^{-1}(y)} p_x } " class="latex" /></p>
<p>In this situation people say <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> <b>pushed forward along <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /></b>, and they say <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a <b>measure-preserving function</b>.</p>
<p>So, we&#8217;ve used <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}" class="latex" /> to describe another important category:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}" class="latex" /> be the category of finite probability measure spaces and measure-preserving functions between them.</p>
<p>I can&#8217;t resist mentioning another variation:</p>
<div align="center">
<img height="200" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/measure-preserving_stochastic_map.jpg" />
</div>
<p>A commuting triangle like this is a <b>measure-preserving stochastic map</b>.  In other words, <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> gives a probability measure on <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> gives a probability measure on <img src="https://s0.wp.com/latex.php?latex=Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cleadsto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;leadsto Y" class="latex" /> is a stochastic map with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_y+%3D+%5Csum_%7Bx+%5Cin+X%7D+f_%7Byx%7D+p_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_y = &#92;sum_{x &#92;in X} f_{yx} p_x } " class="latex" /></p>
<h3> FinStat </h3>
<p>The category we really need for relative entropy is a bit more subtle.  An object is a finite probability measure space:</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>but a morphism looks like this:</p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>The whole diagram doesn&#8217;t commute, but the two equations I wrote down hold.  The first equation says that <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Cto+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: X &#92;to Y" class="latex" /> is a measure-preserving function.  In other words, this triangle, which we&#8217;ve seen before, commutes:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>The second equation says that <img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ s" class="latex" /> is the identity, or in math jargon, <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is a <a href="https://en.wikipedia.org/wiki/Section_%28category_theory%29"><b>section</b></a> for <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>But what does that <i>really mean?</i></p>
<p>The idea is that <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> is the set of &#8216;states&#8217; of some system, while <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> is a set of possible &#8216;observations&#8217; you might make.  The function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a &#8216;measurement process&#8217;. You &#8216;measure&#8217; the system using <img src="https://s0.wp.com/latex.php?latex=f%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f," class="latex" /> and if the system is in the the state <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> you get the observation <img src="https://s0.wp.com/latex.php?latex=f%28x%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x)." class="latex" />  The probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> says the probability that the system is any given state, while <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> says the probability that you get any given observation when you do your measurement.</p>
<p>Note: are assuming for now that that there&#8217;s no random noise in the observation process!  That&#8217;s why <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a function instead of a stochastic map.</p>
<p>But what about <img src="https://s0.wp.com/latex.php?latex=s%3F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s?" class="latex" />  That&#8217;s the fun part: <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> describes your &#8216;hypothesis&#8217; about the system&#8217;s state given a particular measurement!  If you measure the system and get a result <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y," class="latex" /> you guess it&#8217;s in the state <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> with probability <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy}." class="latex" /></p>
<p>And we don&#8217;t want this hypothesis to be really dumb: that&#8217;s what</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+s+%3D+1_Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ s = 1_Y" class="latex" /></p>
<p>says.  You see, this equation says that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+X%7D+%5Cdelta_%7By%27%2C+f%28x%29%7D+s_%7Bxy%7D+%3D+%5Cdelta_%7By%27+y%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in X} &#92;delta_{y&#039;, f(x)} s_{xy} = &#92;delta_{y&#039; y} " class="latex" /></p>
<p>or in other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx+%5Cin+f%5E%7B-1%7D%28y%27%29%7D+s_%7Bxy%7D+%3D+%5Cdelta_%7By%27+y%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{x &#92;in f^{-1}(y&#039;)} s_{xy} = &#92;delta_{y&#039; y} " class="latex" /></p>
<p>If you think about it, this implies <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy} = 0" class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y." class="latex" /></p>
<p>So, if you make an observation <img src="https://s0.wp.com/latex.php?latex=y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y," class="latex" /> you will guess the system is in state <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> with <i>probability zero</i> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y." class="latex" />  In short, you won&#8217;t make a really dumb guess about the system&#8217;s state.</p>
<p>Here&#8217;s how we compose morphisms:</p>
<div align="center">
<img height="250" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_composition.jpg" />
</div>
<p>We get a measure-preserving function <img src="https://s0.wp.com/latex.php?latex=g+%5Ccirc+f+%3A+X+%5Cto+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;circ f : X &#92;to Z" class="latex" /> and a stochastic map going back, <img src="https://s0.wp.com/latex.php?latex=s+%5Ccirc+t+%3A+Z+%5Cto+Z.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s &#92;circ t : Z &#92;to Z." class="latex" /> You can check that these obey the required equations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%5Ccirc+f+%5Ccirc+p+%3D+r&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;circ f &#92;circ p = r" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%5Ccirc+f+%5Ccirc+s+%5Ccirc+t+%3D+1_Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;circ f &#92;circ s &#92;circ t = 1_Z" class="latex" /></p>
<p>So, we get a category:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> be the category where an object is a finite probability measure space:</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>a morphism is a diagram obeying these equations:</p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>and composition is defined as above.</p>
<h3> FP  </h3>
<p>As we&#8217;ve just seen, a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> consists of a &#8216;measurement process&#8217; <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and a &#8216;hypothesis&#8217; <img src="https://s0.wp.com/latex.php?latex=s%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s:" class="latex" /></p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>But sometimes we&#8217;re lucky and our hypothesis is optimal, in the sense that</p>
<p><img src="https://s0.wp.com/latex.php?latex=s+%5Ccirc+q+%3D+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s &#92;circ q = p" class="latex" /></p>
<p>Conceptually, this says that if you take the probability distribution <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> on our observations and use it to guess a probability distribution for the system&#8217;s state using our hypothesis <img src="https://s0.wp.com/latex.php?latex=s%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s," class="latex" /> you <i>get the correct answer</i>: <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" /></p>
<p>Mathematically, it says that this diagram commutes:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/measure-preserving_stochastic_map_2.jpg" />
</div>
<p>In other words, <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is a measure-preserving stochastic map.</p>
<p>There&#8217;s a subcategory of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> with all the same objects, but only these &#8216;optimal&#8217; morphisms.  It&#8217;s important, but the name we have for it is not very exciting:</p>
<p><b>Definition.</b>  Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> be the subcategory of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> where an object is a finite probability measure space</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>and a morphism is a diagram obeying these equations:</p>
<div align="center">
<img height="260" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/FP_morphism.jpg" />
</div>
<p>Why do we call this category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" />?  Because it&#8217;s a close relative of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}," class="latex" /> where a morphism, you&#8217;ll remember, looks like this:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>The point is that for a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}," class="latex" /> the conditions on <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> are so strong that they completely determine it <i>unless there are observations that happen with probability zero</i>&#8212;that is, unless there are <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=q_y+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_y = 0." class="latex" />  To see this, note that</p>
<p><img src="https://s0.wp.com/latex.php?latex=s+%5Ccirc+q+%3D+p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s &#92;circ q = p" class="latex" /></p>
<p>actually says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7By+%5Cin+Y%7D+s_%7Bxy%7D+q_y+%3D+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{y &#92;in Y} s_{xy} q_y = p_x " class="latex" /></p>
<p>for any choice of <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X." class="latex" />   But we&#8217;ve already seen <img src="https://s0.wp.com/latex.php?latex=s_%7Bxy%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{xy} = 0" class="latex" /> unless <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+y%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(x) = y," class="latex" /> so the sum has just one term, and the equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=s_%7Bx%2Cf%28x%29%7D+q_%7Bf%28x%29%7D+%3D+p_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{x,f(x)} q_{f(x)} = p_x " class="latex" /></p>
<p>We can solve this for <img src="https://s0.wp.com/latex.php?latex=s_%7Bx%2Cf%28x%29%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{x,f(x)}," class="latex" /> so <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is completely determined&#8230; <i>unless <img src="https://s0.wp.com/latex.php?latex=q_%7Bf%28x%29%7D+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_{f(x)} = 0." class="latex" /></i></p>
<p>This covers the case when <img src="https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y = f(x)." class="latex" />   We also can&#8217;t figure out <img src="https://s0.wp.com/latex.php?latex=s_%7Bx%2Cy%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s_{x,y}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> isn&#8217;t in the image of <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>So, to be utterly precise, <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="s" class="latex" /> is determined by <img src="https://s0.wp.com/latex.php?latex=p%2C+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> unless there&#8217;s an element <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in Y" class="latex" /> that has <img src="https://s0.wp.com/latex.php?latex=q_y+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_y = 0." class="latex" />  Except for this special case, a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> is just a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}." class="latex" />  But in this special case, a morphism in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> has a little extra information: an arbitrary probability distribution on the inverse image of each point <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y" class="latex" /> with this property.</p>
<p>In short, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> is the same as <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}" class="latex" /> except that our observer&#8217;s &#8216;optimal hypothesis&#8217; must provide a guess about the state of the system given an observation, <i>even in cases of observations that occur with probability zero.</i></p>
<p>I&#8217;m going into these nitpicky details for two reasons.  First, we&#8217;ll need <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FP}" class="latex" /> for our characterization of relative entropy.  But second, Tom Leinster <i>already ran into this category</i> in his work on entropy and category theory!  He discussed it here:</p>
<p>&bull; Tom Leinster, <a href="http://golem.ph.utexas.edu/category/2011/05/an_operadic_introduction_to_en.html">An operadic introduction to entropy</a>.</p>
<p>Despite the common theme of entropy, he arrived at it from a very different starting-point.</p>
<h3> Conclusion </h3>
<p>So, I hope that next time I can show you something like this:</p>
<div align="center">
<img height="150" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_object.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a probability distribution on the states of some system!&#8221;  Intuitively, you should think of the wiggly arrow <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as picking out a &#8216;random element&#8217; of the set <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" /></p>
<p>I hope I can show you this:</p>
<div align="center">
<img height="200" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/FinProb_morphism.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a deterministic measurement process, sending a probability distribution on the states of the measured system to a probability distribution on observations!&#8221;</p>
<p>I hope I can show you this:</p>
<div align="center">
<img height="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/FinStat_morphism.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a deterministic measurement process, together with a hypothesis about the system&#8217;s state, given what is observed!&#8221;</p>
<p>And I hope I can show you this:</p>
<div align="center">
<img height="260" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/FP_morphism.jpg" />
</div>
<p>and you&#8217;ll say &#8220;Oh, that&#8217;s a deterministic measurement process, together with an <i>optimal</i> hypothesis about the system&#8217;s state, given what is observed!&#8221;</p>
<p>I don&#8217;t count on it&#8230; but I can hope.</p>
<h3> Postscript </h3>
<p>And speaking of unrealistic hopes, if I were <i>really</i> optimistic I would hope you noticed that <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}" class="latex" /> and  <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb}," class="latex" /> which underlie the more fancy categories I&#8217;ve discussed today, were themselves constructed starting from linear algebra over the nonnegative numbers <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> in <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Part 1</a>.   That &#8216;foundational&#8217; work is not really needed for what we&#8217;re doing now.  However, I like the fact that we&#8217;re ultimately getting the concept of relative entropy starting from very little: just linear algebra, using only nonnegative numbers!</p>
<p>For more details, here&#8217;s the actual paper:</p>
<p>‚Ä¢ John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>, <i><a href="http://www.tac.mta.ca/tac/volumes/29/16/29-16abs.html">Theory and Applications of Categories</a></i> <b>29</b> (2014), 421&ndash;456.</p>
<p>And here&#8217;s my whole series of blog articles about it:</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy (Part 1)</a>: how various structures important in probability theory arise naturally when you do linear algebra using only the nonnegative real numbers.</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Relative Entropy (Part 2)</a>: a category related to statistical inference, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and how relative entropy defines a functor on this category.</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Relative Entropy (Part 3)</a>: how to characterize relative entropy as a functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty])." class="latex" /></p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/">Relative Entropy (Part 4)</a>: wrap-up, and an invitation to read more about the underlying math at the <a href="https://golem.ph.utexas.edu/category/2014/02/relative_entropy.html"><i>n</i>-Category Caf&eacute;</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/#comments">11 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/" rel="bookmark" title="Permanent Link to Relative Entropy (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16174 post type-post status-publish format-standard hentry category-information-and-entropy category-probability" id="post-16174">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/" rel="bookmark">Relative Entropy (Part&nbsp;1)</a></h2>
				<small>20 June, 2013</small><br />


				<div class="entry">
					<p>I&#8217;m trying to finish off a paper that Tobias Fritz and I have been working on, which gives a category-theoretic (and Bayesian!) characterization of <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">relative entropy</a>.  It&#8217;s a kind of sequel to <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/">our paper with Tom Leinster</a>, in which we characterized entropy.</p>
<p>That earlier paper was developed in conversations <a href="http://ncatlab.org/johnbaez/show/A+characterization+of+entropy+in+terms+of+information+loss">on the <i>n</i>-Category Caf&eacute;</a>.  It was a lot of fun; I sort of miss that style of working.    Also, to get warmed up, I need to think through some things I&#8217;ve thought about before.   So, I might as well write them down here.</p>
<h3> The idea </h3>
<p>There are many categories related to probability theory, and they&#8217;re related in many ways.  Last summer&#8212;on the 24th of August 2012, according to my notes here&#8212;Jamie Vicary, Brendan Fong and I worked through a bunch of these relationships.  I need to write them down now, even if they&#8217;re not all vitally important to my paper with Tobias.  They&#8217;re sort of buzzing around my brain like flies.</p>
<p>(Tobias knows this stuff too, and this is how we think about probability theory, but we weren&#8217;t planning to stick it in our paper.  Maybe we should.)</p>
<p>Let&#8217;s restrict attention to probability measures on <i>finite sets</i>, and related structures.  We could study these questions more generally, and we should, but not today.  What we&#8217;ll do is give a unified purely algebraic description of:</p>
<p>&bull; finite sets</p>
<p>&bull; measures on finite sets</p>
<p>&bull; probability measures on finite sets</p>
<p>and various kinds of maps between these:</p>
<p>&bull; functions</p>
<p>&bull; bijections</p>
<p>&bull; measure-preserving functions</p>
<p>&bull; stochastic maps</p>
<h3>  Finitely generated free [0,&infin;)-modules </h3>
<p>People often do linear algebra over a <a href="http://en.wikipedia.org/wiki/Field_%28mathematics%29">field</a>, which is&#8212;roughly speaking&#8212;a number system where you can add, subtract, multiply and divide.  But algebraists have long realized that a lot of linear algebra still works with a <a href="http://en.wikipedia.org/wiki/Commutative_ring">commutative ring</a>, where you can&#8217;t necessarily divide.  It gets more complicated, but also a lot more interesting.</p>
<p>But in fact, a lot still works with a commutative <a href="http://ncatlab.org/nlab/show/rig">rig</a>, where we can&#8217;t necessarily subtract either!  Something I keep telling everyone is that linear algebra over rigs is a good idea for studying things like probability theory, thermodynamics, and the principle of least action.</p>
<p>Today we&#8217;ll start with the rig of nonnegative real numbers with their usual addition and multiplication; let&#8217;s call this <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ." class="latex" />   The idea is that measure theory, and probability theory, are closely related to linear algebra over this rig.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> be the category with of <a href="http://en.wikipedia.org/wiki/Finitely-generated_module">finitely generated</a> <a href="http://en.wikipedia.org/wiki/Free_module">free</a> <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" />-modules as objects, and <a href="http://en.wikipedia.org/wiki/Module_homomorphism#Submodules_and_homomorphisms">module homomorphisms</a> as morphisms.   I&#8217;ll call these morphisms <b>maps</b>.</p>
<p><b>Puzzle.</b> Do we need to say &#8216;free&#8217; here?  Are there finitely generated modules over <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" /> that aren&#8217;t free?</p>
<p>Every finitely generated free <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" />-module is isomorphic to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S " class="latex" /> for some finite set <img src="https://s0.wp.com/latex.php?latex=S+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S ." class="latex" /> In other words, it&#8217;s isomorphic to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5En+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^n " class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=n+%3D+0%2C+1%2C+2%2C+%5Cdots+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 0, 1, 2, &#92;dots ." class="latex" />   So, <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is equivalent to the category where objects are natural numbers, a morphism from <img src="https://s0.wp.com/latex.php?latex=m+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m " class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n " class="latex" /> is an <img src="https://s0.wp.com/latex.php?latex=m+%5Ctimes+n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m &#92;times n " class="latex" /> matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ," class="latex" /> and composition is done by matrix multiplication.  I&#8217;ll also call this equivalent category <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /></p>
<p>We can take tensor products of finitely generated free modules, and this makes  <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> into a <a href="http://ncatlab.org/nlab/show/symmetric+monoidal+dagger-category">symmetric monoidal &dagger;-category</a>.  This means we can draw maps using <a href="http://arxiv.org/abs/0903.0340">string diagrams</a> in the usual way.  However, I&#8217;m feeling lazy so I&#8217;ll often write equations when I could be drawing diagrams.</p>
<p>One of the rules of the game is that all these equations will make sense in <i>any</i> symmetric monoidal &dagger;-category.  So we could, if we wanted, generalize ideas from probability theory this way.  If you want to do this, you&#8217;ll need to know that <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> is the unit for the tensor product in <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" />   We&#8217;ll be seeing this guy <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> a lot.  So if you want to generalize, replace <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> by any symmetric monoidal &dagger;-category, and replace <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> by the unit for the tensor product.</p>
<h3> Finite sets </h3>
<p>There&#8217;s a way to see the category of finite sets lurking in <img src="https://s0.wp.com/latex.php?latex=C%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C," class="latex" /> which we can borrow from this paper:</p>
<p>&bull; Bob Coecke, Dusko Pavlovic and Jamie Vicary, <a href="http://arxiv.org/abs/0810.0812">A new description of orthogonal bases</a>.</p>
<p>For any finite set <img src="https://s0.wp.com/latex.php?latex=S+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S ," class="latex" /> we get a free finitely generated <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" />-module, namely <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S ." class="latex" />  This comes with some structure:</p>
<p>&bull; a multiplication <img src="https://s0.wp.com/latex.php?latex=m%3A+%5B0%2C%5Cinfty%29%5ES+%5Cotimes+%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C%5Cinfty%29%5ES+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m: [0,&#92;infty)^S &#92;otimes [0,&#92;infty)^S &#92;to [0,&#92;infty)^S ," class="latex" /> coming from pointwise multiplication of <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" />-valued functions on <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /></p>
<p>&bull; the unit for this multiplication, an element of <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S," class="latex" /> which we can write as a morphism <img src="https://s0.wp.com/latex.php?latex=i%3A+%5B0%2C%5Cinfty%29+%5Cto+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i: [0,&#92;infty) &#92;to [0,&#92;infty)^S " class="latex" /></p>
<p>&bull; a comultiplication, obtained by taking the diagonal map <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%3A+S+%5Cto+S+%5Ctimes+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta : S &#92;to S &#92;times S " class="latex" /> and promoting it to a linear map <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%3A+%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C+%5Cinfty%29%5ES+%5Cotimes+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta : [0,&#92;infty)^S &#92;to [0, &#92;infty)^S &#92;otimes [0,&#92;infty)^S " class="latex" /></p>
<p>&bull; a counit for this comultiplication, obtained by taking the unique map to the terminal set <img src="https://s0.wp.com/latex.php?latex=%21+%3A+S+%5Cto+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="! : S &#92;to 1 " class="latex" /> and promoting it to a linear map <img src="https://s0.wp.com/latex.php?latex=e%3A+%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e: [0,&#92;infty)^S &#92;to [0,&#92;infty)" class="latex" /></p>
<p>These morphisms <img src="https://s0.wp.com/latex.php?latex=m%2C+i%2C+%5CDelta%2C+e+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="m, i, &#92;Delta, e " class="latex" /> make</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%3D+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x = [0,&#92;infty)^S " class="latex" /></p>
<p>into a commutative Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C ." class="latex" />  That&#8217;s a thing where the unit, counit, multiplication and comultiplication obey these laws:</p>
<div align="center">
<img src="https://i0.wp.com/math.ucr.edu/home/baez/commutative_frobenius_algebra.jpg" alt="" />
</div>
<p>(I drew these back when I was feeling less lazy.)   This Frobenius algebra is also &#8216;special&#8217;, meaning it obeys this:</p>
<div align="center">
<img width="300" src="https://i2.wp.com/math.ucr.edu/home/baez/separability.jpg" alt="" />
</div>
<p>And it&#8217;s also a &dagger;-Frobenius algebra, meaning that the counit and comultiplication are obtained from the unit and multiplication by &#8216;flipping&#8217; them using the <a href="http://ncatlab.org/nlab/show/dagger-category">&dagger;category</a> structure.  (If we think of a morphism in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> as a matrix, its dagger is its transpose.)</p>
<p>Conversely, suppose we have <i>any</i> special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ." class="latex" />  Then using the ideas in the paper by Coecke, Pavlovich and Vicary we can recover a basis for <img src="https://s0.wp.com/latex.php?latex=x+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ," class="latex" /> consisting of the vectors <img src="https://s0.wp.com/latex.php?latex=e_i+%5Cin+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e_i &#92;in x " class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta%28e_i%29+%3D+e_i+%5Cotimes+e_i+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta(e_i) = e_i &#92;otimes e_i " class="latex" /></p>
<p>This basis forms a set <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Ccong+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;cong [0,&#92;infty)^S " class="latex" /></p>
<p>for some <i>specified</i> isomorphism in <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" />  Furthermore, this is an isomorphism of special commutative &dagger;-Frobenius algebras!</p>
<p>In case you&#8217;re wondering, these vectors <img src="https://s0.wp.com/latex.php?latex=e_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e_i" class="latex" /> correspond to the functions on <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> that are zero everywhere except at one point <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+S%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in S," class="latex" /> where they equal 1.</p>
<p>In short, a special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is just a fancy way of talking about a finite set.  This may seem silly, but it&#8217;s a way to start describing probability theory using linear algebra very much as we do with quantum theory.  This analogy between quantum theory and probability theory is so interesting that it deserves a <a href="http://math.ucr.edu/home/baez/stoch_stable.pdf">book</a>.</p>
<h3> Functions and bijections </h3>
<p>Now suppose we have two special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y ." class="latex" /></p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /> is a Frobenius algebra homomorphism: that is, a map preserving <i>all</i> the structure&#8212;the unit, counit, multiplication and comultiplication.  Then it comes from an isomorphism of finite sets.   This lets us find <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinSet%7D_0+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinSet}_0 ," class="latex" /> the groupoid of finite sets and bijections, inside <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /></p>
<p>Alternatively, suppose <img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /> is just a coalgebra homomorphism: that is a map preserving just the counit and comultiplication.  Then it comes from an arbitrary function between finite sets.  This lets us find <img src="https://s0.wp.com/latex.php?latex=FinSet+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="FinSet ," class="latex" /> the category of finite sets and functions, inside <img src="https://s0.wp.com/latex.php?latex=C+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C ." class="latex" /></p>
<p>But what if <img src="https://s0.wp.com/latex.php?latex=f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f " class="latex" /> preserves just the counit?  This sounds like a dry, formal question.  But it&#8217;s not: the answer is something useful, a &#8216;stochastic map&#8217;.</p>
<h3>  Stochastic maps </h3>
<p>A <b>stochastic map</b> from a finite set <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> to a finite set <img src="https://s0.wp.com/latex.php?latex=T+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T " class="latex" /> is a map sending each point of <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> to a probability measure on <img src="https://s0.wp.com/latex.php?latex=T+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T ." class="latex" /></p>
<p>We can think of this as a <img src="https://s0.wp.com/latex.php?latex=T+%5Ctimes+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;times S " class="latex" />-shaped matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ," class="latex" /> where a given column gives the probability that a given point in <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> goes to any point in <img src="https://s0.wp.com/latex.php?latex=T+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T ." class="latex" />  The sum of the numbers in each column will be 1.  And conversely, any  <img src="https://s0.wp.com/latex.php?latex=T+%5Ctimes+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;times S " class="latex" />-shaped matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ," class="latex" /> where each column sums to 1, gives a stochastic map from <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=T+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T ." class="latex" /></p>
<p>But now let&#8217;s describe this idea using the category <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" />  We&#8217;ve seen a finite set is the same as a special commutative &dagger;-Frobenius algebra.  So, say we have two of these, <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y ." class="latex" />  Our matrix of numbers in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) " class="latex" /> is just a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: x &#92;to y " class="latex" /></p>
<p>So, we just need a way to state the condition that each column in the matrix sums to 1.  And this condition simply says that <img src="https://s0.wp.com/latex.php?latex=f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f " class="latex" /> preserves the counit:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+%5Ccirc+f+%3D+%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y &#92;circ f = &#92;epsilon_x " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_x+%3A+x+%5Cto+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_x : x &#92;to [0,&#92;infty) " class="latex" /> is the counit for <img src="https://s0.wp.com/latex.php?latex=x+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ," class="latex" /> and similarly for <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y ." class="latex" /></p>
<p>To understand this, note that if we use the canonical isomorphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Ccong+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;cong [0,&#92;infty)^S " class="latex" /></p>
<p>the counit <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_x " class="latex" /> can be seen as the map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+%5Cto+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S &#92;to [0,&#92;infty) " class="latex" /></p>
<p>that takes any <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" />-tuple of numbers and sums them up.   In other words, it&#8217;s integration with respect to counting measure.  So, the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+%5Ccirc+f+%3D+%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y &#92;circ f = &#92;epsilon_x " class="latex" /></p>
<p>says that if we take any <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" />-tuple of numbers, multiply it by the matrix <img src="https://s0.wp.com/latex.php?latex=f+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f ," class="latex" /> and then sum up the entries of the resulting <img src="https://s0.wp.com/latex.php?latex=T+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T " class="latex" />-tuple, it&#8217;s the same as if we summed up the original <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" />-tuple.  But this says precisely that each column of the matrix <img src="https://s0.wp.com/latex.php?latex=f+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f " class="latex" /> sums to 1.</p>
<p>So, we can use our formalism to describe <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStoch%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStoch}," class="latex" /> the category with finite sets as objects and stochastic maps as morphisms.  We&#8217;ve seen this category is equivalent to the category with special commutative &dagger;-Frobenius algebras in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> as objects and counit-preserving maps as morphisms.</p>
<h3> Finite measure spaces </h3>
<p>Now let&#8217;s use our formalism to describe finite measure spaces&#8212;by which, beware, I mean a finite sets equipped with measures!  To do this, we&#8217;ll use a special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> together with any map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>Starting from these, we get a specified isomorphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%5Ccong+%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;cong [0,&#92;infty)^S " class="latex" /></p>
<p>and <img src="https://s0.wp.com/latex.php?latex=%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu " class="latex" /> sends the number 1 to a vector in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29%5ES+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)^S " class="latex" />: that is, a function on <img src="https://s0.wp.com/latex.php?latex=S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S " class="latex" /> taking values in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ." class="latex" />   Multiplying this function by counting measure, we get a <i>measure</i> on <img src="https://s0.wp.com/latex.php?latex=S+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S ." class="latex" /></p>
<p><b>Puzzle.</b>  How can we describe this measure without the annoying use of counting measure?</p>
<p>Conversely, any measure on a finite set gives a special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C " class="latex" /> equipped with a map from <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty) ." class="latex" /></p>
<p>So, we can say a finite measure space is a special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C " class="latex" /> equipped with a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>And given two of these,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+%2C+%5Cqquad++%5Cnu%3A+%5B0%2C%5Cinfty%29+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x , &#92;qquad  &#92;nu: [0,&#92;infty) &#92;to y" class="latex" /></p>
<p>and a coalgebra morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /></p>
<p>obeying this equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+%5Cmu++%3D+%5Cnu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ &#92;mu  = &#92;nu " class="latex" /></p>
<p>then we get a measure-preserving function between finite measure spaces!    If you&#8217;re a category theorist, you&#8217;ll draw this equation as a commutative triangle:</p>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/relative_entropy_commutative_triangle.jpg" />
</div>
<p>Conversely, any measure-preserving function between finite measure spaces obeys this equation.  So, we get an algebraic way of describing the category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinMeas%7D+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinMeas} ," class="latex" /> with finite measure spaces as objects and measure-preserving maps as morphisms.</p>
<h3> Finite probability measure spaces </h3>
<p>I&#8217;m mainly interested in probability measures.  So suppose <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> is a special commutative &dagger;-Frobenius algebra in <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> equipped with a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>We&#8217;ve seen this gives a finite measure space.  But this is a probability measure space if and only if</p>
<p><img src="https://s0.wp.com/latex.php?latex=e+%5Ccirc+%5Cmu+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e &#92;circ &#92;mu = 1 " class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=e+%3A+x+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e : x &#92;to [0,&#92;infty)" class="latex" /></p>
<p>is the counit for <img src="https://s0.wp.com/latex.php?latex=x+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x ." class="latex" />  The equation simply says the total integral of our measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu " class="latex" /> is 1.</p>
<p>So, we get a way of describing the category <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinProb%7D+%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinProb} ," class="latex" /> which has finite probability measure spaces as objects and measure-preserving maps as objects.  Given finite probability measure spaces described this way:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+%2C+%5Cqquad++%5Cnu%3A+%5B0%2C%5Cinfty%29+%5Cto+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x , &#92;qquad  &#92;nu: [0,&#92;infty) &#92;to y" class="latex" /></p>
<p>a measure-preserving function is a coalgebra morphism</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : x &#92;to y " class="latex" /></p>
<p>such that the obvious triangle commutes:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+%5Cmu++%3D+%5Cnu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ &#92;mu  = &#92;nu " class="latex" /></p>
<h3> Measure-preserving stochastic maps </h3>
<p>Say we have two finite measure spaces.  Then we can ask whether a stochastic map from one to the other is measure-preserving.  And we can answer this question in the language of <img src="https://s0.wp.com/latex.php?latex=C+.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C ." class="latex" /></p>
<p>Remember, a finite measure space is a special commutative &dagger;-Frobenius algebra <img src="https://s0.wp.com/latex.php?latex=x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x " class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=C+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C " class="latex" /> together with a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu%3A+%5B0%2C%5Cinfty%29+%5Cto+x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu: [0,&#92;infty) &#92;to x " class="latex" /></p>
<p>Say we have another one:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnu%3A+%5B0%2C%5Cinfty%29+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu: [0,&#92;infty) &#92;to y " class="latex" /></p>
<p>A stochastic map is just a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%3A+x+%5Cto+y+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: x &#92;to y " class="latex" /></p>
<p>that preserves the counit:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_y+%5Ccirc+f+%3D+%5Cepsilon_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;epsilon_y &#92;circ f = &#92;epsilon_x " class="latex" /></p>
<p>But it&#8217;s a <b>measure-preserving stochastic map</b> if also</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+%5Cmu++%3D+%5Cnu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;circ &#92;mu  = &#92;nu " class="latex" /></p>
<h3> Addendum </h3>
<p>In this article I didn&#8217;t get anywhere near to talking about what Tobias and I were actually doing!  But it was good to get this basic stuff written down.</p>
<p>Here&#8217;s the paper we eventually wrote:</p>
<p>‚Ä¢ John Baez and Tobias Fritz, <a href="http://arxiv.org/abs/1402.3067">A Bayesian characterization of relative entropy</a>, <i><a href="http://www.tac.mta.ca/tac/volumes/29/16/29-16abs.html">Theory and Applications of Categories</a></i> <b>29</b> (2014), 421&ndash;456.</p>
<p>And here&#8217;s my whole series of blog articles about it:</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/">Relative Entropy (Part 1)</a>: how various structures important in probability theory arise naturally when you do linear algebra using only the nonnegative real numbers.</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/07/02/relative-entropy-part-2/">Relative Entropy (Part 2)</a>: a category related to statistical inference, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}," class="latex" /> and how relative entropy defines a functor on this category.</p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2013/12/25/relative-entropy-part-3/">Relative Entropy (Part 3)</a>: how to characterize relative entropy as a functor from <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BFinStat%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{FinStat}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%5D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty]." class="latex" /></p>
<p>‚Ä¢ <a href="https://johncarlosbaez.wordpress.com/2014/02/16/relative-entropy-part-4/">Relative Entropy (Part 4)</a>: wrap-up, and an invitation to read more about the underlying math at the <a href="https://golem.ph.utexas.edu/category/2014/02/relative_entropy.html"><i>n</i>-Category Caf&eacute;</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/#comments">19 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/06/20/relative-entropy-part-1/" rel="bookmark" title="Permanent Link to Relative Entropy (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-14821 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy" id="post-14821">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/02/21/maximum-entropy-and-ecology/" rel="bookmark">Maximum Entropy and&nbsp;Ecology</a></h2>
				<small>21 February, 2013</small><br />


				<div class="entry">
					<p>I already talked about <a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/">John Harte&#8217;s book on how to stop global warming</a>.   Since I&#8217;m trying to apply information theory and thermodynamics to ecology, I was also interested in this book of his:</p>
<p>&bull; <a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a>, <i>Maximum Entropy and Ecology</i>, Oxford U. Press, Oxford, 2011.</p>
<p>There&#8217;s a lot in this book, and I haven&#8217;t absorbed it all, but let me try to briefly summarize his <b>maximum entropy theory of ecology</b>. This aims to be &#8220;a comprehensive, parsimonious, and testable theory of the distribution, abundance, and energetics of species across spatial scales&#8221;.  One great thing is that he makes quantitative predictions using this theory and compares them to a lot of real-world data.  But let me just tell you about the theory.  </p>
<p>It&#8217;s heavily based on the <a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy">principle of maximum entropy</a> (MaxEnt for short), and there are two parts:</p>
<blockquote><p>
Two MaxEnt calculations are at the core of the theory: the first yields all the metrics that describe abundance and energy distributions, and the second describes the spatial scaling properties of species&#8217; distributions.
</p></blockquote>
<h3> Abundance and energy distributions </h3>
<p>The first part of Harte&#8217;s theory is all about a conditional probability distribution </p>
<p><img src="https://s0.wp.com/latex.php?latex=R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(n,&#92;epsilon | S_0, N_0, E_0) " class="latex" /></p>
<p>which he calls the <b>ecosystem structure function</b>.  Here:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=S_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_0" class="latex" />: the total number of species under consideration in some area.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=N_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N_0" class="latex" />: the total number of individuals under consideration in that area.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=E_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_0" class="latex" />: the total rate of metabolic energy consumption of all these individuals.</p>
<p>Given this, </p>
<p><img src="https://s0.wp.com/latex.php?latex=R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%5C%2C+d+%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(n,&#92;epsilon | S_0, N_0, E_0) &#92;, d &#92;epsilon " class="latex" /></p>
<p>is the probability that given <img src="https://s0.wp.com/latex.php?latex=S_0%2C+N_0%2C+E_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S_0, N_0, E_0," class="latex" /> if a species is picked from the collection of species, then it has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> individuals, and if an individual is picked at random from that species, then its rate of metabolic energy consumption is in the interval <img src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C+%5Cepsilon+%2B+d+%5Cepsilon%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;epsilon, &#92;epsilon + d &#92;epsilon)." class="latex" /></p>
<p>Here of course <img src="https://s0.wp.com/latex.php?latex=d+%5Cepsilon&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d &#92;epsilon" class="latex" /> is &#8216;infinitesimal&#8217;, meaning that we take a limit where it goes to zero to make this idea precise (if we&#8217;re doing analytical work) or take it to be very small (if we&#8217;re estimating <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R" class="latex" /> from data).  </p>
<p>I believe that when we &#8216;pick a species&#8217; we&#8217;re treating them all as equally probable, not weighting them according to their number of individuals.  </p>
<p>Clearly <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R" class="latex" /> obeys some constraints.  First, since it&#8217;s a probability distribution, it obeys the normalization condition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n &#92;int d &#92;epsilon &#92;; R(n,&#92;epsilon | S_0, N_0, E_0) = 1 }" class="latex" /></p>
<p>Second, since the average number of individuals per species is <img src="https://s0.wp.com/latex.php?latex=N_0%2FS_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N_0/S_0," class="latex" /> we have:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+n+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%3D+N_0+%2F+S_0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n &#92;int d &#92;epsilon &#92;; n R(n,&#92;epsilon | S_0, N_0, E_0) = N_0 / S_0 }" class="latex" /></p>
<p>Third, since the average over species of the total rate of metabolic energy consumption of individuals within the species is <img src="https://s0.wp.com/latex.php?latex=E_0%2F+S_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_0/ S_0," class="latex" /> we have:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+n+%5Cepsilon+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%3D+E_0+%2F+S_0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n &#92;int d &#92;epsilon &#92;; n &#92;epsilon R(n,&#92;epsilon | S_0, N_0, E_0) = E_0 / S_0 }" class="latex" /></p>
<p>Harte&#8217;s theory is that <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R" class="latex" /> <i>maximizes entropy subject to these three constraints</i>.  Here entropy is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Csum_n+%5Cint+d+%5Cepsilon+%5C%3B+R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29+%5Cln%28R%28n%2C%5Cepsilon+%7C+S_0%2C+N_0%2C+E_0%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;sum_n &#92;int d &#92;epsilon &#92;; R(n,&#92;epsilon | S_0, N_0, E_0) &#92;ln(R(n,&#92;epsilon | S_0, N_0, E_0)) } " class="latex" /></p>
<p>Harte uses this theory to calculate <img src="https://s0.wp.com/latex.php?latex=R%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R," class="latex" /> and tests the results against data from about 20 ecosystems.  For example, he predicts the abundance of species as a function of their rank, with rank 1 being the most abundant, rank 2 being the second most abundant, and so on.  And he gets results like this:</p>
<div align="center">
<a href="http://math.ucr.edu/home/baez/ecological/harte_species_abundance_distribution.jpg"><img src="https://i2.wp.com/math.ucr.edu/home/baez/ecological/harte_species_abundance_distribution.jpg" /></a></div>
<p>The data here are from:</p>
<p>&bull; Green, Harte, and Ostling&#8217;s work on a serpentine grassland, </p>
<p>&bull; Luquillo&#8217;s work on a 10.24-hectare tropical forest, and</p>
<p>&bull; Cocoli&#8217;s work on a 2-hectare wet tropical forest.</p>
<p>The fit looks good to me&#8230; but I should emphasize that I haven&#8217;t had time to study these matters in detail.  For more, you can read this paper, at least if your institution subscribes to this journal:</p>
<p>&bull; J. Harte, T. Zillio, E. Conlisk and A. Smith, Maximum entropy and the state-variable approach to macroecology, <a href="http://www.esajournals.org/doi/full/10.1890/07-1369.1"><i>Ecology</i></a> <b>89</b> (2008), 2700&ndash;2711.</p>
<h3> Spatial abundance distribution </h3>
<p>The second part of Harte&#8217;s theory is all about a conditional probability distribution </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Pi(n | A, n_0, A_0) " class="latex" /></p>
<p>This is the probability that <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> individuals of a species are found in a region of area <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> given that it has <img src="https://s0.wp.com/latex.php?latex=n_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_0" class="latex" /> individuals in a larger region of area <img src="https://s0.wp.com/latex.php?latex=A_0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_0." class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Pi" class="latex" /> obeys two constraints.  First, since it&#8217;s a probability distribution, it obeys the normalization condition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n++%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n  &#92;Pi(n | A, n_0, A_0) = 1 }" class="latex" /></p>
<p>Second, since the mean value of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> across regions of area <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> equals <img src="https://s0.wp.com/latex.php?latex=n_0+A%2FA_0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n_0 A/A_0," class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_n+n+%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29+%3D+n_0+A%2FA_0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_n n &#92;Pi(n | A, n_0, A_0) = n_0 A/A_0 }" class="latex" /></p>
<p>Harte&#8217;s theory is that <img src="https://s0.wp.com/latex.php?latex=%5CPi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Pi" class="latex" /> <i>maximizes entropy subject to these two constraints</i>.   Here entropy is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B-+%5Csum_n++%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29%5Cln%28%5CPi%28n+%7C+A%2C+n_0%2C+A_0%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{- &#92;sum_n  &#92;Pi(n | A, n_0, A_0)&#92;ln(&#92;Pi(n | A, n_0, A_0)) } " class="latex" /></p>
<p>Harte explains two approaches to use this idea to derive &#8216;scaling laws&#8217; for how <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> varies with <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" />.   And again, he compares his predictions to real-world data, and get results that look good to my (amateur, hasty) eye!</p>
<p>I hope sometime I can dig deeper into this subject.  Do you have any ideas, or knowledge about this stuff?</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/02/21/maximum-entropy-and-ecology/#comments">27 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/02/21/maximum-entropy-and-ecology/" rel="bookmark" title="Permanent Link to Maximum Entropy and&nbsp;Ecology">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-14094 post type-post status-publish format-standard hentry category-economics category-information-and-entropy category-mathematics category-networks" id="post-14094">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/01/15/network-theory-for-economists/" rel="bookmark">Network Theory for&nbsp;Economists</a></h2>
				<small>15 January, 2013</small><br />


				<div class="entry">
					<p>Tomorrow I&#8217;m giving a talk in the econometrics seminar at U.C. Riverside.  I was invited to speak on my work on network theory, so I don&#8217;t feel too bad about the fact that I&#8217;ll be saying only a little about economics and practically nothing about econometrics.  Still, I&#8217;ve tried to slant the talk in a way that emphasizes possible applications to economics and game theory.  Here are the slides:</p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/econ.pdf">Network Theory</a>.</p>
<p>For long-time readers here the fun comes near the end.  I explain how <a href="http://math.ucr.edu/home/baez/networks/networks_17.html">reaction networks</a> can be used to describe <a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/">evolutionary games</a>.  I point out that in certain classes of evolutionary games, evolution tends to increase <a href="http://en.wikipedia.org/wiki/Fitness_%28biology%29">&#8216;fitness&#8217;</a>, and/or lead the players to a <a href="http://en.wikipedia.org/wiki/Nash_equilibrium">&#8216;Nash equilibrium&#8217;</a>.  For precise theorems you&#8217;ll have to click the links in my talk and read the references!   </p>
<p>I conclude with an example: a game with three strategies and 7 Nash equilibria.  Here evolution makes the proportion of these three strategies follow these flow lines, at least in the limit of large numbers of players:</p>
<div align="center"><a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf"><img width="450" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/123_coordination_game_sandholm.jpg" /></a>
</div>
<p>This picture is from William Sandholm&#8217;s nice expository paper:</p>
<p>&bull; William H. Sandholm, <a href="http://www.ssc.wisc.edu/~whs/research/egt.pdf">Evolutionary game theory</a>, 2007.</p>
<p>I mentioned it before in <a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/">Information Geometry (Part 12)</a>, en route to showing a proof that some quantity always decreases in a class of evolutionary games.  Sometime I want to tell the whole story linking:</p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/networks/networks_17.html">reaction networks</a><br />
&bull; <a href="https://johncarlosbaez.wordpress.com/2012/06/24/information-geometry-part-12/">evolutionary games</a><br />
&bull; <a href="https://johncarlosbaez.wordpress.com/2012/08/24/more-second-laws-of-thermodynamics/">the 2nd law of thermodynamics</a></p>
<p>and</p>
<p>&bull; <a href="http://download.bioon.com.cn/view/upload/month_0807/20080731_9f738b4390be637eb03aE2coCuyuHBL2.attach.pdf">Fisher&#8217;s fundamental theorem of natural selection</a>.</p>
<p>But not today!  Think of these talk slides as a little appetizer.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/01/15/network-theory-for-economists/#respond">Leave a Comment &#187;</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/economics/" rel="category tag">economics</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/networks/" rel="category tag">networks</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/01/15/network-theory-for-economists/" rel="bookmark" title="Permanent Link to Network Theory for&nbsp;Economists">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-12832 post type-post status-publish format-standard hentry category-carbon-emissions category-climate category-economics category-energy category-information-and-entropy category-sustainability" id="post-12832">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/" rel="bookmark">John Harte</a></h2>
				<small>27 October, 2012</small><br />


				<div class="entry">
					<p>Earlier this week I gave a talk on the <a href="http://math.ucr.edu/home/baez/planet/planet_usc.pdf">Mathematics of Planet Earth</a> at the University of Southern California, and someone there recommended that I look into John Harte&#8217;s work on maximum entropy methods in ecology.  He works at U.C. Berkeley.  </p>
<div align="center"><a href="http://www.forbes.com/sites/michaeltobias/2011/08/29/climate-shock-uc-berkeley-scientist-dr-john-harte-puts-the-world-on-notice/"><img width="300" src="https://i0.wp.com/blogs-images.forbes.com/michaeltobias/files/2011/08/John-Harte1.jpg" /></a></div>
<p>I checked out <a href="http://socrates.berkeley.edu/~hartelab/">his website</a> and found that his goals resemble mine: save the planet and understand its ecosystems.  He&#8217;s a lot further along than I am, since he comes from a long background in ecology while I&#8217;ve just recently blundered in from mathematical physics.  I can&#8217;t really say what I think of his work since I&#8217;m just learning about it.  But I thought I should point out its existence.</p>
<p>This free book is something a lot of people would find interesting:</p>
<p>&bull; John and Mary Ellen Harte, <a href="http://www.cooltheearth.us/download.php"><i>Cool the Earth, Save the Economy: Solving the Climate Crisis Is EASY</i></a>, 2008.</p>
<p>EASY?  Well, it&#8217;s an acronym.  Here&#8217;s the basic idea of the US-based plan described in this book:</p>
<blockquote>
<p>Any proposed energy policy should include these two components:</p>
<p>&bull; <b>Technical/Behavioral</b>: What resources and technologies are to be used to supply energy? On the demand side, what technologies and lifestyle changes are being proposed to consumers?</p>
<p>&bull; <b>Incentives/Economic Policy</b>: How are the desired supply and demand options to be encouraged or forced? Here the options include taxes, subsidies, regulations, permits, research and development, and education.</p>
<p>And a successful energy policy should satisfy the AAA criteria:</p>
<p>&bull; <b>Availability</b>. The climate crisis will rapidly become costly to society if we do not take action expeditiously. We need to adopt now those technologies that are currently available, provided they meet the following two additional criteria:</p>
<p>&bull; <b>Affordability</b>. Because of the central role of energy in our society, its cost to consumers should not increase significantly. In fact, a successful energy policy could ultimately save consumers money.</p>
<p>&bull; <b>Acceptability</b>. All energy strategies have environmental, land use, and health and safety implications; these must be acceptable to the public. Moreover, while some interest groups will undoubtedly oppose any particular energy policy, political acceptability at a broad scale is necessary.</p>
<p>Our strategy for preventing climate catastrophe and achieving energy independence includes:</p>
<p>&bull; <b>Energy Efficient Technology</b> at home and at the workplace. Huge reductions in home energy use can be achieved with available technologies, including more efficient appliances such as refrigerators, water heaters, and light bulbs. Home retrofits and new home design features such as ‚Äúsmart‚Äù window coatings, lighter-colored roofs where there are hot summers, better home insulation, and passive solar designs can also reduce energy use. Together, energy efficiency in home and industry can save the U.S. up to approximately half of the energy currently consumed in those sectors, and at no net cost&#8212;just by making different choices. Sounds good, doesn‚Äôt it?</p>
<p>&bull; <b>Automobile Fuel Efficiency</b>. Phase in higher Corporate Average Fuel Economy (CAFE) standards for automobiles, SUVs and light trucks by requiring vehicles to go 35 miles per gallon of gas (mpg) by 2015, 45 mpg by 2020, and 60 mpg by 2030. This would rapidly wipe out our dependence on foreign oil and cut emissions from the vehicle sector by two-thirds. A combination of plug-in hybrid, lighter car body materials, re-design and other innovations could readily achieve these standards. This sounds good, too!</p>
<p>&bull; <b>Solar and Wind Energy</b>. Rooftop photovoltaic panels and solar water heating units should be phased in over the next 20 years, with the goal of solar installation on 75% of U.S. homes and commercial buildings by 2030. (Not all roofs receive sufficient sunlight to make solar panels practical for them.) Large wind farms, solar photovoltaic stations, and solar thermal stations should also be phased in so that by 2030, all U.S. electricity demand will be supplied by existing hydroelectric, existing and possibly some new nuclear, and, most importantly, new solar and wind units. This will require investment in expansion of the grid to bring the new supply to the demand, and in research and development to improve overnight storage systems. Achieving this goal would reduce our dependence on coal to practically zero. More good news!</p>
<p>&bull; <b>You</b> are part of the answer. Voting wisely for leaders who promote the first three components is one of the most important individual actions one can make. Other actions help, too. Just as molecules make up mountains, individual actions taken collectively have huge impacts. Improved driving skills, automobile maintenance, reusing and recycling, walking and biking, wearing sweaters in winter and light clothing in summer, installing timers on thermostats and insulating houses, carpooling, paying attention to energy efficiency labels on appliances, and many other simple practices and behaviors hugely influence energy consumption. A major education campaign, both in schools for youngsters and by the media for everyone, should be mounted to promote these consumer practices.</p>
<p>No part of EASY can be left out; all parts are closely integrated. Some parts might create much larger changes&#8212;for example, more efficient home appliances and automobiles&#8212;but all parts are essential. If, for example, we do not achieve the decrease in electricity demand that can be brought about with the E of EASY, then it is extremely doubtful that we could meet our electricity needs with the S of EASY.</p>
<p>It is equally urgent that once we start implementing the plan, we aggressively export it to other major emitting nations. We can reduce our own emissions all we want, but the planet will continue to warm if we can‚Äôt convince other major global emitters to reduce their emissions substantially, too.</p>
<p><b>What EASY will achieve.</b> If no actions are taken to reduce carbon dioxide emissions, in the year 2030 the U.S. will be emitting about 2.2 billion tons of carbon in the form of carbon dioxide. This will be an increase of 25% from today‚Äôs emission rate of about 1.75 billion tons per year of carbon. By following the EASY plan, the U.S. share in a global effort to solve the climate crisis (that is, prevent catastrophic warming) will result in U.S emissions of only about 0.4 billion tons of carbon by 2030, which represents a little less than 25% of 2007 carbon dioxide emissions.128 Stated differently, the plan provides a way to eliminate 1.8 billion tons per year of carbon by that date.</p>
<p>We must act urgently: in the 14 months it took us to write this book, atmospheric CO<sub>2</sub> levels rose by several billion tons of carbon, and more climatic consequences have been observed. Let‚Äôs assume that we conserve our forests and other natural carbon reservoirs at our current levels, as well as maintain our current nuclear and hydroelectric plants (or replace them with more solar and wind generators). Here‚Äôs what implementing EASY will achieve, as illustrated by Figure 3.1 on the next page.
</p></blockquote>
<div align="center">
<img src="https://i1.wp.com/math.ucr.edu/home/baez/ecological/harte_EASY.jpg" />
</div>
<p>Please check out this book and help me figure out if the numbers add up!  I could also use help understanding <a href="http://socrates.berkeley.edu/~hartelab/Publications.html">his research</a>, for example:</p>
<p>&bull; John Harte, <i>Maximum Entropy and Ecology: A Theory of Abundance, Distribution, and Energetics</i>, Oxford University Press, Oxford, 2011.</p>
<p>The book is not free but the <a href="http://fds.oup.com/www.oup.com/pdf/13/9780199593422_chapter1.pdf">first chapter</a> is.</p>
<p>This paper looks really interesting too:</p>
<p>&bull; J. Harte, T. Zillio, E. Conlisk and A. B. Smith, Maximum entropy and the state-variable approach to macroecology, <i><a href="http://www.esajournals.org/doi/abs/10.1890/07-1369.1">Ecology</a></i> <b>89</b> (2008), 2700‚Äì-2711.  </p>
<p>Again, it&#8217;s not freely available&#8212;tut tut. <img src="https://i0.wp.com/math.ucr.edu/home/baez/emoticons/shame_on_you.gif" />  Ecologists should follow physicists and make their work free online; if you&#8217;re serious about saving the planet you should let everyone know what you&#8217;re doing!  However, the abstract is visible to all, and of course I can use my academic superpowers to get ahold of the paper for myself:</p>
<blockquote><p>
<b>Abstract</b>: The biodiversity scaling metrics widely studied in macroecology include the species-area relationship (SAR), the scale-dependent species-abundance distribution (SAD), the distribution of masses or metabolic energies of individuals within and across species, the abundance-energy or abundance-mass relationship across species, and the species-level occupancy distributions across space. We propose a theoretical framework for predicting the scaling forms of these and other metrics based on the state-variable concept and an analytical method derived from information theory. In statistical physics, a method of inference based on information entropy results in a complete macro-scale description of classical thermodynamic systems in terms of the state variables volume, temperature, and number of molecules. In analogy, we take the state variables of an ecosystem to be its total area, the total number of species within any specified taxonomic group in that area, the total number of individuals across those species, and the summed metabolic energy rate for all those individuals. In terms solely of ratios of those state variables, and without invoking any specific ecological mechanisms, we show that realistic functional forms for the macroecological metrics listed above are inferred based on information entropy. The Fisher log series SAD emerges naturally from the theory. The SAR is predicted to have negative curvature on a log-log plot, but as the ratio of the number of species to the number of individuals decreases, the SAR becomes better and better approximated by a power law, with the predicted slope z in the range of 0.14-0.20. Using the 3/4 power mass-metabolism scaling relation to relate energy requirements and measured body sizes, the Damuth scaling rule relating mass and abundance is also predicted by the theory. We argue that the predicted forms of the macroecological metrics are in reasonable agreement with the patterns observed from plant census data across habitats and spatial scales. While this is encouraging, given the absence of adjustable fitting parameters in the theory, we further argue that even small discrepancies between data and predictions can help identify ecological mechanisms that influence macroecological patterns.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/#comments">14 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/" rel="category tag">carbon emissions</a>, <a href="https://johncarlosbaez.wordpress.com/category/climate/" rel="category tag">climate</a>, <a href="https://johncarlosbaez.wordpress.com/category/economics/" rel="category tag">economics</a>, <a href="https://johncarlosbaez.wordpress.com/category/energy/" rel="category tag">energy</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/sustainability/" rel="category tag">sustainability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/27/john-harte/" rel="bookmark" title="Permanent Link to John Harte">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-12363 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-12363">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/10/08/the-mathematical-origin-of-irreversibility/" rel="bookmark">The Mathematical Origin of&nbsp;Irreversibility</a></h2>
				<small>8 October, 2012</small><br />


				<div class="entry">
					<p><i>guest post by <b><a href="http://aei-mpg.academia.edu/MatteoSmerlak/Papers">Matteo Smerlak</a></b></i></p>
<h3> Introduction </h3>
<p>Thermodynamical dissipation and adaptive evolution are two faces of the same Markovian coin!</p>
<p>Consider this. The <a href="http://en.wikipedia.org/wiki/Second_law_of_thermodynamics">Second Law of Thermodynamics</a> states that the entropy of an isolated thermodynamic system can never decrease; <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">Landauer&#8217;s principle</a> maintains that the erasure of information inevitably causes dissipation; <a href="http://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection">Fisher&#8217;s fundamental theorem of natural selection</a> asserts that any fitness difference within a population leads to adaptation in an evolution process governed by natural selection. Diverse as they are, these statements have two common characteristics: </p>
<p>1. they express the <i>irreversibility</i> of certain natural phenomena, and </p>
<p>2. the dynamical processes underlying these phenomena involve an element of <i>randomness</i>. </p>
<p>Doesn&#8217;t this suggest to you the following question: Could it be that thermal phenomena, forgetful information processing and adaptive evolution are governed by <i>the same stochastic mechanism?</i> </p>
<p>The answer is‚Äîyes! The key to this rather profound connection resides in a universal property of <a href="http://en.wikipedia.org/wiki/Markov_process">Markov processes</a> discovered recently in the context of non-equilibrium statistical mechanics, and known as the <a href="http://en.wikipedia.org/wiki/Fluctuation_theorem">&#8216;fluctuation theorem&#8217;</a>. Typically stated in terms of &#8216;dissipated work&#8217; or &#8216;entropy production&#8217;, this result can be seen as an extension of the Second Law of Thermodynamics to <i>small</i> systems, where thermal fluctuations cannot be neglected. But <i>it is actually much more than this</i>: it is the mathematical underpinning of irreversibility itself, be it thermodynamical, evolutionary, or else. To make this point clear, let me start by giving a general formulation of the fluctuation theorem that makes no reference to physics concepts such as &#8216;heat&#8217; or &#8216;work&#8217;.</p>
<h3> The mathematical fact </h3>
<p>Consider a system randomly jumping between states <img src="https://s0.wp.com/latex.php?latex=a%2C+b%2C%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a, b,&#92;dots" class="latex" /> with (possibly time-dependent) transition rates <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Ba+b%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{a b}(t)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> is the state prior to the jump, while <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b" class="latex" /> is the state after the jump. I&#8217;ll assume that this dynamics defines a (continuous-time) Markov process, namely that the numbers <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Ba+b%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma_{a b}" class="latex" /> are the matrix entries of an <a href="http://math.ucr.edu/home/baez/networks/networks_20.html">infinitesimal stochastic</a> matrix, which means that its off-diagonal entries are non-negative and that its columns sum up to zero. </p>
<p>Now, each possible history <img src="https://s0.wp.com/latex.php?latex=%5Comega%3D%28%5Comega_t%29_%7B0%5Cleq+t%5Cleq+T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega=(&#92;omega_t)_{0&#92;leq t&#92;leq T}" class="latex" /> of this process can be characterized by the sequence of occupied states <img src="https://s0.wp.com/latex.php?latex=a_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j}" class="latex" /> and by the times <img src="https://s0.wp.com/latex.php?latex=%5Ctau_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau_{j}" class="latex" /> at which the transitions <img src="https://s0.wp.com/latex.php?latex=a_%7Bj-1%7D%5Clongrightarrow+a_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j-1}&#92;longrightarrow a_{j}" class="latex" /> occur <img src="https://s0.wp.com/latex.php?latex=%280%5Cleq+j%5Cleq+N%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(0&#92;leq j&#92;leq N)" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega%3D%28%5Comega_%7B0%7D%3Da_%7B0%7D%5Coverset%7B%5Ctau_%7B0%7D%7D%7B%5Clongrightarrow%7D+a_%7B1%7D+%5Coverset%7B%5Ctau_%7B1%7D%7D%7B%5Clongrightarrow%7D%5Ccdots+%5Coverset%7B%5Ctau_%7BN%7D%7D%7B%5Clongrightarrow%7D+a_%7BN%7D%3D%5Comega_%7BT%7D%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega=(&#92;omega_{0}=a_{0}&#92;overset{&#92;tau_{0}}{&#92;longrightarrow} a_{1} &#92;overset{&#92;tau_{1}}{&#92;longrightarrow}&#92;cdots &#92;overset{&#92;tau_{N}}{&#92;longrightarrow} a_{N}=&#92;omega_{T})." class="latex" /></p>
<p>Define the <b>skewness</b> <img src="https://s0.wp.com/latex.php?latex=%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_{j}(&#92;tau_{j})" class="latex" /> of each of these transitions to be the logarithmic ratio of transition rates:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29%3A%3D%5Cln%5Cfrac%7B%5Cgamma_%7Ba_%7Bj%7Da_%7Bj-1%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7B%5Cgamma_%7Ba_%7Bj-1%7Da_%7Bj%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;sigma_{j}(&#92;tau_{j}):=&#92;ln&#92;frac{&#92;gamma_{a_{j}a_{j-1}}(&#92;tau_{j})}{&#92;gamma_{a_{j-1}a_{j}}(&#92;tau_{j})}}" class="latex" /></p>
<p>Also define the <a href="http://en.wikipedia.org/wiki/Self-information"><b>self-information</b></a> of the system in state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=i_a%28t%29%3A%3D+-%5Cln%5Cpi_%7Ba%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i_a(t):= -&#92;ln&#92;pi_{a}(t)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Ba%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_{a}(t)" class="latex" /> is the probability that the system is in state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />, given some prescribed initial distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Ba%7D%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_{a}(0)" class="latex" />.  This quantity is also sometimes called the <b>surprisal</b>, as it measures the &#8216;surprise&#8217; of finding out that the system is in state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.</p>
<p>Then the following identity&#8212;the <b>detailed fluctuation theorem</b>&#8212;holds:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BProb%7D%5B%5CDelta+i-%5CSigma%3D-A%5D+%3D+e%5E%7B-A%7D%5C%3B%5Cmathrm%7BProb%7D%5B%5CDelta+i-%5CSigma%3DA%5D+%5C%3B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Prob}[&#92;Delta i-&#92;Sigma=-A] = e^{-A}&#92;;&#92;mathrm{Prob}[&#92;Delta i-&#92;Sigma=A] &#92;;" class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5CSigma%3A%3D%5Csum_%7Bj%7D%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;Sigma:=&#92;sum_{j}&#92;sigma_{j}(&#92;tau_{j})}" class="latex" /></p>
<p>is the <b>cumulative skewness</b> along a trajectory of the system, and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+i%3D+i_%7Ba_N%7D%28T%29-i_%7Ba_0%7D%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta i= i_{a_N}(T)-i_{a_0}(0)" class="latex" /> </p>
<p>is the <b>variation of self-information</b> between the end points of this trajectory.  </p>
<p>This identity has an immediate consequence: if <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5C%2C%5Ccdot%5C%2C%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle&#92;,&#92;cdot&#92;,&#92;rangle" class="latex" /> denotes the average over all realizations of the process, then we have the <b>integral fluctuation theorem</b>: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+e%5E%7B-%5CDelta+i%2B%5CSigma%7D%5Crangle%3D1%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle e^{-&#92;Delta i+&#92;Sigma}&#92;rangle=1," class="latex" /></p>
<p>which, by the convexity of the exponential and <a href="http://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen&#8217;s inequality</a>, implies:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5CDelta+i%5Crangle%3D%5CDelta+S%5Cgeq%5Clangle%5CSigma%5Crangle.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;Delta i&#92;rangle=&#92;Delta S&#92;geq&#92;langle&#92;Sigma&#92;rangle." class="latex" /></p>
<p>In short: <i>the mean variation of self-information, aka the variation of Shannon entropy</i> </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28t%29%3A%3D+%5Csum_%7Ba%7D%5Cpi_%7Ba%7D%28t%29i_a%28t%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(t):= &#92;sum_{a}&#92;pi_{a}(t)i_a(t) }" class="latex" /></p>
<p><i>is bounded from below by the mean cumulative skewness of the underlying stochastic trajectory.</i>  </p>
<p>This is the fundamental mathematical fact underlying irreversibility. To unravel its physical and biological consequences, it suffices to consider the origin and interpretation of the &#8216;skewness&#8217; term in different contexts. (By the way, people usually call <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> the &#8216;entropy production&#8217; or &#8216;dissipation function&#8217;&#8212;but how tautological is that?)</p>
<h3> The physical and biological consequences </h3>
<p>Consider first the standard stochastic-thermodynamic scenario where a physical system is kept in contact with a thermal reservoir at inverse temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> and undergoes thermally induced transitions between states <img src="https://s0.wp.com/latex.php?latex=a%2C+b%2C%5Cdots&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a, b,&#92;dots" class="latex" />. By virtue of the <a href="http://en.wikipedia.org/wiki/Detailed_balance"><b>detailed balance condition</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+e%5E%7B-%5Cbeta+E_%7Ba%7D%28t%29%7D%5Cgamma_%7Ba+b%7D%28t%29%3De%5E%7B-%5Cbeta+E_%7Bb%7D%28t%29%7D%5Cgamma_%7Bb+a%7D%28t%29%2C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ e^{-&#92;beta E_{a}(t)}&#92;gamma_{a b}(t)=e^{-&#92;beta E_{b}(t)}&#92;gamma_{b a}(t),}" class="latex" /></p>
<p>the skewness <img src="https://s0.wp.com/latex.php?latex=%5Csigma_%7Bj%7D%28%5Ctau_%7Bj%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sigma_{j}(&#92;tau_{j})" class="latex" /> of each such transition is <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta " class="latex" /> times the energy difference between the states <img src="https://s0.wp.com/latex.php?latex=a_%7Bj%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=a_%7Bj-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_{j-1}" class="latex" />, namely the <i>heat</i> received from the reservoir during the transition. Hence, the mean cumulative skewness <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5CSigma%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;Sigma&#92;rangle" class="latex" /> is nothing but <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Clangle+Q%5Crangle%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta&#92;langle Q&#92;rangle," class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> the total heat received by the system along the process. It follows from the detailed fluctuation theorem that </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+e%5E%7B-%5CDelta+i%2B%5Cbeta+Q%7D%5Crangle%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle e^{-&#92;Delta i+&#92;beta Q}&#92;rangle=1" class="latex" /></p>
<p>and therefore </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+S%5Cgeq%5Cbeta%5Clangle+Q%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta S&#92;geq&#92;beta&#92;langle Q&#92;rangle" class="latex" /> </p>
<p>which is of course <a href="http://en.wikipedia.org/wiki/Clausius_theorem">Clausius&#8217; inequality</a>. In a computational context where the control parameter is the entropy variation itself (such as in a bit-erasure protocol, where <img src="https://s0.wp.com/latex.php?latex=%5CDelta+S%3D-%5Cln+2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta S=-&#92;ln 2" class="latex" />), this inequality in turn expresses Landauer&#8217;s principle: it impossible to decrease the self-information of the system&#8217;s state without dissipating a minimal amount of heat into the environment (in this case <img src="https://s0.wp.com/latex.php?latex=-Q+%5Cgeq+k+T%5Cln2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-Q &#92;geq k T&#92;ln2" class="latex" />, the <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">&#8216;Landauer bound&#8217;</a>). More general situations (several types of reservoirs, <a href="http://en.wikipedia.org/wiki/Maxwell_demon">Maxwell-demon</a>-like feedback controls) can be treated along the same lines, and the various forms of the Second Law derived from the detailed fluctuation theorem. </p>
<p>Now, many would agree that evolutionary dynamics is a wholly different business from thermodynamics; in particular, notions such as &#8216;heat&#8217; or &#8216;temperature&#8217; are clearly irrelevant to Darwinian evolution. However, the stochastic framework of Markov processes <i>is</i> relevant to describe the genetic evolution of a population, and this fact alone has important consequences. As a simple example, consider the time evolution of mutant fixations <img src="https://s0.wp.com/latex.php?latex=x_%7Ba%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_{a}" class="latex" /> in a population, with <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> ranging over the possible genotypes. In a &#8216;symmetric mutation scheme&#8217;, which I understand is biological parlance for &#8216;reversible Markov process&#8217;, meaning one that obeys <a href="http://en.wikipedia.org/wiki/Detailed_balance">detailed balance</a>, the ratio between the <img src="https://s0.wp.com/latex.php?latex=a%5Cmapsto+b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a&#92;mapsto b" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=b%5Cmapsto+a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b&#92;mapsto a" class="latex" /> transition rates is completely determined by the <a href="http://en.wikipedia.org/wiki/Fitness_landscape"><b>fitnesses</b></a> <img src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_{a}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f_b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_b" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b" class="latex" />, according to </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cgamma_%7Ba+b%7D%7D%7B%5Cgamma_%7Bb+a%7D%7D+%3D%5Cleft%28%5Cfrac%7Bf_%7Bb%7D%7D%7Bf_%7Ba%7D%7D%5Cright%29%5E%7B%5Cnu%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;gamma_{a b}}{&#92;gamma_{b a}} =&#92;left(&#92;frac{f_{b}}{f_{a}}&#92;right)^{&#92;nu} }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nu" class="latex" /> is a model-dependent function of the effective population size [Sella2005]. Along a given history of mutant fixations, the cumulated skewness <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> is therefore given by minus the <b>fitness flux</b>: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5CPhi%3D%5Cnu%5Csum_%7Bj%7D%28%5Cln+f_%7Ba_j%7D-%5Cln+f_%7Ba_%7Bj-1%7D%7D%29.%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;Phi=&#92;nu&#92;sum_{j}(&#92;ln f_{a_j}-&#92;ln f_{a_{j-1}}).}" class="latex" /></p>
<p>The integral fluctuation theorem then becomes the <b>fitness flux theorem</b>: </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+e%5E%7B-%5CDelta+i+-%5CPhi%7D%5Crangle%3D1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle e^{-&#92;Delta i -&#92;Phi}&#92;rangle=1}" class="latex" /></p>
<p>discussed recently by Mustonen and L&auml;ssig [Mustonen2010] and implying Fisher&#8217;s fundamental theorem of natural selection as a special case. (Incidentally, the &#8216;fitness flux theorem&#8217; derived in this reference is more general than this; for instance, it does not rely on the &#8216;symmetric mutation scheme&#8217; assumption above.) The ensuing inequality </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5CPhi%5Crangle%5Cgeq-%5CDelta+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle &#92;Phi&#92;rangle&#92;geq-&#92;Delta S " class="latex" /> </p>
<p>shows that a positive fitness flux is &#8220;an almost universal evolutionary principle of biological systems&#8221; [Mustonen2010], with negative contributions limited to time intervals with a systematic loss of adaptation (<img src="https://s0.wp.com/latex.php?latex=%5CDelta+S+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta S &gt; 0" class="latex" />). This statement may well be the closest thing to a version of the Second Law of Thermodynamics applying to evolutionary dynamics. </p>
<p>It is really quite remarkable that thermodynamical dissipation and Darwinian evolution can be reduced to the same stochastic mechanism, and that notions such as &#8216;fitness flux&#8217; and &#8216;heat&#8217; can arise as two faces of the same mathematical coin, namely the &#8216;skewness&#8217; of Markovian transitions. After all, the phenomenon of life is in itself a direct challenge to thermodynamics, isn&#8217;t it? When thermal phenomena tend to increase the world&#8217;s disorder, life strives to bring about and maintain exquisitely fine spatial and chemical structures&#8212;which is why Schr&ouml;dinger famously proposed to <i>define</i> life as <i>negative entropy</i>. Could there be a more striking confirmation of his intuition&#8212;and a reconciliation of evolution and thermodynamics in the same go&#8212;than the fundamental inequality of adaptive evolution <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5CPhi%5Crangle%5Cgeq-%5CDelta+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle&#92;Phi&#92;rangle&#92;geq-&#92;Delta S" class="latex" />?</p>
<p>Surely the detailed fluctuation theorem for Markov processes has other applications, pertaining neither to thermodynamics nor adaptive evolution. Can you think of any?</p>
<h3> Proof of the fluctuation theorem </h3>
<p>I am a physicist, but knowing that many readers of John&#8217;s blog are mathematicians, I&#8217;ll do my best to frame&#8212;and prove&#8212;the FT as an actual theorem. </p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C%5Cmathcal%7BT%7D%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega,&#92;mathcal{T},p)" class="latex" /> be a probability space and <img src="https://s0.wp.com/latex.php?latex=%28%5C%2C%5Ccdot%5C%2C%29%5E%7B%5Cdagger%7D%3D%5COmega%5Cto+%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;,&#92;cdot&#92;,)^{&#92;dagger}=&#92;Omega&#92;to &#92;Omega" class="latex" /> a measurable involution of <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" />. Denote <img src="https://s0.wp.com/latex.php?latex=p%5E%7B%5Cdagger%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^{&#92;dagger}" class="latex" /> the pushforward probability measure through this involution, and </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+R%3D%5Cln+%5Cfrac%7Bd+p%7D%7Bd+p%5E%5Cdagger%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ R=&#92;ln &#92;frac{d p}{d p^&#92;dagger} }" class="latex" /></p>
<p>the logarithm of the corresponding Radon-Nikodym derivative (we assume <img src="https://s0.wp.com/latex.php?latex=p%5E%5Cdagger&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p^&#92;dagger" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> are mutually absolutely continuous). Then the following lemmas are true, with <img src="https://s0.wp.com/latex.php?latex=%281%29%5CRightarrow%282%29%5CRightarrow%283%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1)&#92;Rightarrow(2)&#92;Rightarrow(3)" class="latex" />:</p>
<p><b>Lemma 1.</b> The detailed fluctuation relation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cforall+A%5Cin%5Cmathbb%7BR%7D+%5Cquad++p%5Cbig%28R%5E%7B-1%7D%28-A%29+%5Cbig%29%3De%5E%7B-A%7Dp+%5Cbig%28R%5E%7B-1%7D%28A%29+%5Cbig%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;forall A&#92;in&#92;mathbb{R} &#92;quad  p&#92;big(R^{-1}(-A) &#92;big)=e^{-A}p &#92;big(R^{-1}(A) &#92;big)" class="latex" /></p>
<p><b>Lemma 2.</b>  The integral fluctuation relation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2Ce%5E%7B-R%28%5Comega%29%7D%3D1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,e^{-R(&#92;omega)}=1 }" class="latex" /></p>
<p><b>Lemma 3.</b>  The positivity of the Kullback-Leibler divergence:</p>
<p><img src="https://s0.wp.com/latex.php?latex=D%28p%5C%2C%5CVert%5C%2C+p%5E%7B%5Cdagger%7D%29%3A%3D%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2CR%28%5Comega%29%5Cgeq+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="D(p&#92;,&#92;Vert&#92;, p^{&#92;dagger}):=&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,R(&#92;omega)&#92;geq 0." class="latex" /></p>
<p>These are basic facts which anyone can show: <img src="https://s0.wp.com/latex.php?latex=%282%29%5CRightarrow%283%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(2)&#92;Rightarrow(3)" class="latex" /> by Jensen&#8217;s inequality, <img src="https://s0.wp.com/latex.php?latex=%281%29%5CRightarrow%282%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1)&#92;Rightarrow(2)" class="latex" /> trivially, and <img src="https://s0.wp.com/latex.php?latex=%281%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(1)" class="latex" /> follows from <img src="https://s0.wp.com/latex.php?latex=R%28%5Comega%5E%7B%5Cdagger%7D%29%3D-R%28%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(&#92;omega^{&#92;dagger})=-R(&#92;omega)" class="latex" /> and the change of variables theorem, as follows,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+%5Cint_%7BR%5E%7B-1%7D%28-A%29%7D+d+p%28%5Comega%29%7D+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_%7BR%5E%7B-1%7D%28A%29%7Dd+p%5E%7B%5Cdagger%7D%28%5Comega%29+%7D+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_%7BR%5E%7B-1%7D%28A%29%7D+d+p%28%5Comega%29%5C%2C+e%5E%7B-R%28%5Comega%29%7D+%7D+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+e%5E%7B-A%7D+%5Cint_%7BR%5E%7B-1%7D%28A%29%7D+d+p%28%5Comega%29%7D+.%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ &#92;int_{R^{-1}(-A)} d p(&#92;omega)} &amp;=&amp; &#92;displaystyle{ &#92;int_{R^{-1}(A)}d p^{&#92;dagger}(&#92;omega) } &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ &#92;int_{R^{-1}(A)} d p(&#92;omega)&#92;, e^{-R(&#92;omega)} } &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ e^{-A} &#92;int_{R^{-1}(A)} d p(&#92;omega)} .&#92;end{array}" class="latex" /></p>
<p>But here is the beauty: if </p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C%5Cmathcal%7BT%7D%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega,&#92;mathcal{T},p)" class="latex" /> is actually a Markov process defined over some time interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2CT%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,T]" class="latex" /> and valued in some (say discrete) state space <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" />, with the instantaneous probability <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Ba%7D%28t%29%3Dp%5Cbig%28%5C%7B%5Comega_%7Bt%7D%3Da%5C%7D+%5Cbig%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_{a}(t)=p&#92;big(&#92;{&#92;omega_{t}=a&#92;} &#92;big)" class="latex" /> of each state <img src="https://s0.wp.com/latex.php?latex=a%5Cin%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a&#92;in&#92;Sigma" class="latex" /> satisfying the <b>master equation</b> (aka Kolmogorov equation)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%5Cpi_%7Ba%7D%28t%29%7D%7Bdt%7D%3D%5Csum_%7Bb%5Cneq+a%7D%5CBig%28%5Cgamma_%7Bb+a%7D%28t%29%5Cpi_%7Ba%7D%28t%29-%5Cgamma_%7Ba+b%7D%28t%29%5Cpi_%7Bb%7D%28t%29%5CBig%29%2C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d&#92;pi_{a}(t)}{dt}=&#92;sum_{b&#92;neq a}&#92;Big(&#92;gamma_{b a}(t)&#92;pi_{a}(t)-&#92;gamma_{a b}(t)&#92;pi_{b}(t)&#92;Big),} " class="latex" /></p>
<p>and</p>
<p>&bull;  the dagger involution is time-reversal, that is <img src="https://s0.wp.com/latex.php?latex=%5Comega%5E%7B%5Cdagger%7D_%7Bt%7D%3A%3D%5Comega_%7BT-t%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega^{&#92;dagger}_{t}:=&#92;omega_{T-t}," class="latex" /></p>
<p>then for a given path</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Comega%3D%28%5Comega_%7B0%7D%3Da_%7B0%7D%5Coverset%7B%5Ctau_%7B0%7D%7D%7B%5Clongrightarrow%7D+a_%7B1%7D+%5Coverset%7B%5Ctau_%7B1%7D%7D%7B%5Clongrightarrow%7D%5Ccdots+%5Coverset%7B%5Ctau_%7BN%7D%7D%7B%5Clongrightarrow%7D+a_%7BN%7D%3D%5Comega_%7BT%7D%29%5Cin%5COmega%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;omega=(&#92;omega_{0}=a_{0}&#92;overset{&#92;tau_{0}}{&#92;longrightarrow} a_{1} &#92;overset{&#92;tau_{1}}{&#92;longrightarrow}&#92;cdots &#92;overset{&#92;tau_{N}}{&#92;longrightarrow} a_{N}=&#92;omega_{T})&#92;in&#92;Omega}" class="latex" /></p>
<p>the logarithmic ratio <img src="https://s0.wp.com/latex.php?latex=R%28%5Comega%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="R(&#92;omega)" class="latex" /> decomposes into &#8216;variation of self-information&#8217; and &#8216;cumulative skewness&#8217; along <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+R%28%5Comega%29%3D%5Cunderbrace%7B%5CBig%28%5Cln%5Cpi_%7Ba_0%7D%280%29-%5Cln%5Cpi_%7Ba_N%7D%28T%29+%5CBig%29%7D_%7B%5CDelta+i%28%5Comega%29%7D-%5Cunderbrace%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7D%5Cln%5Cfrac%7B%5Cgamma_%7Ba_%7Bj%7Da_%7Bj-1%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7B%5Cgamma_%7Ba_%7Bj-1%7Da_%7Bj%7D%7D%28%5Ctau_%7Bj%7D%29%7D%7D_%7B%5CSigma%28%5Comega%29%7D.%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ R(&#92;omega)=&#92;underbrace{&#92;Big(&#92;ln&#92;pi_{a_0}(0)-&#92;ln&#92;pi_{a_N}(T) &#92;Big)}_{&#92;Delta i(&#92;omega)}-&#92;underbrace{&#92;sum_{j=1}^{N}&#92;ln&#92;frac{&#92;gamma_{a_{j}a_{j-1}}(&#92;tau_{j})}{&#92;gamma_{a_{j-1}a_{j}}(&#92;tau_{j})}}_{&#92;Sigma(&#92;omega)}.}" class="latex" /></p>
<p>This is easy to see if one writes the probability of a path explicitly as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7Bp%28%5Comega%29%3D%5Cpi_%7Ba_%7B0%7D%7D%280%29%5Cleft%5B%5Cprod_%7Bj%3D1%7D%5E%7BN%7D%5Cphi_%7Ba_%7Bj-1%7D%7D%28%5Ctau_%7Bj-1%7D%2C%5Ctau_%7Bj%7D%29%5Cgamma_%7Ba_%7Bj-1%7Da_%7Bj%7D%7D%28%5Ctau_%7Bj%7D%29%5Cright%5D%5Cphi_%7Ba_%7BN%7D%7D%28%5Ctau_%7BN%7D%2CT%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{p(&#92;omega)=&#92;pi_{a_{0}}(0)&#92;left[&#92;prod_{j=1}^{N}&#92;phi_{a_{j-1}}(&#92;tau_{j-1},&#92;tau_{j})&#92;gamma_{a_{j-1}a_{j}}(&#92;tau_{j})&#92;right]&#92;phi_{a_{N}}(&#92;tau_{N},T)}" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cphi_%7Ba%7D%28%5Ctau%2C%5Ctau%27%29%3D%5Cphi_%7Ba%7D%28%5Ctau%27%2C%5Ctau%29%3D%5Cexp%5CBig%28-%5Csum_%7Bb%5Cneq+a%7D%5Cint_%7B%5Ctau%7D%5E%7B%5Ctau%27%7Ddt%5C%2C+%5Cgamma_%7Ba+b%7D%28t%29%5CBig%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;phi_{a}(&#92;tau,&#92;tau&#039;)=&#92;phi_{a}(&#92;tau&#039;,&#92;tau)=&#92;exp&#92;Big(-&#92;sum_{b&#92;neq a}&#92;int_{&#92;tau}^{&#92;tau&#039;}dt&#92;, &#92;gamma_{a b}(t)&#92;Big)}" class="latex" /></p>
<p>is the probability that the process remains in the state <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> between the times <img src="https://s0.wp.com/latex.php?latex=%5Ctau&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctau%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tau&#039;" class="latex" />. It follows from the above lemma that</p>
<p><b>Theorem.</b> Let <img src="https://s0.wp.com/latex.php?latex=%28%5COmega%2C%5Cmathcal%7BT%7D%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(&#92;Omega,&#92;mathcal{T},p)" class="latex" /> be a Markov process and let <img src="https://s0.wp.com/latex.php?latex=i%2C%5CSigma%3A%5COmega%5Crightarrow+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i,&#92;Sigma:&#92;Omega&#92;rightarrow &#92;mathbb{R}" class="latex" /> be defined as above. Then we have</p>
<p>1. The detailed fluctuation theorem:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cforall+A%5Cin%5Cmathbb%7BR%7D%2C+p%5Cbig%28%28%5CDelta+i-%5CSigma%29%5E%7B-1%7D%28-A%29+%5Cbig%29%3De%5E%7B-A%7Dp+%5Cbig%28%28%5CDelta+i-%5CSigma%29%5E%7B-1%7D%28A%29+%5Cbig%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;forall A&#92;in&#92;mathbb{R}, p&#92;big((&#92;Delta i-&#92;Sigma)^{-1}(-A) &#92;big)=e^{-A}p &#92;big((&#92;Delta i-&#92;Sigma)^{-1}(A) &#92;big)" class="latex" /></p>
<p>2.  The integral fluctuation theorem:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2Ce%5E%7B-%5CDelta+i%28%5Comega%29%2B%5CSigma%28%5Comega%29%7D%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,e^{-&#92;Delta i(&#92;omega)+&#92;Sigma(&#92;omega)}=1" class="latex" /></p>
<p>3.  The &#8216;Second Law&#8217; inequality:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5CDelta+S%3A%3D%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2C%5CDelta+i%28%5Comega%29%5Cgeq+%5Cint_%7B%5COmega%7D+d+p%28%5Comega%29%5C%2C%5CSigma%28%5Comega%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;Delta S:=&#92;int_{&#92;Omega} d p(&#92;omega)&#92;,&#92;Delta i(&#92;omega)&#92;geq &#92;int_{&#92;Omega} d p(&#92;omega)&#92;,&#92;Sigma(&#92;omega)} " class="latex" /></p>
<p>The same theorem can be formulated for other kinds of Markov processes as well, including diffusion processes (in which case it follows from the <a href="http://en.wikipedia.org/wiki/Girsanov_theorem">Girsanov theorem</a>).</p>
<h3> References </h3>
<p>Landauer&#8217;s principle was introduced here:</p>
<p>&bull; [Landauer1961]  R. Landauer, Irreversibility and heat generation in the computing process}, <i>IBM Journal of Research and Development</i> <b>5</b>, (1961) 183&#8211;191.</p>
<p>and is now being verified experimentally by various groups worldwide.</p>
<p>The &#8216;fundamental theorem of natural selection&#8217; was derived by Fisher in his book:</p>
<p>&bull; [Fisher1930]  R. Fisher, <i>The Genetical Theory of Natural Selection</i>, Clarendon Press, Oxford, 1930.</p>
<p>His derivation has long been considered obscure, even perhaps wrong, but apparently the theorem is now well accepted. I believe the first Markovian models of genetic evolution appeared here:</p>
<p>&bull; [Fisher1922]  R. A. Fisher, On the dominance ratio, <i>Proc. Roy. Soc. Edinb.</i> <b>42</b> (1922), 321&#8211;341.</p>
<p>&bull; [Wright1931]  S. Wright, Evolution in Mendelian populations, <i>Genetics</i> <b>16</b> (1931), 97&#8211;159.</p>
<p>Fluctuation theorems are reviewed here:</p>
<p>&bull; [Sevick2008]  E. Sevick, R. Prabhakar, S. R. Williams, and D. J. Searles, <a href="http://arxiv.org/abs/0709.3888">Fluctuation theorems</a>, <i>Ann. Rev. Phys. Chem.</i> <b>59</b> (2008), 603&#8211;633.</p>
<p>Two of the key ideas for the &#8216;detailed fluctuation theorem&#8217; discussed here are due to Crooks: </p>
<p>&bull; [Crooks1999]  Gavin Crooks, <a href="http://arxiv.org/abs/cond-mat/9901352">The entropy production fluctuation theorem and the nonequilibrium work relation for free energy differences</a>, <a href="http://dx.doi.org/10.1103/PhysRevE.60.2721"><i>Phys. Rev. E</i></a> <b>60</b> (1999), 2721&#8211;2726.</p>
<p>who identified <img src="https://s0.wp.com/latex.php?latex=%28E_%7Ba%7D%28%5Ctau_%7Bj%7D%29-E_%7Ba%7D%28%5Ctau_%7Bj-1%7D%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(E_{a}(&#92;tau_{j})-E_{a}(&#92;tau_{j-1}))" class="latex" /> as heat, and Seifert:</p>
<p>&bull; [Seifert2005]  Udo Seifert, <a href="http://arxiv.org/abs/cond-mat/0503686">Entropy production along a stochastic trajectory and an integral fluctuation theorem</a>, <a href="http://dx.doi.org/10.1103/PhysRevLett.95.040602"><i>Phys. Rev. Lett.</i></a> <b>95</b> (2005), 4.</p>
<p>who understood the relevance of the self-information in this context. </p>
<p>The connection between statistical physics and evolutionary biology is discussed here:</p>
<p>&bull; [Sella2005] G. Sella and A.E. Hirsh, <a href="http://www.pnas.org/content/102/27/9541.full.pdf+html">The application of statistical physics to evolutionary biology</a>, <a href="http://www.pnas.org/content/102/27/9541.short"><i>Proc. Nat. Acad. Sci. USA</i></a> <b>102</b> (2005), 9541&#8211;9546.</p>
<p>and the &#8216;fitness flux theorem&#8217; is derived in </p>
<p>&bull; [Mustonen2010]  V. Mustonen and M. L&auml;ssig, <a href="http://www.pnas.org/content/107/9/4248.full.pdf+html">Fitness flux and ubiquity of adaptive evolution</a>, <a href="http://www.pnas.org/content/107/9/4248.short"><i>Proc. Nat. Acad. Sci. USA</i></a> <b>107</b> (2010), 4248&#8211;4253.</p>
<p>Schr&ouml;dinger&#8217;s famous discussion of the physical nature of life was published here:</p>
<p>&bull; [Schr&ouml;dinger1944]  E. Schr&ouml;dinger, <i>What is Life?</i>, Cambridge University Press, Cambridge, 1944.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/08/the-mathematical-origin-of-irreversibility/#comments">57 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/10/08/the-mathematical-origin-of-irreversibility/" rel="bookmark" title="Permanent Link to The Mathematical Origin of&nbsp;Irreversibility">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/7/" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/5/" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the information and entropy category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what‚Äôs on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin‚Äôs environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/6/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

				<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = JSON.parse( decodeURIComponent( '%7B%22settings%22%3A%7B%22id%22%3A%22content%22%2C%22ajaxurl%22%3A%22https%3A%5C%2F%5C%2Fjohncarlosbaez.wordpress.com%5C%2F%3Finfinity%3Dscrolling%22%2C%22type%22%3A%22scroll%22%2C%22wrapper%22%3Atrue%2C%22wrapper_class%22%3A%22infinite-wrap%22%2C%22footer%22%3Atrue%2C%22click_handle%22%3A%221%22%2C%22text%22%3A%22Older%20posts%22%2C%22totop%22%3A%22Scroll%20back%20to%20top%22%2C%22currentday%22%3A%2208.10.12%22%2C%22order%22%3A%22DESC%22%2C%22scripts%22%3A%5B%5D%2C%22styles%22%3A%5B%5D%2C%22google_analytics%22%3Afalse%2C%22offset%22%3A6%2C%22history%22%3A%7B%22host%22%3A%22johncarlosbaez.wordpress.com%22%2C%22path%22%3A%22%5C%2Fcategory%5C%2Finformation-and-entropy%5C%2Fpage%5C%2F%25d%5C%2F%22%2C%22use_trailing_slashes%22%3Atrue%2C%22parameters%22%3A%22%22%7D%2C%22query_args%22%3A%7B%22paged%22%3A6%2C%22category_name%22%3A%22information-and-entropy%22%2C%22error%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A0%2C%22post_parent%22%3A%22%22%2C%22subpost%22%3A%22%22%2C%22subpost_id%22%3A%22%22%2C%22attachment%22%3A%22%22%2C%22attachment_id%22%3A0%2C%22name%22%3A%22%22%2C%22pagename%22%3A%22%22%2C%22page_id%22%3A0%2C%22second%22%3A%22%22%2C%22minute%22%3A%22%22%2C%22hour%22%3A%22%22%2C%22day%22%3A0%2C%22monthnum%22%3A0%2C%22year%22%3A0%2C%22w%22%3A0%2C%22tag%22%3A%22%22%2C%22cat%22%3A23375499%2C%22tag_id%22%3A%22%22%2C%22author%22%3A%22%22%2C%22author_name%22%3A%22%22%2C%22feed%22%3A%22%22%2C%22tb%22%3A%22%22%2C%22meta_key%22%3A%22%22%2C%22meta_value%22%3A%22%22%2C%22preview%22%3A%22%22%2C%22s%22%3A%22%22%2C%22sentence%22%3A%22%22%2C%22title%22%3A%22%22%2C%22fields%22%3A%22%22%2C%22menu_order%22%3A%22%22%2C%22embed%22%3A%22%22%2C%22category__in%22%3A%5B%5D%2C%22category__not_in%22%3A%5B%5D%2C%22category__and%22%3A%5B%5D%2C%22post__in%22%3A%5B%5D%2C%22post__not_in%22%3A%5B%5D%2C%22post_name__in%22%3A%5B%5D%2C%22tag__in%22%3A%5B%5D%2C%22tag__not_in%22%3A%5B%5D%2C%22tag__and%22%3A%5B%5D%2C%22tag_slug__in%22%3A%5B%5D%2C%22tag_slug__and%22%3A%5B%5D%2C%22post_parent__in%22%3A%5B%5D%2C%22post_parent__not_in%22%3A%5B%5D%2C%22author__in%22%3A%5B%5D%2C%22author__not_in%22%3A%5B%5D%2C%22lazy_load_term_meta%22%3Afalse%2C%22posts_per_page%22%3A10%2C%22ignore_sticky_posts%22%3Afalse%2C%22suppress_filters%22%3Afalse%2C%22cache_results%22%3Afalse%2C%22update_post_term_cache%22%3Atrue%2C%22update_post_meta_cache%22%3Atrue%2C%22post_type%22%3A%22%22%2C%22nopaging%22%3Afalse%2C%22comments_per_page%22%3A%22100%22%2C%22no_found_rows%22%3Afalse%2C%22order%22%3A%22DESC%22%7D%2C%22query_before%22%3A%222021-09-26%2017%3A22%3A05%22%2C%22last_post_date%22%3A%222012-10-08%2017%3A27%3A53%22%2C%22body_class%22%3A%22infinite-scroll%20neverending%22%2C%22loading_text%22%3A%22Loading%20new%20page%22%2C%22stats%22%3A%22blog%3D12777403%26v%3Dwpcom%26tz%3D0%26user_id%3D0%26subd%3Djohncarlosbaez%26x_pagetype%3Dinfinite%22%7D%7D' ) );
		//]]>
		</script>
		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>
		<div id="infinite-footer">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://johncarlosbaez.wordpress.com/" rel="home">
						Azimuth					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a> 				</div>
			</div>
		</div><!-- #infinite-footer -->
		
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s1.wp.com/_static/??-eJyFy00OQDAQQOELGUP8xUKcpWoiZVTTadO4vVhY2LB8L/kwOdCHDWQD7hEcx8VYQa38EYUYJRlHHqZoZ6Zci2T4I1YKTukNnvGFgmGaYVHM5M933Wzch7Ktir5p+rpbL+lvP34=?cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F10%2F08%2Fthe-mathematical-origin-of-irreversibility%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s0.wp.com/_static/??-eJyNkN1uwyAMhV9oDkouKvVi2rMQcCNT/oZN07z9yNRMbTZFu8LH8J2DreYMJkXBKMqxsngjg/neOX5TT1ehQvZ1osiK4oUiyfJTHLzVNlCEURcVNAuWVoEUba68h1q2+6xYlsfRzdmkALmk+wIFW49lYygaXy3yCjWJYUTbtaCDj8xkJxRWXEc2hbJQivyXH8+XNDo0sjfbnG5kMSnNvNq5h84F+ddIT/FGl1QZvXIouY0PW+OAEfJoYdLer1t5Uf9Igu/97WTjPsJ7fxrO534YTr37Ai2cwB8='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			(function() {
				var extend = function(out) {
					out = out || {};

					for (var i = 1; i < arguments.length; i++) {
						if (!arguments[i])
						continue;

						for (var key in arguments[i]) {
						if (arguments[i].hasOwnProperty(key))
							out[key] = arguments[i][key];
						}
					}

					return out;
				};
				extend( window.infiniteScroll.settings.scripts, ["postmessage","mobile-useragent-info","rlt-proxy","jquery-core","jquery-migrate","jquery","wpcom-actionbar-placeholder","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","wpcom-masterbar-tracks-js","jquery.wpcom-proxy-request","wp-embed","jetpack-subscriptions-js","swfobject","videopress","jetpack-carousel","tiled-gallery","carousel-wpcom"] );
				extend( window.infiniteScroll.settings.styles, ["the-neverending-homepage","infinity-contempt","wp-block-library","mediaelement","wp-mediaelement","jetpack-layout-grid","jetpack-ratings","coblocks-frontend","wpcom-core-compat-playlist-styles","wpcom-text-widget-styles","wpcom-bbpress2-staff-css","contempt","geo-location-flair","reblogging","a8c-global-print","h4-global","global-styles","jetpack-global-styles-frontend-style","jetpack-carousel-swiper-css","jetpack-carousel","tiled-gallery"] );
			})();
		</script>
				<span id="infinite-aria" aria-live="polite"></span>
		<script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZVNXlxTkwsfk8sWlcscy95ZjBFUzJ3anQuZXBGVz1wQjdjSXRMYTN0SUomOXhFcnolVG4vLDM9OXhzYUlkREdofkNlbSw2d1lsLlR0WytfM1RtLS5mRzFkMFNnNzdlQ1smR2RBOHFnLHZyfDY3MDBpSlBlW3Q4Tm10c1ElSzddVnMmSS1ON1pkMFBHbWN4d3VQM3FQN2ZLTVVXTGwtVituR3paeUtHamU1bytzJWE2fHxfcU9TaTIlZEFjL28taEJDTE80ZEU='}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>