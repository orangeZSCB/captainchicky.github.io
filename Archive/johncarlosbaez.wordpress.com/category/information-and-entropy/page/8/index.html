<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>information and entropy | Azimuth | Page 8</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; information and entropy Category Feed" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IlqVOdbxwfBYOEVOBZSBYeXuz1e6sVrveMDl9yU9AHrOwUyJIJGMTOTSPqUpMz5iQ+mIMttYbuVFMLxChytyMPJXFTBfcGfKNXQPFc6aAfFNq2A9KmobBSRMm+yoCmqJLl5V6gKURJhua4zGHKiM41BB46qxo5eSgOxQRwGvbh4hpG+fc2v8G/S3+pJSbAWU9S9Z9aiR8QfdD9r9bFE2YfN3A7fSF7QY18t4cVlqC4nd29WTzzjkes768+BXsiM4DMV7PtiB4v45kHiOMyQVqFXxGbFF8/pSZe4qP6u5W3T/sx3F3+ACgX/YH?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F06%2F04%2Finformation-geometry-part-10%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e6f6fdfb46","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"ffcb185558\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/information-and-entropy\/page\/8\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Finformation-and-entropy%2Fpage%2F8%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,228 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2012%2F06%2F04%2Finformation-geometry-part-10%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="information and entropy &#8211; Page 8 &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about information and entropy written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive paged category category-information-and-entropy category-23375499 paged-8 category-paged-8 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-9984 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-probability" id="post-9984">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/" rel="bookmark">Information Geometry (Part&nbsp;10)</a></h2>
				<small>4 June, 2012</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/">Last time</a> I began explaining the tight relation between three concepts:</p>
<p>&bull; entropy, </p>
<p>&bull; information&mdash;or more precisely, lack of information, </p>
<p>and</p>
<p>&bull; biodiversity.</p>
<p>The idea is to consider <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> different species of &#8216;replicators&#8217;.  A replicator is any entity that can reproduce itself, like an organism, a gene, or a meme.  A replicator can come in different kinds, and a &#8216;species&#8217; is just our name for one of these kinds.  If <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> is the population of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, we can interpret the fraction</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_j+P_j%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_j P_j} } " class="latex" /></p>
<p>as a probability: the probability that a randomly chosen replicator belongs to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  This suggests that we define <i><a href="http://en.wikipedia.org/wiki/Entropy_%28statistical_thermodynamics%29">entropy</a></i> just as we do in statistical mechanics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Csum_i+p_i+%5Cln%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;sum_i p_i &#92;ln(p_i) } " class="latex" /></p>
<p>In the study of statistical inference, entropy is a measure of uncertainty, or lack of <i><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">information</a></i>.  But now we can interpret it as a measure of <i><a href="http://www.loujost.com/Statistics%20and%20Physics/Diversity%20and%20Similarity/JostEntropy%20AndDiversity.pdf">biodiversity</a></i>: it&#8217;s zero when just one species is present, and small when a few species have much larger populations than all the rest, but gets big otherwise.  </p>
<p>Our goal here is play these viewpoints off against each other.  In short, we want to think of natural selection, and even biological evolution, as a process of statistical inference&mdash;or in simple terms, <i>learning</i>.  </p>
<p>To do this, let&#8217;s think about how entropy changes with time.  Last time we introduced a simple model called the <b><a href="http://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) &#92;, P_i } " class="latex" /></p>
<p>where each population grows at a rate proportional to some &#8216;fitness functions&#8217; <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" />.  We can get some intuition by looking at the pathetically simple case where these functions are actually <i>constants</i>, so</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i &#92;, P_i } " class="latex" /></p>
<p>The equation then becomes trivial to solve:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+P_i%28t%29+%3D+e%5E%7Bt+f_i+%7D+P_i%280%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ P_i(t) = e^{t f_i } P_i(0)} " class="latex" /></p>
<p>Last time I showed that in this case, the entropy will eventually decrease.  It will go to zero as <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;to +&#92;infty" class="latex" /> whenever one species is fitter than all the rest and starts out with a nonzero population&#8212;since then this species will eventually take over.  </p>
<p>But remember, the entropy of a probability distribution is its <i>lack</i> of information.  So the decrease in entropy signals an increase in information.  And last time I argued that this makes perfect sense.   As the fittest species takes over and biodiversity drops, <i>the population is acquiring information about its environment</i>.  </p>
<p>However, I never said the entropy is <i>always</i> decreasing, because that&#8217;s false!  Even in this pathetically simple case, entropy can increase.</p>
<p>Suppose we start with many replicators belonging to one very unfit species, and a few belonging to various more fit species.  The probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> will start out sharply peaked, so the entropy will start out low:</p>
<div align="center">
<img width="250" src="https://i0.wp.com/math.ucr.edu/home/baez/mathematical/biodiversity_0.png" />
</div>
<p>Now think about what happens when time passes.  At first the unfit species will rapidly die off, while the population of the other species slowly grows:</p>
<div align="center">
<img width="250" src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/biodiversity_1.png" />
</div>
<p>&nbsp;</p>
<div align="center">
<img width="250" src="https://i1.wp.com/math.ucr.edu/home/baez/mathematical/biodiversity_2.png" />
</div>
<p>So the probability distribution will, for a while, become less sharply peaked.  Thus, <i>for a while</i>, the entropy will increase!  </p>
<p>This seems to conflict with our idea that the population&#8217;s entropy should decrease as it acquires information about its environment.  But in fact this phenomenon is familiar in the study of statistical inference.  If you start out with strongly held <i>false</i> beliefs about a situation, the first effect of learning more is to become <i>less</i> certain about what&#8217;s going on! </p>
<p>Get it?  Say you start out by assigning a high probability to some wrong guess about a situation.   The entropy of your probability distribution is low: you&#8217;re quite certain about what&#8217;s going on.  But you&#8217;re wrong.  When you first start suspecting you&#8217;re wrong, you become more uncertain about what&#8217;s going on.   Your probability distribution flattens out, and the entropy goes up. </p>
<p>So, sometimes learning involves a decrease in information&#8212;<i>false</i> information.  There&#8217;s nothing about the mathematical concept of information that says this information is <i>true</i>.</p>
<p>Given this, it&#8217;s good to work out a formula for the rate of change of entropy, which will let us see more clearly when it goes down and when it goes up.  To do this, first let&#8217;s derive a completely general formula for the time derivative of the entropy of a probability distribution.  Following Sir Isaac Newton, we&#8217;ll use a dot to stand for a time derivative:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B++%5Cdot%7BS%7D%7D+%26%3D%26+%5Cdisplaystyle%7B+-++%5Cfrac%7Bd%7D%7Bdt%7D+%5Csum_i+p_i+%5Cln+%28p_i%29%7D+%5C%5C+++%5C%5C++%26%3D%26+-+%5Cdisplaystyle%7B+%5Csum_i+%5Cdot%7Bp%7D_i+%5Cln+%28p_i%29+%2B+%5Cdot%7Bp%7D_i+%7D++%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{  &#92;dot{S}} &amp;=&amp; &#92;displaystyle{ -  &#92;frac{d}{dt} &#92;sum_i p_i &#92;ln (p_i)} &#92;&#92;   &#92;&#92;  &amp;=&amp; - &#92;displaystyle{ &#92;sum_i &#92;dot{p}_i &#92;ln (p_i) + &#92;dot{p}_i }  &#92;end{array}" class="latex" /></p>
<p>In the last term we took the derivative of the logarithm and got a factor of <img src="https://s0.wp.com/latex.php?latex=1%2Fp_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/p_i" class="latex" /> which cancelled the factor of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />.  But since </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Csum_i+p_i+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;sum_i p_i = 1 } " class="latex" /></p>
<p>we know</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_i+%5Cdot%7Bp%7D_i+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_i &#92;dot{p}_i = 0 } " class="latex" /></p>
<p>so this last term vanishes:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7BS%7D%3D+-%5Csum_i+%5Cdot%7Bp%7D_i+%5Cln+%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{S}= -&#92;sum_i &#92;dot{p}_i &#92;ln (p_i) } " class="latex" /></p>
<p>Nice!   To go further, we need a formula for <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i" class="latex" />.  For this we might as well return to the general replicator equation, dropping the pathetically special assumption that the fitness functions are actually constants.   Then we saw last time that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle++%5CBig%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle  &#92;Big) &#92;, p_i }" class="latex" /></p>
<p>where we used the abbreviation</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_i%28P%29+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P) = f_i(P_1, &#92;dots, P_n) " class="latex" /></p>
<p>for the fitness of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, and defined the <b>mean fitness</b> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_i+f_i%28P%29+p_i++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_i f_i(P) p_i  } " class="latex" /></p>
<p>Using this cute formula for <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}_i" class="latex" />, we get the final result:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7BS%7D+%3D+-+%5Csum_i+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle+%5CBig%29+%5C%2C+p_i+%5Cln+%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{S} = - &#92;sum_i &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle &#92;Big) &#92;, p_i &#92;ln (p_i) } " class="latex" /></p>
<p>This is strikingly similar to the formula for entropy itself.  But now each term in the sum includes a factor saying how much more fit than average, or less fit, that species is.  The quantity <img src="https://s0.wp.com/latex.php?latex=-+p_i+%5Cln%28p_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- p_i &#92;ln(p_i)" class="latex" /> is always nonnegative, since the graph of <img src="https://s0.wp.com/latex.php?latex=-x+%5Cln%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-x &#92;ln(x)" class="latex" /> looks like this:</p>
<div align="center">
<img src="https://i2.wp.com/math.ucr.edu/home/baez/mathematical/-xlnx.png" />
</div>
<p>So, the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th term contributes positively to the change in entropy if the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species is fitter than average, but negatively if it&#8217;s less fit than average.</p>
<p>This may seem counterintuitive!</p>
<p><b>Puzzle 1.</b> How can we reconcile this fact with our earlier observations about the case when the fitness of each species is population-independent?  Namely: a) if initially most of the replicators belong to one very unfit species, the entropy will rise at first, but b) in the long run, when the fittest species present take over, the entropy drops?  </p>
<p>If this seems too tricky, look at some examples!  The first illustrates observation a); the second illustrates observation b):</p>
<p><b>Puzzle 2.</b>  Suppose we have two species, one with fitness equal to 1 initially constituting 90% of the population, the other with fitness equal to 10 initially constituting just 10% of the population:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccc%7D+f_1+%3D+1%2C+%26+%26++p_1%280%29+%3D+0.9+%5C%5C+%5C%5C++++++++++++++++++++++++++++f_2+%3D+10+%2C+%26+%26+p_2%280%29+%3D+0.1+++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccc} f_1 = 1, &amp; &amp;  p_1(0) = 0.9 &#92;&#92; &#92;&#92;                            f_2 = 10 , &amp; &amp; p_2(0) = 0.1   &#92;end{array} " class="latex" /></p>
<p>At what rate does the entropy change at <img src="https://s0.wp.com/latex.php?latex=t+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = 0" class="latex" />?  Which species is responsible for most of this change?</p>
<p><b>Puzzle 3.</b>  Suppose we have two species, one with fitness equal to 10 initially constituting 90% of the population, and the other with fitness equal to 1 initially constituting just 10% of the population:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccc%7D+f_1+%3D+10%2C+%26+%26++p_1%280%29+%3D+0.9+%5C%5C+%5C%5C++++++++++++++++++++++++++++f_2+%3D+1+%2C+%26+%26+p_2%280%29+%3D+0.1+++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccc} f_1 = 10, &amp; &amp;  p_1(0) = 0.9 &#92;&#92; &#92;&#92;                            f_2 = 1 , &amp; &amp; p_2(0) = 0.1   &#92;end{array} " class="latex" /></p>
<p>At what rate does the entropy change at <img src="https://s0.wp.com/latex.php?latex=t+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = 0" class="latex" />?  Which species is responsible for most of this change?</p>
<p>I had to work through these examples to understand what&#8217;s going on.  Now I do, and it all makes sense.</p>
<h3> Next time </h3>
<p>Still, it would be nice if there were some quantity that <i>always goes down</i> with the passage of time, reflecting our naive idea that the population gains information from its environment, and thus loses entropy, as time goes by.  </p>
<p>Often there <i>is</i> such a quantity. But it&#8217;s not the naive entropy: it&#8217;s the <i>relative</i> entropy.  I&#8217;ll talk about that next time.  In the meantime, if you want to prepare, please reread <a href="http://math.ucr.edu/home/baez/information/information_geometry_6.html">Part 6</a> of this series, where I explained this concept.  Back then, I argued that <i>whenever you&#8217;re tempted to talk about entropy, you should talk about relative entropy</i>.  So, we should try that here.</p>
<p>There&#8217;s a big idea lurking here: <i>information is relative</i>.  How much information a signal gives you depends on your prior assumptions about what that signal is likely to be.  If this is true, perhaps biodiversity is relative too.  </p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/#comments">28 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/04/information-geometry-part-10/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;10)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-9904 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy category-probability" id="post-9904">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/" rel="bookmark">Information Geometry (Part&nbsp;9)</a></h2>
				<small>1 June, 2012</small><br />


				<div class="entry">
					<div align="center">
<a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx"><br />
<img width="450" src="https://i2.wp.com/math.ucr.edu/home/baez/barcelona_biodiversity_poster.jpg" /><br />
</a>
</div>
<p>It&#8217;s time to continue this <a href="http://math.ucr.edu/home/baez/information/">information geometry</a> series, because I&#8217;ve promised to give the following talk at a  <a href="http://www.crm.cat/en/Activities/Pages/ActivityDescriptions/Exploratory-Conference-on-the-Mathematics-of-Biodiversity.aspx">conference on the mathematics of biodiversity</a> in early July&#8230; and I still need to do some of the research!  <img src="https://i2.wp.com/math.ucr.edu/home/baez/emoticons/uhh.gif" alt="" /></p>
<blockquote>
<h4>Diversity, information geometry and learning</h4>
<p>As is well known, some measures of biodiversity are formally identical to measures of information developed by Shannon and others.  Furthermore, Marc Harper has shown that the replicator equation in evolutionary game theory is formally identical to a process of Bayesian inference, which is studied in the field of machine learning using ideas from information geometry. Thus, in this simple model, a population of organisms can be thought of as a &#8216;hypothesis&#8217; about how to survive, and natural selection acts to update this hypothesis according to Bayes&#8217; rule.  The question thus arises to what extent natural changes in biodiversity can be usefully seen as analogous to a form of learning. However, some of the same mathematical structures arise in the study of chemical reaction networks, where the increase of entropy, or more precisely decrease of free energy, is not usually considered a form of &#8216;learning&#8217;. We report on some preliminary work on these issues.
</p></blockquote>
<p>So, let&#8217;s dive in!  To some extent I&#8217;ll be explaining these two papers:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>However, I hope to bring in some more ideas from physics, the study of biodiversity, and the theory of stochastic Petri nets, also known as chemical reaction networks.  So, this series may start to overlap with my <a href="http://math.ucr.edu/home/baez/networks/">network theory</a> posts.  We&#8217;ll see.  We won&#8217;t get far today: for now, I just want to review and expand on what we did <a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/">last time</a>.</p>
<h3> The replicator equation </h3>
<p>The <b><a href="https://en.wikipedia.org/wiki/Replicator_equation">replicator equation</a></b> is a simplified model of how populations change.  Suppose we have <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> types of self-replicating entity.  I&#8217;ll call these entities <b>replicators</b>.  I&#8217;ll call the types of replicators <b>species</b>, but they don&#8217;t need to be species in the biological sense.  For example, the replicators could be genes, and the types could be <a href="http://en.wikipedia.org/wiki/Allele">alleles</a>.  Or the replicators could be restaurants, and the types could be restaurant chains.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=P_i%28t%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t)," class="latex" /> or just <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> for short, be the population of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species at time <img src="https://s0.wp.com/latex.php?latex=t.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t." class="latex" />  Then the replicator equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) &#92;, P_i } " class="latex" /></p>
<p>So, the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> changes at a rate proportional to <img src="https://s0.wp.com/latex.php?latex=P_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i," class="latex" /> but the &#8216;constant of proportionality&#8217; need not be constant: it can be any smooth function <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> of the populations of all the species.  We call <img src="https://s0.wp.com/latex.php?latex=f_i%28P_1%2C+%5Cdots%2C+P_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P_1, &#92;dots, P_n)" class="latex" /> the <b><a href="http://en.wikipedia.org/wiki/Fitness_%28biology%29">fitness</a></b> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.</p>
<p>Of course this model is absurdly general, while still leaving out lots of important effects, like the spatial variation of populations, or the ability for the population of some species to start at zero and become nonzero&#8212;which happens thanks to mutation.  Nonetheless this model is worth taking a good look at.</p>
<p>Using the magic of vectors we can write</p>
<p><img src="https://s0.wp.com/latex.php?latex=P+%3D+%28P_1%2C+%5Cdots+%2C+P_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P = (P_1, &#92;dots , P_n)" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+%3D+%28f_1%28P%29%2C+%5Cdots%2C+f_n%28P%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) = (f_1(P), &#92;dots, f_n(P))" class="latex" /></p>
<p>This lets us write the replicator equation a wee bit more tersely as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P%7D%7Bd+t%7D+%3D+f%28P%29+P%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P}{d t} = f(P) P} " class="latex" /></p>
<p>where on the right I&#8217;m multiplying vectors componentwise, the way your teachers tried to brainwash you into never doing:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28P%29+P+%3D+%28f%28P%29_1+P_1%2C+%5Cdots%2C+f%28P%29_n+P_n%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P) P = (f(P)_1 P_1, &#92;dots, f(P)_n P_n) " class="latex" /></p>
<p>In other words, I&#8217;m thinking of <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" /> as functions on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}" class="latex" /> and multiplying them pointwise.  This will be a nice way of thinking if we want to replace this finite set by some more general space.</p>
<p>Why would we want to do that?  Well, we might be studying lizards with different length tails, and we might find it convenient to think of the set of possible tail lengths as the half-line <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="[0,&#92;infty)" class="latex" /> instead of a finite set.</p>
<p>Or, just to get started, we might want to study the pathetically simple case where <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" /> doesn&#8217;t depend on <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" />  Then we just have a fixed function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and a time-dependent function <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P%7D%7Bd+t%7D+%3D+f+P%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P}{d t} = f P} " class="latex" /></p>
<p>If we&#8217;re physicists, we might write <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> more suggestively as <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> and write the operator multiplying by <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=-+H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- H." class="latex" />  Then our equation becomes</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+%5Cpsi%7D%7Bd+t%7D+%3D+-+H+%5Cpsi+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d &#92;psi}{d t} = - H &#92;psi } " class="latex" /></p>
<p>This looks a lot like Schr&ouml;dinger&#8217;s equation, but since there&#8217;s no factor of <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B-1%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sqrt{-1}," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> is real-valued, it&#8217;s more like the heat equation or the &#8216;master equation&#8217;, the basic equation of stochastic mechanics.</p>
<p>For an explanation of Schr&ouml;dinger&#8217;s equation and the master equation, try <a href="http://math.ucr.edu/home/baez/networks/networks_12.html">Part 12</a> of the network theory series.  In that post I didn&#8217;t include a minus sign in front of the <img src="https://s0.wp.com/latex.php?latex=H.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H." class="latex" />  That&#8217;s no big deal: it&#8217;s just a different convention than the one I want today.  A more serious issue is that in stochastic mechanics, <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;psi" class="latex" /> stands for a <i>probability distribution</i>.  This suggests that we should get probabilities into the game somehow.</p>
<h3> The replicator equation in terms of probabilities </h3>
<p>Luckily, that&#8217;s exactly what people usually do!   Instead of talking about the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species, they talk about the <i>probability</i> <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> that one of our organisms will belong to the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th species.  This amounts to normalizing our populations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_j+P_j%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  p_i = &#92;frac{P_i}{&#92;sum_j P_j} } " class="latex" /></p>
<p>Don&#8217;t you love it when notations work out well?  Our big <b>P</b>opulation <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> has gotten normalized to give little <b>p</b>robability <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /></p>
<p>How do these probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> change with time?  Now is the moment for that least loved rule of elementary calculus to come out and take a bow: the quotient rule for derivatives!</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%5Csum_j+P_j+%5Cquad+-+%5Cquad+P_i+%5Csum_j+%5Cfrac%7Bd+P_j%7D%7Bd+t%7D%5Cright%29+%5Cbig%7B%2F%7D+%5Cleft%28++%5Csum_j+P_j+%5Cright%29%5E2+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;left(&#92;frac{d P_i}{d t} &#92;sum_j P_j &#92;quad - &#92;quad P_i &#92;sum_j &#92;frac{d P_j}{d t}&#92;right) &#92;big{/} &#92;left(  &#92;sum_j P_j &#92;right)^2 }" class="latex" /></p>
<p>Using our earlier version of the replicator equation, this gives:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D++%5Cleft%28f_i%28P%29+P_i+%5Csum_j+P_j+%5Cquad+-+%5Cquad+P_i+%5Csum_j+f_j%28P%29+P_j+%5Cright%29+%5Cbig%7B%2F%7D+%5Cleft%28++%5Csum_j+P_j+%5Cright%29%5E2+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} =  &#92;left(f_i(P) P_i &#92;sum_j P_j &#92;quad - &#92;quad P_i &#92;sum_j f_j(P) P_j &#92;right) &#92;big{/} &#92;left(  &#92;sum_j P_j &#92;right)^2 }" class="latex" /></p>
<p>Using the definition of <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> this simplifies to:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D++f_i%28P%29+p_i+%5Cquad+-+%5Cquad+%5Cleft%28+%5Csum_j+f_j%28P%29+p_j+%5Cright%29+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} =  f_i(P) p_i &#92;quad - &#92;quad &#92;left( &#92;sum_j f_j(P) p_j &#92;right) p_i }" class="latex" /></p>
<p>The stuff in parentheses actually has a nice meaning: it&#8217;s just the <b>mean fitness</b>.  In other words, it&#8217;s the average, or expected, fitness of an organism chosen at random from the whole population.  Let&#8217;s write it like this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+f%28P%29+%5Crangle+%3D+%5Csum_j+f_j%28P%29+p_j++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle f(P) &#92;rangle = &#92;sum_j f_j(P) p_j  } " class="latex" /></p>
<p>So, we get the <b><a>replicator equation</a></b> in its classic form:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5CBig%28+f_i%28P%29+-+%5Clangle+f%28P%29+%5Crangle+%5CBig%29+%5C%2C+p_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d p_i}{d t} = &#92;Big( f_i(P) - &#92;langle f(P) &#92;rangle &#92;Big) &#92;, p_i }" class="latex" /></p>
<p>This has a nice meaning: for the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type to increase, their fitness must exceed the mean fitness.  If you&#8217;re trying to increase <a href="http://en.wikipedia.org/wiki/Market_share">market share</a>, what matters is not how good you are, but how much <i>better than average</i> you are.  If everyone else is lousy, you&#8217;re in luck.</p>
<h3> Entropy </h3>
<p>Now for something a bit new.  Once we&#8217;ve gotten a probability distribution into the game, its entropy is sure to follow:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28p%29+%3D+-+%5Csum_i+p_i+%5C%2C+%5Cln%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(p) = - &#92;sum_i p_i &#92;, &#92;ln(p_i) } " class="latex" /></p>
<p>This says how &#8216;smeared-out&#8217; the overall population is among the various different species.  Alternatively, it says how much <i>information</i> it takes, on average, to say which species a randomly chosen organism belongs to.   For example, if there are <img src="https://s0.wp.com/latex.php?latex=2%5EN&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2^N" class="latex" /> species, all with equal populations, the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> works out to <img src="https://s0.wp.com/latex.php?latex=N+%5Cln+2.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N &#92;ln 2." class="latex" />  So in this case, it takes <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> bits of information to say which species a randomly chosen organism belongs to.</p>
<p>In biology, entropy is one of many ways people measure biodiversity.  For a quick intro to some of the issues involved, try:</p>
<p>&bull; Tom Leinster, <a href="https://johncarlosbaez.wordpress.com/2011/11/07/measuring-biodiversity/">Measuring biodiversity</a>, <i>Azimuth</i>, 7 November 2011.</p>
<p>&bull; Lou Jost, <a href="http://www.loujost.com/Statistics%20and%20Physics/Diversity%20and%20Similarity/JostEntropy%20AndDiversity.pdf">Entropy and diversity</a>, <i>Oikos</i> <b>113</b> (2006), 363&#8211;375.</p>
<p>But we don&#8217;t need to understand this stuff to see how entropy is connected to the replicator equation.  Marc Harper&#8217;s paper explains this in detail:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763">The replicator equation as an inference dynamic</a>.</p>
<p>and I hope to go through quite a bit of it here.  But not today!  Today I just want to look at a pathetically simple, yet still interesting, example.</p>
<h3> Exponential growth </h3>
<p>Suppose the fitness of each species is independent of the populations of all the species.   In other words, suppose each fitness <img src="https://s0.wp.com/latex.php?latex=f_i%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i(P)" class="latex" /> is actually a constant, say <img src="https://s0.wp.com/latex.php?latex=f_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i." class="latex" />  Then the replicator equation reduces to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i+%5C%2C+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i &#92;, P_i } " class="latex" /></p>
<p>so it&#8217;s easy to solve:</p>
<p><img src="https://s0.wp.com/latex.php?latex=P_i%28t%29+%3D+e%5E%7Bt+f_i%7D+P_i%280%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t) = e^{t f_i} P_i(0)" class="latex" /></p>
<p>You don&#8217;t need a detailed calculation to see what&#8217;s going to happen to the probabilities</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Csum_j+P_j%28t%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i(t) = &#92;frac{P_i(t)}{&#92;sum_j P_j(t)}} " class="latex" /></p>
<p>The most fit species present will eventually take over!   If one species, say the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th one, has a fitness greater than the rest, then the population of this species will eventually grow faster than all the rest, at least if its population starts out greater than zero.  So as <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%2B%5Cinfty%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;to +&#92;infty," class="latex" /> we&#8217;ll have</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) &#92;to 1" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_j%28t%29+%5Cto+0+%5Cquad+%5Cmathrm%7Bfor%7D+%5Cquad+j+%5Cne+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_j(t) &#92;to 0 &#92;quad &#92;mathrm{for} &#92;quad j &#92;ne i" class="latex" /></p>
<p>Thus the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> will become more sharply peaked, and <i>its entropy will eventually approach zero</i>.</p>
<p>With a bit more thought you can see that even if more than one species shares the maximum possible fitness, the entropy will eventually decrease, though not approach zero.</p>
<p>In other words, <i>the biodiversity will eventually drop</i> as all but the most fit species are overwhelmed.  Of course, this is only true in our simple idealization.  In reality, biodiversity behaves in more complex ways&amp;mdash;in part because species interact, and in part because mutation tends to smear out the probability distribution <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />  We&#8217;re not looking at these effects yet.  They&#8217;re extremely important&#8230; in ways we can only fully understand if we start by looking at what happens when they&#8217;re not present.</p>
<p>In still other words, <i>the population will absorb information from its environment</i>.  This should make intuitive sense: the process of natural selection resembles &#8216;learning&#8217;.  As fitter organisms become more common and less fit ones die out, the environment puts its stamp on the probability distribution <img src="https://s0.wp.com/latex.php?latex=p.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p." class="latex" />  So, this probability distribution should gain information.</p>
<p>While intuitively clear, this last claim also follows more rigorously from thinking of entropy as negative information.  Admittedly, it&#8217;s always easy to get confused by minus signs when relating entropy and information.   A while back I said the entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S%28p%29+%3D+-+%5Csum_i+p_i+%5C%2C+%5Cln%28p_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S(p) = - &#92;sum_i p_i &#92;, &#92;ln(p_i) } " class="latex" /></p>
<p>was the average information required to say which species a randomly chosen organism belongs to.  If this entropy is going down, isn&#8217;t the population <i>losing</i> information?</p>
<p>No, this is a classic sign error.  It&#8217;s like the concept of &#8216;work&#8217; in physics.  We can talk about the work some system does on its environment, or the work done by the environment on the system, and these are almost the same&#8230; <i>except one is minus the other!</i></p>
<p>When you are very ignorant about some system&#8212;say, some rolled dice&mdash;your estimated probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> for its various possible states are very smeared-out, so the entropy <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> is large.  As you gain information, you revise your probabilities and they typically become more sharply peaked, so <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> goes down.   When you know as much as you possibly can, <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> equals zero.</p>
<p>So, the entropy <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> is the amount of information you have left to learn: the amount of information you <i>lack</i>, not the amount you <i>have</i>.  As you gain information, this goes down.  There&#8217;s no paradox here.</p>
<p>It works the same way with our population of replicators&#8212;at least in the special case where the fitness of each species is independent of its population.  The probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is like a &#8216;hypothesis&#8217; assigning to each species <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> the probability <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> that it&#8217;s the best at self-replicating.   As some replicators die off while others prosper, they gather information their environment, and this hypothesis gets refined.  So, the entropy <img src="https://s0.wp.com/latex.php?latex=S%28p%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p)" class="latex" /> drops.</p>
<h3> Next time </h3>
<p>Of course, to make closer contact to reality, we need to go beyond the special case where the fitness of each species is a constant!   Marc Harper does this, and I want to talk about his work someday, but first I have a few more remarks to make about the pathetically simple special case I&#8217;ve been focusing on. I&#8217;ll save these for next time, since I&#8217;ve probably strained your patience already.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comments">17 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;9)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-7944 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-7944">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/" rel="bookmark">Quantropy (Part 2)</a></h2>
				<small>10 February, 2012</small><br />


				<div class="entry">
					<p>In my <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/">first post in this series</a>, we saw that filling in a well-known analogy between statistical mechanics and quantum mechanics requires a new concept: &#8216;quantropy&#8217;.  To get some feeling for this concept, we should look at some examples.  But to do that, we need to develop some tools to compute quantropy.  That&#8217;s what we&#8217;ll do today.</p>
<p>All these tools will be borrowed from statistical mechanics.  So, let me remind you how to compute the entropy of a system in thermal equilibrium starting if we know the energy of every state.  Then we&#8217;ll copy this and get a formula for the quantropy of a system if we know the action of every history.</p>
<h3> Computing entropy </h3>
<p>Everything in this section is bog-standard.  In case you don&#8217;t know, that&#8217;s British slang for &#8216;extremely, perhaps even depressingly, familiar&#8217;.  Apparently it rains so much in England that bogs are not only standard, they&#8217;re the <i>standard</i> of what counts as standard!</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be a measure space: physically, the set of states of some system.  In statistical mechanics we suppose the system occupies states with probabilities given by some probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3A+X+%5Cto+%5B0%2C%5Cinfty%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p : X &#92;to [0,&#92;infty) " class="latex" /></p>
<p>where of course </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_X+p%28x%29+%5C%2C+dx+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_X p(x) &#92;, dx = 1 " class="latex" /></p>
<p>The <b>entropy</b> of this probability distribution is</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Cint_X+p%28x%29+%5Cln%28p%28x%29%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;int_X p(x) &#92;ln(p(x)) &#92;, dx " class="latex" /></p>
<p>There&#8217;s a nice way to compute the entropy when our system is in thermal equilibrium.  This idea makes sense when we have a function </p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3A+X+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H : X &#92;to &#92;mathbb{R} " class="latex" /> </p>
<p>saying the <b>energy</b> of each state.  Our system is in <b>thermal equilibrium</b> when <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> maximizes entropy subject to a constraint on the expected value of energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+%5Cint_X+H%28x%29+p%28x%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = &#92;int_X H(x) p(x) &#92;, dx " class="latex" /></p>
<p>A famous calculation shows that thermal equilibrium occurs precisely when <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is the so-called <b>Gibbs state</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p%28x%29+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta+H%28x%29%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p(x) = &#92;frac{e^{-&#92;beta H(x)}}{Z} } " class="latex" /></p>
<p>for some real number <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalization factor called the <b>partition function</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cint_X+e%5E%7B-%5Cbeta+H%28x%29%7D+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;int_X e^{-&#92;beta H(x)} &#92;, dx " class="latex" /></p>
<p>The number <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is called the <b>coolness</b>, since physical considerations say that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cbeta+%3D+%5Cfrac%7B1%7D%7BT%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;beta = &#92;frac{1}{T} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the <b>temperature</b> in units where Boltzmann&#8217;s constant is 1.</p>
<p>There&#8217;s a famous way to compute the entropy of the Gibbs state; I don&#8217;t know who did it first, but it&#8217;s both straightforward and tremendously useful.  We take the formula for entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Cint_X+p%28x%29+%5Cln%28p%28x%29%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;int_X p(x) &#92;ln(p(x)) &#92;, dx " class="latex" /></p>
<p>and substitute the Gibbs state</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p%28x%29+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta+H%28x%29%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p(x) = &#92;frac{e^{-&#92;beta H(x)}}{Z} } " class="latex" /></p>
<p>getting</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+S+%26%3D%26+%5Cint_X+p%28x%29+%5Cleft%28+%5Cbeta+H%28x%29+-+%5Cln+Z+%5Cright%29%5C%2C+dx+%5C%5C+++%5C%5C++%26%3D%26+%5Cbeta+%5C%2C+%5Clangle+H+%5Crangle+-+%5Cln+Z+%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} S &amp;=&amp; &#92;int_X p(x) &#92;left( &#92;beta H(x) - &#92;ln Z &#92;right)&#92;, dx &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;beta &#92;, &#92;langle H &#92;rangle - &#92;ln Z &#92;end{array}  " class="latex" /></p>
<p>Reshuffling this a little bit, we obtain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+T+%5Cln+Z+%3D+%5Clangle+H+%5Crangle+-+T+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- T &#92;ln Z = &#92;langle H &#92;rangle - T S" class="latex" /></p>
<p>If we define the <b>free energy</b> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-+T+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = - T &#92;ln Z" class="latex" /></p>
<p>then we&#8217;ve shown that</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+H+%5Crangle+-+T+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle H &#92;rangle - T S " class="latex" /></p>
<p>This justifies the term &#8216;free energy&#8217;: it&#8217;s the expected energy minus the energy in the form of heat, namely <img src="https://s0.wp.com/latex.php?latex=T+S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T S." class="latex" /></p>
<p>It&#8217;s nice that we can compute the free energy purely in terms of the partition function and the temperature, or equivalently the coolness <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F+%3D+-+%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F = - &#92;frac{1}{&#92;beta} &#92;ln Z }" class="latex" /></p>
<p>Can we also do this for the entropy?  Yes!  First we&#8217;ll do it for the expected energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Clangle+H+%5Crangle+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_X+H%28x%29+p%28x%29+%5C%2C+dx+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%7D+%5Cint_X+H%28x%29+e%5E%7B-%5Cbeta+H%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cint_X+e%5E%7B-%5Cbeta+H%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7BdZ%7D%7Bd+%5Cbeta%7D+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;langle H &#92;rangle &amp;=&amp; &#92;displaystyle{ &#92;int_X H(x) p(x) &#92;, dx } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{1}{Z} &#92;int_X H(x) e^{-&#92;beta H(x)} &#92;, dx } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{d}{d &#92;beta} &#92;int_X e^{-&#92;beta H(x)} &#92;, dx } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{dZ}{d &#92;beta} } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;frac{d}{d &#92;beta} &#92;ln Z } &#92;end{array} " class="latex" /></p>
<p>This gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+S+%26%3D%26+%5Cbeta+%5C%2C+%5Clangle+H+%5Crangle+-+%5Cln+Z+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+-+%5Cbeta+%5C%2C+%5Cfrac%7Bd+%5Cln+Z%7D%7Bd+%5Cbeta%7D+-+%5Cln+Z+%7D%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} S &amp;=&amp; &#92;beta &#92;, &#92;langle H &#92;rangle - &#92;ln Z &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ - &#92;beta &#92;, &#92;frac{d &#92;ln Z}{d &#92;beta} - &#92;ln Z }&#92;end{array} " class="latex" /></p>
<p>So, if we know the partition function of a system in thermal equilibrium as a function of the temperature, we can work out its entropy, expected energy and free energy.  </p>
<h3> Computing quantropy </h3>
<p>Now we&#8217;ll repeat everything for quantropy!  The idea is simply to replace the energy by action and the temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=i+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;hbar" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is Planck&#8217;s constant.  It&#8217;s harder to get the integrals to converge in interesting examples.  But we&#8217;ll worry about that next time, that when we actually do an example!  </p>
<p>It&#8217;s annoying that in physics <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> stands for both entropy and action, since in this article we need to think about both.  People also use <img src="https://s0.wp.com/latex.php?latex=H&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H" class="latex" /> to stand for entropy, but that&#8217;s no better, since that letter also stands for &#8216;Hamiltonian&#8217;!  To avoid this let&#8217;s use <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> to stand for action.  This letter is also used to mean &#8216;Helmholtz free energy&#8217;, but we&#8217;ll just have to live with that.  It would be real bummer if we failed to unify physics just because we ran out of letters.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be a measure space: physically, the set of histories of some system.  In quantum mechanics we suppose the system carries out histories with amplitudes given by some function</p>
<p><img src="https://s0.wp.com/latex.php?latex=a+%3A+X+%5Cto+%5Cmathbb%7BC%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a : X &#92;to &#92;mathbb{C} " class="latex" /></p>
<p>where perhaps surprisingly</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cint_X+a%28x%29+%5C%2C+dx+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;int_X a(x) &#92;, dx = 1 " class="latex" /></p>
<p>The <b>quantropy</b> of this function is</p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+%3D+-+%5Cint_X+a%28x%29+%5Cln%28a%28x%29%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = - &#92;int_X a(x) &#92;ln(a(x)) &#92;, dx " class="latex" /></p>
<p>There&#8217;s a nice way to compute the entropy in Feynman&#8217;s path integral formalism.  This formalism makes sense when we have a function </p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%3A+X+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A : X &#92;to &#92;mathbb{R} " class="latex" /> </p>
<p>saying the <b>action</b> of each history.  Feynman proclaimed that in this case we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a%28x%29+%3D+%5Cfrac%7Be%5E%7Bi+A%28x%29%2F%5Chbar%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a(x) = &#92;frac{e^{i A(x)/&#92;hbar}}{Z} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is Planck&#8217;s constant and <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" /> is a normalization factor called the <b>partition function</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Cint_X+e%5E%7Bi+A%28x%29%2F%5Chbar%7D+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;int_X e^{i A(x)/&#92;hbar} &#92;, dx " class="latex" /></p>
<p>Last time I showed that we obtain Feynman&#8217;s prescription for <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> by demanding that it&#8217;s a stationary point for the <b>quantropy</b> </p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+%3D+-+%5Cint_X+a%28x%29+%5C%2C+%5Cln+%28a%28x%29%29+%5C%2C+dx&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = - &#92;int_X a(x) &#92;, &#92;ln (a(x)) &#92;, dx" class="latex" /></p>
<p>subject to a constraint on the <b>expected action</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Cint_X+A%28x%29+a%28x%29+%5C%2C+dx+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;int_X A(x) a(x) &#92;, dx " class="latex" /></p>
<p>As I mentioned <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/">last time</a>, the formula for quantropy is dangerous, since we&#8217;re taking the logarithm of a complex-valued function.  There&#8217;s not really a &#8216;best&#8217; logarithm for a complex number: if we have one choice we can add any multiple of <img src="https://s0.wp.com/latex.php?latex=2+%5Cpi+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2 &#92;pi i" class="latex" /> and get another.  So in general, to define quantropy we need to pick a choice of <img src="https://s0.wp.com/latex.php?latex=%5Cln+%28a%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (a(x))" class="latex" /> for each point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X." class="latex" />  That&#8217;s a lot of ambiguity! </p>
<p>Luckily, the ambiguity is much less when we use Feynman&#8217;s prescription for <img src="https://s0.wp.com/latex.php?latex=a.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a." class="latex" />  Why?  Because then <img src="https://s0.wp.com/latex.php?latex=a%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a(x)" class="latex" /> is defined in terms of an exponential, and it&#8217;s easy to take the logarithm of an exponential!  So, we can declare that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cln+%28a%28x%29%29+%3D+%5Cdisplaystyle%7B+%5Cln+%5Cleft%28+%5Cfrac%7Be%5E%7BiA%28x%29%2F%5Chbar%7D%7D%7BZ%7D%5Cright%29+%7D+%3D+%5Cfrac%7Bi%7D%7B%5Chbar%7D+A%28x%29+-+%5Cln+Z++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (a(x)) = &#92;displaystyle{ &#92;ln &#92;left( &#92;frac{e^{iA(x)/&#92;hbar}}{Z}&#92;right) } = &#92;frac{i}{&#92;hbar} A(x) - &#92;ln Z  " class="latex" /></p>
<p>Once we choose a logarithm for <img src="https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z" class="latex" />, this formula will let us define <img src="https://s0.wp.com/latex.php?latex=%5Cln+%28a%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ln (a(x))" class="latex" /> and thus the quantropy.</p>
<p>So let&#8217;s do this, and say the quantropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+-+%5Cint_X+a%28x%29+%5Cleft%28+%5Cfrac%7Bi%7D%7B%5Chbar%7D+A%28x%29+-+%5Cln+Z+%5Cright%29%5C%2C+dx+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = - &#92;int_X a(x) &#92;left( &#92;frac{i}{&#92;hbar} A(x) - &#92;ln Z &#92;right)&#92;, dx } " class="latex" /></p>
<p>We can simplify this a bit, since the integral of <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> is 1:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+%5Cfrac%7B1%7D%7Bi+%5Chbar%7D+%5Clangle+A+%5Crangle+%2B+%5Cln+Z+%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = &#92;frac{1}{i &#92;hbar} &#92;langle A &#92;rangle + &#92;ln Z }  " class="latex" /></p>
<p>Reshuffling this a little bit, we obtain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=-+i+%5Chbar+%5Cln+Z+%3D+%5Clangle+A+%5Crangle+-+i+%5Chbar+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="- i &#92;hbar &#92;ln Z = &#92;langle A &#92;rangle - i &#92;hbar Q" class="latex" /></p>
<p>By analogy to free energy in statistical mechanics, let&#8217;s define the <b>free action</b> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+-+i+%5Chbar+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = - i &#92;hbar &#92;ln Z" class="latex" /></p>
<p>I&#8217;m using the same letter for free energy and free action, but they play exactly analogous roles, so it&#8217;s not so bad.  Indeed we now have</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+A+%5Crangle+-+i+%5Chbar+Q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle A &#92;rangle - i &#92;hbar Q " class="latex" /></p>
<p>which is the analogue of a formula we saw for free energy in thermodynamics.  </p>
<p>It&#8217;s nice that we can compute the free action purely in terms of the partition function and Planck&#8217;s constant. Can we also do this for the quantropy?  Yes!  </p>
<p>It&#8217;ll be convenient to introduce a parameter</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cbeta+%3D+%5Cfrac%7B1%7D%7Bi+%5Chbar%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;beta = &#92;frac{1}{i &#92;hbar} }" class="latex" /></p>
<p>which is analogous to &#8216;coolness&#8217;.  We could call it &#8216;quantum coolness&#8217;, but a better name might be <b>classicality</b>, since it&#8217;s big when our system is close to classical.  Whatever we call it, the main thing is that unlike ordinary coolness, it&#8217;s imaginary!    </p>
<p>In terms of classicality, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta+A%28x%29%7D%7D%7BZ%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a(x) = &#92;frac{e^{- &#92;beta A(x)}}{Z} } " class="latex" /></p>
<p>Now we can compute the expected action just as we computed the expected energy in thermodynamics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Clangle+A+%5Crangle+%26%3D%26+%5Cdisplaystyle%7B+%5Cint_X+A%28x%29+a%28x%29+%5C%2C+dx+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%7D+%5Cint_X+A%28x%29+e%5E%7B-%5Cbeta+A%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+++%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cint_X+e%5E%7B-%5Cbeta+A%28x%29%7D+%5C%2C+dx+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%7D+%5Cfrac%7BdZ%7D%7Bd+%5Cbeta%7D+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z+%7D+%5Cend%7Barray%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;langle A &#92;rangle &amp;=&amp; &#92;displaystyle{ &#92;int_X A(x) a(x) &#92;, dx } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{1}{Z} &#92;int_X A(x) e^{-&#92;beta A(x)} &#92;, dx } &#92;&#92;   &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{d}{d &#92;beta} &#92;int_X e^{-&#92;beta A(x)} &#92;, dx } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{1}{Z} &#92;frac{dZ}{d &#92;beta} } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ - &#92;frac{d}{d &#92;beta} &#92;ln Z } &#92;end{array}" class="latex" /></p>
<p>This gives:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+Q+%26%3D%26+%5Cbeta+%5C%2C%5Clangle+A+%5Crangle+-+%5Cln+Z+%5C%5C+%5C%5C+%26%3D%26+%5Cdisplaystyle%7B+-+%5Cbeta+%5C%2C%5Cfrac%7Bd+%5Cln+Z%7D%7Bd+%5Cbeta%7D+-+%5Cln+Z+%7D+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} Q &amp;=&amp; &#92;beta &#92;,&#92;langle A &#92;rangle - &#92;ln Z &#92;&#92; &#92;&#92; &amp;=&amp; &#92;displaystyle{ - &#92;beta &#92;,&#92;frac{d &#92;ln Z}{d &#92;beta} - &#92;ln Z } &#92;end{array} " class="latex" /></p>
<p>So, if we can compute the partition function in the path integral approach to quantum mechanics, we can also work out the quantropy, expected action and free action!   </p>
<p>Next time I&#8217;ll use these formulas to compute quantropy in an example: the free particle.  We&#8217;ll see some strange and interesting things.</p>
<p><a name="summary"></p>
<h3> Summary </h3>
<p></a></p>
<p>Here&#8217;s where our analogy stands now:</p>
<table border="1">
<tr>
<td><b>Statistical Mechanics</b></td>
<td><b>Quantum Mechanics</b></td>
</tr>
<tr>
<td>states: <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /></td>
<td>histories: <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /></td>
</tr>
<tr>
<td>probabilities: <img src="https://s0.wp.com/latex.php?latex=p%3A+X+%5Cto+%5B0%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p: X &#92;to [0,&#92;infty)" class="latex" /></td>
<td>amplitudes: <img src="https://s0.wp.com/latex.php?latex=a%3A+X+%5Cto+%5Cmathbb%7BC%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a: X &#92;to &#92;mathbb{C} " class="latex" /></td>
</tr>
<tr>
<td>energy: <img src="https://s0.wp.com/latex.php?latex=H%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H: X &#92;to &#92;mathbb{R}" class="latex" /></td>
<td>action: <img src="https://s0.wp.com/latex.php?latex=A%3A+X+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A: X &#92;to &#92;mathbb{R}" class="latex" /> </td>
</tr>
<tr>
<td>temperature: <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /></td>
<td>Planck&#8217;s constant times <i>i</i>: <img src="https://s0.wp.com/latex.php?latex=i+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;hbar" class="latex" /></td>
</tr>
<tr>
<td>coolness: <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T" class="latex" /></td>
<td>classicality: <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2Fi+%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/i &#92;hbar" class="latex" /></td>
</tr>
<tr>
<td>partition function: <img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-%5Cbeta+H%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{-&#92;beta H(x)}" class="latex" /></td>
<td>partition function: <img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+X%7D+e%5E%7B-%5Cbeta+A%28x%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z = &#92;sum_{x &#92;in X} e^{-&#92;beta A(x)}" class="latex" /></td>
</tr>
<tr>
<td>Boltzmann distribution: <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+e%5E%7B-%5Cbeta+H%28x%29%7D%2FZ&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(x) = e^{-&#92;beta H(x)}/Z" class="latex" /></td>
<td>Feynman sum over histories: <img src="https://s0.wp.com/latex.php?latex=a%28x%29+%3D+e%5E%7B-%5Cbeta+A%28x%29%7D%2FZ&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a(x) = e^{-&#92;beta A(x)}/Z" class="latex" /></td>
</tr>
<tr>
<td>entropy: <img src="https://s0.wp.com/latex.php?latex=S+%3D+-+%5Csum_%7Bx+%5Cin+X%7D+p%28x%29+%5Cln%28p%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = - &#92;sum_{x &#92;in X} p(x) &#92;ln(p(x))" class="latex" /></td>
<td>quantropy: <img src="https://s0.wp.com/latex.php?latex=Q+%3D+-+%5Csum_%7Bx+%5Cin+X%7D+a%28x%29+%5Cln%28a%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = - &#92;sum_{x &#92;in X} a(x) &#92;ln(a(x))" class="latex" /></td>
</tr>
<tr>
<td>expected energy: <img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+%5Csum_%7Bx+%5Cin+X%7D+p%28x%29+H%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = &#92;sum_{x &#92;in X} p(x) H(x) " class="latex" /></td>
<td>expected action: <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+%5Csum_%7Bx+%5Cin+X%7D+a%28x%29+A%28x%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = &#92;sum_{x &#92;in X} a(x) A(x) " class="latex" /></td>
</tr>
<tr>
<td>free energy: <img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+H+%5Crangle+-+TS&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle H &#92;rangle - TS" class="latex" /></td>
<td>free action: <img src="https://s0.wp.com/latex.php?latex=F+%3D+%5Clangle+A+%5Crangle+-+i+%5Chbar+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = &#92;langle A &#92;rangle - i &#92;hbar Q" class="latex" /></td>
</tr>
<tr>
<td>  <img src="https://s0.wp.com/latex.php?latex=%5Clangle+H+%5Crangle+%3D+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle H &#92;rangle = - &#92;frac{d}{d &#92;beta} &#92;ln Z" class="latex" />  </td>
<td> <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%3D+-+%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle = - &#92;frac{d}{d &#92;beta} &#92;ln Z" class="latex" /></td>
</tr>
<tr>
<td> <img src="https://s0.wp.com/latex.php?latex=F+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = -&#92;frac{1}{&#92;beta} &#92;ln Z" class="latex" /> </td>
<td> <img src="https://s0.wp.com/latex.php?latex=F+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = -&#92;frac{1}{&#92;beta} &#92;ln Z" class="latex" />    </td>
</tr>
<tr>
<td>  <img src="https://s0.wp.com/latex.php?latex=S+%3D++%5Cln+Z+-+%5Cbeta+%5C%2C%5Cfrac%7Bd%7D%7Bd+%5Cbeta%7D%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S =  &#92;ln Z - &#92;beta &#92;,&#92;frac{d}{d &#92;beta}&#92;ln Z " class="latex" /> </td>
<td> <img src="https://s0.wp.com/latex.php?latex=Q+%3D+%5Cln+Z+-+%5Cbeta+%5C%2C%5Cfrac%7Bd+%7D%7Bd+%5Cbeta%7D%5Cln+Z+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q = &#92;ln Z - &#92;beta &#92;,&#92;frac{d }{d &#92;beta}&#92;ln Z " class="latex" />
</td>
</tr>
</table>
<p>I should also say a word about units and dimensional analysis.  There&#8217;s enormous flexibility in how we do dimensional analysis.  Amateurs often don&#8217;t realize this, because they&#8217;ve just learned one system, but experts take full advantage of this flexibility to pick a setup that&#8217;s convenient for what they&#8217;re doing.  The fewer independent units you use, the fewer dimensionful constants like the speed of light, Planck&#8217;s constant and Boltzmann&#8217;s constant you see in your formulas.  That&#8217;s often good.  But here I don&#8217;t want to set Planck&#8217;s constant equal to 1 because I&#8217;m treating it as analogous to temperature&#8212;so it&#8217;s important, and I want to <i>see</i> it.  I&#8217;m also finding dimensional analysis useful to check my formulas. </p>
<p>So, I&#8217;m using units where mass, length and time count as independent dimensions in the sense of dimensional analysis.  On the other hand, I&#8217;m not treating temperature as an independent dimension: instead, I&#8217;m setting Boltzmann&#8217;s constant to 1 and using that to translate from temperature into energy.   This is fairly common in some circles. And for me, treating temperature as an independent dimension would be analogous to treating Planck&#8217;s constant as having its own independent dimension!  I don&#8217;t feel like doing that.</p>
<p>So, here&#8217;s how the dimensional analysis works in my setup:</p>
<table border="1">
<tr>
<td><b>Statistical Mechanics</b></td>
<td><b>Quantum Mechanics</b></td>
</tr>
<tr>
<td>probabilities: dimensionless</td>
<td>amplitudes: dimensionless </td>
</tr>
<tr>
<td>energy: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /> </td>
<td>action: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /> </td>
</tr>
<tr>
<td>temperature: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /></td>
<td>Planck&#8217;s constant: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /></td>
</tr>
<tr>
<td>coolness: <img src="https://s0.wp.com/latex.php?latex=T%5E2%2FML&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^2/ML" class="latex" /></td>
<td>classicality: <img src="https://s0.wp.com/latex.php?latex=T%2FML+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T/ML " class="latex" /></td>
</tr>
<tr>
<td>partition function: dimensionless </td>
<td>partition function: dimensionless </td>
</tr>
<td>entropy: dimensionless </td>
<td>quantropy: dimensionless </td>
<tr>
<td>expected energy: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /></td>
<td>expected action: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /></td>
</tr>
<tr>
<td>free energy: <img src="https://s0.wp.com/latex.php?latex=ML%2FT%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T^2" class="latex" /></td>
<td>free action: <img src="https://s0.wp.com/latex.php?latex=ML%2FT&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="ML/T" class="latex" /></td>
</tr>
</table>
<p>I like this setup because I often think of entropy as closely allied to information, measured in bits or nats depending on whether I&#8217;m using base 2 or base <i>e</i>.  From this viewpoint, it should be dimensionless.  </p>
<p>Of course, in thermodynamics it&#8217;s common to put a factor of Boltzmann&#8217;s constant in front of the formula for entropy.  Then entropy has units of energy/temperature.  But I&#8217;m using units where Boltzmann&#8217;s constant is 1 and temperature has the same units as energy!  So for me, entropy is dimensionless.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/#comments">19 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/02/10/quantropy-part-2/" rel="bookmark" title="Permanent Link to Quantropy (Part 2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-7682 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-7682">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/02/01/entropic-forces/" rel="bookmark">Entropic Forces</a></h2>
				<small>1 February, 2012</small><br />


				<div class="entry">
					<p>&nbsp;</p>
<div align="center"><img src="https://i2.wp.com/math.ucr.edu/home/baez/warning_high_entropy_area.png" alt="" /></div>
<p>In 2009, Erik Verlinde argued that gravity is an entropic force. This created a <a href="http://en.wikipedia.org/wiki/Entropic_gravity#Erik_Verlinde.27s_theory">big stir</a>&#8212;and it helped him win about <a href="http://www.math.columbia.edu/~woit/wordpress/?p=3781">$6,500,000 in prize money and grants!</a>  But what the heck is an &#8216;entropic force&#8217;, anyway?  </p>
<p>Entropic forces are nothing unusual: you&#8217;ve felt one if you&#8217;ve ever stretched a rubber band.  Why does a rubber band pull back when you stretch it?  You might think it&#8217;s because a stretched rubber band has <i>more energy</i> than an unstretched one.  That would indeed be a fine explanation for a metal spring.  But rubber doesn&#8217;t work that way.  Instead, a stretched rubber band mainly has <i>less entropy</i> than an unstretched one&#8212;and this too can cause a force.</p>
<p>You see, molecules of rubber are like long chains.  When unstretched, these chains can curl up in lots of random wiggly ways.  &#8216;Lots of random ways&#8217; means lots of entropy.  But when you stretch one of these chains, the number of ways it can be shaped decreases, until it&#8217;s pulled taut and there&#8217;s just one way!   Only past that point does stretching the molecule take a lot of energy; before that, you&#8217;re mainly decreasing its entropy.  </p>
<p>So, the force of a stretched rubber band is an entropic force.</p>
<p>But how can changes in either energy or entropy give rise to forces?  That&#8217;s what I want to explain.  But instead of talking about force, I&#8217;ll start out talking about pressure.  This too arises both from changes in energy and changes in entropy.</p>
<h3>Entropic pressure &#8212; a sloppy derivation </h3>
<p>If you&#8217;ve ever studied thermodynamics you&#8217;ve probably heard about an <a href="http://en.wikipedia.org/wiki/Ideal_gas">ideal gas</a>.  You can think of this as a gas consisting of point particles that almost never collide with each other&#8212;because they&#8217;re just points&#8212;and bounce elastically off the walls of the container they&#8217;re in.  If you have a box of gas like this, it&#8217;ll push on the walls with some pressure.  But the cause of this pressure is <i>not</i> that slowly making the box smaller increases the energy of the gas inside: in fact, it doesn&#8217;t!  The cause is that making the box smaller decreases the <i>entropy</i> of the gas.</p>
<p>To understand how pressure has an &#8216;energetic&#8217; part and an &#8216;entropic&#8217; part, let&#8217;s start with the basic equation of thermodynamics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=d+U+%3D+T+d+S+-+P+d+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d U = T d S - P d V" class="latex" /></p>
<p>What does this mean?  It means the internal energy <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> of a box of stuff changes when you heat or cool it, meaning that you change its entropy <img src="https://s0.wp.com/latex.php?latex=S%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S," class="latex" /> but also when you shrink or expand it, meaning that you change its volume <img src="https://s0.wp.com/latex.php?latex=V.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V." class="latex" />  Increasing its entropy raises its internal energy at a rate proportional to its temperature <img src="https://s0.wp.com/latex.php?latex=T.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T." class="latex" />  Increasing its volume lowers its internal energy at a rate proportional to its pressure <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" /></p>
<p>We can already see that both changes in energy, <img src="https://s0.wp.com/latex.php?latex=U%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U," class="latex" /> and entropy, <img src="https://s0.wp.com/latex.php?latex=S%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S," class="latex" /> can affect <img src="https://s0.wp.com/latex.php?latex=P+d+V.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P d V." class="latex" />  Pressure is like force&#8212;indeed it&#8217;s just force per area&#8212;so we should try to solve for <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" />  </p>
<p>First let&#8217;s do it in a sloppy way.  One reason people don&#8217;t like thermodynamics is that they don&#8217;t understand partial derivatives when there are lots different coordinate systems floating around&#8212;which is what thermodynamics is all about!   So, they manipulate these partial derivatives sloppily, feeling a sense of guilt and unease, and sometimes it works, but other times it fails disastrously.  The cure is <i>not</i> to learn more thermodynamics; the cure is to learn about <a href="http://en.wikipedia.org/wiki/Differential_form">differential forms</a>.  All the expressions in the basic equation <img src="https://s0.wp.com/latex.php?latex=d+U+%3D+T+d+S+-+P+d+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d U = T d S - P d V" class="latex" /> are differential forms.  If you learn what they are and how to work with them, you&#8217;ll never get in trouble with partial derivatives in thermodynamics&#8212;as long as you proceed slowly and carefully.</p>
<p>But let&#8217;s act like we don&#8217;t know this!  Let&#8217;s start with the basic equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=d+U+%3D+T+d+S+-+P+d+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d U = T d S - P d V" class="latex" /></p>
<p>and solve for <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" />  First we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=P+d+V+%3D+T+d+S+-+d+U+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P d V = T d S - d U " class="latex" /></p>
<p>This is fine.  Then we divide by <img src="https://s0.wp.com/latex.php?latex=d+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d V" class="latex" /> and get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+P+%3D+T+%5Cfrac%7Bd+S%7D%7Bd+V%7D+-+%5Cfrac%7Bd+U%7D%7Bd+V%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ P = T &#92;frac{d S}{d V} - &#92;frac{d U}{d V} }" class="latex" /></p>
<p>This is not so fine: here the guilt starts to set in.  After all, we&#8217;ve been told that we need to use &#8216;partial derivatives&#8217; when we have functions of several variables&#8212;and the main fact about partial derivatives, the one that everybody remembers, is that these are written with with curly d&#8217;s, not ordinary letter d&#8217;s.  So we must have done something wrong.  So, we make the d&#8217;s curly:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+P+%3D+T+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D+-+%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ P = T &#92;frac{&#92;partial S}{&#92;partial V} - &#92;frac{&#92;partial U}{&#92;partial V} }" class="latex" /></p>
<p>But we still feel guilty.  First of all, who gave us the right to make those d&#8217;s curly?  Second of all, a partial derivative like <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial S}{&#92;partial V}" class="latex" /> makes no sense unless <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> is one of a set of coordinate functions: only then we can talk about how much some function changes as we change <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> <i>while keeping the other coordinates fixed</i>.  The value of <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial S}{&#92;partial V}" class="latex" /> actually depends on what other coordinates we&#8217;re keeping fixed! So what coordinates are we using?</p>
<p>Well, it seems like one of them is <img src="https://s0.wp.com/latex.php?latex=V%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V," class="latex" /> and the other is&#8230; we don&#8217;t know!  It could be <img src="https://s0.wp.com/latex.php?latex=S%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S," class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=P%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P," class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /> or perhaps even <img src="https://s0.wp.com/latex.php?latex=P.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P." class="latex" />  This is where real unease sets in.  If we&#8217;re taking a test, we might in desperation think something like this: &#8220;Since the easiest things to control about our box of stuff are its volume and its temperature, let&#8217;s take these as our coordinates!&#8221;  And then we might write</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+P+%3D+T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+-+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ P = T &#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T - &#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T } " class="latex" /></p>
<p>And then we might do okay on this problem, because this formula is in fact <i>correct!</i>  But I hope you agree that this is an unsatisfactory way to manipulate partial derivatives: we&#8217;re shooting in the dark and hoping for luck.</p>
<h4> Entropic pressure and entropic force </h4>
<p>So, I want to show you a better way to get this result.  But first let&#8217;s take a break and think about what it means.   It means there are two possible reasons a box of gas may push back with pressure as we try to squeeze it smaller while keeping its temperature constant.  One is that the energy may go up:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -&#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T } " class="latex" /></p>
<p>will be positive if the internal energy goes up as we squeeze the box smaller.  But the other reason is that entropy may go down:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  T &#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T }" class="latex" /></p>
<p>will be positive if the entropy goes down as we squeeze the box smaller, assuming <img src="https://s0.wp.com/latex.php?latex=T+%3E+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &gt; 0." class="latex" />  </p>
<p>Let&#8217;s turn this fact into a result about force.  Remember that pressure is just force per area.  Say we have some stuff in a cylinder with a piston on top.  Say the the position of the piston is given by some coordinate <img src="https://s0.wp.com/latex.php?latex=x%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x," class="latex" /> and its area is <img src="https://s0.wp.com/latex.php?latex=A.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A." class="latex" />  Then the stuff will push on the piston with a force</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+P+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = P A" class="latex" /></p>
<p>and the change in the cylinder&#8217;s volume as the piston moves is</p>
<p><img src="https://s0.wp.com/latex.php?latex=d+V+%3D+A+d+x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d V = A d x" class="latex" />  </p>
<p>Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++P+%3D+T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+-+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  P = T &#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T - &#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T }" class="latex" /></p>
<p>gives us</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F+%3D+T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+x%7D%5Cright%7C_T+-+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+x%7D%5Cright%7C_T+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F = T &#92;left.&#92;frac{&#92;partial S}{&#92;partial x}&#92;right|_T - &#92;left.&#92;frac{&#92;partial U}{&#92;partial x}&#92;right|_T }" class="latex" /></p>
<p>So, the force consists of two parts: the <b>energetic force</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F_%7B%5Cmathrm%7Benergetic%7D%7D+%3D+-+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+x%7D%5Cright%7C_T+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F_{&#92;mathrm{energetic}} = - &#92;left.&#92;frac{&#92;partial U}{&#92;partial x}&#92;right|_T }" class="latex" /></p>
<p>and the <b><a href="http://en.wikipedia.org/wiki/Entropic_force">entropic force</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F_%7B%5Cmathrm%7Bentropic%7D%7D+%3D++T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+x%7D%5Cright%7C_T%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F_{&#92;mathrm{entropic}} =  T &#92;left.&#92;frac{&#92;partial S}{&#92;partial x}&#92;right|_T}" class="latex" /></p>
<p>Energetic forces are familiar from classical statics: for example, a rock pushes down on the table because its energy would decrease if it could go down.  Entropic forces enter the game when we generalize to thermal statics, as we&#8217;re doing now.  But when we set <img src="https://s0.wp.com/latex.php?latex=T+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = 0," class="latex" /> these entropic forces go away and we&#8217;re back to classical statics!</p>
<h4> Entropic pressure&#8212;a better derivation </h4>
<p>Okay, enough philosophizing.  To conclude, let&#8217;s derive  </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+P+%3D+T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+-+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ P = T &#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T - &#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T } " class="latex" /></p>
<p>in a less sloppy way.  We start with</p>
<p><img src="https://s0.wp.com/latex.php?latex=d+U+%3D+T+d+S+-+P+d+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d U = T d S - P d V" class="latex" /></p>
<p>which is true no matter what coordinates we use.  We can choose 2 of the 5 variables here as local coordinates, generically at least, so let&#8217;s choose <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=T.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T." class="latex" />  Then </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+d+U+%3D+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+d+V+%2B+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+T%7D%5Cright%7C_V+d+T+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ d U = &#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T d V + &#92;left.&#92;frac{&#92;partial U}{&#92;partial T}&#92;right|_V d T } " class="latex" /></p>
<p>and similarly </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+d+S+%3D+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+d+V+%2B+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+T%7D%5Cright%7C_V+d+T+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ d S = &#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T d V + &#92;left.&#92;frac{&#92;partial S}{&#92;partial T}&#92;right|_V d T } " class="latex" /></p>
<p>Using these, our equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=d+U+%3D+T+d+S+-+P+d+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d U = T d S - P d V" class="latex" /></p>
<p>becomes</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+d+V+%2B+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+T%7D%5Cright%7C_V+d+T+%3D+T+%5Cleft%28%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+d+V+%2B+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+T%7D%5Cright%7C_V+d+T+%5Cright%29+-+P+dV+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T d V + &#92;left.&#92;frac{&#92;partial U}{&#92;partial T}&#92;right|_V d T = T &#92;left(&#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T d V + &#92;left.&#92;frac{&#92;partial S}{&#92;partial T}&#92;right|_V d T &#92;right) - P dV }" class="latex" /></p>
<p>If you know about differential forms, you know that the differentials of the coordinate functions, namely <img src="https://s0.wp.com/latex.php?latex=d+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d T" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=d+V%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d V," class="latex" /> form a basis of 1-forms.  Thus we can equate the coefficients of <img src="https://s0.wp.com/latex.php?latex=d+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d V" class="latex" /> in the equation above and get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+%3D+T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+-+P+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T = T &#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T - P } " class="latex" /></p>
<p>and thus:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+P+%3D+T+%5Cleft.%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+V%7D%5Cright%7C_T+-+%5Cleft.%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+V%7D%5Cright%7C_T+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ P = T &#92;left.&#92;frac{&#92;partial S}{&#92;partial V}&#92;right|_T - &#92;left.&#92;frac{&#92;partial U}{&#92;partial V}&#92;right|_T } " class="latex" /></p>
<p>which is what we wanted!  There should be no bitter aftertaste of guilt this time.</p>
<h3> The big picture </h3>
<p>That&#8217;s almost all I want to say: a simple exposition of well-known stuff that&#8217;s not quite as well-known as it should be.  If you know some thermodynamics and are feeling mildly ambitious, you can now work out the pressure of an ideal gas and show that it&#8217;s <i>completely</i> entropic in origin: only the first term in the right-hand side above is nonzero.   If you&#8217;re feeling a lot more ambitious, you can try to read Verlinde&#8217;s papers and explain them to me.  But my own goal was not to think about gravity.  Instead, it was to ponder a question raised by Allen Knutson: how does the &#8216;entropic force&#8217; idea fit into my ruminations on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/">classical mechanics versus thermodynamics?</a></p>
<p>It seems to fit in this way: as we go from classical statics (governed by the principle of least energy) to thermal statics at fixed temperature (governed by the principle of least <i>free</i> energy), the definition of force familiar in classical statics must be adjusted.  In classical statics we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F_i+%3D+-+%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+q%5Ei%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F_i = - &#92;frac{&#92;partial U}{&#92;partial q^i}} " class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=U%3A+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U: Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is the energy as a function of some coordinates <img src="https://s0.wp.com/latex.php?latex=q%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i" class="latex" /> on the configuration space of our system, some manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />  But in thermal statics at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> our system will try to minimize, not the energy <img src="https://s0.wp.com/latex.php?latex=U%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U," class="latex" /> but the <a href="http://en.wikipedia.org/wiki/Helmholtz_free_energy">Helmholtz free energy</a></p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%3D+U+-+T+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A = U - T S" class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3A+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S : Q &#92;to &#92;mathbb{R}" class="latex" /> </p>
<p>is the entropy.  So now we should define force by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+F_i+%3D+-+%5Cfrac%7B%5Cpartial+A%7D%7B%5Cpartial+q%5Ei%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ F_i = - &#92;frac{&#92;partial A}{&#92;partial q^i}} " class="latex" /></p>
<p>and we see that force has an entropic part and an energetic part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++F_i+%3D+T+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q%5Ei%7D%7D+-++%5Cfrac%7B%5Cpartial+U%7D%7B%5Cpartial+q%5Ei%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  F_i = T &#92;frac{&#92;partial S}{&#92;partial q^i}} -  &#92;frac{&#92;partial U}{&#92;partial q^i} " class="latex" /></p>
<p>When <img src="https://s0.wp.com/latex.php?latex=T+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = 0," class="latex" /> the entropic part goes away and we&#8217;re back to classical statics!</p>
<hr />
<p><i>I&#8217;m subject to the natural forces.</i> &#8211; <a href="http://www.youtube.com/watch?v=lbZn_Z5s-yA">Lyle Lovett</a></p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/02/01/entropic-forces/#comments">18 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/02/01/entropic-forces/" rel="bookmark" title="Permanent Link to Entropic Forces">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-7696 post type-post status-publish format-standard hentry category-information-and-entropy category-physics category-quantum-technologies" id="post-7696">
				<h2><a href="https://johncarlosbaez.wordpress.com/2012/01/29/a-quantum-hammersley-clifford-theorem/" rel="bookmark">A Quantum Hammersley&#8211;Clifford Theorem</a></h2>
				<small>29 January, 2012</small><br />


				<div class="entry">
					<p>I&#8217;m at this workshop:</p>
<p>&bull; <a href="http://www.physics.usyd.edu.au/quantum/Coogee2012/">Sydney Quantum Information Theory Workshop: Coogee 2012</a>, 30 January &#8211; 2 February 2012, Coogee Bay Hotel, Coogee, Sydney, organized by Stephen Bartlett, Gavin Brennen, Andrew Doherty and Tom Stace.</p>
<p>Right now David Poulin is speaking about a quantum version of the Hammersley&#8211;Clifford theorem, which is a theorem about Markov networks.   Let me quickly say a bit about what he proved!   This will be a bit rough, since I&#8217;m doing it live&#8230;</p>
<p>The <a href="http://en.wikipedia.org/wiki/Mutual_information"><b>mutual information</b></a> between two random variables is </p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%29+%3D+S%28A%29+%2B+S%28B%29+-+S%28A%2CB%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B) = S(A) + S(B) - S(A,B)" class="latex" /></p>
<p>The <a href="http://en.wikipedia.org/wiki/Conditional_mutual_information"><b>conditional mutual information</b></a> between three random variables <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+%5Csum_c+p%28C%3Dc%29+I%28A%3AB%7CC%3Dc%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = &#92;sum_c p(C=c) I(A:B|C=c)" class="latex" /></p>
<p>It&#8217;s the average amount of information about <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> learned by measuring <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> when you already knew <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /> </p>
<p>All this works for both classical (Shannon) and quantum (von Neumann) entropy. So, when we say &#8216;random variable&#8217; above, we<br />
could mean it in the traditional classical sense or in the quantum sense.</p>
<p>If <img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = 0" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=A%2C+C%2C+B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A, C, B" class="latex" /> has the following <b>Markov property</b>: if you know <img src="https://s0.wp.com/latex.php?latex=C%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C," class="latex" /> learning <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> tells you nothing new about <img src="https://s0.wp.com/latex.php?latex=B.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B." class="latex" />  In condensed matter physics, say a spin system, we get (quantum) random variables from measuring what&#8217;s going on in regions, and we have <b>short range entanglement</b> if <img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = 0" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> corresponds to some sufficiently thick region separating the regions <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=B.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B." class="latex" />   We&#8217;ll get this in any Gibbs state of a spin chain with a local Hamiltonian.</p>
<p>A <b>Markov network</b> is a graph with random variables at vertices (and thus subsets of vertices) such that  <img src="https://s0.wp.com/latex.php?latex=I%28A%3AB%7CC%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(A:B|C) = 0" class="latex" /> whenever <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is a subset of vertices that completely &#8216;shields&#8217; the subset <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> from the subset <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" />: any path from <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B" class="latex" /> goes through a vertex in a <img src="https://s0.wp.com/latex.php?latex=C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C." class="latex" /></p>
<p>The <a href="http://en.wikipedia.org/wiki/Hammersley%E2%80%93Clifford_theorem"><b>Hammersley&#8211;Clifford theorem</b></a> says that in the classical case we can get any Markov network from the Gibbs state </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-&#92;beta H)" class="latex" /></p>
<p>of a local Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H," class="latex" /> and vice versa.   Here a Hamiltonian is <b>local</b> if it is a sum of terms, one depending on the degrees of freedom in each <a href="http://en.wikipedia.org/wiki/Clique_%28graph_theory%29">clique</a> in the graph:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Csum_%7BC+%5Cin+%5Cmathrm%7Bcliques%7D%7D+h_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H = &#92;sum_{C &#92;in &#92;mathrm{cliques}} h_C" class="latex" /></p>
<p>Hayden, Jozsa, Petz and Winter gave a quantum generalization of one direction of this result to graphs that are just &#8216;chains&#8217;, like this:</p>
<p>o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o&#8212;o</p>
<p>Namely: for such graphs, any quantum Markov network is the Gibbs state of some local Hamiltonian.  Now Poulin has shown the same for <i>all</i> graphs.  But the converse is, in general, <i>false</i>.  If the different terms <img src="https://s0.wp.com/latex.php?latex=h_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="h_C" class="latex" /> in a local Hamiltonian all commute, its Gibbs state will have the Markov property.  But otherwise, it may not.  </p>
<p>For some related material, see:</p>
<p>&bull; David Poulin, <a href="http://cnls.lanl.gov/CQIT/poulin.pdf">Quantum graphical models and belief propagation</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/01/29/a-quantum-hammersley-clifford-theorem/#comments">1 Comment</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/" rel="category tag">quantum technologies</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2012/01/29/a-quantum-hammersley-clifford-theorem/" rel="bookmark" title="Permanent Link to A Quantum Hammersley&#8211;Clifford Theorem">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-6940 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-6940">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/" rel="bookmark">Quantropy (Part 1)</a></h2>
				<small>22 December, 2011</small><br />


				<div class="entry">
					<p>I wish you all happy holidays!  My wife Lisa and I are going to Bangkok on Christmas Eve, and thence to Luang Prabang, a town in Laos where the Nam Khan river joins the Mekong.  We&#8217;ll return to Singapore on the 30th.  See you then!  And in the meantime, here&#8217;s a little present&#8212;something to mull over.</p>
<div align="center"><img height="200" src="https://i1.wp.com/math.ucr.edu/home/baez/ludwig_boltzmann_and_richard_feynman.jpg" /></div>
<h4>Statistical mechanics versus quantum mechanics</h4>
<p>There&#8217;s a famous analogy between statistical mechanics and quantum mechanics.   In statistical mechanics, a system can be in any state, but its probability of being in a state with energy <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E" class="latex" /> is proportional to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-E%2FT%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-E/T) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the temperature in units where Boltzmann&#8217;s constant is 1.  In quantum mechanics, a system can move along any path, but its amplitude for moving along a path with action <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is proportional to</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28i+S%2F%5Chbar%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(i S/&#92;hbar)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is Planck&#8217;s constant.  So, we have an analogy where Planck&#8217;s constant is like an imaginary temperature:</p>
<div align="center">
<table border="1">
<tr>
<td><b>Statistical Mechanics</b></td>
<td><b>Quantum Mechanics</b></td>
</tr>
<tr>
<td>probabilities</td>
<td>amplitudes</td>
</tr>
<tr>
<td>energy</td>
<td>action</td>
</tr>
<tr>
<td>temperature</td>
<td>Planck&#8217;s constant times <i>i</i></td>
</tr>
</table>
</div>
<p>In other words, making the replacements</p>
<p><img src="https://s0.wp.com/latex.php?latex=E+%5Cmapsto+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E &#92;mapsto S" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=T+%5Cmapsto+i+%5Chbar+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T &#92;mapsto i &#92;hbar " class="latex" /></p>
<p>formally turns the probabilities for states in statistical mechanics into the amplitudes for paths, or &#8216;histories&#8217;, in quantum mechanics.</p>
<p>But the probabilities <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-E%2FT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(-E/T)" class="latex" /> arise naturally from maximizing entropy subject to a constraint on the expected energy.  So what about the amplitudes <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28i+S%2F%5Chbar%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;exp(i S/&#92;hbar)" class="latex" />?</p>
<p>Following the analogy without thinking too hard, we&#8217;d guess it arises from minimizing something subject to a constraint on the expected action.</p>
<p>But now we&#8217;re dealing with complex numbers, so &#8216;minimizing&#8217; doesn&#8217;t sound right.  It&#8217;s better talk about finding a &#8216;stationary point&#8217;: a place where the derivative of something is zero.</p>
<p>More importantly, what is this something?  We&#8217;ll have to see&#8212;indeed, we&#8217;ll have to see if this whole idea makes sense!  But for now, let&#8217;s just call it &#8216;quantropy&#8217;.  This is a goofy word whose only virtue is that it quickly gets the idea across: just as the main ideas in statistical mechanics follow from the idea of maximizing entropy, we&#8217;d like the main ideas in quantum mechanics to follow from maximizing&#8230; err, well, finding a stationary point&#8230; of &#8216;quantropy&#8217;.</p>
<p>I don&#8217;t know how well this idea works, but there&#8217;s no way to know except by trying, so I&#8217;ll try it here.  I got this idea thanks to a nudge from Uwe Stroinski and WebHubTel, who started talking about <a href="https://johncarlosbaez.wordpress.com/2011/12/05/probabilities-versus-amplitudes/#comment-11558">the principle of least action and the principle of maximum entropy</a> at a moment when I was thinking hard about <a href="http://math.ucr.edu/home/baez/prob/">probabilities versus amplitudes</a>.</p>
<p>Of course, if this idea makes sense, someone probably had it already.  If you know where, please tell me.</p>
<p>Here&#8217;s the story&#8230;</p>
<h4>Statics </h4>
<p>Static systems at temperature zero obey the <a href="http://en.wikipedia.org/wiki/Principle_of_minimum_energy"><b>principle of minimum energy</b></a>.  <a href="http://en.wikipedia.org/wiki/Energy"><b>Energy</b></a> is typically the sum of kinetic and potential energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=E+%3D+K+%2B+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E = K + V" class="latex" /></p>
<p>where the <a href="http://en.wikipedia.org/wiki/Potential_energy"><b>potential energy</b></a> <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V" class="latex" /> depends only on the system&#8217;s position, while the <a href="http://en.wikipedia.org/wiki/Kinetic_energy"><b>kinetic energy</b></a> <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> also depends on its velocity.  The kinetic energy is often (but not always) a quadratic function of velocity with a minimum at velocity zero.  In classical physics this lets our system minimize energy in a two-step way.  First it will minimize kinetic energy, <img src="https://s0.wp.com/latex.php?latex=K%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K," class="latex" />  by staying still.  Then it will go on to minimize potential energy, <img src="https://s0.wp.com/latex.php?latex=V%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V," class="latex" /> by choosing the right place to stay still.</p>
<p>This is actually somewhat surprising: usually minimizing the sum of two things involves an interesting tradeoff.  But sometimes it doesn&#8217;t!</p>
<p>In quantum physics, a tradeoff <i>is</i> required, thanks to the uncertainty principle.  We can&#8217;t know the position and velocity of a particle simultaneously, so we can&#8217;t simultaneously minimize potential and kinetic energy.  This makes minimizing their sum much more interesting, as you&#8217;ll know if you&#8217;ve ever worked out the lowest-energy state of a harmonic oscillator or hydrogen atom.</p>
<p>But in classical physics, minimizing energy often forces us into &#8216;statics&#8217;: the boring part of physics, the part that studies things that don&#8217;t move.  And people usually say statics at temperature zero is governed by the <a href="http://en.wikipedia.org/wiki/Minimum_total_potential_energy_principle" rel="nofollow"><b>principle of minimum potential energy</b></a>.</p>
<p>Next let&#8217;s turn up the heat.  What about static systems at nonzero temperature?  This is what people study in the subject called &#8216;thermostatics&#8217;, or more often, &#8216;equilibrium thermodynamics&#8217;.</p>
<p>In classical or quantum thermostatics at any fixed temperature, a closed system will obey the <b>principle of minimum free energy</b>.   Now it will minimize</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+E+-+T+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = E - T S" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is the temperature and <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is the entropy.  Note that this principle reduces to the principle of minimum energy when <img src="https://s0.wp.com/latex.php?latex=T+%3D+0.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T = 0." class="latex" />  But as <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> gets bigger, the second term in the above formula becomes more important, so the system gets more interested in having lots of entropy.  That&#8217;s why water forms orderly ice crystals at low temperatures (more or less minimizing energy despite low entropy) and a wild random gas at high temperatures (more or less maximizing entropy despite high energy).</p>
<p>But where does the principle of minimum free energy come from?</p>
<p>One nice way to understand it uses probability theory.  Suppose for simplicity that our system has a finite set of states, say <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> and the energy of the state <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=E_x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E_x." class="latex" />   Instead of our system occupying a single definite state, let&#8217;s suppose it can be in <i>any</i> state, with a probability <img src="https://s0.wp.com/latex.php?latex=p_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_x" class="latex" /> of being in the state <img src="https://s0.wp.com/latex.php?latex=x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x." class="latex" />  Then its <b><a href="http://en.wikipedia.org/wiki/Entropy_in_thermodynamics_and_information_theory">entropy</a></b> is, by definition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+S+%3D+-+%5Csum_x+p_x+%5Cln%28p_x%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ S = - &#92;sum_x p_x &#92;ln(p_x) }" class="latex" /></p>
<p>The expected value of the energy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+E+%3D+%5Csum_x+p_x+E_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ E = &#92;sum_x p_x E_x } " class="latex" /></p>
<p>Now suppose our system maximizes entropy subject to a constraint on the expected value of energy.   Thanks to the Lagrange multiplier trick, this is the same as maximizing</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+-+%5Cbeta+E+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S - &#92;beta E " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is a Lagrange multiplier.  When we go ahead and maximize this, we see the system chooses a <a href="http://en.wikipedia.org/wiki/Boltzmann_distribution"><b>Boltzmann distribution</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Cfrac%7B%5Cexp%28-%5Cbeta+E_x%29%7D%7B%5Csum_x+%5Cexp%28-%5Cbeta+E_x%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;frac{&#92;exp(-&#92;beta E_x)}{&#92;sum_x &#92;exp(-&#92;beta E_x)}} " class="latex" /></p>
<p>This is just a calculation; you must do it for yourself someday, and I will not rob you of that joy.</p>
<p>But what does this mean?   We could call <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> the <a href="http://en.wikipedia.org/wiki/Thermodynamic_beta"><b>coolness</b></a>, since its inverse is the <b><a href="http://en.wikipedia.org/wiki/Temperature">temperature</a></b>, <img src="https://s0.wp.com/latex.php?latex=T%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T," class="latex" /> at least in units where Boltzmann&#8217;s constant is set to 1.   So, when the temperature is positive, maximizing <img src="https://s0.wp.com/latex.php?latex=S+-+%5Cbeta+E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S - &#92;beta E" class="latex" /> is the same as minimizing the <a href="http://en.wikipedia.org/wiki/Helmholtz_free_energy"><b>free energy</b></a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F+%3D+E+-+T+S+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F = E - T S " class="latex" /></p>
<p>(For negative temperatures, maximizing <img src="https://s0.wp.com/latex.php?latex=S+-+%5Cbeta+E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S - &#92;beta E" class="latex" /> would amount to <i>maximizing</i> free energy.)</p>
<p>So, every minimum or maximum principle described so far can be seen as a special case or limiting case of the <a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy"><b>principle of maximum entropy</b></a>, as long as we admit that sometimes we need to maximize entropy <i>subject to constraints</i>.</p>
<p>Why &#8216;limiting case&#8217;?  Because the principle of least energy only shows up as the low-temperature limit, or <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to &#92;infty" class="latex" /> limit, of the idea of maximizing entropy subject to a constraint on expected energy.  But that&#8217;s good enough for me.</p>
<h4>Dynamics </h4>
<p>Now suppose things are changing as time passes, so we&#8217;re doing &#8216;dynamics&#8217; instead of mere &#8216;statics&#8217;.  In classical mechanics we can imagine a system tracing out a path <img src="https://s0.wp.com/latex.php?latex=%5Cgamma%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma(t)" class="latex" /> as time passes from one time to another, for example from <img src="https://s0.wp.com/latex.php?latex=t+%3D+t_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = t_0" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=t+%3D+t_1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t = t_1." class="latex" />   The <a href="http://en.wikipedia.org/wiki/Action_%28physics%290"><b>action</b></a> of this path is typically the integral of the kinetic minus potential energy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A%28%5Cgamma%29+%3D+%5Cdisplaystyle%7B+%5Cint_%7Bt_0%7D%5E%7Bt_1%7D++%28K%28t%29+-+V%28t%29%29+%5C%2C+dt+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A(&#92;gamma) = &#92;displaystyle{ &#92;int_{t_0}^{t_1}  (K(t) - V(t)) &#92;, dt }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=K%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K(t)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=V%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V(t)" class="latex" /> depend on the path <img src="https://s0.wp.com/latex.php?latex=%5Cgamma.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma." class="latex" />  Note that now I&#8217;m calling action <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> instead of the more usual <img src="https://s0.wp.com/latex.php?latex=S%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S," class="latex" /> since we&#8217;re already using <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> for entropy and I don&#8217;t want things to get any more confusing than necessary.</p>
<p>The <a href="http://en.wikipedia.org/wiki/Principle_of_least_action"><b>principle of least action</b></a> says that if we fix the endpoints of this path, that is the points <img src="https://s0.wp.com/latex.php?latex=%5Cgamma%28t_0%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma(t_0)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cgamma%28t_1%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma(t_1)," class="latex" /> the system will follow the path that minimizes the action subject to these constraints.</p>
<p>Why is there a minus sign in the definition of action?  How did people come up with principle of least action?   How is it related to the principle of least energy in statics?   These are all fascinating questions.  But I have a half-written book that tackles these questions, so I won&#8217;t delve into them here:</p>
<p> John Baez and Derek Wise, <i><a href="http://math.ucr.edu/home/baez/classical/texfiles/2005/book/classical.pdf">Lectures on Classical Mechanics</a></i>.</p>
<p>Instead, let&#8217;s go straight to dynamics in quantum mechanics.  Here Feynman proposed that instead of our following a single definite path, it can follow <i>any</i> path, with an amplitude <img src="https://s0.wp.com/latex.php?latex=a%28%5Cgamma%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a(&#92;gamma)" class="latex" /> of following the path <img src="https://s0.wp.com/latex.php?latex=%5Cgamma.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma." class="latex" />   And he proposed this prescription for the amplitude:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a%28%5Cgamma%29+%3D+%5Cfrac%7B%5Cexp%28i+A%28%5Cgamma%29%2F%5Chbar%29%7D%7B%5Cint++%5Cexp%28i+A%28%5Cgamma%29%2F%5Chbar%29+%5C%2C+d+%5Cgamma%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a(&#92;gamma) = &#92;frac{&#92;exp(i A(&#92;gamma)/&#92;hbar)}{&#92;int  &#92;exp(i A(&#92;gamma)/&#92;hbar) &#92;, d &#92;gamma}} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar" class="latex" /> is <a href="http://en.wikipedia.org/wiki/Planck_constant"><b>Planck&#8217;s constant</b></a>.  He also gave a heuristic argument showing that as <img src="https://s0.wp.com/latex.php?latex=%5Chbar+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar &#92;to 0" class="latex" />, this prescription reduces to the principle of least action!</p>
<p>Unfortunately the integral over all paths&#8212;called a &#8216;path integral&#8217;&#8212;is hard to make rigorous except in certain special cases.   And it&#8217;s a bit of a distraction for what I&#8217;m talking about now.  So let&#8217;s talk more abstractly about &#8216;histories&#8217; instead of paths with fixed endpoints, and consider a system whose possible &#8216;histories&#8217; form a finite set, say <img src="https://s0.wp.com/latex.php?latex=X.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X." class="latex" />  Systems of this sort frequently show up as discrete approximations to continuous ones, but they also show up in other contexts, like quantum cellular automata and topological quantum field theories.  Don&#8217;t worry if you don&#8217;t know what those things are.  I&#8217;d just prefer to write sums instead of integrals now, to make everything easier.</p>
<p>Suppose the action of the history <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=A_x.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_x." class="latex" />  Then Feynman&#8217;s <a href="http://en.wikipedia.org/wiki/Path_integral_formulation"><b>sum over histories formulation</b></a> of quantum mechanics says the amplitude of the history <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> is:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28i+A_x+%2F%5Chbar%29%7D%7B%5Csum_x++%5Cexp%28i+A_x+%2F%5Chbar%29+%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(i A_x /&#92;hbar)}{&#92;sum_x  &#92;exp(i A_x /&#92;hbar) }} " class="latex" /></p>
<p>This looks very much like the Boltzmann distribution:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Cfrac%7B%5Cexp%28-E_x%2FT%29%7D%7B%5Csum_x+%5Cexp%28-+E_x%2FT%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;frac{&#92;exp(-E_x/T)}{&#92;sum_x &#92;exp(- E_x/T)}} " class="latex" /></p>
<p>Indeed, the only serious difference is that we&#8217;re taking the exponential of an imaginary quantity instead of a real one.</p>
<p>So far everything has been a review of very standard stuff.  Now comes something weird and new&#8212;at least, new to me.</p>
<h4> Quantropy </h4>
<p>I&#8217;ve described statics and dynamics, and a famous analogy between them, but there are some missing items in the analogy, which would be good to fill in:</p>
<div align="center">
<table border="1">
<tr>
<td><b>Statics</b></td>
<td><b>Dynamics</b></td>
</tr>
<tr>
<td>statistical mechanics</td>
<td>quantum mechanics</td>
</tr>
<tr>
<td>probabilities</td>
<td>amplitudes</td>
</tr>
<tr>
<td>Boltzmann distribution</td>
<td>Feynman sum over histories</td>
</tr>
<tr>
<td>energy</td>
<td>action</td>
</tr>
<tr>
<td>temperature</td>
<td>Planck&#8217;s constant times <i>i</i></td>
</tr>
<tr>
<td>entropy</td>
<td>???</td>
</tr>
<tr>
<td>free energy</td>
<td>???</td>
</tr>
</table>
</div>
<p>Since the Boltzmann distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_x+%3D+%5Cfrac%7B%5Cexp%28-E_x%2FT%29%7D%7B%5Csum_x+%5Cexp%28-+E_x%2FT%29%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_x = &#92;frac{&#92;exp(-E_x/T)}{&#92;sum_x &#92;exp(- E_x/T)}} " class="latex" /></p>
<p>comes from the principle of maximum entropy, you might hope Feynman&#8217;s sum over histories formulation of quantum mechanics:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28i+A_x+%2F%5Chbar%29%7D%7B%5Csum_x++%5Cexp%28i+A_x+%2F%5Chbar%29+%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(i A_x /&#92;hbar)}{&#92;sum_x  &#92;exp(i A_x /&#92;hbar) }} " class="latex" /></p>
<p>comes from a maximum principle too!</p>
<p>Unfortunately Feynman&#8217;s sum over histories involves complex numbers, and it doesn&#8217;t make sense to maximize a complex function.   However, when we say nature likes to minimize or maximize something, it often behaves like a bad freshman who applies the first derivative test and quits there: it just finds a <a href="http://en.wikipedia.org/wiki/Stationary_point"><b>stationary point</b></a>, where the first derivative is zero.  For example, in statics we have &#8216;stable&#8217; equilibria, which are local minima of the energy, but also &#8216;unstable&#8217; equilibria, which are still stationary points of the energy, but not local minima.  This is good for us, because stationary points still make sense for complex functions.</p>
<p>So let&#8217;s try to derive Feynman&#8217;s prescription from some sort of &#8216;principle of stationary quantropy&#8217;.</p>
<p>Suppose we have a finite set of <b>histories</b>, <img src="https://s0.wp.com/latex.php?latex=X%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X," class="latex" /> and each history <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in X" class="latex" /> has a complex <b>amplitude</b> <img src="https://s0.wp.com/latex.php?latex=a_x++%5Cin+%5Cmathbb%7BC%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x  &#92;in &#92;mathbb{C}." class="latex" />  We&#8217;ll assume these amplitudes are normalized so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_x+a_x+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_x a_x = 1 " class="latex" /></p>
<p>since that&#8217;s what Feynman&#8217;s normalization actually achieves.  We can try to define the <b>quantropy</b> of <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a" class="latex" /> by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+-+%5Csum_x+a_x+%5Cln%28a_x%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = - &#92;sum_x a_x &#92;ln(a_x) }" class="latex" /></p>
<p>You might fear this is ill-defined when <img src="https://s0.wp.com/latex.php?latex=a_x+%3D+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x = 0," class="latex" /> but that&#8217;s not the worst problem; in the study of entropy we typically set</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%5Cln+0+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;ln 0 = 0" class="latex" /></p>
<p>and everything works fine.   The worst problem is that the logarithm has different branches: we can add any multiple of <img src="https://s0.wp.com/latex.php?latex=2+%5Cpi+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2 &#92;pi i" class="latex" /> to our logarithm and get another equally good logarithm.  For now suppose we&#8217;ve chosen a specific logarithm for each number <img src="https://s0.wp.com/latex.php?latex=a_x%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x," class="latex" /> and suppose that when we vary them they don&#8217;t go through zero, so we can smoothly change the logarithm as we move them.  This should let us march ahead for now, but clearly it&#8217;s a disturbing issue which we should revisit someday.</p>
<p>Next, suppose each history <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> has an <b>action</b> <img src="https://s0.wp.com/latex.php?latex=A_x+%5Cin+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_x &#92;in &#92;mathbb{R}." class="latex" />  Let&#8217;s seek amplitudes <img src="https://s0.wp.com/latex.php?latex=a_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x" class="latex" /> that give a stationary point of the quantropy <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> subject to a constraint on the <b>expected action</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+A+%3D+%5Csum_x+a_x+A_x+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ A = &#92;sum_x a_x A_x } " class="latex" /></p>
<p>The term &#8216;expected action&#8217; is a bit odd, since the numbers <img src="https://s0.wp.com/latex.php?latex=a_x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="a_x" class="latex" /> are amplitudes rather than probabilities.   While I could try to justify it from how expected values are computed in Feynman&#8217;s formalism, I&#8217;m mainly using this term because <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is analogous to the expected value of the energy, which we saw earlier.  We can worry later what all this stuff really means; right now I&#8217;m just trying to push forwards with an analogy and do a calculation.</p>
<p>So, let&#8217;s look for a stationary point of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> subject to a constraint on <img src="https://s0.wp.com/latex.php?latex=A.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A." class="latex" />  To do this, I&#8217;d be inclined to use <a href="http://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a> and look for a stationary point of</p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A" class="latex" /></p>
<p>But there&#8217;s another constraint, too, namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_x+a_x+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_x a_x = 1 " class="latex" /></p>
<p>So let&#8217;s write</p>
<p><img src="https://s0.wp.com/latex.php?latex=B+%3D+%5Csum_x+a_x+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="B = &#92;sum_x a_x " class="latex" /></p>
<p>and look for stationary points of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> subject to the constraints</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%3D+%5Calpha+%2C+%5Cqquad+B+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A = &#92;alpha , &#92;qquad B = 1" class="latex" /></p>
<p>To do this, the Lagrange multiplier recipe says we should find stationary points of</p>
<p><img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A+-+%5Cmu+B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A - &#92;mu B" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> are Lagrange multipliers.  The Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> is really interesting.  It&#8217;s analogous to &#8216;coolness&#8217;, <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1%2FT%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 1/T," class="latex" /> so our analogy chart suggests that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1%2Fi%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = 1/i&#92;hbar" class="latex" /></p>
<p>This says that when <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> gets big our system becomes close to classical.  So, we could call <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> the <b>classicality</b> of our system.   The Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu" class="latex" /> is less interesting&#8212;or at least I haven&#8217;t thought about it much.</p>
<p>So, we&#8217;ll follow the usual Lagrange multiplier recipe and look for amplitudes for which</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%3D+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+%5Cleft%28Q+-+%5Clambda+A+-+%5Cmu+B+%5Cright%29+%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 = &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} &#92;left(Q - &#92;lambda A - &#92;mu B &#92;right) }  " class="latex" /></p>
<p>holds, along with the constraint equations.  We begin by computing the derivatives we need:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bcclcl%7D+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+Q++%7D++%26%3D%26+-+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+%5C%3B+a_x+%5Cln%28a_x%29%7D++%26%3D%26+-+%5Cln%28a_x%29+-+1+%5C%5C++++%5C%5C++++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D%5C%3B+A++%7D++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+a_x+A_x%7D+%26%3D%26+A_x+%5C%5C++++%5C%5C+++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+B++%7D++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D%5C%3B+a_x+%7D+%26%3D%26+1+%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{cclcl} &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} Q  }  &amp;=&amp; - &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} &#92;; a_x &#92;ln(a_x)}  &amp;=&amp; - &#92;ln(a_x) - 1 &#92;&#92;    &#92;&#92;    &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x}&#92;; A  }  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} a_x A_x} &amp;=&amp; A_x &#92;&#92;    &#92;&#92;   &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} B  }  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x}&#92;; a_x } &amp;=&amp; 1 &#92;end{array} " class="latex" /></p>
<p>Thus, we need</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%3D+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+a_x%7D+%5Cleft%28Q+-+%5Clambda+A+-+%5Cmu+B+%5Cright%29+%3D+-%5Cln%28a_x%29+-+1-+%5Clambda+A_x+-+%5Cmu+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 = &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial a_x} &#92;left(Q - &#92;lambda A - &#92;mu B &#92;right) = -&#92;ln(a_x) - 1- &#92;lambda A_x - &#92;mu } " class="latex" /></p>
<p>or</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28-%5Clambda+A_x%29%7D%7B%5Cexp%28%5Cmu+%2B+1%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(-&#92;lambda A_x)}{&#92;exp(&#92;mu + 1)} } " class="latex" /></p>
<p>The constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_x+a_x+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_x a_x = 1" class="latex" /></p>
<p>then forces us to choose:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cexp%28%5Cmu+%2B+1%29+%3D+%5Csum_x+%5Cexp%28-%5Clambda+A_x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;exp(&#92;mu + 1) = &#92;sum_x &#92;exp(-&#92;lambda A_x) } " class="latex" /></p>
<p>so we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28-%5Clambda+A_x%29%7D%7B%5Csum_x+%5Cexp%28-%5Clambda+A_x%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(-&#92;lambda A_x)}{&#92;sum_x &#92;exp(-&#92;lambda A_x)} } " class="latex" /></p>
<p>Hurrah!  This is precisely Feynman&#8217;s sum over histories formulation of quantum mechanics if</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1%2Fi%5Chbar&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda = 1/i&#92;hbar" class="latex" /></p>
<p>We could go further with the calculation, but this is the punchline, so I&#8217;ll stop here.    I&#8217;ll just note that the final answer:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+a_x+%3D+%5Cfrac%7B%5Cexp%28iA_x%2F%5Chbar%29%7D%7B%5Csum_x+%5Cexp%28iA_x%2F%5Chbar%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ a_x = &#92;frac{&#92;exp(iA_x/&#92;hbar)}{&#92;sum_x &#92;exp(iA_x/&#92;hbar)} } " class="latex" /></p>
<p>does two equivalent things in one blow:</p>
<p> It gives a stationary point of quantropy subject to the constraints that the amplitudes sum to 1 and the expected action takes some fixed value.</p>
<p> It gives a stationary point of the <b>free action</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+-+i+%5Chbar+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A - i &#92;hbar Q" class="latex" /></p>
<p>subject to the constraint that the amplitudes sum to 1.</p>
<p>In case the second point is puzzling, note that the &#8216;free action&#8217; is the quantum analogue of &#8216;free energy&#8217;, <img src="https://s0.wp.com/latex.php?latex=E+-+T+S.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="E - T S." class="latex" />  It&#8217;s also just <img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A" class="latex" /> times <img src="https://s0.wp.com/latex.php?latex=-i+%5Chbar%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-i &#92;hbar," class="latex" /> and we already saw that finding stationary points of <img src="https://s0.wp.com/latex.php?latex=Q+-+%5Clambda+A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q - &#92;lambda A" class="latex" /> is another way of finding stationary points of quantropy with a constraint on the expected action.</p>
<p>Note also that when <img src="https://s0.wp.com/latex.php?latex=%5Chbar+%5Cto+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;hbar &#92;to 0" class="latex" />, free action reduces to action, so we recover the principle of least action&#8212;or at least <i>stationary</i> action&#8212;in classical mechanics.</p>
<blockquote><p>
<b>Summary.</b>  We recover Feynman&#8217;s sum over histories formulation of quantum mechanics from assuming that all histories have complex amplitudes, that these amplitudes sum to one, and that the amplitudes give a stationary point of quantropy subject to a constraint on the expected action.  Alternatively, we can assume the amplitudes sum to one and that they give a stationary point of free action.
</p></blockquote>
<p>That&#8217;s sort of nice!  So, here&#8217;s our analogy chart, all filled in:</p>
<div align="center">
<table border="1">
<tr>
<td><b>Statics</b></td>
<td><b>Dynamics</b></td>
</tr>
<tr>
<td>statistical mechanics</td>
<td>quantum mechanics</td>
</tr>
<tr>
<td>probabilities</td>
<td>amplitudes</td>
</tr>
<tr>
<td>Boltzmann distribution</td>
<td>Feynman sum over histories</td>
</tr>
<tr>
<td>energy</td>
<td>action</td>
</tr>
<tr>
<td>temperature</td>
<td>Planck&#8217;s constant times <i>i</i></td>
</tr>
<tr>
<td>entropy</td>
<td>quantropy</td>
</tr>
<tr>
<td>free energy</td>
<td>free action</td>
</tr>
</table>
</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/#comments">133 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/12/22/quantropy/" rel="bookmark" title="Permanent Link to Quantropy (Part 1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-5991 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics" id="post-5991">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/10/28/the-complexity-barrier/" rel="bookmark">The Complexity Barrier</a></h2>
				<small>28 October, 2011</small><br />


				<div class="entry">
					<p>Could we grow the whole universe with all its seeming complexity starting from a little seed?  How much can you do with just a little information?</p>
<p>People have contests about this.  My programmer friend Bruce Smith <a href="https://johncarlosbaez.wordpress.com/2011/10/06/chaitins-theorem-and-the-surprise-examination-paradox/#comment-8876">points out</a> this animation, produced using a program less than 4 kilobytes long:</p>
<p><span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="450" height="254" src="https://www.youtube.com/embed/FWmv1ykGzis?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span> </p>
<p>As he notes:</p>
<blockquote><p>
&#8230;to be fair, the complexity of some of the OS, graphics drivers, and hardware should be included, but this is a lot less than you might think if you imagine rewriting it purely for compactness rather than for speed, and only including what this sort of program needs to produce output.
</p></blockquote>
<p><a href="http://decoy.iki.fi/">Sampo Syreeni</a> pointed me to this video, all generated from under 64 kilobytes of x86 code:</p>
<p><span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="450" height="254" src="https://www.youtube.com/embed/Y3n3c_8Nn2Y?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p>
<p>As he points out, one trick is to use <i>symmetry</i>.</p>
<p>These fancy images produced from tiny amounts of information are examples of the &#8216;demoscene&#8217;. a computer art subculture where people produce <a href="http://en.wikipedia.org/wiki/Demo_%28computer_programming%29">demos</a>: non-interactive audio-visual computer presentations that run in real time.  </p>
<p>According to the <a href="http://en.wikipedia.org/wiki/Demoscene">Wikipedia article</a> on the demoscene:</p>
<blockquote><p>
Recent computer hardware advancements include faster processors, more memory, faster video graphics processors, and hardware 3D acceleration. With many of the past&#8217;s challenges removed, the focus in making demos has moved from squeezing as much out of the computer as possible to making stylish, beautiful, well-designed real time artwork [&#8230;]</p>
<p>The old tradition still lives on, though. <a href="http://en.wikipedia.org/wiki/Demoparty">Demo parties</a> have competitions with varying limitations in program size or platform (different series are called <a href="http://en.wikipedia.org/wiki/Compo_%28demoscene%29">compos</a>). On a modern computer the executable size may be limited to 64 kB or 4 kB. Programs of limited size are usually called <a href="http://en.wikipedia.org/wiki/Demo_%28computer_programming%29#Intros">intros</a>. In other compos the choice of platform is restricted; only old computers, like the 8-bit Commodore 64, or the 16-bit Amiga, or Atari ST, or mobile devices like handheld phones or PDAs are allowed. Such restrictions provide a challenge for coders, musicians and graphics artists and bring back the old motive of making a device do more than was intended in its original design.
</p></blockquote>
<p>What else can you do with just a little information?  Bruce listed a couple more things:</p>
<blockquote>
<p>&bull; Bill Gates first commercial success was an implementation of a useful version of BASIC in about 4000 bytes;</p>
<p>&bull; the complete genetic code of an organism can be as short as a few hundred thousand bytes, and that has to be encoded in a way that doesnt allow for highly clever compression schemes.</p>
</blockquote>
<p>So if quite complex things can be compressed into fairly little information, you can&#8217;t help but wonder: how complex can something be?  </p>
<p>The answer: arbitrarily complex!  At least that&#8217;s true if we&#8217;re talking about the <b>Kolmogorov complexity</b> of a string of bits: namely, the length of the shortest computer program that prints it out.   Lots of long strings of bits can&#8217;t be compressed.   You can&#8217;t print most of them out using short programs, since there aren&#8217;t enough short programs to go around.</p>
<p>Of course, we need to fix a computer language ahead of time, so this is well-defined.  And we need to make sure the programs are written in binary, so the comparison is fair.  </p>
<p>So, things can be arbitrarily complex.  But here&#8217;s a more interesting question: how complex can we <i>prove</i> something to be?  </p>
<p>The answer is one of the most astounding facts I know.  It&#8217;s called <a href="http://en.wikipedia.org/wiki/Chaitin%27s_incompleteness_theorem#Chaitin.27s_incompleteness_theorem">Chaitin&#8217;s incompleteness theorem</a>.  It says, very roughly: </p>
<blockquote><p>
<b>There&#8217;s a number <i>L</i> such that we can&#8217;t prove the Kolmogorov complexity of any specific string of bits is bigger than <i>L</i>.</b>
</p></blockquote>
<p>Make sure you understand this.  For any number,<br />
we can prove there are infinitely many bit strings with  Kolmogorov complexity bigger than that.  But we can&#8217;t point to any <i>particular</i> bit string and prove its Kolmogorov complexity is bigger than <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />! </p>
<p>Over on Google+, Allen Knutson wrote:</p>
<blockquote>
<p>That&#8217;s an incredibly disturbing theorem, like driving to the edge of the universe and finding a wall.
</p></blockquote>
<p>I call <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> the <b>complexity barrier</b>.  So one question is, how big is <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />?  It&#8217;s hard, or perhaps even impossible, to find the smallest <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> that does the job.  But we can certainly find numbers <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> that work.  And they&#8217;re surprisingly small!</p>
<p>My friend Bruce estimates that the complexity barrier is <i>a few kilobytes</i>.  </p>
<p>I&#8217;d like to see a program a few kilobytes long that produces a video showing a big bang, the formation of stars and galaxies, then planets, including one where life evolves, then intelligent life, then the development of computers&#8230; and finally someone writing the very same program.  </p>
<p>I can&#8217;t prove it&#8217;s possible&#8230; but <i>you can&#8217;t prove it isn&#8217;t!</i></p>
<p>Let&#8217;s see why.  </p>
<p>For starters, we need to choose some axioms for our system of math, so we know what &#8216;provable&#8217; means.  We need a system that&#8217;s powerful enough to prove a bunch of basic facts about arithmetic, but simple enough that a computer program can check if a proof in this system is valid.  </p>
<p>There are lots of systems like this.  Three famous ones are <a href="http://en.wikipedia.org/wiki/Peano_axioms#First-order_theory_of_arithmetic">Peano arithmetic</a>, <a href="http://en.wikipedia.org/wiki/Robinson_arithmetic">Robinson arithmetic</a> (which is less powerful) and <a href="">Zermelo-Fraenkel set theory</a> (which is more powerful).</p>
<p>When you have a system of math like this, <a href="http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#First_incompleteness_theorem">G&ouml;del&#8217;s first incompleteness theorem</a> kicks in: if the system is consistent,  it can&#8217;t be complete.  In other words, there are some questions it leaves unsettled.  This is why we shouldn&#8217;t be utterly shocked that while a bunch of bit strings have complexity more than <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />, we can&#8217;t <i>prove</i> this.</p>
<p><a href="http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#Second_incompleteness_theorem">G&ouml;del&#8217;s second incompleteness theorem</a> also kicks in: if the system can prove that it&#8217;s consistent, it&#8217;s not!  (If it&#8217;s not consistent, it can prove <i>anything</i>, so you shouldn&#8217;t trust it.)  So, there&#8217;s a sense in which we can never be completely sure that our system of math is consistent.  But let&#8217;s assume it is.</p>
<p>Given this, Chaitin&#8217;s theorem says:</p>
<blockquote><p> There exists a constant <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> such that no string of bits has Kolmogorov complexity that provably exceeds <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />.
</p></blockquote>
<p>How can we get a number that does the job?  Any number <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=U+%2B+%5Clog_2%28L%29+%2B+C+%3C+L+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U + &#92;log_2(L) + C &lt; L " class="latex" /></p>
<p>will do.  Here:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> is the length of a program where if you input a natural number <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />, it will search through all proofs in Peano arithmetic until it finds one that proves some bit string has Kolmogorov complexity <img src="https://s0.wp.com/latex.php?latex=%3E+i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&gt; i" class="latex" />.  If it finds one, then it outputs this bit string.  If it never finds one, it grinds on endlessly.  (Of course, if <img src="https://s0.wp.com/latex.php?latex=i+%3D+L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = L" class="latex" />, it will never find one!)</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" /> is a small overhead cost: the length of the extra &#039;glue&#039; to create a bigger program that takes the number <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />, written out in binary, and feeds it into the program described above.  </p>
<p>The length of <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> written out in binary is about <img src="https://s0.wp.com/latex.php?latex=%5Clog_2%28L%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;log_2(L)" class="latex" />.  This bigger program thus has length </p>
<p><img src="https://s0.wp.com/latex.php?latex=U+%2B+%5Clog_2%28L%29+%2B+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U + &#92;log_2(L) + C" class="latex" /> </p>
<p>and for the proof of Chaitin&#8217;s incompleteness theorem to work, we need this to be smaller than <img src="https://s0.wp.com/latex.php?latex=L.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L." class="latex" />  Obviously we can accomplish this by making <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> big enough, since <img src="https://s0.wp.com/latex.php?latex=%5Clog_2+L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;log_2 L" class="latex" /> grows slower than <img src="https://s0.wp.com/latex.php?latex=L.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L." class="latex" /></p>
<p>Given all the stuff I&#8217;ve told you, the proof of Chaitin&#8217;s theorem almost writes itself!  You run this bigger program I just described.  If there were a bit string whose Kolmogorov complexity is provably greater than <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />, this program would print one out.  But that&#8217;s a contradiction, because this program has length less than <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />.</p>
<p>So, we just need to pick a computer language and a suitable system of math, and estimate <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> and, less importantly because it&#8217;s so much smaller, <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="C" class="latex" />.   Then <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> will be just a bit bigger than <img src="https://s0.wp.com/latex.php?latex=U+%2B+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U + C" class="latex" />.</p>
<p>I picked the language C and Peano arithmetic and asked Bruce if he could guess, roughly, what answer we get for <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" />.  He replied:</p>
<blockquote><p>
I don&#8217;t think it can be done in C, since C semantics are not well-defined unless you specify a particular finite machine size. (Since C programs can do things like convert pointers to integers and back, tell you the size of any datatype, and convert data of any specified datatype to bytes and back.) On a finite machine of <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="N" class="latex" /> bits, all programs either finish in time less than about <img src="https://s0.wp.com/latex.php?latex=2%5EN&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2^N" class="latex" /> or take forever.</p>
<p>But if you take &#8220;C without size-specific operations&#8221;, or a higher level language like Python, or for that matter a different sort of low-level language like a Turing machine, then that&#8217;s not an issue &#8212; you can define a precise semantics that allows it to run a program for an arbitrarily long time and allocate an arbitrary number of objects in memory which contain pointers to each other. (To stick with the spirit of the question, for whatever language you choose, you&#8217;d want to disallow use of any large external batch of information like a &#8220;standard library&#8221;, except for whatever is so basic that you think of it as part of the native language. This is not a serious handicap for this problem.)</p>
<p>The main things that the program &#8216;<img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" />&#8216; (I&#8217;d rather call the program itself &#8216;U&#8217; than call its length &#8216;<img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" />&#8216;) needs to do are:</p>
<p>&bull; recognize a syntactically correct statement or proof;</p>
<p>&bull; check the validity of a purported proof;</p>
<p>&bull; recognize certain statements as saying or implying &#8220;The Kolmogorov complexity of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> is more than <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />&#8221; for some <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />. (It&#8217;s not necessary to recognize all such statements, just at least one for each <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />; so it can just recognize a statement that consists of some template with specific values of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> inserted into it at certain places.)</p>
<p>Assuming that <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> expresses the proofs it wants to check in a practical proof language (which will be more like what a practical theorem-prover like <a href="http://en.wikipedia.org/wiki/Coq">Coq</a> uses than like what a traditional logician would recognize as &#8220;straight Peano arithmetic&#8221;, but which will not be excessively powerful in the spirit of this question), I&#8217;d estimate that the most complex part is checking proof validity, but that that can still be expressed in at most a few dozen syntactic rules, each expressible in a few lines of code. (The authors of a system like Coq, which includes code to actually do that, would know better, as long as they remember that the vast majority of their system&#8217;s actual code is not needed for this problem.)</p>
<p>This makes me think that even without trying to compact it much, in a reasonable language we could write <img src="https://s0.wp.com/latex.php?latex=U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="U" class="latex" /> in a few hundred lines of code, or (after a bit of simple compression) a few thousand bytes. (And perhaps much less if we tried hard to compact the whole program in clever ways.)</p>
<p>So <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> will also be &#8220;a few thousand&#8221; (bytes or digits), or perhaps less, rather than some number you can never possibly count to.</p>
</blockquote>
<p>Does anyone know if Chaitin or someone actually went ahead a wrote a program that showed a specific value of <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L" class="latex" /> would work?</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/28/the-complexity-barrier/#comments">66 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/10/28/the-complexity-barrier/" rel="bookmark" title="Permanent Link to The Complexity Barrier">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3832 post type-post status-publish format-standard hentry category-information-and-entropy category-probability" id="post-3832">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/" rel="bookmark">A Characterization of&nbsp;Entropy</a></h2>
				<small>2 June, 2011</small><br />


				<div class="entry">
					<p><a href="http://golem.ph.utexas.edu/category/2011/05/an_operadic_introduction_to_en.html">Over at the <i>n</i>-Category Caf</a> some of us have been trying an experiment: writing a math paper in full public view, both on that blog and on its associated wiki: the <a href="http://nlab.mathforge.org/nlab/show/HomePage"><i>n</i>Lab</a>.  One great thing about doing things this way is that  people can easily chip in with helpful suggestions.  It&#8217;s also more fun!  Both these tend to speed the process.</p>
<p>Like Frankenstein&#8217;s monster, our paper&#8217;s main result was initially jolted into life by huge blasts of power: in this case, not lightning but category theory.  It was awesome to behold, but too scary for <i>this</i> blog.</p>
<div align="center">
<img width="440" src="https://bosquechica.files.wordpress.com/2009/02/frankenstein11.jpg?w=440" />
</div>
<p>First <a href="http://www.maths.gla.ac.uk/~tl/">Tom Leinster</a> realized that the concept of entropy fell out &#8212; unexpectedly, but very naturally &#8212; from considerations involving <a href="http://en.wikipedia.org/wiki/Operad_theory">&#8216;operads&#8217;</a>, which are collections of abstract operations.  He was looking at a particular operad where the operations are &#8216;convex linear combinations&#8217;, and he discovered that this operad has entropy lurking in its heart.  Then <a href="http://users.icfo.es/Tobias.Fritz/">Tobias Fritz</a> figured out a nice way to state Tom&#8217;s result without mentioning operads.  By now we&#8217;ve taught the monster table manners, found it shoes that fit, and it&#8217;s ready for polite society:</p>
<p> John Baez, Tobias Fritz and Tom Leinster, <a href="http://arxiv.org/abs/1106.1791">A characterization of entropy in terms of information loss</a>,  <a href="http://www.mdpi.com/1099-4300/13/11/1945"><i>Entropy</i></a> <b>13</b> (2011), 1945&ndash;1957.</p>
<p>The idea goes like this.   Say you&#8217;ve got a finite set <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with a <b><a href="http://en.wikipedia.org/wiki/Probability_measure">probability measure</a></b> <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> on it, meaning a number <img src="https://s0.wp.com/latex.php?latex=0+%5Cle+p_i+%5Cle+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le p_i &#92;le 1" class="latex" /> for each point <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" />, obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi+%5Cin+X%7D+p_i+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;sum_{i &#92;in X} p_i = 1" class="latex" /></p>
<p>Then the <b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a></b> of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%2C+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{i &#92;in X} p_i &#92;, &#92;ln(p_i) " class="latex" /></p>
<p>This funny-looking formula can be justified in many ways.  Our new way involves focusing not on entropy itself, but on <i>changes</i> in entropy.  This makes sense for lots of reasons.  For example, in physics we don&#8217;t usually measure entropy directly.  Instead, we measure changes in entropy, using the fact that a system at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> absorbing a tiny amount of heat <img src="https://s0.wp.com/latex.php?latex=%5CDelta+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta Q" class="latex" /> in a reversible way will experience an entropy change of <img src="https://s0.wp.com/latex.php?latex=%5CDelta+Q+%2F+T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta Q / T" class="latex" />.  But our real reason for focusing on changes in entropy is that it gives a really slick theorem.</p>
<p>Suppose we have two finite sets with probability measures, say <img src="https://s0.wp.com/latex.php?latex=%28X%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(X,p)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(Y,q)" class="latex" />.  Then we define a <b>morphism</b> <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" /> to be a measure-preserving function: in other words, one for which the probability <img src="https://s0.wp.com/latex.php?latex=q_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_j" class="latex" /> of any point in <img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Y" class="latex" /> is the sum of the probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> of the points in <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=f%28i%29+%3D+j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(i) = j" class="latex" />.</p>
<p>A morphism of this sort is a deterministic process that carries one random situation to another.  For example, if I have a random integer between -10 and 10, chosen according to some probability distribution, and I square it, I get a random integer between 0 and 100.   A process of this sort always <i>decreases</i> the entropy: given any morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28q%29+%5Cle+S%28p%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q) &#92;le S(p) " class="latex" /></p>
<p>Since the second law of thermodynamics says that entropy always <i>increases</i>, this may seem counterintuitive or even paradoxical!   But there&#8217;s no paradox here.  It makes more intuitive sense if you think of entropy as information, and the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> as some kind of data processing that doesn&#8217;t introduce any additional randomness. Such a process can only decrease the amount of information.  For example, squaring the number -5 gives the same answer as squaring 5, so if I tell you &#8220;this number squared is 25&#8221;, I&#8217;m giving you less information than if I said &#8220;this number is -5&#8221;.</p>
<p>For this reason, we call the difference <img src="https://s0.wp.com/latex.php?latex=S%28p%29+-+S%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) - S(q)" class="latex" /> the <b>information loss</b> of the morphism <img src="https://s0.wp.com/latex.php?latex=f+%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : (X,p) &#92;to (Y,q)" class="latex" />.  And here&#8217;s our characterization of Shannon entropy in terms of information loss:</p>
<p>First, let&#8217;s write a morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=f+%3A+p+%5Cto+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f : p &#92;to q" class="latex" /> for short.  Suppose <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> is a function that assigns to any such morphism a number <img src="https://s0.wp.com/latex.php?latex=F%28f%29+%5Cin+%5B0%2C%5Cinfty%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f) &#92;in [0,&#92;infty)," class="latex" /> which we think of as its information loss.  And suppose that <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> obeys three axioms:</p>
<ol>
<li>Functoriality.  Whenever we can compose morphisms <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />, we demand that </li>
</ol>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f+%5Ccirc+g%29+%3D+F%28f%29+%2B+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f &#92;circ g) = F(f) + F(g)" class="latex" /></p>
<p>In other words: <b>when we do a process consisting of two stages, the amount of information lost in the whole process is the sum of the amounts lost in each stage!</b></p>
<ol>
<li>Convex linearity.   Suppose we have two finite sets equipped with probability measures, say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, and a real number <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5B0%2C+1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda &#92;in [0, 1]" class="latex" />.  Then there is a probability measure <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p+%5Coplus+%281+-+%5Clambda%29+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p &#92;oplus (1 - &#92;lambda) q" class="latex" /> on the disjoint union of the two sets, obtained by weighting the two measures by <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=1+-+%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 - &#92;lambda" class="latex" />, respectively.  Similarly, given morphisms <img src="https://s0.wp.com/latex.php?latex=f%3A+p+%5Cto+p%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: p &#92;to p&#039;" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g%3A+q+%5Cto+q%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g: q &#92;to q&#039;" class="latex" /> there is an obvious morphism from <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p+%5Coplus+%281+-+%5Clambda%29+q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p &#92;oplus (1 - &#92;lambda) q" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Clambda+p%27+%5Coplus+%281+-+%5Clambda%29+q%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda p&#039; &#92;oplus (1 - &#92;lambda) q&#039;" class="latex" />.   Let&#8217;s call this morphism <img src="https://s0.wp.com/latex.php?latex=%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda f &#92;oplus (1 - &#92;lambda) g" class="latex" />.  We demand that </li>
</ol>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g%29+%3D+%5Clambda+F%28f%29+%2B+%281+-+%5Clambda%29+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(&#92;lambda f &#92;oplus (1 - &#92;lambda) g) = &#92;lambda F(f) + (1 - &#92;lambda) F(g)" class="latex" /></p>
<p>In other words: <b>if we flip a probability- coin to decide whether to do one process or another, the information lost is  times the information lost by the first process plus (1 &#8211; ) times the information lost by the second!</b></p>
<ol>
<li>Continuity.  The same function between finite sets can be thought of as a measure-preserving map in different ways, by changing the measures on these sets.   In this situation the quantity <img src="https://s0.wp.com/latex.php?latex=F%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f)" class="latex" /> should depend continuously on the measures in question.</li>
</ol>
<p>In other words: <b>if we slightly change what we do a process to, the information it loses changes only slightly</b>.</p>
<p>Then we conclude that there exists a constant <img src="https://s0.wp.com/latex.php?latex=c+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="c &#92;ge 0" class="latex" /> such that for any morphism <img src="https://s0.wp.com/latex.php?latex=f%3A+%28X%2Cp%29+%5Cto+%28Y%2Cq%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f: (X,p) &#92;to (Y,q)" class="latex" />, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28f%29+%3D+c%28S%28p%29+-+S%28q%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(f) = c(S(p) - S(q)) " class="latex" /></p>
<p>In other words: <b>the information loss is some multiple of the change in Shannon entropy!</b></p>
<p>What&#8217;s pleasing about this theorem is that the three axioms are pretty natural, and it&#8217;s hard to see the formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28p%29+%3D+-+%5Csum_%7Bi+%5Cin+X%7D+p_i+%5C%2C+%5Cln%28p_i%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(p) = - &#92;sum_{i &#92;in X} p_i &#92;, &#92;ln(p_i) " class="latex" /></p>
<p>hiding in them&#8230; but it&#8217;s actually there.</p>
<p>(We also prove a version of this theorem for <a href="http://en.wikipedia.org/wiki/Tsallis_entropy">Tsallis entropy</a>, in case you care.   This obeys a mutant version of axiom 2, namely:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Clambda+f+%5Coplus+%281+-+%5Clambda%29+g%29+%3D+%5Clambda%5E%5Calpha+F%28f%29+%2B+%281+-+%5Clambda%29%5E%5Calpha+F%28g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(&#92;lambda f &#92;oplus (1 - &#92;lambda) g) = &#92;lambda^&#92;alpha F(f) + (1 - &#92;lambda)^&#92;alpha F(g)" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> is a parameter with <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha%3C+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &lt; &#92;alpha&lt; &#92;infty" class="latex" />.  Tsallis entropy is a close relative of Rnyi entropy, which I discussed here <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">earlier</a>.  Just as  Rnyi entropy is a kind of <i>q</i>-derivative of the free energy, the Tsallis entropy is a <i>q</i>-derivative of the partition function.  I&#8217;m not sure either of them are really important, but when you&#8217;re trying to uniquely characterize Shannon entropy, it&#8217;s nice for it to have some competitors to fight against, and these are certainly the main two.  Both of them depend on a parameter and reduce to the Shannon entropy at a certain value of that parameter.)</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/#comments">32 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/06/02/a-characterization-of-entropy/" rel="bookmark" title="Permanent Link to A Characterization of&nbsp;Entropy">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3688 post type-post status-publish format-standard hentry category-biology category-information-and-entropy" id="post-3688">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/" rel="bookmark">Information Geometry (Part&nbsp;8)</a></h2>
				<small>26 May, 2011</small><br />


				<div class="entry">
					<p>Now this series on information geometry will take an unexpected turn toward &#8216;green mathematics&#8217;.  Lately I&#8217;ve been talking about relative entropy.  Now I&#8217;ll say how this concept shows up in the study of evolution!</p>
<p>That&#8217;s an unexpected turn to me, at least.  I learned of this connection just two days ago in a conversation with <a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a>, a mathematician who is a postdoc in bioinformatics at UCLA, working with my friend <a href="http://bioinfo.mbi.ucla.edu/leelab/">Chris Lee</a>.  I was visiting Chris for a couple of days after attending the thesis defenses of some grad students of mine who just finished up at U.C. Riverside.   Marc came by and told me about this paper:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>.</p>
<p>and now I can&#8217;t resist telling you.</p>
<p>First of all: what does information theory have to do with biology?  Let me start with a very general answer: biology is different from physics because biological systems are packed with information you can&#8217;t afford to ignore.  </p>
<p>Physicists love to think about systems that take only a little information to describe.  So when they get a system that takes a <i>lot</i> of information to describe, they use a trick called &#8216;statistical mechanics&#8217;, where you try to ignore most of this information and focus on a few especially important variables.  For example, if you hand a physicist a box of gas, they&#8217;ll try to avoid thinking about the state of each atom, and instead focus on a few macroscopic quantities like the volume and total energy. Ironically, the mathematical concept of information arose first here&mdash;although they didn&#8217;t call it information back then; they called it &#8216;entropy&#8217;.  The entropy of a box of gas is precisely the amount of information you&#8217;ve decided to forget when you play this trick of focusing on the macroscopic variables.  Amazingly, remembering just this&mdash;the sheer <i>amount</i> of information you&#8217;ve forgotten&mdash;can be extremely useful&#8230; at least for the systems physicists like best.</p>
<p>But biological systems are different.  They store lots of information (for example in DNA), transmit lots of information (for example in the form of biochemical signals), and collect a lot of information from their environment.  And this information isn&#8217;t uninteresting &#8216;noise&#8217;, like the positions of atoms in a gas.  The details really matter.   Thus, we need to keep track of lots of information to have a chance of understanding any particular biological system.   </p>
<p>So, part of doing biology is developing new ways to think about physical systems that contain lots of <i>relevant</i> information.  This is why physicists consider biology &#8216;messy&#8217;.  It&#8217;s also why biology and computers go hand in hand in the subject called &#8216;bioinformatics&#8217;.  There&#8217;s no avoiding this: in fact, it will probably force us to <i>automate the scientific method!</i>  That&#8217;s what Chris Lee and Marc Harper are really working on:</p>
<p>&bull; Chris Lee, <a href="http://vimeo.com/23235162">General information metrics for automated experiment planning</a>, presentation in the UCLA Chemistry &amp; Biochemistry Department faculty luncheon series, 2 May 2011.</p>
<p>But more about that some other day.  Let me instead give <i>another</i> answer to the question of what information theory has to do with biology.  </p>
<p>There&#8217;s an analogy between evolution and the scientific method.  Simply put, life is an experiment to see what works; natural selection weeds out the bad guesses, and over time the better guesses predominate.  This process transfers information from the world to the &#8216;experimenter&#8217;: the species that&#8217;s doing the evolving, or the scientist.   Indeed, the only way the experimenter can get information is by making guesses that can be wrong.  </p>
<p>All this is simple enough, but the nice thing is that we can make it more precise.</p>
<p>On the one hand, there&#8217;s a simple model of the scientific method called <a href="http://en.wikipedia.org/wiki/Bayesian_inference">&#8216;Bayesian inference&#8217;</a>.  Assume there&#8217;s a set of mutually exclusive alternatives: possible ways the world can be.   And suppose we start with a <a href="http://en.wikipedia.org/wiki/Prior_probability">&#8216;prior probability distribution&#8217;</a>: a preconceived notion of how probable each alternative is.   Say we do an experiment and get a result that depends on which alternative is true.  We can work out how likely this result was given our prior, and&mdash;using a marvelously simple formula called <a href="http://en.wikipedia.org/wiki/Bayes'_theorem">Bayes&#8217; rule</a>&mdash;we can use this to update our prior and obtain a new improved probability distribution, called the <a href="http://en.wikipedia.org/wiki/Posterior_probability_distribution">&#8216;posterior probability distribution&#8217;</a>.  </p>
<p>On the other hand, suppose we have a species with several different possible genotypes.  A population of this species will start with some number of organisms with each genotype.  So, we get a probability distribution saying how likely it is that an organism has any given genotype.  These genotypes are our &#8216;mutually exclusive alternatives&#8217;, and this probability distribution is our &#8216;prior&#8217;.  Suppose each generation the organisms have some expected number of offspring that depends on their genotype.   Mathematically, it turns out this is just like updating our prior using Bayes&#8217; rule!   The result is a new probability distribution of genotypes: the &#8216;posterior&#8217;.</p>
<p>I learned about this from Chris Lee on the 19th of December, 2006.  In my <a href="http://math.ucr.edu/home/baez/diary/december_2006.html#december19.06">diary</a> that day, I wrote:</p>
<blockquote><p>
The analogy is mathematically precise, and fascinating.  In rough terms, it says that <i>the process of natural selection resembles the process of Bayesian inference</i>.  A population of organisms can be thought of as having various &#8216;hypotheses&#8217; about how to survive&mdash;each hypothesis corresponding to a different <a href="http://en.wikipedia.org/wiki/Allele"> allele</a>.  (Roughly, an allele is one of several alternative versions of a gene.)  In each successive generation, the process of natural selection modifies the proportion of organisms having each hypothesis, according to Bayes&#8217; rule!</p>
<p>Now let&#8217;s be more precise:</p>
<p><a href="http://en.wikipedia.org/wiki/Bayes'_theorem#Alternative_forms_of_Bayes.27_theorem">Bayes&#8217; rule</a> says if we start with a &#8216;prior probability&#8217; for some hypothesis to be true, divide it by the probability that some observation is made, then multiply by the &#8216;conditional probability&#8217; that this observation will be made given that the hypothesis is true, we&#8217;ll get the &#8216;posterior probability&#8217; that the hypothesis is true <i>given that the observation is made</i>.</p>
<p>Formally, the exact same equation shows up in population genetics!  In fact, Chris showed it to me&mdash;it&#8217;s equation 9.2 on page 30 of this<br />
book:</p>
<p>&bull; R. B&uuml;rger, <em>The Mathematical Theory of Selection, Recombination and Mutation</em>, section I.9: Selection at a single locus, Wiley, 2000.  </p>
<p>But, now all the terms in the equation have different meanings!  </p>
<p>Now, instead of a &#8216;prior probability&#8217; for a hypothesis to be true, we have the frequency of occurrence of some <a href="http://en.wikipedia.org/wiki/Allele">allele</a> in some generation of a population.  Instead of the probability that we make some observation, we have the expected number of offspring of an organism. Instead of the &#8216;conditional probability&#8217; of making the observation, we have the expected number of offspring of an organism <i>given</i> that it has this allele. And, instead of the &#8216;posterior probability&#8217; of our hypothesis, we have the frequency of occurrence of that allele in the next generation.</p>
<p>(Here we are assuming, for simplicity, an asexually reproducing &#8216;haploid&#8217; population &#8211; that is, one with just a single set of chromosomes.)</p>
<p>This is a great idea&mdash;Chris felt sure someone must have already had it. A natural context would be research on <a href="http://en.wikipedia.org/wiki/Genetic_programming">genetic programming</a>, a machine learning technique that uses an evolutionary algorithm to optimize a population of computer programs according to a fitness landscape determined by their ability to perform a given task.  Since there has also been a lot of work on Bayesian approaches to machine learning, surely someone has noticed their mathematical relationship?
</p></blockquote>
<p>I see at least <a href="http://www.iq.harvard.edu/blog/sss/archives/2007/01/bayesian_infere.shtml">one person</a> found these ideas as new and exciting as I did.  But I still can&#8217;t believe Chris was the first to clearly formulate them, so I&#8217;d still like to know who did.</p>
<p>Marc Harper actually went to work with Chris after reading that diary entry of mine.  By now he&#8217;s gone a lot further with this analogy by focusing on the role of <i>information</i>.    As we keep updating our prior using Bayes&#8217; rule, we should be gaining information about the real world.  This idea has been made very precise in the theory of <a href="http://en.wikipedia.org/wiki/Machine_learning">&#8216;machine learning&#8217;</a>. Similarly, as a population evolves through natural selection, it should be gaining information about its environment.  </p>
<p>I&#8217;ve been talking about Bayesian updating as a discrete-time process: something that happens once each generation for our population.  That&#8217;s fine and dandy, definitely worth studying, but Marc&#8217;s paper focuses on a continuous-time version called the <a href="http://en.wikipedia.org/wiki/Replicator_equation">&#8216;replicator equation&#8217;</a>.  It goes like this.  Let <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="X" class="latex" /> be the set of alternative genotypes.  For each <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i &#92;in X" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> be the number of organisms that have the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />.   Say that </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i P_i } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is the <b>fitness</b> of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype.   Let <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> be the probability that at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" />, a randomly chosen organism will have the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7BP_i%7D%7B%5Csum_%7Bi+%5Cin+X%7D+P_i+%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{P_i}{&#92;sum_{i &#92;in X} P_i } }" class="latex" /></p>
<p>Then a little calculus gives the <b>replicator equation</b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7Bd+p_i%7D%7Bd+t%7D+%3D+%5Cleft%28+f_i++-+%5Clangle+f+%5Crangle+%5Cright%29+%5C%2C+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{d p_i}{d t} = &#92;left( f_i  - &#92;langle f &#92;rangle &#92;right) &#92;, p_i } " class="latex" /></p>
<p>where </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+f+%5Crangle+%3D+%5Csum_%7Bi+%5Cin+X%7D++f_i++p_i++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle f &#92;rangle = &#92;sum_{i &#92;in X}  f_i  p_i  " class="latex" /></p>
<p>is the <b>mean fitness</b> of the organisms.  So, the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type grows at a rate proportional to the fitness of that type <i>minus the mean fitness</i>.   It ain&#8217;t enough to be good: you gotta be better than average.</p>
<p>Note that all this works not just when each fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is a mere number, but also when it&#8217;s a function of the whole list of probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />.  That&#8217;s good, because in the real world, the fitness of one kind of bug may depend on the fraction of bugs of various kinds.  </p>
<p>But what does all this have to do with <i>information?</i>  </p>
<p>Marc&#8217;s paper has a lot to say about this!   But just to give you a taste, here&#8217;s a simple fact involving relative entropy, which was first discovered by Ethan Atkin.  Suppose evolution as described by the replicator equation brings the whole list of probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />&mdash;let&#8217;s call this list <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />&mdash;closer and closer to some stable equilibrium, say <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  Then if a couple of technical conditions hold, the entropy of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> keeps decreasing, and approaches zero.  </p>
<p>Remember what I told you about <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">relative entropy</a>.  In Bayesian inference, the entropy <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> is how much information we gain if we start with <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> as our prior and then do an experiment that pushes us to the posterior <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  </p>
<p>So, in simple rough terms: <i><b>as it approaches a stable equilibrium, the amount of information a species has left to learn keeps dropping, and goes to zero!</b></i></p>
<p>I won&#8217;t fill in the precise details, because I bet you&#8217;re tired already.  You can find them in Section 3.5, which is called &#8220;Kullback-Leibler Divergence is a Lyapunov function for the Replicator Dynamic&#8221;.  If you know all the buzzwords here, you&#8217;ll be in buzzword heaven now.  &#8216;Kullback-Leibler divergence&#8217; is just another term for relative entropy.  &#8216;Lyapunov function&#8217; means that it keeps dropping and goes to zero.  And the &#8216;replicator dynamic&#8217; is the replicator equation I described above.</p>
<p>Perhaps next time I&#8217;ll say more about this stuff.  For now, I just hope you see why it makes me so happy.  </p>
<p>First, it uses information geometry to make precise the sense in which evolution is a process of acquiring information.  That&#8217;s very cool.  We&#8217;re looking at a simplified model&mdash;the replicator equation&mdash;but doubtless this is just the beginning of a very long story that keeps getting deeper as we move to less simplified models.  </p>
<p>Second, if you read my summary of <a href="https://johncarlosbaez.wordpress.com/2011/05/02/networks-and-population-biology/">Chris Canning&#8217;s talks on evolutionary game theory</a>, you&#8217;ll see everything I just said meshes nicely with that.  He was taking the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> to be</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_i+%3D+%5Csum_%7Bj+%5Cin+X%7D+A_%7Bi+j%7D+p_j+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i = &#92;sum_{j &#92;in X} A_{i j} p_j " class="latex" /></p>
<p>where the <b>payoff matrix</b> <img src="https://s0.wp.com/latex.php?latex=A_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_{i j}" class="latex" /> describes the &#8216;winnings&#8217; of an organism with the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype when it meets an organism with the <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="j" class="latex" />th genotype.  This gives a particularly nice special case of the replicator equation.</p>
<p>Third, this particularly nice special case happens to be the <a href="https://johncarlosbaez.wordpress.com/2011/04/03/network-theory-part-3/">rate equation</a> for a certain stochastic Petri net.  So, we&#8217;ve succeeded in connecting the &#8216;diagram theory&#8217; discussion to the &#8216;information geometry&#8217; discussion!  This has all sort of implications, which will take quite a while to explore.   </p>
<p>As the saying goes, in mathematics:</p>
<blockquote><p>
Everything sufficiently beautiful is connected to all other beautiful things.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/#comments">105 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/05/26/information-geometry-part-8/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;8)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-3380 post type-post status-publish format-standard hentry category-information-and-entropy category-physics" id="post-3380">
				<h2><a href="https://johncarlosbaez.wordpress.com/2011/04/30/crooks-fluctuation-theorem/" rel="bookmark">Crooks&#8217; Fluctuation Theorem</a></h2>
				<small>30 April, 2011</small><br />


				<div class="entry">
					<p><i>guest post by <b>Eric Downes</b></i></p>
<p>Christopher Jarzynski, Gavin Crooks, and some others have made a big splash by providing general equalities that relate free energy differences to non-equilibrium work values.  The best place to start is the first two chapters of Gavin Crooks&#8217; thesis:</p>
<p>&bull; Gavin Crooks, <a href="http://threeplusone.com/pubs/GECthesis">Excursions in Statistical Dynamics</a>, Ph.D. Thesis, Department of Chemistry, U.C. Berkeley, 1999.</p>
<p>Here is the ~1 kiloword summary:</p>
<p>If we consider the work <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W" class="latex" /> done <i>on</i> a system, <a href="http://en.wikipedia.org/wiki/Clausius_theorem">Clausius&#8217; Inequality</a> states that</p>
<p><img src="https://s0.wp.com/latex.php?latex=W+%5Cge+%5CDelta+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W &#92;ge &#92;Delta F" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5CDelta+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta F" class="latex" /> is the change in free energy.  One must perform more work along a non-equilibrium path because of the second law of thermodynamics.  The excess work</p>
<p><img src="https://s0.wp.com/latex.php?latex=W-+%5CDelta+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W- &#92;Delta F" class="latex" /></p>
<p>is dissipated as heat, and is basically the entropy change in the universe, measured in different units.  But who knows how large the excess work will be&#8230;</p>
<p>One considers a small system for which we imagine there exists a distribution of thermodynamic work values <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W" class="latex" /> (more on that below) in moving a system through phase space.  We start at a macrostate with free energy <img src="https://s0.wp.com/latex.php?latex=F_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_1" class="latex" />, and (staying in touch with a thermal reservoir at inverse temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" />) move in finite time to a new non-equilibrium state.  When this new state is allowed to equilibriate it will have free energy</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_1+%2B+%5CDelta+F+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F_1 + &#92;Delta F " class="latex" /></p>
<p>You can do this by changing the spin-spin coupling, compressing a gas, etc: you&#8217;re changing one of the parameters in the system&#8217;s Hamiltonian in a completely deterministic way, such that the structure of the Hamiltonian does not change, and the system still has well-defined microstates at all intervening times.  Your total accumulated work values will follow</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+%5Cexp%28-%5Cbeta+W%29+%5Crangle+%3D+%5Cexp%28-%5Cbeta+%5CDelta+F%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle &#92;exp(-&#92;beta W) &#92;rangle = &#92;exp(-&#92;beta &#92;Delta F)} " class="latex" /></p>
<p>where the expectation value is over a distribution of all possible paths through the classical  phase space.  This is the <a href="http://en.wikipedia.org/wiki/Jarzynski_equality">Jarzynski Equality</a>.</p>
<p>It has an analogue for quantum systems, which appears to be <a href="http://arxiv.org/abs/0711.2059">related to supersymmetry</a>, somehow.  But the proof for classical systems simply relies on a <a href="http://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> that moves through state space and an appropriate definition for work (see below).  I can dig up the reference if anyone wants.</p>
<p>This is actually a specific case of a more fundamental theorem discovered about a decade ago by Gavin Crooks: the <a href="http://en.wikipedia.org/wiki/Crooks_fluctuation_theorem">Crooks fluctuation theorem</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cexp%28-%5Cbeta%28W-+%5CDelta+F%29%29+%3D+%5Cfrac%7BP_%7B%5Cmathrm%7Bfwd%7D%7D%7D%7BP_%7B%5Cmathrm%7Brev%7D%7D%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;exp(-&#92;beta(W- &#92;Delta F)) = &#92;frac{P_{&#92;mathrm{fwd}}}{P_{&#92;mathrm{rev}}} }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cmathrm%7Bfwd%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_{&#92;mathrm{fwd}}" class="latex" /> is the probability of a particular forward path which requires work <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="W" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cmathrm%7Brev%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_{&#92;mathrm{rev}}" class="latex" /> is the probability of its time-reversal dual (see Gavin Crooks&#8217; thesis for more precise definitions).</p>
<p>How do we assign a thermodynamic work value to a path of microstates?  At the risk of ruining it for you: It turns out that one can write a first law analog for a subsystem Hamiltonian.  We start with:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cmathrm%7Btot%7D%7D+%3D+H_%7B%5Cmathrm%7Bsubsys%7D%7D+%2B+H_%7B%5Cmathrm%7Benviron%7D%7D+%2B+H_%7B%5Cmathrm%7Binteract%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;mathrm{tot}} = H_{&#92;mathrm{subsys}} + H_{&#92;mathrm{environ}} + H_{&#92;mathrm{interact}} " class="latex" /></p>
<p>As with Gibbs&#8217; derivation of the canonical ensemble, we never specify what <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cmathrm%7Benviron%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;mathrm{environ}}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cmathrm%7Binteract%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;mathrm{interact}}" class="latex" /> are, only that the number of degrees of freedom in <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cmathrm%7Benviron%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;mathrm{environ}}" class="latex" /> is very large, and <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cmathrm%7Binteract%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;mathrm{interact}}" class="latex" /> is a small coupling.  You make the observation that work can be associated with changing the energy-levels of the microstates in <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cmathrm%7Bsubsys%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;mathrm{subsys}}" class="latex" />, while heat is associated with the energy change when the (sub)system jumps from one microstate to another (due to <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Cmathrm%7Binteract%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="H_{&#92;mathrm{interact}}" class="latex" />) with no change in the spectrum of available energies.  This implies a rather deep connection between the Hamiltonian and thermodynamic work.  The second figure in Gavin&#8217;s thesis explained everything for me, after that you can basically derive it yourself.</p>
<p>The only physical applications I am aware of are to Monte Carlo simulations and mesoscopic systems in nano- or molecular biophysics.  In that regard John Baez&#8217; <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/">recent relation between free energy and R&eacute;nyi entropy</a> is a nice potential competitor for the efficient calculation of free energy differences (which apparently normally requires multiple simulations at intervening temperaturess, calculating the specific heat at each.)</p>
<p>But the relation to Markov chains is much more interesting to me, because this is a very general mathematical object which can be related to a much broader class of problems.  Heat ends up being associated with fluctuations in the system&#8217;s state, and the (phenomenological) energy values are kind of the &#8220;relative unlikelihood&#8221; of each state.  The excess work turns out to be related to the Kullback-Leibler divergence between the forward and reverse path-probabilities.</p>
<p>For visual learners with a background in stat mech, this is all developed in a pedagogical talk I gave in Fall 2010 at U. Wisconsin-Madison&#8217;s Condensed Matter Theory Seminar; talk available <a href="http://web.mit.edu/edown/www/JE_talk.pdf">here</a>.  I&#8217;m licensing it cc-by-sa-nc through the <a href="http://en.wikipedia.org/wiki/Creative_commons">Creative Commons License</a>.  I&#8217;ve been sloppy with references, but I emphasize that this is not original work; it is my presentation of Crooks&#8217; and Jarzynski&#8217;s. Nonetheless, any errors you find are my own.  Hokay, have a nice day!</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/30/crooks-fluctuation-theorem/#comments">8 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2011/04/30/crooks-fluctuation-theorem/" rel="bookmark" title="Permanent Link to Crooks&#8217; Fluctuation Theorem">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/9/" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/7/" >Next Entries &raquo;</a></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the information and entropy category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/">Classical Mechanics versus Thermodynamics (Part&nbsp;4)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Toby Bartels" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172598">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172597">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Toby Bartels" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://tobybartels.name/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/68c7b083965f5073c50bf7c8d2aac358?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://tobybartels.name/" rel="nofollow">Toby Bartels</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/26/classical-mechanics-versus-thermodynamics-part-4/#comment-172596">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="David Corfield" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://lh3.googleusercontent.com/a/AATXAJxXcoKzwm_cY3LJp3qldhAQvZVoBimQd4xe5tDl=s96-c' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">David Corfield on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172590">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="amarashiki" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (479)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (205)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see whats on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkins environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,228 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/8/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="ffcb185558" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,577 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

				<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = JSON.parse( decodeURIComponent( '%7B%22settings%22%3A%7B%22id%22%3A%22content%22%2C%22ajaxurl%22%3A%22https%3A%5C%2F%5C%2Fjohncarlosbaez.wordpress.com%5C%2F%3Finfinity%3Dscrolling%22%2C%22type%22%3A%22scroll%22%2C%22wrapper%22%3Atrue%2C%22wrapper_class%22%3A%22infinite-wrap%22%2C%22footer%22%3Atrue%2C%22click_handle%22%3A%221%22%2C%22text%22%3A%22Older%20posts%22%2C%22totop%22%3A%22Scroll%20back%20to%20top%22%2C%22currentday%22%3A%2230.04.11%22%2C%22order%22%3A%22DESC%22%2C%22scripts%22%3A%5B%5D%2C%22styles%22%3A%5B%5D%2C%22google_analytics%22%3Afalse%2C%22offset%22%3A8%2C%22history%22%3A%7B%22host%22%3A%22johncarlosbaez.wordpress.com%22%2C%22path%22%3A%22%5C%2Fcategory%5C%2Finformation-and-entropy%5C%2Fpage%5C%2F%25d%5C%2F%22%2C%22use_trailing_slashes%22%3Atrue%2C%22parameters%22%3A%22%22%7D%2C%22query_args%22%3A%7B%22paged%22%3A8%2C%22category_name%22%3A%22information-and-entropy%22%2C%22error%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A0%2C%22post_parent%22%3A%22%22%2C%22subpost%22%3A%22%22%2C%22subpost_id%22%3A%22%22%2C%22attachment%22%3A%22%22%2C%22attachment_id%22%3A0%2C%22name%22%3A%22%22%2C%22pagename%22%3A%22%22%2C%22page_id%22%3A0%2C%22second%22%3A%22%22%2C%22minute%22%3A%22%22%2C%22hour%22%3A%22%22%2C%22day%22%3A0%2C%22monthnum%22%3A0%2C%22year%22%3A0%2C%22w%22%3A0%2C%22tag%22%3A%22%22%2C%22cat%22%3A23375499%2C%22tag_id%22%3A%22%22%2C%22author%22%3A%22%22%2C%22author_name%22%3A%22%22%2C%22feed%22%3A%22%22%2C%22tb%22%3A%22%22%2C%22meta_key%22%3A%22%22%2C%22meta_value%22%3A%22%22%2C%22preview%22%3A%22%22%2C%22s%22%3A%22%22%2C%22sentence%22%3A%22%22%2C%22title%22%3A%22%22%2C%22fields%22%3A%22%22%2C%22menu_order%22%3A%22%22%2C%22embed%22%3A%22%22%2C%22category__in%22%3A%5B%5D%2C%22category__not_in%22%3A%5B%5D%2C%22category__and%22%3A%5B%5D%2C%22post__in%22%3A%5B%5D%2C%22post__not_in%22%3A%5B%5D%2C%22post_name__in%22%3A%5B%5D%2C%22tag__in%22%3A%5B%5D%2C%22tag__not_in%22%3A%5B%5D%2C%22tag__and%22%3A%5B%5D%2C%22tag_slug__in%22%3A%5B%5D%2C%22tag_slug__and%22%3A%5B%5D%2C%22post_parent__in%22%3A%5B%5D%2C%22post_parent__not_in%22%3A%5B%5D%2C%22author__in%22%3A%5B%5D%2C%22author__not_in%22%3A%5B%5D%2C%22lazy_load_term_meta%22%3Afalse%2C%22posts_per_page%22%3A10%2C%22ignore_sticky_posts%22%3Afalse%2C%22suppress_filters%22%3Afalse%2C%22cache_results%22%3Afalse%2C%22update_post_term_cache%22%3Atrue%2C%22update_post_meta_cache%22%3Atrue%2C%22post_type%22%3A%22%22%2C%22nopaging%22%3Afalse%2C%22comments_per_page%22%3A%22100%22%2C%22no_found_rows%22%3Afalse%2C%22order%22%3A%22DESC%22%7D%2C%22query_before%22%3A%222021-09-26%2017%3A27%3A01%22%2C%22last_post_date%22%3A%222011-04-30%2003%3A55%3A39%22%2C%22body_class%22%3A%22infinite-scroll%20neverending%22%2C%22loading_text%22%3A%22Loading%20new%20page%22%2C%22stats%22%3A%22blog%3D12777403%26v%3Dwpcom%26tz%3D0%26user_id%3D0%26subd%3Djohncarlosbaez%26x_pagetype%3Dinfinite%22%7D%7D' ) );
		//]]>
		</script>
		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-68c7b083965f5073c50bf7c8d2aac358">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-7d52fbe20c8ac05886a296e9ee2159b1">
	</div>
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	</div>
		<div id="infinite-footer">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://johncarlosbaez.wordpress.com/" rel="home">
						Azimuth					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a> 				</div>
			</div>
		</div><!-- #infinite-footer -->
		
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s1.wp.com/_static/??-eJyFy00OQDAQQOELGUP8xUKcpWoiZVTTadO4vVhY2LB8L/kwOdCHDWQD7hEcx8VYQa38EYUYJRlHHqZoZ6Zci2T4I1YKTukNnvGFgmGaYVHM5M933Wzch7Ktir5p+rpbL+lvP34=?cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"805b669379","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2011%2F04%2F30%2Fcrooks-fluctuation-theorem%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s0.wp.com/_static/??-eJyNkN1uwyAMhV9oDkouKvVi2rMQcCNT/oZN07z9yNRMbTZFu8LH8J2DreYMJkXBKMqxsngjg/neOX5TT1ehQvZ1osiK4oUiyfJTHLzVNlCEURcVNAuWVoEUba68h1q2+6xYlsfRzdmkALmk+wIFW49lYygaXy3yCjWJYUTbtaCDj8xkJxRWXEc2hbJQivyXH8+XNDo0sjfbnG5kMSnNvNq5h84F+ddIT/FGl1QZvXIouY0PW+OAEfJoYdLer1t5Uf9Igu/97WTjPsJ7fxrO534YTr37Ai2cwB8='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			(function() {
				var extend = function(out) {
					out = out || {};

					for (var i = 1; i < arguments.length; i++) {
						if (!arguments[i])
						continue;

						for (var key in arguments[i]) {
						if (arguments[i].hasOwnProperty(key))
							out[key] = arguments[i][key];
						}
					}

					return out;
				};
				extend( window.infiniteScroll.settings.scripts, ["postmessage","mobile-useragent-info","rlt-proxy","jquery-core","jquery-migrate","jquery","wpcom-actionbar-placeholder","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","wpcom-masterbar-tracks-js","jquery.wpcom-proxy-request","wp-embed","jetpack-subscriptions-js","swfobject","videopress","jetpack-carousel","tiled-gallery","carousel-wpcom"] );
				extend( window.infiniteScroll.settings.styles, ["the-neverending-homepage","infinity-contempt","wp-block-library","mediaelement","wp-mediaelement","jetpack-layout-grid","jetpack-ratings","coblocks-frontend","wpcom-core-compat-playlist-styles","wpcom-text-widget-styles","wpcom-bbpress2-staff-css","contempt","geo-location-flair","reblogging","a8c-global-print","h4-global","global-styles","jetpack-global-styles-frontend-style","jetpack-carousel-swiper-css","jetpack-carousel","tiled-gallery"] );
			})();
		</script>
				<span id="infinite-aria" aria-live="polite"></span>
		<script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZVeHRpLXB0U3J1elBvZCZiTkEzXXlsLi1PU25PbltLcyZONi1Lay1ia3RtU0ZEcmxjdDdab2FqP3lnNTdjREcvfi93LG1GJS5WSk1jVm9BP0F6XWJteXFBeUJTLW56ZVkmb2ZCS3w4RyVlSEplQ3pUP3gxVC02aWI0cm4mLVRYY0Q2b0trJVl2aiVicnwyWTQ4cj1UQ3I2L3lIZFFZejYsTCtGbWF4fmZ+PVJnQ2pdZE98X2hUUUhXY3VNTSs1ZUZuOGZoVWZ6'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>