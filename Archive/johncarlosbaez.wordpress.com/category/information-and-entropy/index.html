<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>information and entropy | Azimuth</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; information and entropy Category Feed" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/feed/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IlqVOdbxwfBYOEVOBZSBYeXuz1e6sVrveMDl9yU9AHrOwUyJIJGMTOTSPqUpMz5iQ+mIMttYbuVFMLxChytyMPJXFTBfcGfKNXQPFc6aAfFNq2A9KmobBSRMm+yoCmqJLl5V6gKURJhua4zGHKiM41BB46qxo5eSgOxQRwGvbh4hpG+fc2v8G/S3+pJSbAWU9S9Z9aiR8QfdD9r9bFE2YfN3A7fSF7QY18t4cVlqC4nd29WTzzjkes768+BXsiM4DMV7PtiB4v45kHiOMyQVqFXxGbFF8/pSZe4qP6u5W3T/sx3F3+ACgX/YH?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F17%2Finformation-geometry-part-21%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"677eca7dd0","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"0272f16312\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/category\/information-and-entropy\/","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2Fcategory%2Finformation-and-entropy%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,227 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F17%2Finformation-geometry-part-21%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="information and entropy &#8211; Azimuth" />
<meta property="og:url" content="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="description" content="Posts about information and entropy written by John Baez" />
<style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="archive category category-information-and-entropy category-23375499 customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-31635 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31635">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/" rel="bookmark">Information Geometry (Part&nbsp;21)</a></h2>
				<small>17 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Last time</a> I ended with a formula for the &#8216;Gibbs distribution&#8217;: the probability distribution that maximizes entropy subject to constraints on the expected values of some observables.</p>
<p>This formula is well-known, but I&#8217;d like to derive it here.   My argument won&#8217;t be up to the highest standards of rigor: I&#8217;ll do a bunch of computations, and it would take more work to state conditions under which these computations are justified.   But even a nonrigorous approach is worthwhile, since the computations will give us more than the mere formula for the Gibbs distribution.</p>
<p>I&#8217;ll start by reminding you of what I claimed last time.   I&#8217;ll state it in a way that removes all unnecessary distractions, so go back to <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Part 20</a> if you want more explanation.</p>
<h3> The Gibbs distribution</h3>
<p>Take a measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu." class="latex" />    Suppose there is a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> that maximizes the <b>entropy</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-%5Cint_%5COmega+%5Cpi%28x%29+%5Cln+%5Cpi%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -&#92;int_&#92;Omega &#92;pi(x) &#92;ln &#92;pi(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>subject to the requirement that some integrable functions <img src="https://s0.wp.com/latex.php?latex=A%5E1%2C+%5Cdots%2C+A%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^1, &#92;dots, A^n" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> have expected values equal to some chosen list of numbers <img src="https://s0.wp.com/latex.php?latex=q%5E1%2C+%5Cdots%2C+q%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^1, &#92;dots, q^n." class="latex" /></p>
<p>(Unlike last time, now I&#8217;m writing <img src="https://s0.wp.com/latex.php?latex=A%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i" class="latex" /> with superscripts rather than subscripts, because I&#8217;ll be using the Einstein summation convention: I&#8217;ll sum over any repeated index that appears once as a a superscript and once as a subscript.)</p>
<p>Furthermore, suppose <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> depends smoothly on <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in &#92;mathbb{R}^n." class="latex" />    I&#8217;ll call it <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> to indicate its dependence on <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   Then, I claim <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the so-called <b>Gibbs distribution</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cpi_q%28x%29+%3D%5Cfrac%7Be%5E%7B-p_i+A%5Ei%28x%29%7D%7D%7B%5Cint_%5COmega+e%5E%7B-p_i+A%5Ei%28x%29%7D+%5C%2C+d%5Cmu%28x%29%7D+++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;pi_q(x) =&#92;frac{e^{-p_i A^i(x)}}{&#92;int_&#92;Omega e^{-p_i A^i(x)} &#92;, d&#92;mu(x)}   }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%28q%29%7D%7B%5Cpartial+q%5Ei%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f(q)}{&#92;partial q^i} }" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Cint_%5COmega+%5Cpi_q%28x%29+%5Cln+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;int_&#92;Omega &#92;pi_q(x) &#92;ln &#92;pi_q(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>is the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>Let&#8217;s show this is true!</p>
<h3> Finding the Gibbs distribution</h3>
<p>So, we are trying to find a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> that maximizes entropy subject to these constraints:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cpi%28x%29+A%5Ei%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q%5Ei+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;pi(x) A^i(x) &#92;, d&#92;mu(x) = q^i } " class="latex" /></p>
<p>We can solve this problem using <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>.  We need one Lagrange multiplier, say <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i," class="latex" /> for each of the above constraints.  But it&#8217;s easiest if we start by letting <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> range over all of <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega)," class="latex" /> that is, the space of all integrable functions on <img src="https://s0.wp.com/latex.php?latex=%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega." class="latex" />  Then, because we want <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> to be a probability distribution, we need to impose one extra constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cpi%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;pi(x) &#92;, d&#92;mu(x) = 1 } " class="latex" /></p>
<p>To do this we need an extra Lagrange multiplier, say <img src="https://s0.wp.com/latex.php?latex=%5Cgamma.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma." class="latex" /></p>
<p>So, that&#8217;s what we&#8217;ll do!  We&#8217;ll look for critical points of this function on <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%29%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega):" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+-+%5Cbeta_i++%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+-+%5Cgamma+%5Cint+%5Cpi%5C%2C++d%5Cmu++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu - &#92;beta_i  &#92;int &#92;pi A^i &#92;, d&#92;mu - &#92;gamma &#92;int &#92;pi&#92;,  d&#92;mu  }" class="latex" /></p>
<p>Here I&#8217;m using some tricks to keep things short.  First, I&#8217;m dropping the dummy variable <i>x</i> which appeared in all of the integrals we had: I&#8217;m leaving it implicit.  Second, all my integrals are over <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> so I won&#8217;t say that.  And third, I&#8217;m using the Einstein summation convention, so there&#8217;s a sum over <i>i</i> implicit here.</p>
<p>Okay, now let&#8217;s do the <a href="https://en.wikipedia.org/wiki/Functional_derivative">variational derivative</a> required to find a critical point of this function.   When I was a math major taking physics classes, the way physicists did variational derivatives seemed like black magic to me.  Then I spent months reading how mathematicians rigorously justified these techniques.  I don&#8217;t feel like a massive digression into this right now, so I&#8217;ll just do the calculations&#8212;and if they seem like black magic, I&#8217;m sorry!</p>
<p>We need to find <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cleft%28-+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+-+%5Cbeta_i++%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+-+%5Cgamma+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%5Cright%29+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;left(- &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu - &#92;beta_i  &#92;int &#92;pi A^i &#92;, d&#92;mu - &#92;gamma &#92;int &#92;pi &#92;, d&#92;mu &#92;right) = 0 } " class="latex" /></p>
<p>or in other words</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cleft%28%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+%2B+%5Cbeta_i++%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+%2B+%5Cgamma+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%5Cright%29+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;left(&#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu + &#92;beta_i  &#92;int &#92;pi A^i &#92;, d&#92;mu + &#92;gamma &#92;int &#92;pi &#92;, d&#92;mu &#92;right) = 0 } " class="latex" /></p>
<p>First we need to simplify this expression.   The only part that takes any work, if you know how to do variational derivatives, is the first term.  Since the derivative of <img src="https://s0.wp.com/latex.php?latex=z+%5Cln+z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z &#92;ln z" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=1+%2B+%5Cln+z%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 + &#92;ln z," class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu++%3D+1+%2B+%5Cln+%5Cpi%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu  = 1 + &#92;ln &#92;pi(x) } " class="latex" /></p>
<p>The second and third terms are easy, so we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cleft%28+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+%2B+%5Cbeta_i+%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+%2B+%5Cgamma+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%5Cright%29+%3D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;left( &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu + &#92;beta_i &#92;int &#92;pi A^i &#92;, d&#92;mu + &#92;gamma &#92;int &#92;pi &#92;, d&#92;mu &#92;right) =} " class="latex" /></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp;  <img src="https://s0.wp.com/latex.php?latex=1+%2B+%5Cln+%5Cpi%28x%29+%2B+%5Cbeta_i+A%5Ei%28x%29+%2B+%5Cgamma++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 + &#92;ln &#92;pi(x) + &#92;beta_i A^i(x) + &#92;gamma  " class="latex" /></p>
<p>Thus, we need to solve this equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+1+%2B+%5Cln+%5Cpi%28x%29+%2B+%5Cbeta_i+A%5Ei%28x%29+%2B+%5Cgamma++%3D+0%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ 1 + &#92;ln &#92;pi(x) + &#92;beta_i A^i(x) + &#92;gamma  = 0} " class="latex" /></p>
<p>That&#8217;s easy to do:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi%28x%29+%3D+e%5E%7B-1+-+%5Cgamma+-+%5Cbeta_i+A%5Ei%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi(x) = e^{-1 - &#92;gamma - &#92;beta_i A^i(x)} " class="latex" /></p>
<p>Good!   It&#8217;s starting to look like the Gibbs distribution!</p>
<p>We now need to choose the Lagrange multipliers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> to make the constraints hold.  To satisfy this constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int &#92;pi &#92;, d&#92;mu = 1 } " class="latex" /></p>
<p>we must choose <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint++e%5E%7B-1+-+%5Cgamma+-+%5Cbeta_i+A%5Ei+%7D+%5C%2C+d%5Cmu+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int  e^{-1 - &#92;gamma - &#92;beta_i A^i } &#92;, d&#92;mu = 1 } " class="latex" /></p>
<p>or in other words</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+e%5E%7B1+%2B+%5Cgamma%7D+%3D+%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ e^{1 + &#92;gamma} = &#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu } " class="latex" /></p>
<p>Plugging this into our earlier formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi%28x%29+%3D+e%5E%7B-1+-+%5Cgamma+-+%5Cbeta_i+A%5Ei%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi(x) = e^{-1 - &#92;gamma - &#92;beta_i A^i(x)} " class="latex" /></p>
<p>we get this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi_q%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi_q(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{&#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu} } " class="latex" /></p>
<p>Great!  Even more like the Gibbs distribution!</p>
<p>By the way, you must have noticed the &#8220;1&#8221; that showed up here:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu++%3D+1+%2B+%5Cln+%5Cpi%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu  = 1 + &#92;ln &#92;pi(x) } " class="latex" /></p>
<p>It buzzed around like an annoying fly in the otherwise beautiful calculation, but eventually went away.  This is the same irksome &#8220;1&#8221; that showed up in <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Part 19</a>.   Someday I&#8217;d like to say a bit more about it.</p>
<p>Now, where were we?  We were trying to show that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cpi_q%28x%29+%3D%5Cfrac%7Be%5E%7B-p_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-p_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;pi_q(x) =&#92;frac{e^{-p_i A^i(x)}}{&#92;int e^{-p_i A^i} &#92;, d&#92;mu}   }" class="latex" /></p>
<p>minimizes entropy subject to our constraints.  So far we&#8217;ve shown</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{&#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu} } " class="latex" /></p>
<p>is a critical point.   It&#8217;s clear that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi%28x%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi(x) &#92;ge 0" class="latex" /></p>
<p>so <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> really is a probability distribution.  We should show it actually <i>maximizes</i> entropy subject to our constraints, but I will skip that.   Given that, <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> will be our claimed Gibbs distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> if we can show</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;beta_i" class="latex" /></p>
<p>This is interesting!   It&#8217;s saying our Lagrange multipliers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> actually equal the so-called <b>conjugate variables</b> <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q^i} }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q)" class="latex" /> is the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q:" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Cint_%5COmega+%5Cpi_q%28x%29+%5Cln+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;int_&#92;Omega &#92;pi_q(x) &#92;ln &#92;pi_q(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>There are two ways to show this: the easy way and the hard way.  The easy way is to reflect on the meaning of Lagrange multipliers, and I&#8217;ll sketch that way first.  The hard way is to use brute force: just compute <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and show it equals <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i." class="latex" />   This is a good test of our computational muscle&#8212;but more importantly, it will help us discover some interesting facts about the Gibbs distribution.</p>
<h3> The easy way</h3>
<p>Consider a simple Lagrange multiplier problem where you&#8217;re trying to find a critical point of a smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+%5Cmathbb%7BR%7D%5E2+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon &#92;mathbb{R}^2 &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>subject to the constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = c" class="latex" /></p>
<p>for some smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%5Ccolon+%5Cmathbb%7BR%7D%5E2+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;colon &#92;mathbb{R}^2 &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>and constant <i>c</i>.   (The function <i>f</i> here has nothing to do with the <i>f</i> in the previous sections.)   To answer this we introduce a Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and seek points where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla+%28+f+-+%5Clambda+g%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla ( f - &#92;lambda g) = 0" class="latex" /></p>
<p>This works because the above equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f+%3D+%5Clambda+%5Cnabla+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla f = &#92;lambda &#92;nabla g" class="latex" /></p>
<p>Geometrically this means we&#8217;re at a point where the gradient of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> points at right angles to the level surface of <img src="https://s0.wp.com/latex.php?latex=g%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g:" class="latex" /></p>
<p><a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"><img loading="lazy" data-attachment-id="31685" data-permalink="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/lagrange_multipliers/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png" data-orig-size="2560,1843" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lagrange_multipliers" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=450" src="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=450&#038;h=324" alt="" class="aligncenter size-full wp-image-31685" width="450" height="324" srcset="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=450&amp;h=324 450w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=900&amp;h=648 900w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=150&amp;h=108 150w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=300&amp;h=216 300w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=768&amp;h=553 768w" sizes="(max-width: 450px) 100vw, 450px" /></a></p>
<p>Thus, to first order we can&#8217;t change <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> by moving along the level surface of <img src="https://s0.wp.com/latex.php?latex=g.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g." class="latex" /></p>
<p>But also, if we start at a point where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f+%3D+%5Clambda+%5Cnabla+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla f = &#92;lambda &#92;nabla g" class="latex" /></p>
<p>and we begin moving in any direction, the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> will change at a rate equal to <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> times the rate of change of <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />.  That&#8217;s just what the equation says!   And this fact gives a conceptual meaning to the Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Clambda.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda." class="latex" /></p>
<p>Our situation is more complicated, since our functions are defined on the infinite-dimensional space <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega)," class="latex" /> and we have an <i>n</i>-tuple of constraints  with an <i>n</i>-tuple of Lagrange multipliers.   But the same principle holds.</p>
<p>So, when we are at a solution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> of our constrained entropy-maximization problem, and we start moving the point <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> by changing the value of the <i>i</i>th constraint, namely <img src="https://s0.wp.com/latex.php?latex=q%5Ei%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i," class="latex" /> the rate at which the entropy changes will be <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> times the rate of change of <img src="https://s0.wp.com/latex.php?latex=q%5Ei.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i." class="latex" />    So, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%3D+%5Cbeta_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i} = &#92;beta_i }" class="latex" /></p>
<p>But this is just what we needed to show!</p>
<h3> The hard way</h3>
<p>Here&#8217;s another way to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%3D+%5Cbeta_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i} = &#92;beta_i }" class="latex" /></p>
<p>We start by solving our constrained entropy-maximization problem using Lagrange multipliers.  As already shown, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi_q%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi_q(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{&#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu} } " class="latex" /></p>
<p>Then we&#8217;ll compute the entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28q%29+%3D+-+%5Cint+%5Cpi_q+%5Cln+%5Cpi_q+%5C%2C+d%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q) = - &#92;int &#92;pi_q &#92;ln &#92;pi_q &#92;, d&#92;mu " class="latex" /></p>
<p>Then we&#8217;ll differentiate this with respect to <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and show we get <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i." class="latex" /></p>
<p>Let&#8217;s try it!   The calculation is a bit heavy, so let&#8217;s write <img src="https://s0.wp.com/latex.php?latex=Z%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(q)" class="latex" /> for the so-called <b>partition function</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Z%28q%29+%3D+%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Z(q) = &#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu } " class="latex" /></p>
<p>so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi_q%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7BZ%28q%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi_q(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{Z(q)} } " class="latex" /></p>
<p>and the entropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++f%28q%29+%26%3D%26+-+%5Cdisplaystyle%7B+%5Cint++%5Cpi_q++%5Cln+%5Cleft%28+%5Cfrac%7Be%5E%7B-+%5Cbeta_k+A%5Ek%7D%7D%7BZ%28q%29%7D+%5Cright%29++%5C%2C+d%5Cmu+%7D++%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cint+%5Cpi_q+%5Cleft%28%5Cbeta_k+A%5Ek+%2B+%5Cln+Z%28q%29+%5Cright%29++%5C%2C+d%5Cmu+%7D+%5C%5C+%5C%5C++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  f(q) &amp;=&amp; - &#92;displaystyle{ &#92;int  &#92;pi_q  &#92;ln &#92;left( &#92;frac{e^{- &#92;beta_k A^k}}{Z(q)} &#92;right)  &#92;, d&#92;mu }  &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;int &#92;pi_q &#92;left(&#92;beta_k A^k + &#92;ln Z(q) &#92;right)  &#92;, d&#92;mu } &#92;&#92; &#92;&#92;  &#92;end{array}  " class="latex" /></p>
<p>This is the sum of two terms.  The first term</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint+%5Cpi_q+%5Cbeta_k+A%5Ek+++%5C%2C+d%5Cmu+%3D++%5Cbeta_k+%5Cint+%5Cpi_q+A%5Ek+++%5C%2C+d%5Cmu%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int &#92;pi_q &#92;beta_k A^k   &#92;, d&#92;mu =  &#92;beta_k &#92;int &#92;pi_q A^k   &#92;, d&#92;mu} " class="latex" /></p>
<p>is <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_k" class="latex" /> times the expected value of <img src="https://s0.wp.com/latex.php?latex=A%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^k" class="latex" /> with respect to the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> all summed over <img src="https://s0.wp.com/latex.php?latex=k.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k." class="latex" />    But the expected value of <img src="https://s0.wp.com/latex.php?latex=A%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^k" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=q%5Ek%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^k," class="latex" /> so we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint++%5Cpi_q+%5Cbeta_k+A%5Ek+%5C%2C+d%5Cmu+%7D+%3D++%5Cbeta_k+q%5Ek+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int  &#92;pi_q &#92;beta_k A^k &#92;, d&#92;mu } =  &#92;beta_k q^k " class="latex" /></p>
<p>The second term is easier:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega++%5Cpi_q+%5Cln+Z%28q%29+%5C%2C+d%5Cmu+%3D+%5Cln+Z%28q%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega  &#92;pi_q &#92;ln Z(q) &#92;, d&#92;mu = &#92;ln Z(q) }" class="latex" /></p>
<p>since <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q(x)" class="latex" /> integrates to 1 and the partition function <img src="https://s0.wp.com/latex.php?latex=Z%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(q)" class="latex" /> doesn&#8217;t depend on <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>Putting together these two terms we get an interesting formula for the entropy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28q%29+%3D+%5Cbeta_k+q%5Ek+%2B+%5Cln+Z%28q%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q) = &#92;beta_k q^k + &#92;ln Z(q) " class="latex" /></p>
<p>This formula is one reason this brute-force approach is actually worthwhile!  I&#8217;ll say more about it later.</p>
<p>But for now, let&#8217;s use this formula to show what we&#8217;re trying to show, namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%3D+%5Cbeta_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i} = &#92;beta_i }" class="latex" /></p>
<p>For starters,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D%7D+%26%3D%26+%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cleft%28%5Cbeta_k+q%5Ek+%2B+%5Cln+Z%28q%29+%5Cright%29+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_k+++%5Cfrac%7B%5Cpartial+q%5Ek%7D%7B%5Cpartial+q%5Ei%7D+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29+++%7D++%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_k++%5Cdelta%5Ek_i+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29+++%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_i+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29++%7D++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i}} &amp;=&amp; &#92;displaystyle{&#92;frac{&#92;partial}{&#92;partial q^i} &#92;left(&#92;beta_k q^k + &#92;ln Z(q) &#92;right) } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_k   &#92;frac{&#92;partial q^k}{&#92;partial q^i} + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)   }  &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_k  &#92;delta^k_i + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)   } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_i + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)  }  &#92;end{array}  " class="latex" /></p>
<p>where we played a little Kronecker delta game with the second term.</p>
<p>Now we just need to compute the third term:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29+%7D+%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+Z%28q%29+%7D+%5C%5C++%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D++%5Cint+e%5E%7B-+%5Cbeta_j+A%5Ej%7D++%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cleft%28e%5E%7B-+%5Cbeta_j+A%5Ej%7D%5Cright%29+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D%5Cleft%28+-+%5Cbeta_k+A%5Ek+%5Cright%29++e%5E%7B-+%5Cbeta_j+A%5Ej%7D+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D++A%5Ek+++e%5E%7B-+%5Cbeta_j+A%5Ej%7D+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+-+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint++A%5Ek+++e%5E%7B-+%5Cbeta_j+A%5Ej%7D+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+-+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%7D++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q) } &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;frac{&#92;partial}{&#92;partial q^i} Z(q) } &#92;&#92;  &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;frac{&#92;partial}{&#92;partial q^i}  &#92;int e^{- &#92;beta_j A^j}  &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;int &#92;frac{&#92;partial}{&#92;partial q^i} &#92;left(e^{- &#92;beta_j A^j}&#92;right) &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;int &#92;frac{&#92;partial}{&#92;partial q^i}&#92;left( - &#92;beta_k A^k &#92;right)  e^{- &#92;beta_j A^j} &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ -&#92;frac{1}{Z(q)} &#92;int &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i}  A^k   e^{- &#92;beta_j A^j} &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ - &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} &#92;frac{1}{Z(q)} &#92;int  A^k   e^{- &#92;beta_j A^j} &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ - &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k }  &#92;end{array}  " class="latex" /></p>
<p>Ah, you don&#8217;t know how good it feels, after years of category theory, to be doing calculations like this again!</p>
<p>Now we can finish the job we started:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D%7D+%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_i+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29++%7D+%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_i+-+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek++%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cbeta_i++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i}} &amp;=&amp;  &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_i + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)  } &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_i - &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k  } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;beta_i  &#92;end{array}  " class="latex" /></p>
<p>Voil!</p>
<h3> Conclusions</h3>
<p>We&#8217;ve learned the formula for the probability distribution that maximizes entropy subject to some constraints on the expected values of observables.  But more importantly, we&#8217;ve seen that the anonymous Lagrange multipliers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> that show up in this problem are actually the partial derivatives of entropy!  They equal</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q^i} } " class="latex" /></p>
<p>Thus, they are rich in meaning.   From what we&#8217;ve seen earlier, they are &#8216;surprisals&#8217;.   They are <i>analogous to momentum</i> in classical mechanics and have the meaning of <i>intensive variables</i> in thermodynamics:</p>
<div align="center">
<table border="1">
<tbody>
<tr>
<td></td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;<i>q</i> &nbsp;</td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">probabilities</td>
</tr>
<tr>
<td>&nbsp;<i>p</i>&nbsp;</td>
<td style="text-align:center;"> momentum</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">surprisals</td>
</tr>
<tr>
<td>&nbsp;<i>S</i> &nbsp;</td>
<td style="text-align:center;"> action</td>
<td style="text-align:center;">entropy</td>
<td style="text-align:center;"> Shannon entropy</td>
</tr>
</tbody>
</table>
</div>
<p>Furthermore, by showing <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i+%3D+p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i = p_i" class="latex" /> the hard way we discovered an interesting fact.  There&#8217;s a relation between the entropy and the logarithm of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28q%29+%3D+p_i+q%5Ei+%2B+%5Cln+Z%28q%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q) = p_i q^i + &#92;ln Z(q) " class="latex" /></p>
<p>(We proved this formula with <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> replacing <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> but now we know those are equal.)</p>
<p>This formula suggests that the logarithm of the partition function is important&#8212;and it is!  It&#8217;s closely related to the concept of <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)#Relation_to_thermodynamic_variables">free energy</a>&#8212;even though &#8216;energy&#8217;, free or otherwise, doesn&#8217;t show up at the level of generality we&#8217;re working at now.</p>
<p>This formula should also remind you of the tautological 1-form on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q," class="latex" /> namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%3D+p_i+dq%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta = p_i dq^i" class="latex" /></p>
<p>It should remind you even more of the contact 1-form on the contact manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}," class="latex" /> namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_i+dq%5Ei+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_i dq^i " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is a coordinate on the contact manifold that&#8217;s a kind of abstract stand-in for our entropy function <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>So, it&#8217;s clear there&#8217;s a lot more to say: we&#8217;re seeing hints of things here and there, but not yet the full picture.</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p> <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comments">11 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;21)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31552 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31552">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/" rel="bookmark">Information Geometry (Part&nbsp;20)</a></h2>
				<small>14 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Last time</a> we worked out an analogy between classical mechanics, thermodynamics and probability theory.  The latter two look suspiciously similar:</p>
<div align="center">
<table border="1">
<tbody>
<tr>
<td></td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;<i>q</i> &nbsp;</td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">probabilities</td>
</tr>
<tr>
<td>&nbsp;<i>p</i>&nbsp;</td>
<td style="text-align:center;"> momentum</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">surprisals</td>
</tr>
<tr>
<td>&nbsp;<i>S</i> &nbsp;</td>
<td style="text-align:center;"> action</td>
<td style="text-align:center;">entropy</td>
<td style="text-align:center;"> Shannon entropy</td>
</tr>
</tbody>
</table>
</div>
<p>This is no coincidence.  After all, in the subject of statistical mechanics we <i>explain</i> classical thermodynamics using probability theory&#8212;and entropy is revealed to be Shannon entropy (or its quantum analogue).</p>
<p>Now I want to make this precise.</p>
<p>To connect classical thermodynamics to probability theory, I&#8217;ll start by discussing &#8216;statistical manifolds&#8217;.  I introduced the idea of a statistical manifold in <a href="https://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>: it&#8217;s a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> equipped with a map sending each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> to a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> on some measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega." class="latex" />   Now I&#8217;ll say how these fit into the second column of the above chart.</p>
<p>Then I&#8217;ll talk about statistical manifolds of a special sort used in thermodynamics, which I&#8217;ll call &#8216;Gibbsian&#8217;, since they really go back to Josiah Willard Gibbs.</p>
<div align="center"><a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs"><img src="https://johncarlosbaez.files.wordpress.com/2021/08/josiah_willard_gibbs_-from_mms-.jpg?w=200" alt="" width="200" /></a></div>
<p>In a Gibbsian statistical manifold, for each <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a &#8216;Gibbs distribution&#8217;.   Physically, these Gibbs distributions describe <a href="https://en.wikipedia.org/wiki/Thermodynamic_equilibrium">thermodynamic equilibria</a>.  For example, if you specify the volume, energy and number of particles in a box of gas, there will be a Gibbs distribution describing what the particles do in thermodynamic equilibrium under these conditions.   Mathematically, Gibbs distributions <i>maximize entropy</i> subject to some constraints specified by the point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q." class="latex" /></p>
<p>More precisely: in a Gibbsian statistical manifold we have a list of observables <img src="https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots+%2C+A_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_1, &#92;dots , A_n" class="latex" /> whose expected values serve as coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> for points <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the probability distribution that maximizes entropy subject to the constraint that the expected value of <img src="https://s0.wp.com/latex.php?latex=A_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_i" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   We can derive most of the interesting formulas of thermodynamics starting from this!</p>
<h3> Statistical manifolds</h3>
<p>Let&#8217;s fix a measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu." class="latex" />  A <b>statistical manifold</b> is then a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> equipped with a smooth map <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> assigning to each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> a probability distribution on <img src="https://s0.wp.com/latex.php?latex=%5COmega%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega," class="latex" /> which I&#8217;ll call <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" />   So, <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a function on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cpi_q+%5C%2C+d%5Cmu+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;pi_q &#92;, d&#92;mu = 1 }" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%28x%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q(x) &#92;ge 0" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>The idea here is that the space of <i>all</i> probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> may be too huge to understand in as much detail as we&#8217;d like, so instead we describe <i>some</i> of these probability distributions&#8212;a family parametrized by points of some manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" />&#8212;using the map <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" />   This is the basic idea behind <a href="https://en.wikipedia.org/wiki/Parametric_statistics">parametric statistics</a>.</p>
<p>Information geometry is the geometry of statistical manifolds.  Any statistical manifold comes with a bunch of interesting geometrical structures.   One is the &#8216;Fisher information metric&#8217;, a Riemannian metric I explained in <a href="https://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>.  Another is a 1-parameter family of connections on the tangent bundle <img src="https://s0.wp.com/latex.php?latex=T+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T Q," class="latex" /> which is important in Amari&#8217;s approach to information geometry.   You can read about this here:</p>
<p> Hiroshi Matsuzoe, <a href="https://projecteuclid.org/download/pdf_1/euclid.aspm/1543086326">Statistical manifolds and affine differential geometry</a>, in <i>Advanced Studies in Pure Mathematics 57</i>, pp. 303321.</p>
<p>I don&#8217;t want to talk about it now&#8212;I just wanted to reassure you that I&#8217;m not completely ignorant of it!</p>
<p>I want to focus on the story I&#8217;ve been telling, which is about <i>entropy</i>.   Our statistical manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> comes with a smooth <b>entropy</b> function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++f%28q%29+%3D+-%5Cint_%5COmega+%5Cpi_q%28x%29+%5C%2C+%5Cln+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29++++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  f(q) = -&#92;int_&#92;Omega &#92;pi_q(x) &#92;, &#92;ln &#92;pi_q(x) &#92;, d&#92;mu(x)    } " class="latex" /></p>
<p>We can use this entropy function to do many of the things we usually do in thermodynamics!   For example, at any point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> where this function is differentiable, its differential gives a cotangent vector</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>which has an important physical meaning.   In coordinates we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>and we call <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> the <b>intensive variable conjugate to</b> <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   For example if <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> is energy, <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> will be &#8216;coolness&#8217;: the reciprocal of temperature.</p>
<p>Defining <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> this way gives a Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B+%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+x+%5Cin+M%2C+%5C%3B+p+%3D++%28df%29_x+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{ (q,p) &#92;in T^&#92;ast Q : &#92;; x &#92;in M, &#92;; p =  (df)_x &#92;} " class="latex" /></p>
<p>of the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   We can also get contact geometry into the game by defining a contact manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> and a Legendrian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B+%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+x+%5Cin+M%2C+%5C%3B+p+%3D++%28df%29_q+%2C+S+%3D+f%28q%29+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{ (q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; x &#92;in M, &#92;; p =  (df)_q , S = f(q) &#92;}" class="latex" /></p>
<p>But I&#8217;ve been talking about these ideas for the last three episodes, so I won&#8217;t say more just now!   Instead, I want to throw a new idea into the pot.</p>
<h3> Gibbsian statistical manifolds</h3>
<p>Thermodynamics, and statistical mechanics, spend a lot of time dealing with statistical manifold of a special sort I&#8217;ll call &#8216;Gibbsian&#8217;.  In these, each probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a &#8216;Gibbs distribution&#8217;, meaning that it maximizes entropy subject to certain constraints specified by the point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q." class="latex" /></p>
<p>How does this work?  For starters, an integrable function</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%5Ccolon+%5COmega+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;colon &#92;Omega &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is called a <b>random variable</b>, or in physics an <b>observable</b>.  The <b>expected value</b> of an observable is a smooth real-valued function on our statistical manifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+A+%5Crangle%28q%29+%3D+%5Cint_%5COmega+A%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle A &#92;rangle(q) = &#92;int_&#92;Omega A(x) &#92;pi_q(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>In other words, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle" class="latex" /> is a function whose value at at any point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> is the expected value of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> with respect to the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>Now, suppose our statistical manifold is <i>n</i>-dimensional and we have <i>n</i> observables <img src="https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots%2C+A_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_1, &#92;dots, A_n." class="latex" />   Their expected values will be smooth functions on our manifold&#8212;and sometimes these functions will be a coordinate system!</p>
<p>This may sound rather unlikely, but it&#8217;s really not so outlandish.  Indeed, if there&#8217;s a point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> such that the differentials of the functions <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle" class="latex" /> are linearly independent at this point, these functions will be a coordinate system in some neighborhood of this point, by the <a href="https://en.wikipedia.org/wiki/Inverse_function_theorem">inverse function theorem</a>.  So, we can take this neighborhood, use <i>it</i> as our statistical manifold, and the functions <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle" class="latex" /> will be coordinates.</p>
<p>So, let&#8217;s assume the expected values of our observables give a coordinate system on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   Let&#8217;s call these coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n," class="latex" /> so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle%28q%29+%3D+q_i++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle(q) = q_i  " class="latex" /></p>
<p>Now for the kicker: we say our statistical manifold is <b>Gibbsian</b> if for each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the probability distribution that <i>maximizes entropy subject to the above condition!</i></p>
<p>Which condition?   The condition saying that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+A_i%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega A_i(x) &#92;pi_q(x) &#92;, d&#92;mu(x) = q_i } " class="latex" /></p>
<p>for all <i>i</i>.   This is just the previous equation spelled out so that you can see it&#8217;s a condition on <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>This assumption of the entropy-maximizing nature of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a very powerful, because it implies a useful and nontrivial formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" />   It&#8217;s called the <b><a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs distribution</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cpi_q%28x%29+%3D+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;pi_q(x) = &#92;frac{1}{Z(q)} &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x)&#92;right) }" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the intensive variable conjugate to <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> while <img src="https://s0.wp.com/latex.php?latex=Z%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(q)" class="latex" /> is the <b>partition function</b>: the thing we must divide by to make sure <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> integrates to 1.  In other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Z%28q%29+%3D+%5Cint_%5COmega+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29+%5Cright%29+%5C%2C+d%5Cmu%28x%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Z(q) = &#92;int_&#92;Omega &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x) &#92;right) &#92;, d&#92;mu(x)   } " class="latex" /></p>
<p>By the way, this formula may look confusing at first, since the left side depends on the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in our statistical manifold, while there&#8217;s no <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> visible in the right side!  Do you see what&#8217;s going on?</p>
<p>I&#8217;ll tell you: the conjugate variable <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> sitting on the right side of the above formula, depends on <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   Remember, we got it by taking the partial derivative of the entropy in the <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> direction</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>and the evaluating this derivative at the point <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>But wait a minute!  <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> here is the entropy&#8212;but the entropy of <i>what?</i></p>
<p>The entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> of course!</p>
<p>So there&#8217;s something circular about our formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /> To know <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> you need to know the conjugate variables <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> but to compute these you need to know the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>This is actually okay.  While circular, the formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is still <i>true</i>.  It&#8217;s harder to work with than you might hope.  But it&#8217;s still extremely useful.</p>
<p>Next time I&#8217;ll prove that this formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is true, and do a few things with it.   All this material was discovered by Gibbs in the late 1800&#8217;s, and it&#8217;s lurking any good book on statistical mechanics&#8212;but not phrased in the language of statistical manifolds.  The physics textbooks usually consider special cases, like a box of gas where:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1" class="latex" /> is energy, <img src="https://s0.wp.com/latex.php?latex=p_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1" class="latex" /> is 1/temperature.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_2" class="latex" /> is volume, <img src="https://s0.wp.com/latex.php?latex=p_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_2" class="latex" /> is &ndash;pressure/temperature.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_3" class="latex" /> is the number of particles, <img src="https://s0.wp.com/latex.php?latex=p_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_3" class="latex" /> is chemical potential / pressure.</p>
<p>While these special cases are important and interesting, I&#8217;d rather be general!</p>
<h3> Technical comments</h3>
<p>I said &#8220;Any statistical manifold comes with a bunch of interesting geometrical structures&#8221;, but in fact some conditions are required.  For example, the Fisher information metric is only well-defined and nondegenerate under some conditions on the map <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" />   For example, if <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> maps every point of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> to the same probability distribution, the Fisher information metric will <i>vanish</i>.</p>
<p>Similarly, the entropy function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is only smooth under some conditions on <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" /></p>
<p>Furthermore, the integral</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29+%5Cright%29+%5C%2C+d%5Cmu%28x%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x) &#92;right) &#92;, d&#92;mu(x)   } " class="latex" /></p>
<p>may not converge for all values of the numbers <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cdots%2C+p_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;dots, p_n." class="latex" />   But in my discussion of Gibbsian statistical manifolds, I was assuming that an entropy-maximizing probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+A_i%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega A_i(x) &#92;pi_q(x) &#92;, d&#92;mu(x) = q_i } " class="latex" /></p>
<p>actually <i>exists</i>.   In this case the probability distribution is also unique (almost everywhere).</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p> <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/#comments">16 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;20)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31491 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31491">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/" rel="bookmark">Information Geometry (Part&nbsp;19)</a></h2>
				<small>8 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Last time</a> I figured out the analogue of <i>momentum</i> in probability theory, but I didn&#8217;t say what it&#8217;s called.  Now I will tell you&#8212;thanks to some help from <a href="https://twitter.com/Abelaer/status/1423656405286916098">Abel Jansma</a> and <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/#comment-171310">Toby Bartels</a>.</p>
<p><b>SURPRISE:</b> it&#8217;s called <b>SURPRISAL!</b></p>
<p>This is a well-known concept in information theory.  It&#8217;s also called &#8216;<a href="https://en.wikipedia.org/wiki/Information_content">information content</a>&#8216;.</p>
<p>Let&#8217;s see why.  First, let&#8217;s remember the setup.   We have a manifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+q_i+%3E+0%2C+%5C%3B+%5Csum_%7Bi%3D1%7D%5En+q_i+%3D+1+%5C%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = &#92;{ q &#92;in &#92;mathbb{R}^n : &#92;; q_i &gt; 0, &#92;; &#92;sum_{i=1}^n q_i = 1 &#92;} } " class="latex" /></p>
<p>whose points <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> are nowhere vanishing probability distributions on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}." class="latex" />   We have a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>called the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>, defined by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Csum_%7Bj+%3D+1%7D%5En+q_j+%5Cln+q_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;sum_{j = 1}^n q_j &#92;ln q_j } " class="latex" /></p>
<p>For each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> we define a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+T%5E%5Cast_q+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in T^&#92;ast_q Q" class="latex" /> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q" class="latex" /></p>
<p>As mentioned <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">last time</a>, this is the analogue of momentum in probability theory.   In the second half of this post I&#8217;ll say more about exactly why.   But first let&#8217;s compute it and see what it actually equals!</p>
<p>Let&#8217;s start with a naive calculation, acting as if the probabilities <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> were a coordinate system on the manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   We get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} }" class="latex" /></p>
<p>so using the definition of the Shannon entropy we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++p_i+%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+%5Csum_%7Bj+%3D+1%7D%5En+q_j+%5Cln+q_j++%7D%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+%5Cleft%28+q_i+%5Cln+q_i+%5Cright%29+%7D+%5C%5C+%5C%5C++%26%3D%26+-%5Cln%28q_i%29+-+1++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  p_i &amp;=&amp; &#92;displaystyle{ -&#92;frac{&#92;partial}{&#92;partial q_i} &#92;sum_{j = 1}^n q_j &#92;ln q_j  }&#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;frac{&#92;partial}{&#92;partial q_i} &#92;left( q_i &#92;ln q_i &#92;right) } &#92;&#92; &#92;&#92;  &amp;=&amp; -&#92;ln(q_i) - 1  &#92;end{array}  " class="latex" /></p>
<p>Now, the quantity <img src="https://s0.wp.com/latex.php?latex=-%5Cln+q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;ln q_i" class="latex" /> is called the <b>surprisal</b> of the probability distribution at <img src="https://s0.wp.com/latex.php?latex=i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i." class="latex" />   Intuitively, it&#8217;s a measure of how surprised you should be if an event of probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> occurs.    For example, if you flip a fair coin and it lands heads up, your surprisal is ln 2.   If you flip 100 fair coins and they all land heads up, your surprisal is 100 times ln 2.</p>
<p>Of course &#8216;surprise&#8217; is a psychological term, not a term from math or physics, so we shouldn&#8217;t take it too seriously here.  We can derive the concept of surprisal from three axioms:</p>
<ol>
<li>The surprisal of an event of probability <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is some function of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />, say <img src="https://s0.wp.com/latex.php?latex=F%28q%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(q)." class="latex" /></li>
<li>The less probable an event is, the larger its surprisal is: <img src="https://s0.wp.com/latex.php?latex=q_1+%5Cle+q_2+%5Cimplies++F%28q_1%29+%5Cge+F%28q_2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1 &#92;le q_2 &#92;implies  F(q_1) &#92;ge F(q_2)." class="latex" /></li>
<li>The surprisal of two independent events is the sum of their surprisals: <img src="https://s0.wp.com/latex.php?latex=F%28q_1+q_2%29+%3D+F%28q_1%29+%2B+F%28q_2%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(q_1 q_2) = F(q_1) + F(q_2)." class="latex" /></li>
</ol>
<p>It follows from work on <a href="https://en.wikipedia.org/wiki/Cauchy%27s_functional_equation">Cauchy&#8217;s functional equation</a> that <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F" class="latex" /> must be of this form:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28q%29+%3D+-+%5Clog_b+q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="F(q) = - &#92;log_b q " class="latex" /></p>
<p>for some constant <img src="https://s0.wp.com/latex.php?latex=b+%3E+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b &gt; 1." class="latex" />   We shall choose <img src="https://s0.wp.com/latex.php?latex=b%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="b," class="latex" /> the base of our logarithms, to be <img src="https://s0.wp.com/latex.php?latex=e.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e." class="latex" />   We had a similar freedom of choice in defining the Shannon entropy, and we will use base <img src="https://s0.wp.com/latex.php?latex=e&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="e" class="latex" /> for both to be consistent.  If we chose something else, it would change the surprisal and the Shannon entropy by the same constant factor.</p>
<p>So far, so good.  But what about the irksome &#8220;-1&#8221; in our formula?</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+-%5Cln%28q_i%29+-+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = -&#92;ln(q_i) - 1 " class="latex" /></p>
<p>Luckily it turns out we can just get rid of this!   The reason is that the probabilities <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are not really coordinates on the manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />  They&#8217;re not independent: they must sum to 1.  So, when we change them a little, the sum of their changes must vanish.  Putting it more technically, the tangent space <img src="https://s0.wp.com/latex.php?latex=T_q+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_q Q" class="latex" /> is not all of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n," class="latex" /> but just the subspace consisting of vectors whose components sum to zero:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+T_q+Q+%3D+%5C%7B+v+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+%5Csum_%7Bj+%3D+1%7D%5En+v_j+%3D+0+%5C%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ T_q Q = &#92;{ v &#92;in &#92;mathbb{R}^n : &#92;; &#92;sum_{j = 1}^n v_j = 0 &#92;} }" class="latex" /></p>
<p>The cotangent space is the dual of the tangent space.  The dual of a subspace</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%5Csubseteq+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S &#92;subseteq V" class="latex" /></p>
<p>is the quotient space</p>
<p><img src="https://s0.wp.com/latex.php?latex=V%5E%5Cast%2F%5C%7B+%5Cell+%5Ccolon+V+%5Cto+%5Cmathbb%7BR%7D+%3A+%5C%3B+%5Cforall+v+%5Cin+S+%5C%3B+%5C%2C+%5Cell%28v%29+%3D+0+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="V^&#92;ast/&#92;{ &#92;ell &#92;colon V &#92;to &#92;mathbb{R} : &#92;; &#92;forall v &#92;in S &#92;; &#92;, &#92;ell(v) = 0 &#92;} " class="latex" /></p>
<p>The cotangent space <img src="https://s0.wp.com/latex.php?latex=T_q%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_q^&#92;ast Q" class="latex" /> thus consists of linear functionals <img src="https://s0.wp.com/latex.php?latex=%5Cell+%5Ccolon+%5Cmathbb%7BR%7D%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell &#92;colon &#92;mathbb{R}^n &#92;to &#92;mathbb{R}" class="latex" /> modulo those that vanish on vectors <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> obeying the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bj+%3D+1%7D%5En+v_j+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{j = 1}^n v_j = 0 } " class="latex" /></p>
<p>Of course, we can identify the dual of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> in the usual way, using the Euclidean inner product: a vector <img src="https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="u &#92;in &#92;mathbb{R}^n" class="latex" /> corresponds to the linear functional</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cell%28v%29+%3D+%5Csum_%7Bj+%3D+1%7D%5En+u_j+v_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;ell(v) = &#92;sum_{j = 1}^n u_j v_j } " class="latex" /></p>
<p>From this, you can see that a linear functional <img src="https://s0.wp.com/latex.php?latex=%5Cell&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;ell" class="latex" /> vanishes on all vectors <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> obeying the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bj+%3D+1%7D%5En+v_j+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{j = 1}^n v_j = 0 } " class="latex" /></p>
<p>if and only if its corresponding vector <img src="https://s0.wp.com/latex.php?latex=u&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="u" class="latex" /> has</p>
<p><img src="https://s0.wp.com/latex.php?latex=u_1+%3D+%5Ccdots+%3D+u_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="u_1 = &#92;cdots = u_n " class="latex" /></p>
<p>So, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast_q+Q+%5Ccong+%5Cmathbb%7BR%7D%5En%2F%5C%7B+u+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+u_1+%3D+%5Ccdots+%3D+u_n+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast_q Q &#92;cong &#92;mathbb{R}^n/&#92;{ u &#92;in &#92;mathbb{R}^n : &#92;; u_1 = &#92;cdots = u_n &#92;}" class="latex" /></p>
<p>In words: we can describe cotangent vectors to <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> as lists of <i>n</i> numbers if we want, <i>but</i> we have to remember that adding the same constant to each number in the list doesn&#8217;t change the cotangent vector!</p>
<p>This suggests that our naive formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cln%28q_i%29+-+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;ln(q_i) - 1 " class="latex" /></p>
<p>is on the right track, but we&#8217;re free to get rid of the constant 1 if we want!   And that&#8217;s true.</p>
<p>To check this rigorously, we need to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p%28v%29+%3D+-%5Csum_%7Bj%3D1%7D%5En+%5Cln%28q_i%29+v_i%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p(v) = -&#92;sum_{j=1}^n &#92;ln(q_i) v_i} " class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+T_q+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v &#92;in T_q Q." class="latex" />   We compute:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++p%28v%29+%26%3D%26+df%28v%29+%5C%5C+%5C%5C++%26%3D%26+v%28f%29+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7Bj%3D1%7D%5En+v_j+%5C%2C+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_j%7D+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Csum_%7Bj%3D1%7D%5En+v_j+%28-%5Cln%28q_i%29+-+1%29+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+-%5Csum_%7Bj%3D1%7D%5En+%5Cln%28q_i%29+v_i+%7D++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  p(v) &amp;=&amp; df(v) &#92;&#92; &#92;&#92;  &amp;=&amp; v(f) &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_{j=1}^n v_j &#92;, &#92;frac{&#92;partial f}{&#92;partial q_j} } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;sum_{j=1}^n v_j (-&#92;ln(q_i) - 1) } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ -&#92;sum_{j=1}^n &#92;ln(q_i) v_i }  &#92;end{array}  " class="latex" /></p>
<p>where in the second to last step we used our earlier calculation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_i%7D+%5Csum_%7Bj+%3D+1%7D%5En+q_j+%5Cln+q_j+%3D+-%5Cln%28q_i%29+-+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;partial f}{&#92;partial q_i} = -&#92;frac{&#92;partial}{&#92;partial q_i} &#92;sum_{j = 1}^n q_j &#92;ln q_j = -&#92;ln(q_i) - 1 } " class="latex" /></p>
<p>and in the last step we used</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_j+v_j+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_j v_j = 0 } " class="latex" /></p>
<h3> Back to the big picture </h3>
<p>Now let&#8217;s take stock of where we are.   We can fill in the question marks in the charts from last time, and combine those charts while we&#8217;re at it.</p>
<div align="center">
<table border="1">
<tbody>
<tr>
<td></td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;<i>q</i> &nbsp;</td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">probabilities</td>
</tr>
<tr>
<td>&nbsp;<i>p</i>&nbsp;</td>
<td style="text-align:center;"> momentum</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">surprisals</td>
</tr>
<tr>
<td>&nbsp;<i>S</i> &nbsp;</td>
<td style="text-align:center;"> action</td>
<td style="text-align:center;">entropy</td>
<td style="text-align:center;"> Shannon entropy</td>
</tr>
</tbody>
</table>
</div>
<p>What&#8217;s going on here?   In classical mechanics, action is minimized (or at least the system finds a critical point of the action).  In thermodynamics, entropy is maximized.  In the maximum entropy approach to probability, Shannon entropy is maximized.  This leads to a mathematical analogy that&#8217;s quite precise.   For classical mechanics and thermodynamics, I explained it here:</p>
<p> <a href="https://johncarlosbaez.wordpress.com/2012/01/19/classical-mechanics-versus-thermodynamics-part-1/">Classical mechanics versus thermodynamics (part 1)</a>.</p>
<p> <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/">Classical mechanics versus thermodynamics (part 2)</a>.</p>
<p>These posts may give a more approachable introduction to what I&#8217;m doing now: now I&#8217;m bringing <i>probability theory</i> into the analogy, with a big emphasis on symplectic and contact geometry.</p>
<p>Let me spell out a bit of the analogy more carefully:</p>
<p><b>Classical Mechanics.</b>   In classical mechanics, we have a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> whose points are positions of a particle.  There&#8217;s an important function on this manifold: Hamilton&#8217;s principal function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>What&#8217;s this?   It&#8217;s basically <b>action</b>: <img src="https://s0.wp.com/latex.php?latex=f%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q)" class="latex" /> is the action of the least-action path from the position <img src="https://s0.wp.com/latex.php?latex=q_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_0" class="latex" /> at some earlier time <img src="https://s0.wp.com/latex.php?latex=t_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_0" class="latex" /> to the position <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> at time 0.   The Hamilton&#8211;Jacobi equations say the particle&#8217;s momentum <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> at time 0 is given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p><b>Thermodynamics.</b>  In thermodynamics, we have a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> whose points are equilibrium states of a system.  The coordinates of a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> are called <b>extensive variables</b>.   There&#8217;s an important function on this manifold: the <b>entropy</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>There is a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>The components of this vector are the <b>intensive variables</b> corresponding to the extensive variables.</p>
<p><b>Probability Theory.</b>  In probability theory, we have a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> whose points are nowhere vanishing probability distributions on a finite set.   The coordinates of a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> are <b>probabilities</b>.   There&#8217;s an important function on this manifold: the <b>Shannon entropy</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>There is a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>The components of this vector are the <b>surprisals</b> corresponding to the probabilities.</p>
<p>In all three cases, <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is a symplectic manifold and imposing the constraint <img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q" class="latex" /> picks out a Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B+%28q%2Cp%29+%5Cin+T%5E%5Cast+Q%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{ (q,p) &#92;in T^&#92;ast Q: &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>There is also a contact manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> where the extra dimension comes with an extra coordinate <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> that means</p>
<p> action in classical mechanics,<br />
 entropy in thermodynamics, and<br />
 Shannon entropy in probability theory.</p>
<p>We can then decree that <img src="https://s0.wp.com/latex.php?latex=S+%3D+f%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f(q)" class="latex" /> along with <img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q," class="latex" /> and these constraints pick out a Legendrian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+S+%3D+f%28q%29%2C+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{(q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; S = f(q), &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>There&#8217;s a lot more to do with these ideas, and I&#8217;ll continue next time.</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p> <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/#comments">29 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;19)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31321 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31321">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/" rel="bookmark">Information Geometry (Part&nbsp;18)</a></h2>
				<small>5 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">Last time</a> I sketched how two related forms of geometry, <a href="https://en.wikipedia.org/wiki/Symplectic_geometry">symplectic</a> and <a href="https://en.wikipedia.org/wiki/Contact_geometry">contact</a> geometry, show up in thermodynamics.  Today I want to explain how they show up in probability theory.</p>
<p>For some reason I haven&#8217;t seen much discussion of this!  But people should have looked into this.  After all, statistical mechanics explains thermodynamics in terms of probability theory, so if some mathematical structure shows up in thermodynamics it should appear in statistical mechanics&#8230; and thus ultimately in probability theory.</p>
<p>I just figured out how this works for symplectic and contact geometry.</p>
<p>Suppose a system has <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> possible states.  We&#8217;ll call these <b><a href="https://en.wikipedia.org/wiki/Microstate_(statistical_mechanics)">microstates</a></b>, following the tradition in statistical mechanics.   If you don&#8217;t know what &#8216;microstate&#8217; means, don&#8217;t worry about it!  But the rough idea is that if you have a macroscopic system like a rock, the precise details of what its atoms are doing are described by a microstate, and many different microstates could be indistinguishable unless you look very carefully.</p>
<p>We&#8217;ll call the microstates <img src="https://s0.wp.com/latex.php?latex=1%2C+2%2C+%5Cdots%2C+n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1, 2, &#92;dots, n." class="latex" />  So, if you don&#8217;t want to think about physics, when I say <b>microstate</b> I&#8217;ll just mean an integer from 1 to <em>n</em>.</p>
<p>Next, a <b>probability distribution</b> <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> assigns a real number <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> to each microstate, and these numbers must sum to 1 and be nonnegative.  So, we have <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathbb%7BR%7D%5En%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in &#92;mathbb{R}^n," class="latex" /> though not every vector in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> is a probability distribution.</p>
<p>I&#8217;m sure you&#8217;re wondering why I&#8217;m using <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> rather than <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> to stand for an observable instead of a probability distribution.   Am I just trying to confuse you?</p>
<p>No: I&#8217;m trying to set up an analogy to physics!</p>
<p><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">Last time</a> I introduced symplectic geometry using classical mechanics.  The most important example of a symplectic manifold is the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> of a manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   A point of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is a pair <img src="https://s0.wp.com/latex.php?latex=%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q,p)" class="latex" /> consisting of a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> and a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+T%5E%5Cast_q+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in T^&#92;ast_q Q." class="latex" />  In classical mechanics the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> describes the position of some physical system, while <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> describes its momentum.</p>
<p>So, I&#8217;m going to set up an analogy like this:</p>
<div align="center">
<table border="1">
<tr>
<td> </td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;"><b>Probability Theory</b></td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q " class="latex" /> &nbsp; </td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">&nbsp; probability distribution &nbsp; </td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> &nbsp;</td>
<td style="text-align:center;">momentum</td>
<td style="text-align:center;">???</td>
</tr>
</table>
</div>
<p>But <i>what is to momentum as probability is to position?</i></p>
<p>A big clue is the appearance of symplectic geometry in thermodynamics, which I also outlined <a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/">last time</a>.    We can use this to get some intuition about the analogue of momentum in probability theory.</p>
<p>In thermodynamics, a system has a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> of states.  (These are not the &#8216;microstates&#8217; I mentioned before: we&#8217;ll see the relation later.)  There is a function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>describing the <a href="https://en.wikipedia.org/wiki/Entropy">entropy</a> of the system as a function of its state.   There is a law of thermodynamics saying that</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>This equation picks out a submanifold of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q," class="latex" /> namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{(q,p) &#92;in T^&#92;ast Q : &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>Moreover this submanifold is Lagrangian: the symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> vanishes when restricted to it:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Comega+%7C_%5CLambda+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;omega |_&#92;Lambda = 0 } " class="latex" /></p>
<p>This is very beautiful, but it goes by so fast you might almost miss it!   So let&#8217;s clutter it up a bit with coordinates.  We often use local coordinates on <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> and describe a point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> using these coordinates, getting a point</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28q_1%2C+%5Cdots%2C+q_n%29+%5Cin+%5Cmathbb%7BR%7D%5En+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q_1, &#92;dots, q_n) &#92;in &#92;mathbb{R}^n " class="latex" /></p>
<p>They give rise to local coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n%2C+p_1%2C+%5Cdots%2C+p_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n, p_1, &#92;dots, p_n" class="latex" /> on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   The <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are called <b>extensive variables</b>, because they are typically things that you can measure only by totalling up something over the whole system, like the energy or volume of a cylinder of gas.   The <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are called <b>intensive variables</b>, because they are typically things that you can measure locally at any point, like temperature or pressure.</p>
<p>In these local coordinates, the symplectic structure on <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> is the 2-form given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+dp_1+%5Cwedge+dq_1+%2B+%5Ccdots+%2B+dp_n+%5Cwedge+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = dp_1 &#92;wedge dq_1 + &#92;cdots + dp_n &#92;wedge dq_n " class="latex" /></p>
<p>The equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q" class="latex" /></p>
<p>serves as a law of physics that determines the intensive variables given the extensive ones when our system is in thermodynamic equilibrium.   Written out using coordinates, this law says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>It looks pretty bland here, but in fact it gives formulas for the temperature and pressure of a gas, and many other useful formulas in thermodynamics.</p>
<p>Now we are ready to see how all this plays out in probability theory!  We&#8217;ll get an analogy like this, which goes hand-in-hand with our earlier one:</p>
<div align="center">
<table border="1">
<tr>
<td> </td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q " class="latex" /> &nbsp; </td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">&nbsp; probability distribution &nbsp; </td>
</tr>
<tr>
<td>&nbsp; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> &nbsp;</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">???</td>
</tr>
</table>
</div>
<p>This analogy is clearer than the last, because statistical mechanics reveals that the extensive variables in thermodynamics are really just summaries of probability distributions on microstates.  Furthermore, both thermodynamics and probability theory have a concept of <i>entropy</i>.</p>
<p>So, let&#8217;s take our manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> to consist of probability distributions on the set of microstates I was talking about before: the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}." class="latex" />   Actually, let&#8217;s use <i>nowhere vanishing</i> probability distributions:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Q+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BR%7D%5En+%3A+%5C%3B+q_i+%3E+0%2C+%5C%3B+%5Csum_%7Bi%3D1%7D%5En+q_i+%3D+1+%5C%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Q = &#92;{ q &#92;in &#92;mathbb{R}^n : &#92;; q_i &gt; 0, &#92;; &#92;sum_{i=1}^n q_i = 1 &#92;} } " class="latex" /></p>
<p>I&#8217;m requiring <img src="https://s0.wp.com/latex.php?latex=q_i+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i &gt; 0" class="latex" /> to ensure <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a manifold, and also to make sure <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is differentiable: it ceases to be differentiable when one of the probabilities <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> hits zero.</p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a manifold, its cotangent bundle is a symplectic manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   And here&#8217;s the good news: we have a god-given entropy function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>namely the <b><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a></b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Csum_%7Bi+%3D+1%7D%5En+q_i+%5Cln+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;sum_{i = 1}^n q_i &#92;ln q_i } " class="latex" /></p>
<p>So, everything I just described about thermodynamics works in the setting of plain old probability theory!  Starting from our manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> and the entropy function, we get all the rest, leading up to the Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{(q,p) &#92;in T^&#92;ast Q : &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>that describes the relation between extensive and intensive variables.</p>
<p>For computations it helps to pick coordinates on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   Since the probabilities <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> sum to 1, they aren&#8217;t independent coordinates on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   So, we can either pick all but one of them as coordinates, or learn how to deal with non-independent coordinates, which are already completely standard in <a href="https://en.wikipedia.org/wiki/Homogeneous_coordinates">projective geometry</a>.  Let&#8217;s do the former, just to keep things simple.</p>
<p>These coordinates on <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> give rise in the usual way to coordinates <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   These play the role of extensive and intensive variables, respectively, and it should be very interesting to impose the equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is the Shannon entropy.   This picks out a Lagrangian submanifold <img src="https://s0.wp.com/latex.php?latex=%5CLambda+%5Csubseteq+T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda &#92;subseteq T^&#92;ast Q." class="latex" /></p>
<p>So, the question becomes: what does this mean?  If this formula gives the analogue of momentum for probability theory, what does this analogue of momentum <i>mean?</i></p>
<p>Here&#8217;s a preliminary answer: <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how fast entropy increases as we increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> that our system is in the <i>i</i>th microstate.  So if we think of nature as &#8216;wanting&#8217; to maximize entropy, the quantity <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how eager it is to increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" /></p>
<p>Indeed, you can think of <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> as a bit like <i>pressure</i>&#8212;one of the most famous intensive quantities in thermodynamics.   A gas &#8216;wants&#8217; to expand, and its pressure says precisely how eager it is to expand.   Similarly, a probability distribution &#8216;wants&#8217; to flatten out, to maximize entropy, and <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> says how eager it is to increase the probability <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> in order to do this.</p>
<p>But what can we <i>do</i> with this concept?  And what does symplectic geometry <i>do</i> for probability theory?</p>
<p>I will start tackling these questions next time.</p>
<p>One thing I&#8217;ll show is that when we reduce thermodynamics to probability theory using the ideas of statistical mechanics, the appearance of symplectic geometry in thermodynamics <i>follows</i> from its appearance in probability theory.</p>
<p>Another thing I want to investigate is how other geometrical structures on the space of probability distributions, like the <a href="https://math.ucr.edu/home/baez/information/information_geometry_1.html">Fisher information metric</a>, interact with the symplectic structure on its cotangent bundle.   This will integrate symplectic geometry and information geometry.</p>
<p>I also want to bring contact geometry into the picture.  It&#8217;s already easy to see from our work last time how this should go.  We treat the entropy <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as an independent variable, and replace <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> with a larger manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> having <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as an extra coordinate.  This is a contact manifold with contact form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_1+dq_i+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_1 dq_i + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>This contact manifold has a submanifold <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> where we remember that entropy is a function of the probability distribution <img src="https://s0.wp.com/latex.php?latex=q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q," class="latex" /> and define <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in terms of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> as usual:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+S+%3D+f%28q%29%2C+%5C%3B+p+%3D+%28df%29_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{(q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; S = f(q), &#92;; p = (df)_q &#92;} " class="latex" /></p>
<p>And as we saw last time, <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> is a Legendrian submanifold, meaning</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Calpha%7C_%7B%5CSigma%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;alpha|_{&#92;Sigma} = 0 } " class="latex" /></p>
<p>But again, we want to understand what these ideas from contact geometry really <i>do</i> for probability theory!</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p> <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/#comments">2 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;18)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31330 post type-post status-publish format-standard hentry category-information-and-entropy category-physics category-probability" id="post-31330">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/" rel="bookmark">Information Geometry (Part&nbsp;17)</a></h2>
				<small>27 July, 2021</small><br />


				<div class="entry">
					<p>I&#8217;m getting back into information geometry, which is the geometry of the space of probability distributions, studied using tools from information theory.  I&#8217;ve written a bunch about it already, which you can see here:</p>
<p> <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
<p>Now I&#8217;m fascinated by something new: how <a href="https://en.wikipedia.org/wiki/Symplectic_geometry">symplectic geometry</a> and <a href="https://en.wikipedia.org/wiki/Contact_geometry">contact geometry</a> show up in information geometry.   But before I say anything about this, let me say a bit about how they show up in thermodynamics.  This is more widely discussed, and it&#8217;s a good starting point.</p>
<p>Symplectic geometry was born as the geometry of <a href="https://en.wikipedia.org/wiki/Phase_space">phase space</a> in classical mechanics: that is, the space of possible positions and momenta of a classical system.  The simplest example of a symplectic manifold is the vector space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n}," class="latex" /> with <i>n</i> position coordinates <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <i>n</i> momentum coordinates <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" /></p>
<p>It turns out that symplectic manifolds are always even-dimensional, because we can always cover them with coordinate charts that look like <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n}." class="latex" />  When we change coordinates, it turns out that the splitting of coordinates into positions and momenta is somewhat arbitrary.  For example, the position of a rock on a spring now may determine its momentum a while later, and vice versa.   What&#8217;s not arbitrary?  It&#8217;s the so-called &#8216;<a href="https://en.wikipedia.org/wiki/Symplectic_manifold#Definition">symplectic structure</a>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+dp_1+%5Cwedge+dq_1+%2B+%5Ccdots+%2B+dp_n+%5Cwedge+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = dp_1 &#92;wedge dq_1 + &#92;cdots + dp_n &#92;wedge dq_n " class="latex" /></p>
<p>While far from obvious at first, we know by now that the symplectic structure is exactly what needs to be preserved under valid changes of coordinates in classical mechanics!  In fact, we can develop the whole formalism of classical mechanics starting from a manifold with a symplectic structure.</p>
<p>Symplectic geometry also shows up in thermodynamics.  In thermodynamics we can start with a system in equilibrium whose state is described by some variables <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n." class="latex" />  Its entropy will be a function of these variables, say</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+f%28q_1%2C+%5Cdots%2C+q_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f(q_1, &#92;dots, q_n)" class="latex" /></p>
<p>We can then take the partial derivatives of entropy and call them something:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>These new variables <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are said to be &#8216;<a href="https://en.wikipedia.org/wiki/Conjugate_variables_(thermodynamics)">conjugate</a> to the <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> and they turn out to be very interesting.  For example, if <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> is energy then <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is &#8216;coolness&#8217;: the reciprocal of temperature.  The coolness of a system is its change in entropy per change in energy.</p>
<p>Often the variables <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> are &#8216;<a href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties#Extensive_properties">extensive</a>: that is, you can measure them only by looking at your whole system and totaling up some quantity.  Examples are energy and volume.  Then the new variables <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> are &#8216;<a href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties#Intensive_properties">intensive</a>: that is, you can measure them at any one location in your system.   Examples are coolness and pressure.</p>
<p>Now for a twist: sometimes we do not know the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> ahead of time.   Then we cannot define the <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> as above.  We&#8217;re forced into a different approach where we treat them as independent quantities, at least until someone tells us what <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is.</p>
<p>In this approach, we start with a space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n}" class="latex" /> having <i>n</i> coordinates called <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <i>n</i> coordinates called <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />   This is a symplectic manifold, with the symplectic struture <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> described earlier!</p>
<p>But what about the entropy?   We don&#8217;t yet know what it is as a function of the <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> but we may still want to talk about it.  So, we build a space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}" class="latex" /> having one extra coordinate <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> in addition to the <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i." class="latex" />  This new coordinate stands for entropy.  And this new space has an important 1-form on it:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_1+dq_i+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_1 dq_i + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>This is called the &#8216;contact 1-form&#8217;.</p>
<p>This makes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}" class="latex" /> into an example of a &#8216;contact manifold&#8217;.  Contact geometry is the odd-dimensional partner of symplectic geometry.  Just as symplectic manifolds are always even-dimensional, contact manifolds are always odd-dimensional.</p>
<p>What is the point of the contact 1-form?  Well, suppose someone tells us the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> relating entropy to the coordinates <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   Now we know that we want</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f" class="latex" /></p>
<p>and also</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} }" class="latex" /></p>
<p>So, we can impose these equations, which pick out a subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}." class="latex" />   You can check that this subset, say <img src="https://s0.wp.com/latex.php?latex=%5CSigma%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma," class="latex" /> is an <i>n</i>-dimensional submanifold.  But even better, the contact 1-form vanishes when restricted to this submanifold:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.%5Calpha%5Cright%7C_%5CSigma+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left.&#92;alpha&#92;right|_&#92;Sigma = 0 " class="latex" /></p>
<p>Let&#8217;s see why!   Suppose <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Sigma" class="latex" /> and suppose <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+T_x+%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v &#92;in T_x &#92;Sigma" class="latex" /> is a vector tangent to <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> at this point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" />.  It suffices to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha%28v%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha(v) = 0" class="latex" /></p>
<p>Using the definition of <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> this equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-dS%28v%29+%2B+%5Csum_i+p_i+dq_i%28v%29+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -dS(v) + &#92;sum_i p_i dq_i(v) = 0 } " class="latex" /></p>
<p>But on the surface <img src="https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma" class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=S+%3D+f%2C+%5Cqquad++%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f, &#92;qquad  &#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} }" class="latex" /></p>
<p>So, the equation we&#8217;re trying to show can be written as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-df%28v%29+%2B+%5Csum_i+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+dq_i%28v%29+%3D+0+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -df(v) + &#92;sum_i &#92;frac{&#92;partial f}{&#92;partial q_i} dq_i(v) = 0 }" class="latex" /></p>
<p>But this follows from</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+df+%3D+%5Csum_i+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+dq_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ df = &#92;sum_i &#92;frac{&#92;partial f}{&#92;partial q_i} dq_i } " class="latex" /></p>
<p>which holds because <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is a function only of the coordinates <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" /></p>
<p>So, any formula for entropy <img src="https://s0.wp.com/latex.php?latex=S+%3D+f%28q_1%2C+%5Cdots%2C+q_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S = f(q_1, &#92;dots, q_n)" class="latex" /> picks out a so-called &#8216;Legendrian submanifold&#8217; of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2n%2B1%7D%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{2n+1}:" class="latex" /> that is, an <i>n</i>-dimensional submanifold such that the contact 1-form vanishes when restricted to this submanifold.  And the idea is that this submanifold tells you everything you need to know about a thermodynamic system.</p>
<p>Indeed, V. I. Arnol&#8217;d says this was implicitly known to the great founder of statistical mechanics, Josiah Willard Gibbs.  Arnol&#8217;d calls <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E5&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^5" class="latex" /> with coordinates energy, entropy, temperature, pressure and volume the &#8216;Gibbs manifold&#8217;, and he proclaims:</p>
<blockquote><p>
<b>Gibbs&#8217; thesis</b>: substances are Legendrian submanifolds of the Gibbs manifold.</p></blockquote>
<p>This is from here:</p>
<p> V. I. Arnol&#8217;d, Contact geometry: the geometrical method of Gibbs&#8217; thermodynamics, <i>Proceedings of the Gibbs Symposium (New Haven, CT, 1989)</i>, AMS, Providence, Rhode Island, 1990.</p>
<h3> A bit more detail</h3>
<p>Now I want to say everything again, with a bit of extra detail, assuming more familiarity with manifolds.  Above I was using <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> with coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> to describe the &#8216;extensive&#8217; variables of a thermodynamic system.  But let&#8217;s be a bit more general and use any smooth <i>n</i>-dimensional manifold <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />  Even if <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> is a vector space, this viewpoint is nice because it&#8217;s manifestly coordinate-independent!</p>
<p>So: starting from <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> we build the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   A point in cotangent describes both extensive variables, namely <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> and &#8216;intensive&#8217; variables, namely a cotangent vector <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+T%5E%5Cast_q+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p &#92;in T^&#92;ast_q Q." class="latex" /></p>
<p>The manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> has a 1-form <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta" class="latex" /> on it called the <a href="https://en.wikipedia.org/wiki/Tautological_one-form">tautological 1-form</a>.  We can describe it as follows.  Given a tangent vector <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+T_%7B%28q%2Cp%29%7D+T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v &#92;in T_{(q,p)} T^&#92;ast Q" class="latex" /> we have to say what <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%28v%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta(v)" class="latex" /> is.   Using the projection</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi+%5Ccolon+T%5E%5Cast+Q+%5Cto+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi &#92;colon T^&#92;ast Q &#92;to Q" class="latex" /></p>
<p>we can project <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v" class="latex" /> down to a tangent vector <img src="https://s0.wp.com/latex.php?latex=d%5Cpi%28v%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="d&#92;pi(v)" class="latex" /> at the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" />.  But the 1-form <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> eats tangent vectors at <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and spits out numbers!  So, we set</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctheta%28v%29+%3D+p%28d%5Cpi%28v%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta(v) = p(d&#92;pi(v))" class="latex" /></p>
<p>This is sort of mind-boggling at first, but it&#8217;s worth pondering until it makes sense.  It helps to work out what <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta" class="latex" /> looks like in local coordinates.  Starting with any local coordinates <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> on an open set of <img src="https://s0.wp.com/latex.php?latex=Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q," class="latex" /> we get local coordinates <img src="https://s0.wp.com/latex.php?latex=q_i%2C+p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i, p_i" class="latex" /> on the cotangent bundle of this open set in the usual way.  On this open set you then get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%3D+p_1+dq_1+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta = p_1 dq_1 + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>This is a standard calculation, which is really worth doing!</p>
<p>It follows that we can define a symplectic structure <img src="https://s0.wp.com/latex.php?latex=%5Comega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega" class="latex" /> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+d+%5Ctheta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = d &#92;theta " class="latex" /></p>
<p>and get this formula in local coordinates:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+dp_1+%5Cwedge+dq_1+%2B+%5Ccdots+%2B+dp_n+%5Cwedge+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;omega = dp_1 &#92;wedge dq_1 + &#92;cdots + dp_n &#92;wedge dq_n " class="latex" /></p>
<p>Now, suppose we choose a smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>which describes the entropy.   We get a 1-form <img src="https://s0.wp.com/latex.php?latex=df&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df" class="latex" />, which we can think of as a map</p>
<p><img src="https://s0.wp.com/latex.php?latex=df+%5Ccolon+Q+%5Cto+T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df &#92;colon Q &#92;to T^&#92;ast Q" class="latex" /></p>
<p>assigning to each choice <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> of extensive variables the pair <img src="https://s0.wp.com/latex.php?latex=%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(q,p)" class="latex" /> of extensive and intensive variables where</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+df_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = df_q " class="latex" /></p>
<p>The image of the map <img src="https://s0.wp.com/latex.php?latex=df&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df" class="latex" /> is a &#8216;<a href="https://en.wikipedia.org/wiki/Symplectic_manifold#Lagrangian_and_other_submanifolds">Lagrangian submanifold</a>&#8216; of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q:" class="latex" /> that is, an <i>n</i>-dimensional submanifold <img src="https://s0.wp.com/latex.php?latex=%5CLambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft.%5Comega%5Cright%7C_%7B%5CLambda%7D+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;left.&#92;omega&#92;right|_{&#92;Lambda} = 0" class="latex" /></p>
<p>Lagrangian submanifolds are to symplectic geometry as Legendrian submanifolds are to contact geometry!  What we&#8217;re seeing here is that if Gibbs had preferred symplectic geometry, he could have described substances as Lagrangian submanifolds rather than Legendrian submanifolds.  But this approach would only keep track of the derivatives of entropy, <img src="https://s0.wp.com/latex.php?latex=df%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="df," class="latex" /> not the actual value of the entropy function <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>If we prefer to keep track of the actual value of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> using contact geometry, we can do that.   For this we add an extra dimension to <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q" class="latex" /> and form the manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}." class="latex" />  The extra dimension represents entropy, so we&#8217;ll use <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> as our name for the coordinate on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}." class="latex" /></p>
<p>We can make <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> into a contact manifold with contact 1-form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-d+S+%2B+%5Ctheta+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -d S + &#92;theta " class="latex" /></p>
<p>In local coordinates we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_1+dq_i+%2B+%5Ccdots+%2B+p_n+dq_n+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_1 dq_i + &#92;cdots + p_n dq_n " class="latex" /></p>
<p>just as we had earlier.   And just as before, if we choose a smooth function <img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /> describing entropy, the subset</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+S+%3D+f%28q%29%2C+%5C%3B+p+%3D+df_q+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{(q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; S = f(q), &#92;; p = df_q &#92;} " class="latex" /></p>
<p>is a Legendrian submanifold of <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}." class="latex" /></p>
<p>Okay, this concludes my lightning review of symplectic and contact geometry in thermodynamics!  Next time I&#8217;ll talk about something a bit less well understood: how they show up in statistical mechanics.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/#comments">10 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/27/information-geometry-part-17/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;17)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31205 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-probability" id="post-31205">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/" rel="bookmark">Fisher&#8217;s Fundamental Theorem (Part&nbsp;4)</a></h2>
				<small>13 July, 2021</small><br />


				<div class="entry">
					<p>I wrote a paper that summarizes my work connecting natural selection to information theory:</p>
<p> John Baez, <a href="https://arxiv.org/abs/2107.05610">The fundamental theorem of natural selection</a>.</p>
<p>Check it out! If you have any questions or see any mistakes, please let me know.</p>
<p>Just for fun, here&#8217;s the abstract and introduction.</p>
<p><strong>Abstract.</strong> Suppose we have <i>n</i> different types of self-replicating entity, with the population <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> of the <i>i</i>th type changing at a rate equal to <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> times the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> of that type. Suppose the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is any continuous function of all the populations <img src="https://s0.wp.com/latex.php?latex=P_1%2C+%5Cdots%2C+P_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_1, &#92;dots, P_n" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> be the fraction of replicators that are of the <i>i</i>th type. Then <img src="https://s0.wp.com/latex.php?latex=p+%3D+%28p_1%2C+%5Cdots%2C+p_n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (p_1, &#92;dots, p_n)" class="latex" /> is a time-dependent probability distribution, and we prove that its speed as measured by the Fisher information metric equals the variance in fitness. In rough terms, this says that the speed at which information is updated through natural selection equals the variance in fitness. This result can be seen as a modified version of Fisher&#8217;s fundamental theorem of natural selection. We compare it to Fisher&#8217;s original result as interpreted by Price, Ewens and Edwards.</p>
<h4>Introduction</h4>
<p>In 1930, Fisher stated his &#8220;fundamental theorem of natural selection&#8221; as follows:</p>
<blockquote><p>The rate of increase in fitness of any organism at any time is equal to its genetic variance in fitness at that time</p></blockquote>
<p>Some tried to make this statement precise as follows:</p>
<blockquote><p>The time derivative of the mean fitness of a population equals the variance of its fitness.</p></blockquote>
<p>But this is only true under very restrictive conditions, so a controversy was ignited.</p>
<p>An interesting resolution was proposed by Price, and later amplified by Ewens and Edwards. We can formalize their idea as follows. Suppose we have <i>n</i> types of self-replicating entity, and idealize the population of the <i>i</i>th type as a real-valued function <img src="https://s0.wp.com/latex.php?latex=P_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i(t)" class="latex" />. Suppose</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+P_i%28t%29+%3D+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%2C+P_i%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} P_i(t) = f_i(P_1(t), &#92;dots, P_n(t)) &#92;, P_i(t) } " class="latex" /></p>
<p>where the fitness <img src="https://s0.wp.com/latex.php?latex=f_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i" class="latex" /> is a differentiable function of the populations of every type of replicator. The mean fitness at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t" class="latex" /> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Coverline%7Bf%7D%28t%29+%3D+%5Csum_%7Bi%3D1%7D%5En+p_i%28t%29+%5C%2C+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;overline{f}(t) = &#92;sum_{i=1}^n p_i(t) &#92;, f_i(P_1(t), &#92;dots, P_n(t)) } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t)" class="latex" /> is the fraction of replicators of the <i>i</i>th type:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Cphantom%7B%5CBig%7C%7D+%5Csum_%7Bj+%3D+1%7D%5En+P_j%28t%29+%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i(t) = &#92;frac{P_i(t)}{&#92;phantom{&#92;Big|} &#92;sum_{j = 1}^n P_j(t) } } " class="latex" /></p>
<p>By the product rule, the rate of change of the mean fitness is the sum of two terms:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+%5Coverline%7Bf%7D%28t%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cdot%7Bp%7D_i%28t%29+%5C%2C+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%5C%3B+%2B+%5C%3B+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} &#92;overline{f}(t) = &#92;sum_{i=1}^n &#92;dot{p}_i(t) &#92;, f_i(P_1(t), &#92;dots, P_n(t)) &#92;; + &#92;; }" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+p_i%28t%29+%5C%2C%5Cfrac%7Bd%7D%7Bdt%7D+f_i%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sum_{i=1}^n p_i(t) &#92;,&#92;frac{d}{dt} f_i(P_1(t), &#92;dots, P_n(t)) } " class="latex" /></p>
<p>The <i>first</i> of these two terms equals the variance of the fitness at time <i>t</i>. We give the easy proof in Theorem 1. Unfortunately, the conceptual significance of this first term is much less clear than that of the total rate of change of mean fitness. Ewens concluded that &#8220;the theorem does not provide the substantial biological statement that Fisher claimed&#8221;.</p>
<p>But there is another way out, based on an idea Fisher himself introduced in 1922: Fisher information. Fisher information gives rise to a Riemannian metric on the space of probability distributions on a finite set, called the &#8216;Fisher information metric&#8217;&#8212;or in the context of evolutionary game theory, the &#8216;Shahshahani metric&#8217;. Using this metric we can define the speed at which a time-dependent probability distribution changes with time. We call this its &#8216;Fisher speed&#8217;. Under just the assumptions already stated, we prove in Theorem 2 that the Fisher speed of the probability distribution</p>
<p><img src="https://s0.wp.com/latex.php?latex=p%28t%29+%3D+%28p_1%28t%29%2C+%5Cdots%2C+p_n%28t%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t) = (p_1(t), &#92;dots, p_n(t)) " class="latex" /></p>
<p>is the variance of the fitness at time <i>t</i>.</p>
<p>As explained by Harper, natural selection can be thought of as a learning process, and studied using ideas from information geometry&#8212;that is, the geometry of the space of probability distributions. As <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> changes with time, the rate at which information is updated is closely connected to its Fisher speed. Thus, our revised version of the fundamental theorem of natural selection can be loosely stated as follows:</p>
<blockquote><p>As a population changes with time, the rate at which information is updated equals the variance of fitness.</p></blockquote>
<p>The precise statement, with all the hypotheses, is in Theorem 2. But one lesson is this: variance in fitness may not cause &#8216;progress&#8217; in the sense of increased mean fitness, but it does cause change!</p>
<p>For more details in a user-friendly blog format, read the whole series:</p>
<p> <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-1/">Part 1</a>: the obscurity of Fisher&#8217;s original paper.</p>
<p> <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Part 2</a>: a precise statement of Fisher&#8217;s fundamental theorem of natural selection, and conditions under which it holds.</p>
<p> <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/">Part 3</a>: a modified version of the fundamental theorem of natural selection, which holds much more generally.</p>
<p> <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/">Part 4</a>: my paper on the fundamental theorem of natural selection.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/#comments">6 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/" rel="bookmark" title="Permanent Link to Fisher&#8217;s Fundamental Theorem (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31143 post type-post status-publish format-standard hentry category-biology category-chemistry category-information-and-entropy category-physics" id="post-31143">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/06/16/nonequilibrium-thermodynamics-in-biology-part-2/" rel="bookmark">Nonequilibrium Thermodynamics in Biology (Part&nbsp;2)</a></h2>
				<small>16 June, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg"><img loading="lazy" data-attachment-id="30978" data-permalink="https://johncarlosbaez.wordpress.com/2021/05/11/non-equilibrium-thermodynamics-in-biology/attachment/30978/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg" data-orig-size="3761,1577" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450" src="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450&#038;h=189" alt="" class="aligncenter size-full wp-image-30978" width="450" height="189" srcset="https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=450&amp;h=189 450w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=900&amp;h=378 900w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=150&amp;h=63 150w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=300&amp;h=126 300w, https://johncarlosbaez.files.wordpress.com/2021/05/0.jpg?w=768&amp;h=322 768w" sizes="(max-width: 450px) 100vw, 450px" /></a></p>
<p><a>Larry Li</a>, <a href="https://biology.pnnl.gov/people/bill-cannon">Bill Cannon</a> and I ran a session on non-equilibrium thermodynamics in biology at <a href="https://www.smb2021.org/home">SMB2021</a>, the annual meeting of the Society for Mathematical Biology.  You can see talk slides here!</p>
<p>Here&#8217;s the basic idea:</p>
<blockquote><p>
  Since Lotka, physical scientists have argued that living things belong to a class of complex and orderly systems that exist not despite the second law of thermodynamics, but because of it. Life and evolution, through natural selection of dissipative structures, are based on non-equilibrium thermodynamics. The challenge is to develop an understanding of what the respective physical laws can tell us about flows of energy and matter in living systems, and about growth, death and selection. This session addresses current challenges including understanding emergence, regulation and control across scales, and entropy production, from metabolism in microbes to evolving ecosystems.</p></blockquote>
<p>Click on the links to see slides for most of the talks:</p>
<p> <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_craciun.pdf">Persistence, permanence, and global stability in reaction network models: some results inspired by thermodynamic principles</a></b><br />
Gheorghe Craciun, University of WisconsinMadison</p>
<blockquote><p>
The standard mathematical model for the dynamics of concentrations in biochemical networks is called mass-action kinetics. We describe mass-action kinetics and discuss the connection between special classes of mass-action systems (such as detailed balanced and complex balanced systems) and the Boltzmann equation. We also discuss the connection between the &#8216;global attractor conjecture&#8217; for complex balanced mass-action systems and Boltzmann&#8217;s H-theorem. We also describe some implications for biochemical mechanisms that implement noise filtering and cellular homeostasis.</p></blockquote>
<p> <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_dill.pdf">The  principle of maximum caliber of nonequilibria</a></b><br />
Ken Dill, Stony Brook University</p>
<blockquote><p>
Maximum Caliber is a principle for inferring pathways and rate distributions of kinetic processes. The structure and foundations of MaxCal are much like those of Maximum Entropy for static distributions. We have explored how MaxCal may serve as a general variational principle for nonequilibrium statistical physics&#8212;giving well-known results, such as the Green-Kubo relations, Onsager&#8217;s reciprocal relations and Prigogine&#8217;s Minimum Entropy Production principle near equilibrium, but is also applicable far from equilibrium. I will also discuss some applications, such as finding reaction coordinates in molecular simulations non-linear dynamics in gene circuits, power-law-tail distributions in &#8216;social-physics&#8217; networks, and others.</p></blockquote>
<p> <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_gaspard.pdf">Nonequilibrium biomolecular information processes</a></b><br />
Pierre Gaspard, Universit libre de Bruxelles</p>
<blockquote><p>
Nearly 70 years have passed since the discovery of DNA structure and its role in coding genetic information. Yet, the kinetics and thermodynamics of genetic information processing in DNA replication, transcription, and translation remain poorly understood. These template-directed copolymerization processes are running away from equilibrium, being powered by extracellular energy sources. Recent advances show that their kinetic equations can be exactly solved in terms of so-called iterated function systems. Remarkably, iterated function systems can determine the effects of genome sequence on replication errors, up to a million times faster than kinetic Monte Carlo algorithms. With these new methods, fundamental links can be established between molecular information processing and the second law of thermodynamics, shedding a new light on genetic drift, mutations, and evolution.</p></blockquote>
<p> <b> <a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_harte.pdf">Nonequilibrium dynamics of  disturbed ecosystems</a></b><br />
John Harte, University of California, Berkeley</p>
<blockquote><p>
The Maximum Entropy Theory of Ecology (METE) predicts the shapes of macroecological metrics in relatively static ecosystems, across spatial scales, taxonomic categories, and habitats, using constraints imposed by static state variables. In disturbed ecosystems, however, with time-varying state variables, its predictions often fail. We extend macroecological theory from static to dynamic, by combining the MaxEnt inference procedure with explicit mechanisms governing disturbance. In the static limit, the resulting theory, DynaMETE, reduces to METE but also predicts a new scaling relationship among static state variables. Under disturbances, expressed as shifts in demographic, ontogenic growth, or migration rates, DynaMETE predicts the time trajectories of the state variables as well as the time-varying shapes of macroecological metrics such as the species abundance distribution and the distribution of metabolic rates over<br />
individuals. An iterative procedure for solving the dynamic theory is presented. Characteristic signatures of the deviation from static predictions of macroecological patterns are shown to result from different kinds of disturbance. By combining MaxEnt inference with explicit dynamical mechanisms of disturbance, DynaMETE is a candidate theory of macroecology for ecosystems responding to anthropogenic or natural disturbances.</p></blockquote>
<p> <b> <a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_krishnamurthy.pdf">Stochastic chemical reaction networks</a> </b><br />
Supriya Krishnamurthy, Stockholm University</p>
<blockquote><p>
The study of chemical reaction networks (CRN&#8217;s) is a very active field. Earlier well-known results (Feinberg Chem. Enc. Sci. 42 2229 (1987), Anderson et al Bull. Math. Biol. 72 1947 (2010)) identify a topological quantity called deficiency, easy to compute for CRNs of any size, which, when exactly equal to zero, leads to a unique factorized (non-equilibrium) steady-state for these networks. No general results exist however for the steady states of non-zero-deficiency networks. In recent work, we show how to write the full moment-hierarchy for any non-zero-deficiency CRN obeying mass-action kinetics, in terms of equations for the factorial moments. Using these, we can recursively predict values for lower moments from higher moments, reversing the procedure usually used to solve moment hierarchies. We show, for non-trivial examples, that in this manner we can predict any moment of interest, for CRN&#8217;s with non-zero deficiency and non-factorizable steady states. It is however an open question how scalable these techniques are for large networks.</p></blockquote>
<p> <b> <a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_mast.pdf">Heat flows adjust local ion concentrations in favor of prebiotic chemistry</a></b><br />
Christof Mast, Ludwig-Maximilians-Universitt Mnchen</p>
<blockquote><p>
Prebiotic reactions often require certain initial concentrations of ions. For example, the activity of RNA enzymes requires a lot of divalent magnesium salt, whereas too much monovalent sodium salt leads to a reduction in enzyme function. However, it is known from leaching experiments that prebiotically relevant geomaterial such as basalt releases mainly a lot of sodium and only little magnesium. A natural solution to this problem is heat fluxes through thin rock fractures, through which magnesium is actively enriched and sodium is depleted by thermogravitational convection and thermophoresis. This process establishes suitable conditions for ribozyme function from a basaltic leach. It can take place in a spatially distributed system of rock cracks and is therefore particularly stable to natural fluctuations and disturbances.</p></blockquote>
<p> <b>Deficiency of chemical reaction networks and thermodynamics</b><br />
Matteo Polettini, University of Luxembourg</p>
<blockquote><p>
Deficiency is a topological property of a Chemical Reaction Network linked to important dynamical features, in particular of deterministic fixed points and of stochastic stationary states. Here we link it to thermodynamics: in particular we discuss the validity of a strong vs. weak zeroth law, the existence of time-reversed mass-action kinetics, and the possibility to formulate marginal fluctuation relations. Finally we illustrate some subtleties of the Python module we created for MCMC stochastic simulation of CRNs, soon to be made public.</p></blockquote>
<p> <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_qian.pdf">Large deviations theory and emergent landscapes in biological dynamics</a></b><br />
Hong Qian, University of Washington</p>
<blockquote><p>
The mathematical theory of large deviations provides a nonequilibrium thermodynamic description of complex biological systems that consist of heterogeneous individuals. In terms of the notions of stochastic elementary reactions and pure kinetic species, the continuous-time, integer-valued Markov process dictates a thermodynamic structure that generalizes (i) Gibbs&#8217; microscopic chemical thermodynamics of equilibrium matters to nonequilibrium small systems such as living cells and tissues; and (ii) Gibbs&#8217; potential function to the landscapes for biological dynamics, such as that of C. H. Waddington and S. Wright.</p></blockquote>
<p> <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_vallino.pdf">Using the maximum entropy production  principle to understand and predict microbial biogeochemistry</a></b><br />
Joseph Vallino, Marine Biological Laboratory, Woods Hole</p>
<blockquote><p>
Natural microbial communities contain billions of individuals per liter and can exceed a trillion cells per liter in sediments, as well as harbor thousands of species in the same volume. The high species diversity contributes to extensive metabolic functional capabilities to extract chemical energy from the environment, such as methanogenesis, sulfate reduction, anaerobic photosynthesis, chemoautotrophy, and many others, most of which are only expressed by bacteria and archaea. Reductionist modeling of natural communities is problematic, as we lack knowledge on growth kinetics for most organisms and have even less understanding on the mechanisms governing predation, viral lysis, and predator avoidance in these systems. As a result, existing models that describe microbial communities contain dozens to hundreds of parameters, and state variables are extensively aggregated. Overall, the models are little more than non-linear parameter fitting exercises that have limited, to no, extrapolation potential, as there are few principles governing organization and function of complex self-assembling systems. Over the last decade, we have been developing a systems approach that models microbial communities as a distributed metabolic network that focuses on metabolic function rather than describing individuals or species. We use an optimization approach to determine which metabolic functions in the network should be up regulated versus those that should be down regulated based on the non-equilibrium thermodynamics principle of maximum entropy production (MEP). Derived from statistical mechanics, MEP proposes that steady state systems will likely organize to maximize free energy dissipation rate. We have extended this conjecture to apply to non-steady state systems and have proposed that living systems maximize entropy production integrated over time and space, while non-living systems maximize instantaneous entropy production. Our presentation will provide a brief overview of the theory and approach, as well as present several examples of applying MEP to describe the biogeochemistry of microbial systems in laboratory experiments and natural ecosystems.</p></blockquote>
<p> <b><a href="http://math.ucr.edu/home/baez/SMB2021/SMB2021_wiuf.pdf">Reduction and the quasi-steady state approximation</a></b><br />
Carsten Wiuf, University of Copenhagen</p>
<blockquote><p>
Chemical reactions often occur at different time-scales. In applications of chemical reaction network theory it is often desirable to reduce a reaction network to a smaller reaction network by elimination of fast species or fast reactions. There exist various techniques for doing so, e.g. the Quasi-Steady-State Approximation or the Rapid Equilibrium Approximation. However, these methods are not always mathematically justifiable. Here, a method is presented for which (so-called) non-interacting species are eliminated by means of QSSA. It is argued that this method is mathematically sound. Various examples are given (Michaelis-Menten mechanism, two-substrate mechanism, &#8230;) and older related techniques from the 50s and 60s are briefly discussed.</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/06/16/nonequilibrium-thermodynamics-in-biology-part-2/#respond">Leave a Comment &#187;</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/chemistry/" rel="category tag">chemistry</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/06/16/nonequilibrium-thermodynamics-in-biology-part-2/" rel="bookmark" title="Permanent Link to Nonequilibrium Thermodynamics in Biology (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-28874 post type-post status-publish format-standard hentry category-biology category-information-and-entropy category-probability" id="post-28874">
				<h2><a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/" rel="bookmark">Fisher&#8217;s Fundamental Theorem (Part&nbsp;3)</a></h2>
				<small>8 October, 2020</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Last time</a> we stated and proved a simple version of Fisher&#8217;s fundamental theorem of natural selection, which says that <i>under some conditions</i>, the rate of increase of the mean fitness equals the variance of the fitness.   But the conditions we gave were very restrictive: namely, that the fitness of each species of replicator is constant, not depending on how many of these replicators there are, or any other replicators.</p>
<p>To broaden the scope of Fisher&#8217;s fundamental theorem we need to do one of two things:</p>
<p>1) change the left side of the equation: talk about some other quantity other than rate of change of mean fitness.</p>
<p>2) change the right side of the question: talk about some other quantity than the variance in fitness.</p>
<p>Or we could do both!   <img src="https://i1.wp.com/math.ucr.edu/home/baez/emoticons/tongue2.gif" />  People have spent a lot of time generalizing Fisher&#8217;s fundamental theorem.   I don&#8217;t think there are, or should be, any hard rules on what counts as a generalization.</p>
<p>But today we&#8217;ll take alternative 1).  We&#8217;ll show the square of something called the &#8216;Fisher speed&#8217; <i>always</i> equals the variance in fitness.  One nice thing about this result is that we can drop the restrictive condition I mentioned.   Another nice thing is that the Fisher speed is a concept from information theory!  It&#8217;s defined using the <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">Fisher metric</a> on the space of probability distributions.</p>
<p>And yes&#8212;that metric is named after the same guy who proved Fisher&#8217;s fundamental theorem!  So, arguably, <i>Fisher</i> should have proved this generalization of Fisher&#8217;s fundamental theorem.  But in fact it seems that I was the first to prove it, around <a href="https://math.ucr.edu/home/baez/information/information_geometry_16.html">February 1st, 2017</a>.   Some similar results were already known, and I will discuss those someday.  But they&#8217;re a bit different.</p>
<p>A good way to think about the Fisher speed is that it&#8217;s &#8216;the rate at which information is being updated&#8217;.   A population of replicators of different species gives a probability distribution.  Like any probability distribution, this has information in it.   As the populations of our replicators change, the Fisher speed says the rate at which this information is being updated.  So, in simple terms, we&#8217;ll show</p>
<blockquote><p>
  The square of the rate at which information is updated is equal to the variance in fitness.</p></blockquote>
<p>This is quite a change from Fisher&#8217;s original idea, namely:</p>
<blockquote><p>
  The rate of increase of mean fitness is equal to the variance in fitness.</p></blockquote>
<p>But it has the advantage of always being true&#8230; as long the population dynamics are described by the general framework we introduced last time.  So let me remind you of the general setup, and then prove the result!</p>
<h3> The setup</h3>
<p>We start out with population functions <img src="https://s0.wp.com/latex.php?latex=P_i+%5Ccolon+%5Cmathbb%7BR%7D+%5Cto+%280%2C%5Cinfty%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i &#92;colon &#92;mathbb{R} &#92;to (0,&#92;infty)," class="latex" /> one for each species of replicator <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C%5Cdots%2Cn%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i = 1,&#92;dots,n," class="latex" /> obeying the <b>LotkaVolterra equation</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd+P_i%7D%7Bd+t%7D+%3D+f_i%28P_1%2C+%5Cdots%2C+P_n%29+P_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d P_i}{d t} = f_i(P_1, &#92;dots, P_n) P_i } " class="latex" /></p>
<p>for some differentiable functions <img src="https://s0.wp.com/latex.php?latex=f_i+%5Ccolon+%280%2C%5Cinfty%29+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i &#92;colon (0,&#92;infty) &#92;to &#92;mathbb{R}" class="latex" /> called <b>fitness functions</b>.   The probability of a replicator being in the <i>i</i>th species is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++p_i%28t%29+%3D+%5Cfrac%7BP_i%28t%29%7D%7B%5Csum_j+P_j%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  p_i(t) = &#92;frac{P_i(t)}{&#92;sum_j P_j(t)} } " class="latex" /></p>
<p>Using the LotkaVolterra equation we showed last time that these probabilities obey the <b>replicator equation</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%5Cleft%28+f_i%28P%29+-+%5Coverline+f%28P%29+%5Cright%29++p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = &#92;left( f_i(P) - &#92;overline f(P) &#92;right)  p_i } " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" /> is short for the whole list of populations <img src="https://s0.wp.com/latex.php?latex=%28P_1%28t%29%2C+%5Cdots%2C+P_n%28t%29%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(P_1(t), &#92;dots, P_n(t))," class="latex" /> and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Coverline+f%28P%29+%3D+%5Csum_j+f_j%28P%29+p_j++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;overline f(P) = &#92;sum_j f_j(P) p_j  } " class="latex" /></p>
<p>is the <b>mean fitness</b>.</p>
<h3>The Fisher metric</h3>
<p>The space of probability distributions on the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+n%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;{1, &#92;dots, n&#92;}" class="latex" /> is called the <b>(n-1)-simplex</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D+%3D+%5C%7B+%28x_1%2C+%5Cdots%2C+x_n%29+%3A+%5C%3B+x_i+%5Cge+0%2C+%5C%3B+%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+x_i+%3D+1+%7D+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1} = &#92;{ (x_1, &#92;dots, x_n) : &#92;; x_i &#92;ge 0, &#92;; &#92;displaystyle{ &#92;sum_{i=1}^n x_i = 1 } &#92;} " class="latex" /></p>
<p>It&#8217;s called <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1}" class="latex" /> because it&#8217;s (n-1)-dimensional.  When <img src="https://s0.wp.com/latex.php?latex=n+%3D+3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n = 3" class="latex" /> it looks like the letter <img src="https://s0.wp.com/latex.php?latex=%5CDelta%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta:" class="latex" /></p>
<div align="center">
<a href="https://johncarlosbaez.files.wordpress.com/2020/10/800px-2d-simplex.svg_.png"><img src="https://johncarlosbaez.files.wordpress.com/2020/10/800px-2d-simplex.svg_.png?w=150" alt="" width="150" /></a></div>
<p>The <b>Fisher metric</b> is a Riemannian metric on the interior of the (n-1)-simplex.  That is, given a point <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> in the interior of <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5E%7Bn-1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Delta^{n-1}" class="latex" /> and two tangent vectors <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> at this point the Fisher metric gives a number</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28v%2Cw%29+%3D+%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7Bv_i+w_i%7D%7Bp_i%7D++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(v,w) = &#92;displaystyle{ &#92;sum_{i=1}^n &#92;frac{v_i w_i}{p_i}  } " class="latex" /></p>
<p>Here we are describing the tangent vectors <img src="https://s0.wp.com/latex.php?latex=v%2Cw&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="v,w" class="latex" /> as vectors in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^n" class="latex" /> with the property that the sum of their components is zero: that&#8217;s what makes them tangent to the (n-1)-simplex.  And we&#8217;re demanding that <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> be in the interior of the simplex to avoid dividing by zero, since on the boundary of the simplex we have <img src="https://s0.wp.com/latex.php?latex=p_i+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = 0" class="latex" /> for at least one choice of $i.$</p>
<p>If we have a probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> moving around in the interior of the (n-1)-simplex as a function of time, its <b>Fisher speed</b> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Csqrt%7Bg%28%5Cdot%7Bp%7D%28t%29%2C+%5Cdot%7Bp%7D%28t%29%29%7D+%3D+%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%5Cdot%7Bp%7D_i%28t%29%5E2%7D%7Bp_i%28t%29%7D%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;sqrt{g(&#92;dot{p}(t), &#92;dot{p}(t))} = &#92;sqrt{&#92;sum_{i=1}^n &#92;frac{&#92;dot{p}_i(t)^2}{p_i(t)}} } " class="latex" /></p>
<p>if the derivative <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7Bp%7D%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;dot{p}(t)" class="latex" /> exists.   This is the usual formula for the speed of a curve moving in a Riemannian manifold, specialized to the case at hand.</p>
<p>Now we&#8217;ve got all the formulas we&#8217;ll need to prove the result we want.  But for those who don&#8217;t already know and love it, it&#8217;s worthwhile saying a bit more about the Fisher metric.</p>
<p>The factor of <img src="https://s0.wp.com/latex.php?latex=1%2Fx_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1/x_i" class="latex" /> in the Fisher metric changes the geometry of the simplex so that it becomes <i>round</i>, like a portion of a sphere:</p>
<div align="center">
<a href="https://johncarlosbaez.files.wordpress.com/2020/10/fisher_metric.jpg"><img src="https://johncarlosbaez.files.wordpress.com/2020/10/fisher_metric.jpg?w=250" alt="" width="250" /></a></div>
<p>But the reason the Fisher metric is important, I think, is its connection to relative information.  Given two probability distributions <img src="https://s0.wp.com/latex.php?latex=p%2C+q+%5Cin+%5CDelta%5E%7Bn-1%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p, q &#92;in &#92;Delta^{n-1}," class="latex" /> the <b>information of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> relative to <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /></b> is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28q%2Cp%29+%3D+%5Csum_%7Bi+%3D+1%7D%5En+q_i+%5Cln%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i%7D%5Cright%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(q,p) = &#92;sum_{i = 1}^n q_i &#92;ln&#92;left(&#92;frac{q_i}{p_i}&#92;right)   } " class="latex" /></p>
<p>You can show this is the expected amount of information gained if <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> was your prior distribution and you receive information that causes you to update your prior to <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   So, sometimes it&#8217;s called the <b>information gain</b>.  It&#8217;s also called <b>relative entropy</b> or&#8212;my least favorite, since it sounds so mysterious&#8212;the <b><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KullbackLeibler divergence</a></b>.</p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=p%28t%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t)" class="latex" /> is a smooth curve in the interior of the (n-1)-simplex.  We can ask the rate at which information is gained as time passes.  Perhaps surprisingly, a calculation gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bdt%7D+I%28p%28t%29%2C+p%28t_0%29%29%5CBig%7C_%7Bt+%3D+t_0%7D+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{dt} I(p(t), p(t_0))&#92;Big|_{t = t_0} = 0 } " class="latex" /></p>
<p>That is, in some sense &#8216;to first order&#8217; no information is being gained at any moment <img src="https://s0.wp.com/latex.php?latex=t_0+%5Cin+%5Cmathbb%7BR%7D.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t_0 &#92;in &#92;mathbb{R}." class="latex" />   However, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7D+I%28p%28t%29%2C+p%28t_0%29%29%5CBig%7C_%7Bt+%3D+t_0%7D+%3D++g%28%5Cdot%7Bp%7D%28t_0%29%2C+%5Cdot%7Bp%7D%28t_0%29%29%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;frac{d^2}{dt^2} I(p(t), p(t_0))&#92;Big|_{t = t_0} =  g(&#92;dot{p}(t_0), &#92;dot{p}(t_0))}  " class="latex" /></p>
<p>So, the square of the Fisher speed has a nice interpretation in terms of relative entropy!</p>
<p>For a derivation of these last two equations, see <a href="https://johncarlosbaez.wordpress.com/2011/03/02/information-geometry-part-7/">Part 7</a> of my posts on information geometry.  For more on the meaning of relative entropy, see <a href="https://johncarlosbaez.wordpress.com/2011/01/21/information-geometry-part-6/">Part 6</a>.</p>
<h3> The result</h3>
<p>It&#8217;s now extremely easy to show what we want, but let me state it formally so all the assumptions are crystal clear.</p>
<p><b>Theorem.</b>  Suppose the functions <img src="https://s0.wp.com/latex.php?latex=P_i+%5Ccolon+%5Cmathbb%7BR%7D+%5Cto+%280%2C%5Cinfty%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i &#92;colon &#92;mathbb{R} &#92;to (0,&#92;infty)" class="latex" /> obey the Lotka&#8211;Volterra equations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot+P_i+%3D+f_i%28P%29+P_i%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot P_i = f_i(P) P_i}  " class="latex" /></p>
<p>for some differentiable functions <img src="https://s0.wp.com/latex.php?latex=f_i+%5Ccolon+%280%2C%5Cinfty%29%5En+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i &#92;colon (0,&#92;infty)^n &#92;to &#92;mathbb{R}" class="latex" /> called fitness functions.   Define probabilities and the mean fitness as above, and define the <b>variance of the fitness</b> by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cmathrm%7BVar%7D%28f%28P%29%29+%3D++%5Csum_j+%28+f_j%28P%29+-+%5Coverline+f%28P%29%29%5E2+%5C%2C+p_j+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;mathrm{Var}(f(P)) =  &#92;sum_j ( f_j(P) - &#92;overline f(P))^2 &#92;, p_j } " class="latex" /></p>
<p>Then if none of the populations <img src="https://s0.wp.com/latex.php?latex=P_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_i" class="latex" /> are zero, the square of the Fisher speed of the probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28t%29+%3D+%28p_1%28t%29%2C+%5Cdots+%2C+p_n%28t%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p(t) = (p_1(t), &#92;dots , p_n(t))" class="latex" /> is the variance of the fitness:</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29++%3D+%5Cmathrm%7BVar%7D%28f%28P%29%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g(&#92;dot{p}, &#92;dot{p})  = &#92;mathrm{Var}(f(P)) " class="latex" /></p>
<p><b>Proof.</b> The proof is near-instantaneous.  We take the square of the Fisher speed:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%5Cdot%7Bp%7D_i%28t%29%5E2%7D%7Bp_i%28t%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ g(&#92;dot{p}, &#92;dot{p}) = &#92;sum_{i=1}^n &#92;frac{&#92;dot{p}_i(t)^2}{p_i(t)} } " class="latex" /></p>
<p>and plug in the replicator equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cdot%7Bp%7D_i+%3D+%28f_i%28P%29+-+%5Coverline+f%28P%29%29+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;dot{p}_i = (f_i(P) - &#92;overline f(P)) p_i } " class="latex" /></p>
<p>We obtain:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D+%5Cdisplaystyle%7B+g%28%5Cdot%7Bp%7D%2C+%5Cdot%7Bp%7D%29%7D+%26%3D%26++%5Cdisplaystyle%7B+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%28+f_i%28P%29+-+%5Coverline+f%28P%29+%5Cright%29%5E2+p_i+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cmathrm%7BVar%7D%28f%28P%29%29++%5Cend%7Barray%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl} &#92;displaystyle{ g(&#92;dot{p}, &#92;dot{p})} &amp;=&amp;  &#92;displaystyle{ &#92;sum_{i=1}^n &#92;left( f_i(P) - &#92;overline f(P) &#92;right)^2 p_i } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;mathrm{Var}(f(P))  &#92;end{array} " class="latex" /></p>
<p>as desired.   &nbsp; </p>
<p>It&#8217;s hard to imagine anything simpler than this.  We see that given the LotkaVolterra equation, what causes information to be updated is nothing more and nothing less than variance in fitness!</p>
<hr />
<p>The whole series:</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-1/">Part 1</a>: the obscurity of Fisher&#8217;s original paper.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/09/29/fishers-fundamental-theorem-part-2/">Part 2</a>: a precise statement of Fisher&#8217;s fundamental theorem of natural selection, and conditions under which it holds.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/">Part 3</a>: a modified version of the fundamental theorem of natural selection, which holds much more generally.</p>
<p>&bull; <a href="https://johncarlosbaez.wordpress.com/2021/07/13/fishers-fundamental-theorem-part-4/">Part 4</a>: my paper on the fundamental theorem of natural selection.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/#comments">10 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/10/08/fishers-fundamental-theorem-part-3/" rel="bookmark" title="Permanent Link to Fisher&#8217;s Fundamental Theorem (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-28650 post type-post status-publish format-standard hentry category-biodiversity category-information-and-entropy" id="post-28650">
				<h2><a href="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/" rel="bookmark">Ascendancy vs. Reserve</a></h2>
				<small>22 September, 2020</small><br />


				<div class="entry">
					<p>Why is biodiversity &#8216;good&#8217;?    To what extent is this sort of goodness even relevant to ecosystems&#8212;as opposed to us humans?  I&#8217;d like to study this mathematically.</p>
<p>To do this, we&#8217;d need to extract some answerable questions out of the morass of subtlety and complexity.    For example: what role does biodiversity play in the ability of ecosystems to be robust under sudden changes of external conditions?   This is already plenty hard to study mathematically, since it requires understanding &#8216;biodiversity&#8217; and &#8216;robustness&#8217;.</p>
<p>Luckily there has already been a lot of work on the mathematics of biodiversity and its connection to entropy.  For example:</p>
<p> Tom Leinster, <a href="https://johncarlosbaez.wordpress.com/2011/11/07/measuring-biodiversity/">Measuring biodiversity</a>, <em>Azimuth</em>, 7 November 2011.</p>
<p>But how does biodiversity help robustness?</p>
<p>There&#8217;s been a lot of work on this.   This paper has some inspiring passages:</p>
<p> Robert E. Ulanowicz,, Sally J. Goerner, Bernard Lietaer and Rocio Gomez, <a href="http://wtf.tw/ref/ulanowicz.pdf">Quantifying sustainability: Resilience, efficiency and the return of information theory</a>, <em>Ecological Complexity</em> <strong>6</strong> (2009), 27&#8211;36.</p>
<p>I&#8217;m not sure the math lives up to their claims, but I like these lines:</p>
<blockquote><p>
  In other words, (14) says that the capacity for a system to undergo evolutionary change or self-organization consists of two aspects: It must be capable of exercising sufficient directed power (ascendancy) to maintain its integrity over time. Simultaneously, it must possess a reserve of flexible actions that can be used to meet the exigencies of novel disturbances. According to (14) these two aspects are literally complementary.</p></blockquote>
<p>The two aspects are &#8216;ascendancy&#8217;, which is something like efficiency or being optimized, and &#8216;reserve capacity&#8217;, which is something like random junk that might come in handy if something unexpected comes up.</p>
<p>You know those gadgets you kept in the back of your kitchen drawer and never needed&#8230; until you did?   If you&#8217;re aiming for &#8216;ascendancy&#8217; you&#8217;d clear out those drawers.   But if you keep that stuff, you&#8217;ve got more &#8216;reserve capacity&#8217;.  They both have their good points.   Ideally you want to strike a wise balance.  You&#8217;ve probably sensed this every time you clean out your house: should I keep this thing because I might need it, or should I get rid of it?</p>
<p>I think it would be great to make these concepts precise.  The paper at hand attempts this by taking a matrix of nonnegative numbers <img src="https://s0.wp.com/latex.php?latex=T_%7Bi+j%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T_{i j}" class="latex" /> to describe flows in an ecological network.   They define a kind of entropy for this matrix, very similar in look to <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>. Then they write this as a sum of two parts: a part closely analogous to <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a>, and a part closely analogous to <a href="https://en.wikipedia.org/wiki/Conditional_entropy">conditional entropy</a>.   This decomposition is standard in information theory.   This is their equation (14).</p>
<p>If you want to learn more about the underlying math, click on this picture:</p>
<p><a href="http://www.info612.ece.mcgill.ca/lecture_02.pdf"><img loading="lazy" data-attachment-id="28656" data-permalink="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/entropy-mutual-information-relative-entropy-relation-diagram/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png" data-orig-size="500,308" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Entropy-mutual-information-relative-entropy-relation-diagram" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=450" src="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=450&#038;h=277" alt="" class="aligncenter size-full wp-image-28656" width="450" height="277" srcset="https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=450&amp;h=277 450w, https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=150&amp;h=92 150w, https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png?w=300&amp;h=185 300w, https://johncarlosbaez.files.wordpress.com/2020/09/entropy-mutual-information-relative-entropy-relation-diagram.png 500w" sizes="(max-width: 450px) 100vw, 450px"/></a></p>
<p>The new idea of these authors is that in the context of an ecological network, the mutual information can be understood as &#8216;ascendancy&#8217;, while the conditional entropy can be understood as &#8216;reserve capacity&#8217;.</p>
<p>I don&#8217;t know if I believe this!   But I like the general idea of a balance between ascendancy and reserve capacity.</p>
<p>They write:</p>
<blockquote><p>
  While the dynamics of this dialectic interaction can be quite subtle and highly complex, one thing is boldly clearsystems with either vanishingly small ascendancy or insignificant reserves are destined to perish before long. A system lacking ascendancy has neither the extent of activity nor the internal organization needed to survive. By contrast, systems that are so tightly constrained and honed to a particular environment appear brittle in the sense of Holling (1986) or senescent in the sense of Salthe (1993) and are prone to collapse in the face of even minor novel disturbances. Systems that endure&#8212;that is, are sustainable&#8212;lie somewhere between these extremes.  But, where?</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/#comments">11 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/09/22/ascendancy-vs-reserve/" rel="bookmark" title="Permanent Link to Ascendancy vs. Reserve">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-27387 post type-post status-publish format-standard hentry category-astronomy category-information-and-entropy" id="post-27387">
				<h2><a href="https://johncarlosbaez.wordpress.com/2020/01/25/entropy-in-the-universe/" rel="bookmark">Entropy in the&nbsp;Universe</a></h2>
				<small>25 January, 2020</small><br />


				<div class="entry">
					<p>If you click on this picture, you&#8217;ll see a zoomable image of the Milky Way with <a href="https://www.eso.org/public/news/eso1242/">84 million stars</a>:</p>
<div align="center">
<a href="https://eso.org/public/images/eso1242a/zoomable/"><br />
<img width="450" border="2" src="https://i0.wp.com/math.ucr.edu/home/baez/astronomical/milky_way_gigapixel.jpg" /><br />
</a>
</div>
<p>But stars contribute only a tiny fraction of the total entropy in the observable Universe.  If it&#8217;s random information you want, look elsewhere!</p>
<p>First: what&#8217;s the &#8216;observable Universe&#8217;, exactly?</p>
<p>The further you look out into the Universe, the further you look back in time.  You can&#8217;t see through the hot gas from 380,000 years after the Big Bang.  That &#8216;wall of fire&#8217; marks the limits of the observable Universe.</p>
<div align="center">
<img width="400" src="https://i1.wp.com/math.ucr.edu/home/baez/astronomical/reionization.jpg" />
</div>
<p>But as the Universe expands, the distant ancient stars and gas we see have moved even farther away, so they&#8217;re no longer observable.  Thus, the so-called &#8216;observable Universe&#8217; is really the &#8216;formerly observable Universe&#8217;.  Its edge is 46.5 billion light years away now!</p>
<p>This is true even though the Universe is only 13.8 billion years old.  A standard challenge in understanding general relativity is to figure out how this is possible, given that nothing can move faster than light.</p>
<div align="center">
<img width="440" src="https://i0.wp.com/math.ucr.edu/home/baez/physical/big_bang.jpg" />
</div>
<p>What&#8217;s the total number of stars in the observable Universe? Estimates go up as telescopes improve.  Right now people think there are between 100 and 400 billion stars in the Milky Way.  They think there are between 170 billion and 2 trillion galaxies in the Universe.</p>
<p>In 2009, Chas Egan and Charles Lineweaver estimated the total entropy of all the stars in the observable Universe at 10<sup>81</sup> bits. You should think of these as qubits: it&#8217;s the amount of information to describe the quantum state of <i>everything</i> in all these stars.</p>
<p>But the entropy of interstellar and intergalactic gas and dust is about ten times more the entropy of stars!  It&#8217;s about 10<sup>82</sup> bits.</p>
<p>The entropy in all the photons in the Universe is even more!  The Universe is full of radiation left over from the Big Bang. The photons in the observable Universe left over from the Big Bang have a total entropy of about 10<sup>90</sup> bits.  It&#8217;s called the &#8216;cosmic microwave background radiation&#8217;.</p>
<p>The neutrinos from the Big Bang also carry about 10<sup>90</sup> bitsa bit less than the photons.  The gravitons carry much less, about 10<sup>88</sup> bits.  That&#8217;s because they decoupled from other matter and radiation very early, and have been cooling ever since.  On the other hand, photons in the cosmic microwave background radiation were formed by annihilating<br />
electron-positron pairs until about 10 seconds after the Big Bang. Thus the graviton radiation is expected to be cooler than the microwave background radiation: about 0.6 kelvin as compared to 2.7 kelvin.</p>
<p>Black holes have <i>immensely</i> more entropy than anything listed so far.  Egan and Lineweaver estimate the entropy of stellar-mass black holes in the observable Universe at 10<sup>98</sup> bits.  This is connected to why black holes are so stable: the Second Law says entropy likes to increase.</p>
<p>But the entropy of black holes grows <i>quadratically</i> with mass!  So black holes tend to merge and form bigger black holes  ultimately forming the &#8216;supermassive&#8217; black holes at the centers of most galaxies.  These dominate the entropy of the observable Universe: about 10<sup>104</sup> bits.</p>
<p>Hawking predicted that black holes slowly radiate away their mass when they&#8217;re in a cold enough environment.  But the Universe is much too hot for supermassive black holes to be losing mass now.  Instead, they very slowly <i>grow</i> by eating the cosmic microwave background, even when they&#8217;re not eating stars, gas and dust.</p>
<p>So, only in the far future will the Universe cool down enough for large black holes to start slowly decaying via Hawking radiation. Entropy will continue to increase&#8230; going mainly into photons and gravitons!  This process will take a very long time.  Assuming nothing is falling into it and no unknown effects intervene, a solar-mass black hole takes about 10<sup>67</sup> years to evaporate due to Hawking radiation  while a really big one, comparable to the mass of a galaxy, should take about 10<sup>99</sup> years.</p>
<p>If our current most popular ideas on dark energy are correct, the Universe will continue to expand exponentially.  Thanks to this, there will be a <a href="https://en.wikipedia.org/wiki/Cosmological_horizon#Event_horizon">cosmological event horizon</a> surrounding each observer, which will radiate Hawking radiation at a temperature of roughly 10<sup>-30</sup> kelvin.</p>
<p>In this scenario the Universe in the very far future will mainly consist of massless particles produced as Hawking radiation at this temperature: photons and gravitons.  The entropy within the exponentially expanding ball of space that is <i>today</i> our &#8216;observable Universe&#8217; will continue to increase exponentially&#8230; but more to the point, the entropy density will approach that of a gas of photons and gravitons in thermal equilibrium at 10<sup>-30</sup> kelvin.</p>
<p>Of course, it&#8217;s quite likely that some new physics will turn up, between now and then, that changes the story!  I hope so: this would be a rather dull ending to the Universe.</p>
<p>For more details, go here:</p>
<p>  Chas A. Egan and Charles H. Lineweaver, <a href="https://arxiv.org/abs/0909.3983">A  larger estimate of the entropy of the universe</a>, <i>The Astrophysical Journal</i> <b>710</b> (2010), 1825.</p>
<p>Also read my page on <a href="http://math.ucr.edu/home/baez/information.html">information</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/01/25/entropy-in-the-universe/#comments">19 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/astronomy/" rel="category tag">astronomy</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2020/01/25/entropy-in-the-universe/" rel="bookmark" title="Permanent Link to Entropy in the&nbsp;Universe">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/page/2/" >&laquo; Previous Entries</a></div>
			<div class="alignright"></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You are currently browsing the archives for the information and entropy category.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/07/29/structured-vs-decorated-cospans-part-2/">Structured vs Decorated Cospans (Part&nbsp;2)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="amarashiki" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/#comment-172555">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Robert A. Wilson" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="https://robwilson1.wordpress.com" rel="nofollow"><img alt='' src='https://2.gravatar.com/avatar/24dc7e2371491f36fd4538b3474920b5?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="https://robwilson1.wordpress.com" rel="nofollow">Robert A. Wilson</a> on <a href="https://johncarlosbaez.wordpress.com/2021/04/04/the-koide-formula/#comment-172553">The Koide Formula</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172545">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Wolfgang" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://1.gravatar.com/avatar/d3c6d7ec8069e25c08a0a11263581925?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">Wolfgang on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172543">Classical Mechanics versus The&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499 current-cat"><a aria-current="page" href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (478)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (204)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see whats on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkins environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,227 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/category/information-and-entropy/"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="0272f16312" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,176,888 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

				<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = JSON.parse( decodeURIComponent( '%7B%22settings%22%3A%7B%22id%22%3A%22content%22%2C%22ajaxurl%22%3A%22https%3A%5C%2F%5C%2Fjohncarlosbaez.wordpress.com%5C%2F%3Finfinity%3Dscrolling%22%2C%22type%22%3A%22scroll%22%2C%22wrapper%22%3Atrue%2C%22wrapper_class%22%3A%22infinite-wrap%22%2C%22footer%22%3Atrue%2C%22click_handle%22%3A%221%22%2C%22text%22%3A%22Older%20posts%22%2C%22totop%22%3A%22Scroll%20back%20to%20top%22%2C%22currentday%22%3A%2225.01.20%22%2C%22order%22%3A%22DESC%22%2C%22scripts%22%3A%5B%5D%2C%22styles%22%3A%5B%5D%2C%22google_analytics%22%3Afalse%2C%22offset%22%3A1%2C%22history%22%3A%7B%22host%22%3A%22johncarlosbaez.wordpress.com%22%2C%22path%22%3A%22%5C%2Fcategory%5C%2Finformation-and-entropy%5C%2Fpage%5C%2F%25d%5C%2F%22%2C%22use_trailing_slashes%22%3Atrue%2C%22parameters%22%3A%22%22%7D%2C%22query_args%22%3A%7B%22category_name%22%3A%22information-and-entropy%22%2C%22error%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A0%2C%22post_parent%22%3A%22%22%2C%22subpost%22%3A%22%22%2C%22subpost_id%22%3A%22%22%2C%22attachment%22%3A%22%22%2C%22attachment_id%22%3A0%2C%22name%22%3A%22%22%2C%22pagename%22%3A%22%22%2C%22page_id%22%3A0%2C%22second%22%3A%22%22%2C%22minute%22%3A%22%22%2C%22hour%22%3A%22%22%2C%22day%22%3A0%2C%22monthnum%22%3A0%2C%22year%22%3A0%2C%22w%22%3A0%2C%22tag%22%3A%22%22%2C%22cat%22%3A23375499%2C%22tag_id%22%3A%22%22%2C%22author%22%3A%22%22%2C%22author_name%22%3A%22%22%2C%22feed%22%3A%22%22%2C%22tb%22%3A%22%22%2C%22paged%22%3A0%2C%22meta_key%22%3A%22%22%2C%22meta_value%22%3A%22%22%2C%22preview%22%3A%22%22%2C%22s%22%3A%22%22%2C%22sentence%22%3A%22%22%2C%22title%22%3A%22%22%2C%22fields%22%3A%22%22%2C%22menu_order%22%3A%22%22%2C%22embed%22%3A%22%22%2C%22category__in%22%3A%5B%5D%2C%22category__not_in%22%3A%5B%5D%2C%22category__and%22%3A%5B%5D%2C%22post__in%22%3A%5B%5D%2C%22post__not_in%22%3A%5B%5D%2C%22post_name__in%22%3A%5B%5D%2C%22tag__in%22%3A%5B%5D%2C%22tag__not_in%22%3A%5B%5D%2C%22tag__and%22%3A%5B%5D%2C%22tag_slug__in%22%3A%5B%5D%2C%22tag_slug__and%22%3A%5B%5D%2C%22post_parent__in%22%3A%5B%5D%2C%22post_parent__not_in%22%3A%5B%5D%2C%22author__in%22%3A%5B%5D%2C%22author__not_in%22%3A%5B%5D%2C%22lazy_load_term_meta%22%3Afalse%2C%22posts_per_page%22%3A10%2C%22ignore_sticky_posts%22%3Afalse%2C%22suppress_filters%22%3Afalse%2C%22cache_results%22%3Afalse%2C%22update_post_term_cache%22%3Atrue%2C%22update_post_meta_cache%22%3Atrue%2C%22post_type%22%3A%22%22%2C%22nopaging%22%3Afalse%2C%22comments_per_page%22%3A%22100%22%2C%22no_found_rows%22%3Afalse%2C%22order%22%3A%22DESC%22%7D%2C%22query_before%22%3A%222021-09-25%2021%3A56%3A33%22%2C%22last_post_date%22%3A%222020-01-25%2023%3A53%3A44%22%2C%22body_class%22%3A%22infinite-scroll%20neverending%22%2C%22loading_text%22%3A%22Loading%20new%20page%22%2C%22stats%22%3A%22blog%3D12777403%26v%3Dwpcom%26tz%3D0%26user_id%3D0%26subd%3Djohncarlosbaez%26x_pagetype%3Dinfinite%22%7D%7D' ) );
		//]]>
		</script>
		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	<div class="grofile-hash-map-24dc7e2371491f36fd4538b3474920b5">
	</div>
	<div class="grofile-hash-map-d3c6d7ec8069e25c08a0a11263581925">
	</div>
	</div>
		<div id="infinite-footer">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://johncarlosbaez.wordpress.com/" rel="home">
						Azimuth					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a> 				</div>
			</div>
		</div><!-- #infinite-footer -->
		
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s1.wp.com/_static/??-eJyFy00OQDAQQOELGUP8xUKcpWoiZVTTadO4vVhY2LB8L/kwOdCHDWQD7hEcx8VYQa38EYUYJRlHHqZoZ6Zci2T4I1YKTukNnvGFgmGaYVHM5M933Wzch7Ktir5p+rpbL+lvP34=?cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"bf4790dd91","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F17%2Finformation-geometry-part-21%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/_static/??-eJyNUMtuwzAM+6EpRnIo0MOwb3FsNZDr1yy5af5+ztAMbTYEO0mURVKmmjOYFAWjKMfK4o0M5nvn+E09PYUK2deJIiuKF4oky09zsKttoAijLipoFiytAynaXHlPat7us2JZHqWbs0kBckn3BQq2GcvGoWh8tcgrqUEMI9quGR0cYnRJldErh5KbP2yDA85MdkJhxXVkUygLpch/3cDzJY0OjezFNqUbWUxKM69y7oFzQf4Vw5O9kEcLk/Z+TeUF/eOj8J3fDjbeR3jvT8P53A/DqXdfJ9bAHw=='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			(function() {
				var extend = function(out) {
					out = out || {};

					for (var i = 1; i < arguments.length; i++) {
						if (!arguments[i])
						continue;

						for (var key in arguments[i]) {
						if (arguments[i].hasOwnProperty(key))
							out[key] = arguments[i][key];
						}
					}

					return out;
				};
				extend( window.infiniteScroll.settings.scripts, ["postmessage","mobile-useragent-info","rlt-proxy","jquery-core","jquery-migrate","jquery","wpcom-actionbar-placeholder","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","wpcom-masterbar-tracks-js","jquery.wpcom-proxy-request","wp-embed","jetpack-carousel","jetpack-subscriptions-js","swfobject","videopress","tiled-gallery","carousel-wpcom"] );
				extend( window.infiniteScroll.settings.styles, ["the-neverending-homepage","infinity-contempt","wp-block-library","mediaelement","wp-mediaelement","jetpack-layout-grid","jetpack-ratings","coblocks-frontend","wpcom-core-compat-playlist-styles","wpcom-text-widget-styles","wpcom-bbpress2-staff-css","contempt","geo-location-flair","reblogging","a8c-global-print","h4-global","global-styles","jetpack-global-styles-frontend-style","jetpack-carousel-swiper-css","jetpack-carousel","tiled-gallery"] );
			})();
		</script>
				<span id="infinite-aria" aria-live="polite"></span>
		<script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTY5fiwlL09GczNGJkhCfGxfSzIsTTFVZWlnc0ZuY2gxOWx5cjdVaF0/TFA5c0pUeHpGYS9pL1BFeTE1al1hXXorOFY0MF10eDVpTzcuRDlbSyx5c2xEZF8wbGlUc1dibi58TXdzQ182WExQNUZXS2wvWDJHQkQwb2xMOC5HOXZxbmk4VHwzNHRtSFt6OEtoY3hoa0EvTjBrb2J1ejdXMUE4aE11LUZma3IlMj9wUkYrfHAmTjZEL1BZRVdpWjlQT1FkWndZS3ZL'}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>