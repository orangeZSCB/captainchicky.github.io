<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information Geometry (Part 16)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/</link>
	<description></description>
	<lastBuildDate>Tue, 07 Feb 2017 02:26:40 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Mart Malakoff		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87414</link>

		<dc:creator><![CDATA[Mart Malakoff]]></dc:creator>
		<pubDate>Tue, 07 Feb 2017 02:26:40 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87414</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87405&quot;&gt;Uwe Stroinski&lt;/a&gt;.

&#039;ergodic hypothesis&#039; or theory. is wayyy up there in basic issues. if you look at original proofs of this, by von neumann, weiner, birkhoff  you can see immediately they prove their assumptions--its a variant of the CLT--central limit theorem. it took KAM (kolmogorov/arnold/m to fix this up).  (see &#039;harmonic analyses as the exploitation of symmetry&#039; by G Mackey 1980 --free online--Bulletin of  Am Math Soc--my prof told me to read this--old  classic which i did,its a bible, and he also tried to flunk me, which he coud&#039;nt do since i passed the tests--just didnt go to class).

i looked at your web page. non-commutative relations in economics are quite good. i know of only 2 good papers on ergodic theory in economics, and they do not go far.

time-dependent cobbs-douglass utility functions are better, but only go so far. (and actually if you look, cobbs-douglass utility functins are  a version of entropy).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87405">Uwe Stroinski</a>.</p>
<p>&#8216;ergodic hypothesis&#8217; or theory. is wayyy up there in basic issues. if you look at original proofs of this, by von neumann, weiner, birkhoff  you can see immediately they prove their assumptions&#8211;its a variant of the CLT&#8211;central limit theorem. it took KAM (kolmogorov/arnold/m to fix this up).  (see &#8216;harmonic analyses as the exploitation of symmetry&#8217; by G Mackey 1980 &#8211;free online&#8211;Bulletin of  Am Math Soc&#8211;my prof told me to read this&#8211;old  classic which i did,its a bible, and he also tried to flunk me, which he coud&#8217;nt do since i passed the tests&#8211;just didnt go to class).</p>
<p>i looked at your web page. non-commutative relations in economics are quite good. i know of only 2 good papers on ergodic theory in economics, and they do not go far.</p>
<p>time-dependent cobbs-douglass utility functions are better, but only go so far. (and actually if you look, cobbs-douglass utility functins are  a version of entropy).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Mart Malakoff		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87412</link>

		<dc:creator><![CDATA[Mart Malakoff]]></dc:creator>
		<pubDate>Tue, 07 Feb 2017 01:57:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87412</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87406&quot;&gt;Uwe Stroinski&lt;/a&gt;.

i think u are correct.
the replicator equation in general is already nonlinear. (the way its written makes it look linear). 
Perron-Frobenius theorem is one way to hide all the nonlinearities. (Ruelle goes through all this, though i dont understand the details.)
Langevin equations are the natural way to turn a replicator dynamic into a markov process and then in the limit into a standard stochastic DE (fokker-planck).    This is only true in the infinite limit.
so, its equivalent or its not. if you look at discrete vs continuous dynamics of the logisitic equation in one case it has an  equilbrium solution (continuous case), in the other case its chaotic.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87406">Uwe Stroinski</a>.</p>
<p>i think u are correct.<br />
the replicator equation in general is already nonlinear. (the way its written makes it look linear).<br />
Perron-Frobenius theorem is one way to hide all the nonlinearities. (Ruelle goes through all this, though i dont understand the details.)<br />
Langevin equations are the natural way to turn a replicator dynamic into a markov process and then in the limit into a standard stochastic DE (fokker-planck).    This is only true in the infinite limit.<br />
so, its equivalent or its not. if you look at discrete vs continuous dynamics of the logisitic equation in one case it has an  equilbrium solution (continuous case), in the other case its chaotic.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Uwe Stroinski		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87406</link>

		<dc:creator><![CDATA[Uwe Stroinski]]></dc:creator>
		<pubDate>Mon, 06 Feb 2017 17:19:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87406</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87395&quot;&gt;John Baez&lt;/a&gt;.

John wrote: My “improved Fisher’s theorem”, like the original, deals with the time evolution of probability distributions via a nonlinear equation. I’m pretty sure this can’t be described by Markovian dynamics, even higher-order, because those equations are linear in the probability distribution.

Nonlinearity alone is not sufficient to rule out a Markovian approach as long as you allow infinite dimensional state spaces. E.g. in dynamical systems transfer (or Perron-Frobenius) operators are used to capture properties of non-linear systems (Ruelle, Lasota-Yorke). These operators are linear and, if considered on a measure space, also Markov.

What I have in mind is a result like:

The following are equivalent.
1) The square of the speed is the variance of the fitness.
2) The generator of the whatever (e.g. special Markov chain) satisfies some technical condition.

Now that might not make much sense, but as I said, I am still a beginner and trying to understand what you are doing.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87395">John Baez</a>.</p>
<p>John wrote: My “improved Fisher’s theorem”, like the original, deals with the time evolution of probability distributions via a nonlinear equation. I’m pretty sure this can’t be described by Markovian dynamics, even higher-order, because those equations are linear in the probability distribution.</p>
<p>Nonlinearity alone is not sufficient to rule out a Markovian approach as long as you allow infinite dimensional state spaces. E.g. in dynamical systems transfer (or Perron-Frobenius) operators are used to capture properties of non-linear systems (Ruelle, Lasota-Yorke). These operators are linear and, if considered on a measure space, also Markov.</p>
<p>What I have in mind is a result like:</p>
<p>The following are equivalent.<br />
1) The square of the speed is the variance of the fitness.<br />
2) The generator of the whatever (e.g. special Markov chain) satisfies some technical condition.</p>
<p>Now that might not make much sense, but as I said, I am still a beginner and trying to understand what you are doing.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Uwe Stroinski		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87405</link>

		<dc:creator><![CDATA[Uwe Stroinski]]></dc:creator>
		<pubDate>Mon, 06 Feb 2017 17:04:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87405</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87383&quot;&gt;Mart Malakoff&lt;/a&gt;.

Thanks for the explanation!

My comment was inspired by the ergodic hypothesis. For some systems it is a theorem whereas for others it is not true. I have interpreted John&#039;s &quot;The square of the speed is the variance of the fitness.&quot; as such a hypothesis. True for some systems, like the replicator equation and false for others.

Actually, I have a joint paper with C. Schwarz on non-equilibrium economics. I am quite familiar with this topic.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87383">Mart Malakoff</a>.</p>
<p>Thanks for the explanation!</p>
<p>My comment was inspired by the ergodic hypothesis. For some systems it is a theorem whereas for others it is not true. I have interpreted John&#8217;s &#8220;The square of the speed is the variance of the fitness.&#8221; as such a hypothesis. True for some systems, like the replicator equation and false for others.</p>
<p>Actually, I have a joint paper with C. Schwarz on non-equilibrium economics. I am quite familiar with this topic.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87400</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Mon, 06 Feb 2017 07:41:20 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87400</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87395&quot;&gt;John Baez&lt;/a&gt;.

&lt;blockquote&gt;because my theorem applies to processes for which if p_i(t) = 0 at some particular time then p_i(t) = 0 at all later times. &lt;/blockquote&gt;

what&#039;s if e.g. $latex f_i=P_{i+5}/P_i$ ?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87395">John Baez</a>.</p>
<blockquote><p>because my theorem applies to processes for which if p_i(t) = 0 at some particular time then p_i(t) = 0 at all later times. </p></blockquote>
<p>what&#8217;s if e.g. <img src="https://s0.wp.com/latex.php?latex=f_i%3DP_%7Bi%2B5%7D%2FP_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f_i=P_{i+5}/P_i" class="latex" /> ?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Mart Malakoff		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87399</link>

		<dc:creator><![CDATA[Mart Malakoff]]></dc:creator>
		<pubDate>Mon, 06 Feb 2017 04:13:28 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87399</guid>

					<description><![CDATA[You may find interesting N Behera&#039;s &#039;Optimality principles in evolutionary genetics&#039; in Current Science (1999)--not very technical. He tries to find a lagrangian for this dynamics and finds one L= 1/8 ((your speed of learning) + (variance of fitness))

and points out its limited applications, but seems to come to conclusions simialr to yours. 
  (i can&#039;t do math notation on this computer).
    Here&#039;s a link 
www.iisc.ernet.in/currsci/nov10/GENERALARTICLES.PDF

its the last article of that magazine --last few pages.

James Crow also discussed Berara&#039;s approach  in vol 56 (2002) Evolution in &#039;here&#039;s to fisher, additive genetic variance FTNS&#039;.
  (free on  JSTOR)

I&#039;m trying to get an intuitive picture, but basically the speed of learning or fisher information metric or relative entropy equals the  rate of change of mean fitness.   They are both kinds of distances and evolution (or rate of change) is proportional to these distances. This seems to make sense--in the general case, the mean fitness can always be changing ---go up and down.  (I took that also to be the point of the Curtsinger paper in PNAS i linked to.)  One&#039;s rate of learning is the square of of that---its a kind of like a kinetic energy v**2, while mean fitness is more like velocity v.     (This may not make much sense.) 
   (You likely are right about non/markov processes--- i can&#039;t think now how to turn  replicator dynamic into one.
    Some possible relations may be discussed in https://arxiv.org/abs/cond-mat/0409655   since markov processes are often associated with stochastic DE&#039;s.)]]></description>
			<content:encoded><![CDATA[<p>You may find interesting N Behera&#8217;s &#8216;Optimality principles in evolutionary genetics&#8217; in Current Science (1999)&#8211;not very technical. He tries to find a lagrangian for this dynamics and finds one L= 1/8 ((your speed of learning) + (variance of fitness))</p>
<p>and points out its limited applications, but seems to come to conclusions simialr to yours.<br />
  (i can&#8217;t do math notation on this computer).<br />
    Here&#8217;s a link<br />
<a href="http://www.iisc.ernet.in/currsci/nov10/GENERALARTICLES.PDF" rel="nofollow ugc">http://www.iisc.ernet.in/currsci/nov10/GENERALARTICLES.PDF</a></p>
<p>its the last article of that magazine &#8211;last few pages.</p>
<p>James Crow also discussed Berara&#8217;s approach  in vol 56 (2002) Evolution in &#8216;here&#8217;s to fisher, additive genetic variance FTNS&#8217;.<br />
  (free on  JSTOR)</p>
<p>I&#8217;m trying to get an intuitive picture, but basically the speed of learning or fisher information metric or relative entropy equals the  rate of change of mean fitness.   They are both kinds of distances and evolution (or rate of change) is proportional to these distances. This seems to make sense&#8211;in the general case, the mean fitness can always be changing &#8212;go up and down.  (I took that also to be the point of the Curtsinger paper in PNAS i linked to.)  One&#8217;s rate of learning is the square of of that&#8212;its a kind of like a kinetic energy v**2, while mean fitness is more like velocity v.     (This may not make much sense.)<br />
   (You likely are right about non/markov processes&#8212; i can&#8217;t think now how to turn  replicator dynamic into one.<br />
    Some possible relations may be discussed in <a href="https://arxiv.org/abs/cond-mat/0409655" rel="nofollow ugc">https://arxiv.org/abs/cond-mat/0409655</a>   since markov processes are often associated with stochastic DE&#8217;s.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87395</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 06 Feb 2017 00:22:55 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87395</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87384&quot;&gt;Mart Malakoff&lt;/a&gt;.

My &quot;improved Fisher&#039;s theorem&quot;, like the original, deals with the time evolution of probability distributions via a &lt;i&gt;nonlinear&lt;/i&gt; equation.  I&#039;m pretty sure this can&#039;t be described by Markovian dynamics, even higher-order, because those equations are linear in the probability distribution.

On the other hand, my improved Fisher&#039;s theorem, like the original, does not apply to Markov processes, because my theorem applies to processes for which if $latex p_i(t) = 0$ at some particular time then $latex p_i(t) = 0$ at all later times. In biological terms, it disallows &#039;mutation&#039; or other source of &#039;novelty&#039;.  By &#039;novelty&#039; I mean that the probability of a randomly chosen organism being of the &lt;i&gt;i&lt;/i&gt;th species is zero at some time but nonzero later.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87384">Mart Malakoff</a>.</p>
<p>My &#8220;improved Fisher&#8217;s theorem&#8221;, like the original, deals with the time evolution of probability distributions via a <i>nonlinear</i> equation.  I&#8217;m pretty sure this can&#8217;t be described by Markovian dynamics, even higher-order, because those equations are linear in the probability distribution.</p>
<p>On the other hand, my improved Fisher&#8217;s theorem, like the original, does not apply to Markov processes, because my theorem applies to processes for which if <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) = 0" class="latex" /> at some particular time then <img src="https://s0.wp.com/latex.php?latex=p_i%28t%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i(t) = 0" class="latex" /> at all later times. In biological terms, it disallows &#8216;mutation&#8217; or other source of &#8216;novelty&#8217;.  By &#8216;novelty&#8217; I mean that the probability of a randomly chosen organism being of the <i>i</i>th species is zero at some time but nonzero later.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87386</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 05 Feb 2017 19:03:42 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87386</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87374&quot;&gt;Uwe Stroinski&lt;/a&gt;.

Uwe wrote:

&lt;blockquote&gt;
  [...] but it seems to me that mathematicians should not call “Fisher’s fundamental theorem” a theorem. It is rather a hypothesis. Right?
&lt;/blockquote&gt;

I believe Fisher stated it as a mathematical result with assumptions, a conclusion and a mathematical argument leading from the assumptions to the conclusion.  I haven&#039;t read his original paper, and I should, but researchers considered his proof flawed until George Price fixed it up.  As Wikipedia says:

&lt;blockquote&gt;
  Largely as a result of Fisher’s feud with the American geneticist Sewall Wright about adaptive landscapes, the theorem was widely misunderstood to mean that the average fitness of a population would always increase, even though models showed this not to be the case. In 1972, George R. Price showed that Fisher’s theorem was indeed correct (and that Fisher’s proof was also correct, given a typo or two), but did not find it to be of great significance.
&lt;/blockquote&gt;

Earlier this article says Fisher&#039;s theorem is not a theorem, which seems to contradict this claim!  That&#039;s partially because Wikipedia is written by a multitude of authors, but also it seems to reflect the confusion and controversy among biologists about the status of Fisher&#039;s theorem.

I should read Fisher&#039;s original paper and try to make the Wikipedia article a bit clearer.  In particular, it never clearly states the theorem, including the assumptions that imply the conclusion!

&lt;blockquote&gt;
  You, on the other hand, could show that for your system Fisher’s hypothesis is not true and even more.
&lt;/blockquote&gt;

I can show that for certain systems Fisher&#039;s assumptions don&#039;t hold and his conclusion doesn&#039;t hold either, but &lt;em&gt;my&lt;/em&gt; more general assumptions &lt;em&gt;do&lt;/em&gt; hold and so my conclusions do.

&lt;blockquote&gt;
  You could give a mathematically sound description of a replacement. I would really like to see that on the arxiv to better understand the details.
&lt;/blockquote&gt;

Okay, thanks for your vote.  I&#039;ve been thinking this might be worthwhile.  I&#039;m trying to think of a good title---one that would simultaneously say I&#039;m trying to generalize Fisher&#039;s fundamental theorem and that this new theorem has quite different conclusions.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87374">Uwe Stroinski</a>.</p>
<p>Uwe wrote:</p>
<blockquote><p>
  [&#8230;] but it seems to me that mathematicians should not call “Fisher’s fundamental theorem” a theorem. It is rather a hypothesis. Right?
</p></blockquote>
<p>I believe Fisher stated it as a mathematical result with assumptions, a conclusion and a mathematical argument leading from the assumptions to the conclusion.  I haven&#8217;t read his original paper, and I should, but researchers considered his proof flawed until George Price fixed it up.  As Wikipedia says:</p>
<blockquote><p>
  Largely as a result of Fisher’s feud with the American geneticist Sewall Wright about adaptive landscapes, the theorem was widely misunderstood to mean that the average fitness of a population would always increase, even though models showed this not to be the case. In 1972, George R. Price showed that Fisher’s theorem was indeed correct (and that Fisher’s proof was also correct, given a typo or two), but did not find it to be of great significance.
</p></blockquote>
<p>Earlier this article says Fisher&#8217;s theorem is not a theorem, which seems to contradict this claim!  That&#8217;s partially because Wikipedia is written by a multitude of authors, but also it seems to reflect the confusion and controversy among biologists about the status of Fisher&#8217;s theorem.</p>
<p>I should read Fisher&#8217;s original paper and try to make the Wikipedia article a bit clearer.  In particular, it never clearly states the theorem, including the assumptions that imply the conclusion!</p>
<blockquote><p>
  You, on the other hand, could show that for your system Fisher’s hypothesis is not true and even more.
</p></blockquote>
<p>I can show that for certain systems Fisher&#8217;s assumptions don&#8217;t hold and his conclusion doesn&#8217;t hold either, but <em>my</em> more general assumptions <em>do</em> hold and so my conclusions do.</p>
<blockquote><p>
  You could give a mathematically sound description of a replacement. I would really like to see that on the arxiv to better understand the details.
</p></blockquote>
<p>Okay, thanks for your vote.  I&#8217;ve been thinking this might be worthwhile.  I&#8217;m trying to think of a good title&#8212;one that would simultaneously say I&#8217;m trying to generalize Fisher&#8217;s fundamental theorem and that this new theorem has quite different conclusions.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Mart Malakoff		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87384</link>

		<dc:creator><![CDATA[Mart Malakoff]]></dc:creator>
		<pubDate>Sun, 05 Feb 2017 16:18:59 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87384</guid>

					<description><![CDATA[P.S. I think a lot if not all of this can be done via Markov chains/processes. However these are higher order Markov processes (usually termed non-Markov). The next state depends on current one, and many previous ones---its state dependent---and all states are not created equal---e.g. US electoral college.)]]></description>
			<content:encoded><![CDATA[<p>P.S. I think a lot if not all of this can be done via Markov chains/processes. However these are higher order Markov processes (usually termed non-Markov). The next state depends on current one, and many previous ones&#8212;its state dependent&#8212;and all states are not created equal&#8212;e.g. US electoral college.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Mart Malakoff		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87383</link>

		<dc:creator><![CDATA[Mart Malakoff]]></dc:creator>
		<pubDate>Sun, 05 Feb 2017 16:12:26 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23248#comment-87383</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87374&quot;&gt;Uwe Stroinski&lt;/a&gt;.

I think Fisher&#039;s FT theorem is a theorem, based on assumptions or axioms.  The problem with theorems used for real systems like biology and economics, is that the assumptions are unrealistic (simplified) in most cases. (I think there are a few simple cases where FFT does hold---simple organisms like bacteria which live in a dilute or uncrowded environment under constant conditions.)

If you look at the George Price paper linked to in the blog post you will see on page 2 exactly what assumptions Fisher used to derive his theorem.  It only applies to additive genetic variance---that organism and gene fitnesses do not depend on environmental conditions.

Hence it doesn&#039;t applies to systems with epistasis and dominance (in which the fitness of a gene depends on interactions with other genes), population density (meaning that organisms or genes are indifferent to how crowded they are---a billion rats in a small cage are as fit as if there are 10), frequency dependence (like saying a cook who has a &#039;perfect recipe&#039; involving specific combinations of ingredients is no better than one who just randomly throws in ingredients---e.g. a cup of coffee which is 90% coffee and 10% sugar is as good or &#039;fit&#039; as one which is 10% coffee and 90% sugar), and exogenous l changes in environment don&#039;t matter (e.g. climate change).

This is why FFT in its most commonly used limited form is viewed as bean bag genetics. (If this were true, things like &#039;plant and animal breeding for better varieties&#039; would be impossible.

Price&#039;s form of FFT (which is the full theorem including terms commonly dropped out)  avoids this problem because it includes the &#039;nonlinear&#039; terms usually left out. Its an &#039;ideal gas model&#039;---assumes genes are like Boltzmann&#039;s perfectly elastic colliding point particles.   Real particles have interactions, and are &#039;frustrated&#039;---cannot collide with every other particle due to tie and space constraints.

This is why FFT is similar to general equilibrium eocnomics.  That also is a theorem---that economies like simple genetic systems always move to some optima and eventually an equilibrium.  But this assumes humans have perfect information, interact (trade) with everyone else in the economy, act for self-interest, by maximizing an unchanging utility function or set of preferences.    Assuming that, economies due move to unique equilibria which are &#039;Pareto optimal&#039;.

Once you through in some non-linearities (e.g. interdependent utility functions), or environmental effects (transaction costs, human effect on the environment, or learning---changing preferences), in general there are multiple equilibria or none at all (at least  unique ones)---the system just cycles or is chaotic.

Interesting to me is that even assuming bean bag or ideal gas type models, sometimes you can results in good agreement with observations---just as many physical phenomena are approximately Gaussian, though they don&#039;t arise by some random diffusion process (but rather often by the way data are selected and aggregated).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/02/01/information-geometry-part-16/#comment-87374">Uwe Stroinski</a>.</p>
<p>I think Fisher&#8217;s FT theorem is a theorem, based on assumptions or axioms.  The problem with theorems used for real systems like biology and economics, is that the assumptions are unrealistic (simplified) in most cases. (I think there are a few simple cases where FFT does hold&#8212;simple organisms like bacteria which live in a dilute or uncrowded environment under constant conditions.)</p>
<p>If you look at the George Price paper linked to in the blog post you will see on page 2 exactly what assumptions Fisher used to derive his theorem.  It only applies to additive genetic variance&#8212;that organism and gene fitnesses do not depend on environmental conditions.</p>
<p>Hence it doesn&#8217;t applies to systems with epistasis and dominance (in which the fitness of a gene depends on interactions with other genes), population density (meaning that organisms or genes are indifferent to how crowded they are&#8212;a billion rats in a small cage are as fit as if there are 10), frequency dependence (like saying a cook who has a &#8216;perfect recipe&#8217; involving specific combinations of ingredients is no better than one who just randomly throws in ingredients&#8212;e.g. a cup of coffee which is 90% coffee and 10% sugar is as good or &#8216;fit&#8217; as one which is 10% coffee and 90% sugar), and exogenous l changes in environment don&#8217;t matter (e.g. climate change).</p>
<p>This is why FFT in its most commonly used limited form is viewed as bean bag genetics. (If this were true, things like &#8216;plant and animal breeding for better varieties&#8217; would be impossible.</p>
<p>Price&#8217;s form of FFT (which is the full theorem including terms commonly dropped out)  avoids this problem because it includes the &#8216;nonlinear&#8217; terms usually left out. Its an &#8216;ideal gas model&#8217;&#8212;assumes genes are like Boltzmann&#8217;s perfectly elastic colliding point particles.   Real particles have interactions, and are &#8216;frustrated&#8217;&#8212;cannot collide with every other particle due to tie and space constraints.</p>
<p>This is why FFT is similar to general equilibrium eocnomics.  That also is a theorem&#8212;that economies like simple genetic systems always move to some optima and eventually an equilibrium.  But this assumes humans have perfect information, interact (trade) with everyone else in the economy, act for self-interest, by maximizing an unchanging utility function or set of preferences.    Assuming that, economies due move to unique equilibria which are &#8216;Pareto optimal&#8217;.</p>
<p>Once you through in some non-linearities (e.g. interdependent utility functions), or environmental effects (transaction costs, human effect on the environment, or learning&#8212;changing preferences), in general there are multiple equilibria or none at all (at least  unique ones)&#8212;the system just cycles or is chaotic.</p>
<p>Interesting to me is that even assuming bean bag or ideal gas type models, sometimes you can results in good agreement with observations&#8212;just as many physical phenomena are approximately Gaussian, though they don&#8217;t arise by some random diffusion process (but rather often by the way data are selected and aggregated).</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
