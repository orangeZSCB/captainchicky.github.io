<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Biology as Information Dynamics (Part 2)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/</link>
	<description></description>
	<lastBuildDate>Mon, 01 May 2017 04:24:26 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90334</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 01 May 2017 04:24:26 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90334</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90293&quot;&gt;Derek Wise&lt;/a&gt;.

Cool! Intransitive lizards!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90293">Derek Wise</a>.</p>
<p>Cool! Intransitive lizards!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90333</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 01 May 2017 04:22:16 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90333</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90300&quot;&gt;John Baez&lt;/a&gt;.

I think there can only be a smallest nonzero amount of information if there&#039;s a smallest nonzero probability: if you believe you have a fair n-sided die and I tell you the first side has not landed up, you&#039;ve received

$latex \log_2 n \; - \; \log_2 (n-1) $

bits of information, and this can be arbitrarily small if $latex n$ can be arbitrarily large.

I doubt we can build a fair die with a googolplex sides, but I also don&#039;t feel I&#039;ll learn much about information theory by pondering this issue---at least, I won&#039;t learn much very soon.  &lt;img src=&quot;http://math.ucr.edu/home/baez/emoticons/tongue2.gif&quot;/&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90300">John Baez</a>.</p>
<p>I think there can only be a smallest nonzero amount of information if there&#8217;s a smallest nonzero probability: if you believe you have a fair n-sided die and I tell you the first side has not landed up, you&#8217;ve received</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clog_2+n+%5C%3B+-+%5C%3B+%5Clog_2+%28n-1%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;log_2 n &#92;; - &#92;; &#92;log_2 (n-1) " class="latex" /></p>
<p>bits of information, and this can be arbitrarily small if <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n" class="latex" /> can be arbitrarily large.</p>
<p>I doubt we can build a fair die with a googolplex sides, but I also don&#8217;t feel I&#8217;ll learn much about information theory by pondering this issue&#8212;at least, I won&#8217;t learn much very soon.  <img src="https://i1.wp.com/math.ucr.edu/home/baez/emoticons/tongue2.gif"/></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Derek Wise		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90317</link>

		<dc:creator><![CDATA[Derek Wise]]></dc:creator>
		<pubDate>Sun, 30 Apr 2017 20:41:04 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90317</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90300&quot;&gt;John Baez&lt;/a&gt;.

I miss talking to you too! I&#039;ve got lots of stuff I&#039;d love to talk about, including that book.

I agree that a &quot;bit&quot; is not really the smallest amount of information in any fundamental sense.  It&#039;s more like the smallest kind of question you can ask to get some information -- the answer to a yes/no question.  How much information is contained in the &lt;em&gt;answer&lt;/em&gt; to a yes/no question depends a lot on the question.

However, your talk got me wondering whether, in some finite universe, there really can be a &quot;smallest possible&quot; question that could be asked, and thus a basic unit of information in that universe.  I&#039;m so far just idly wondering, and haven&#039;t tried working it out; maybe it&#039;s obviously nonsense...]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90300">John Baez</a>.</p>
<p>I miss talking to you too! I&#8217;ve got lots of stuff I&#8217;d love to talk about, including that book.</p>
<p>I agree that a &#8220;bit&#8221; is not really the smallest amount of information in any fundamental sense.  It&#8217;s more like the smallest kind of question you can ask to get some information &#8212; the answer to a yes/no question.  How much information is contained in the <em>answer</em> to a yes/no question depends a lot on the question.</p>
<p>However, your talk got me wondering whether, in some finite universe, there really can be a &#8220;smallest possible&#8221; question that could be asked, and thus a basic unit of information in that universe.  I&#8217;m so far just idly wondering, and haven&#8217;t tried working it out; maybe it&#8217;s obviously nonsense&#8230;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90300</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 30 Apr 2017 02:30:34 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90300</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90290&quot;&gt;Derek Wise&lt;/a&gt;.

Hi!  Glad you liked my talk!   Great to hear from you!  I really miss talking to you.  Someday we should finish that book on classical mechanics.

Despite what computer scientists seem to think, there&#039;s no reason to think information comes in integer multiples of bits.  For example, if we transmit data in base 3, it&#039;s easy to transmit a &lt;a href=&quot;https://en.wikipedia.org/wiki/Ternary_numeral_system&quot; rel=&quot;nofollow&quot;&gt;&lt;b&gt;trit&lt;/b&gt;&lt;/a&gt; of information, which is log&lt;sub&gt;2&lt;/sub&gt; 3 &#8776; 1.585 bits.

You&#039;ll note I cleverly didn&#039;t choose a base for my logarithms near the start of my talk; this is why.  In this setup, log 2 of information is one bit of information regardless of the base of the logarithm.  The only reason I did calculations where the answers came out to be integer multiples of a bit is to make it easy for people to follow the calculation.  Perhaps this was misleading!

Later, when talking about physics, I switched to using logarithms base e.  Then information gets measured in &lt;a href=&quot;https://en.wikipedia.org/wiki/Nat_(unit)&quot; rel=&quot;nofollow&quot;&gt;&lt;b&gt;nats&lt;/b&gt;&lt;/a&gt;, which I wish were called &#039;nits&#039;.

By the way, base 3 seems to have &lt;a href=&quot;https://en.wikipedia.org/wiki/Radix_economy&quot; rel=&quot;nofollow&quot;&gt;certain information-theoretic advantages over all other integer bases&lt;/a&gt;, coming from the fact that the closest integer to e is 3.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90290">Derek Wise</a>.</p>
<p>Hi!  Glad you liked my talk!   Great to hear from you!  I really miss talking to you.  Someday we should finish that book on classical mechanics.</p>
<p>Despite what computer scientists seem to think, there&#8217;s no reason to think information comes in integer multiples of bits.  For example, if we transmit data in base 3, it&#8217;s easy to transmit a <a href="https://en.wikipedia.org/wiki/Ternary_numeral_system" rel="nofollow"><b>trit</b></a> of information, which is log<sub>2</sub> 3 &asymp; 1.585 bits.</p>
<p>You&#8217;ll note I cleverly didn&#8217;t choose a base for my logarithms near the start of my talk; this is why.  In this setup, log 2 of information is one bit of information regardless of the base of the logarithm.  The only reason I did calculations where the answers came out to be integer multiples of a bit is to make it easy for people to follow the calculation.  Perhaps this was misleading!</p>
<p>Later, when talking about physics, I switched to using logarithms base e.  Then information gets measured in <a href="https://en.wikipedia.org/wiki/Nat_(unit)" rel="nofollow"><b>nats</b></a>, which I wish were called &#8216;nits&#8217;.</p>
<p>By the way, base 3 seems to have <a href="https://en.wikipedia.org/wiki/Radix_economy" rel="nofollow">certain information-theoretic advantages over all other integer bases</a>, coming from the fact that the closest integer to e is 3.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Derek Wise		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90293</link>

		<dc:creator><![CDATA[Derek Wise]]></dc:creator>
		<pubDate>Sat, 29 Apr 2017 22:26:29 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90293</guid>

					<description><![CDATA[Also ...  The lizards are interesting!  I hadn&#039;t known about this phenomenon, but it immediately made me think of rock-paper-scissors, too, at least initially.

Then I happened to also be reading &lt;a href=&quot;https://gowers.wordpress.com/2017/04/28/a-potential-new-polymath-project-intransitive-dice/&quot; rel=&quot;nofollow&quot;&gt;this post on Tim Gowers&#039;s blog&lt;/a&gt; today, and realized that the lizards are more like &lt;strong&gt;intransitive dice&lt;/strong&gt;!

This is like a probabilistic version of rock-paper-scissors in which, for example, rocks &lt;em&gt;usually&lt;/em&gt; smash scissors, but sometimes the scissors manage to cut up a rock.]]></description>
			<content:encoded><![CDATA[<p>Also &#8230;  The lizards are interesting!  I hadn&#8217;t known about this phenomenon, but it immediately made me think of rock-paper-scissors, too, at least initially.</p>
<p>Then I happened to also be reading <a href="https://gowers.wordpress.com/2017/04/28/a-potential-new-polymath-project-intransitive-dice/" rel="nofollow">this post on Tim Gowers&#8217;s blog</a> today, and realized that the lizards are more like <strong>intransitive dice</strong>!</p>
<p>This is like a probabilistic version of rock-paper-scissors in which, for example, rocks <em>usually</em> smash scissors, but sometimes the scissors manage to cut up a rock.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Derek Wise		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90290</link>

		<dc:creator><![CDATA[Derek Wise]]></dc:creator>
		<pubDate>Sat, 29 Apr 2017 21:39:07 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90290</guid>

					<description><![CDATA[Nice talk, John!

One really basic question about the relative information $latex I(q,p)$:

You picked your examples so that this came out as some multiple of log(2), which you interpreted a &quot;bit&quot; of gained information.  But, what if I(q,p) isn&#039;t just an (integer) number of bits of this size?

For example, suppose you roll a 6-sided die, and then tell me you didn&#039;t roll a &quot;6&quot;.  I&#039;ll update my prior from the uniform distribution to a distribution that&#039;s uniform on the outcomes &quot;1&quot; through &quot;5&quot;, and zero for the outcome &quot;6&quot;.  This seems to give me

$latex I(q,p) = \log(6)-\log(5)$

but how am I to interpret this?

I suppose the &quot;bits&quot; just have a different size in this situation?  Maybe that&#039;s the point of information being &quot;relative&quot;?  Is there a systematic way of figuring out what constitutes one bit of relative information in a given situation? Or, is it rather that this kind of information doesn&#039;t actually come in discrete chunks?]]></description>
			<content:encoded><![CDATA[<p>Nice talk, John!</p>
<p>One really basic question about the relative information <img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p)" class="latex" />:</p>
<p>You picked your examples so that this came out as some multiple of log(2), which you interpreted a &#8220;bit&#8221; of gained information.  But, what if I(q,p) isn&#8217;t just an (integer) number of bits of this size?</p>
<p>For example, suppose you roll a 6-sided die, and then tell me you didn&#8217;t roll a &#8220;6&#8221;.  I&#8217;ll update my prior from the uniform distribution to a distribution that&#8217;s uniform on the outcomes &#8220;1&#8221; through &#8220;5&#8221;, and zero for the outcome &#8220;6&#8221;.  This seems to give me</p>
<p><img src="https://s0.wp.com/latex.php?latex=I%28q%2Cp%29+%3D+%5Clog%286%29-%5Clog%285%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="I(q,p) = &#92;log(6)-&#92;log(5)" class="latex" /></p>
<p>but how am I to interpret this?</p>
<p>I suppose the &#8220;bits&#8221; just have a different size in this situation?  Maybe that&#8217;s the point of information being &#8220;relative&#8221;?  Is there a systematic way of figuring out what constitutes one bit of relative information in a given situation? Or, is it rather that this kind of information doesn&#8217;t actually come in discrete chunks?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90272</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 28 Apr 2017 14:54:05 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90272</guid>

					<description><![CDATA[Kram wrote:

&lt;blockquote&gt;
  Shouldn’t it rather be:
  
  $latex p_i \propto e^{-\frac{E_i}{k T}}$
&lt;/blockquote&gt;

Yes.  Everyone knows this!  I can&#039;t believe I made such a typo and nobody pointed it out during my talk!  I&#039;ll check, and fix it on my slides if needed.

I&#039;ll reply to your less distressing comments later.]]></description>
			<content:encoded><![CDATA[<p>Kram wrote:</p>
<blockquote><p>
  Shouldn’t it rather be:</p>
<p>  <img src="https://s0.wp.com/latex.php?latex=p_i+%5Cpropto+e%5E%7B-%5Cfrac%7BE_i%7D%7Bk+T%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;propto e^{-&#92;frac{E_i}{k T}}" class="latex" />
</p></blockquote>
<p>Yes.  Everyone knows this!  I can&#8217;t believe I made such a typo and nobody pointed it out during my talk!  I&#8217;ll check, and fix it on my slides if needed.</p>
<p>I&#8217;ll reply to your less distressing comments later.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Kram Einsnulldreizwei		</title>
		<link>https://johncarlosbaez.wordpress.com/2017/04/27/biology-as-information-dynamics-part-2/#comment-90265</link>

		<dc:creator><![CDATA[Kram Einsnulldreizwei]]></dc:creator>
		<pubDate>Fri, 28 Apr 2017 07:36:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=23520#comment-90265</guid>

					<description><![CDATA[On the slide showing the proportionality of the probability of a particle having an energy E_i, is that correct?

You had:

$latex p_i \propto e^{-\frac{k E_i}{T}} $

but then the unit in the expression would be the exponential of entropy squared. Shouldn&#039;t it rather be:

$latex p_i \propto e^{-\frac{E_i}{k T}} $

to make it unit free?

Isn&#039;t it pretty much always the case that the mathematically simplest situations are those that are the most symmetric and finely tuned?

So this all works out to a lot of evolutionary pressure meaning quick fixes are to be found. Those fixes may not be globally optimal, they may have problems, but they could be sufficient to give you an edge. And only if the situation &quot;cools down&quot; do you actually have the time thinking about all this in more detail so you can actually fix and improve things much more easily. However, if things are &lt;em&gt;too&lt;/em&gt; cool / there is almost no evolutionary pressure, you will likely also lose interest and nothing is evolved at all, right?

I think that should be relevant to innovation theory: Is there some sort of &quot;optimal temperature&quot; at which innovation is ideally balanced between finding new stuff and optimizing old stuff? - Or if not that, is there an &quot;optimal temperature distribution&quot; (meaning it&#039;s allowed to fluctuate, but in a specific way)? And if there is, could we approximate it for real life, facing a lack of information? (Like, for instance, that we don&#039;t and can&#039;t know what there is left to be invented, or that it&#039;s hard to even properly quantify innovation in retrospect, let alone in the moment.)

Really nice talk!]]></description>
			<content:encoded><![CDATA[<p>On the slide showing the proportionality of the probability of a particle having an energy E_i, is that correct?</p>
<p>You had:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%5Cpropto+e%5E%7B-%5Cfrac%7Bk+E_i%7D%7BT%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;propto e^{-&#92;frac{k E_i}{T}} " class="latex" /></p>
<p>but then the unit in the expression would be the exponential of entropy squared. Shouldn&#8217;t it rather be:</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%5Cpropto+e%5E%7B-%5Cfrac%7BE_i%7D%7Bk+T%7D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &#92;propto e^{-&#92;frac{E_i}{k T}} " class="latex" /></p>
<p>to make it unit free?</p>
<p>Isn&#8217;t it pretty much always the case that the mathematically simplest situations are those that are the most symmetric and finely tuned?</p>
<p>So this all works out to a lot of evolutionary pressure meaning quick fixes are to be found. Those fixes may not be globally optimal, they may have problems, but they could be sufficient to give you an edge. And only if the situation &#8220;cools down&#8221; do you actually have the time thinking about all this in more detail so you can actually fix and improve things much more easily. However, if things are <em>too</em> cool / there is almost no evolutionary pressure, you will likely also lose interest and nothing is evolved at all, right?</p>
<p>I think that should be relevant to innovation theory: Is there some sort of &#8220;optimal temperature&#8221; at which innovation is ideally balanced between finding new stuff and optimizing old stuff? &#8211; Or if not that, is there an &#8220;optimal temperature distribution&#8221; (meaning it&#8217;s allowed to fluctuate, but in a specific way)? And if there is, could we approximate it for real life, facing a lack of information? (Like, for instance, that we don&#8217;t and can&#8217;t know what there is left to be invented, or that it&#8217;s hard to even properly quantify innovation in retrospect, let alone in the moment.)</p>
<p>Really nice talk!</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
