<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: An Entropy Challenge	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/</link>
	<description></description>
	<lastBuildDate>Sat, 01 Sep 2012 02:48:35 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: tqft9999tqft		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19192</link>

		<dc:creator><![CDATA[tqft9999tqft]]></dc:creator>
		<pubDate>Sat, 01 Sep 2012 02:48:35 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19192</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19036&quot;&gt;Boris Borcic&lt;/a&gt;.

If you get a collection of code solving this problem this site would appreciate the donation if possible.
http://rosettacode.org/wiki/Rosetta_Code
&quot;Rosetta Code is a programming chrestomathy site. The idea is to present solutions to the same task in as many different languages as possible, to demonstrate how languages are similar and different&quot;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19036">Boris Borcic</a>.</p>
<p>If you get a collection of code solving this problem this site would appreciate the donation if possible.<br />
<a href="http://rosettacode.org/wiki/Rosetta_Code" rel="nofollow ugc">http://rosettacode.org/wiki/Rosetta_Code</a><br />
&#8220;Rosetta Code is a programming chrestomathy site. The idea is to present solutions to the same task in as many different languages as possible, to demonstrate how languages are similar and different&#8221;</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Marc Harper		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19165</link>

		<dc:creator><![CDATA[Marc Harper]]></dc:creator>
		<pubDate>Fri, 31 Aug 2012 20:25:25 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19165</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19138&quot;&gt;John Baez&lt;/a&gt;.

Here are some simple computations. For 10,000,000 randomly selected pairs of points in the simplex (n=3), 826,289 pairs had both p and q decreasing; of these, 360,420 satisfied the Renyi inequality, and of these pairs, 325,013 had p majorizing q. So the desired proportion is ~10%.

Here is one such pair if anyone wants to verify (Renyi for all beta and p majorizes q):
(0.702035380032679, 0.25269600971504746, 0.04526861025227358)
(0.6012357521664494, 0.32989515938159686, 0.06886908845195372)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19138">John Baez</a>.</p>
<p>Here are some simple computations. For 10,000,000 randomly selected pairs of points in the simplex (n=3), 826,289 pairs had both p and q decreasing; of these, 360,420 satisfied the Renyi inequality, and of these pairs, 325,013 had p majorizing q. So the desired proportion is ~10%.</p>
<p>Here is one such pair if anyone wants to verify (Renyi for all beta and p majorizes q):<br />
(0.702035380032679, 0.25269600971504746, 0.04526861025227358)<br />
(0.6012357521664494, 0.32989515938159686, 0.06886908845195372)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19138</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 31 Aug 2012 09:00:43 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19138</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19132&quot;&gt;Marc Harper&lt;/a&gt;.

Thanks!  I&#039;m not sure why I&#039;d want lots of examples, but here&#039;s something related.

Oscar was asking if this behavior---all R&#233;nyi entropies of p bigger than of q, but p not majorizing q---is unusual.  Maybe without much extra work you could figure out roughly what &#039;percentage&#039; of pairs (p,q) display this behavior, say for probability distributions on a 3-element set?

If you&#039;ve already written code to decide if a given pair of probability distributions displays this behavior, then you just need to 

1) decide what measure you&#039;ll use to define the &#039;percentage&#039; of pairs of probability distributions that display a given behavior,

2) figure out how to estimate that percentage, either using Monte Carlo estimation or some other procedure.

You can think of a probability distribution on a 3-element set as a triple

$latex (x,y,z) $

such that 

$latex 0 \le x, y,z \le 1  $

and 

$latex x + y + z = 1 $

The space of these is an equilateral triangle.  Since the problem involves majorization and (implicitly) convex-linear maps, the symmetries are just the obvious symmetries of the triangle.  So, it makes some sense to just use the obvious measure on this triangle, which is proportional to 

$latex dx \; dy $

So for Monte Carlo, you can just randomly pick $latex x \in [0,1]$ in a uniformly distributed way, then randomly pick $latex y \in [0,1-x]$ in a uniformly distributed way, then let $latex z = 1-x-y$.

As you know, there&#039;s also another measure on this equilateral triangle, coming from the Fisher information metric.  That would give a different answer!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19132">Marc Harper</a>.</p>
<p>Thanks!  I&#8217;m not sure why I&#8217;d want lots of examples, but here&#8217;s something related.</p>
<p>Oscar was asking if this behavior&#8212;all R&eacute;nyi entropies of p bigger than of q, but p not majorizing q&#8212;is unusual.  Maybe without much extra work you could figure out roughly what &#8216;percentage&#8217; of pairs (p,q) display this behavior, say for probability distributions on a 3-element set?</p>
<p>If you&#8217;ve already written code to decide if a given pair of probability distributions displays this behavior, then you just need to </p>
<p>1) decide what measure you&#8217;ll use to define the &#8216;percentage&#8217; of pairs of probability distributions that display a given behavior,</p>
<p>2) figure out how to estimate that percentage, either using Monte Carlo estimation or some other procedure.</p>
<p>You can think of a probability distribution on a 3-element set as a triple</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%2Cz%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="(x,y,z) " class="latex" /></p>
<p>such that </p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%5Cle+x%2C+y%2Cz+%5Cle+1++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &#92;le x, y,z &#92;le 1  " class="latex" /></p>
<p>and </p>
<p><img src="https://s0.wp.com/latex.php?latex=x+%2B+y+%2B+z+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x + y + z = 1 " class="latex" /></p>
<p>The space of these is an equilateral triangle.  Since the problem involves majorization and (implicitly) convex-linear maps, the symmetries are just the obvious symmetries of the triangle.  So, it makes some sense to just use the obvious measure on this triangle, which is proportional to </p>
<p><img src="https://s0.wp.com/latex.php?latex=dx+%5C%3B+dy+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="dx &#92;; dy " class="latex" /></p>
<p>So for Monte Carlo, you can just randomly pick <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in [0,1]" class="latex" /> in a uniformly distributed way, then randomly pick <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+%5B0%2C1-x%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="y &#92;in [0,1-x]" class="latex" /> in a uniformly distributed way, then let <img src="https://s0.wp.com/latex.php?latex=z+%3D+1-x-y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z = 1-x-y" class="latex" />.</p>
<p>As you know, there&#8217;s also another measure on this equilateral triangle, coming from the Fisher information metric.  That would give a different answer!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Marc Harper		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19132</link>

		<dc:creator><![CDATA[Marc Harper]]></dc:creator>
		<pubDate>Fri, 31 Aug 2012 05:37:44 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19132</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19011&quot;&gt;Marc Harper&lt;/a&gt;.

John, I wrote some code to systematically find many examples, and it finds many. Let me know if you have any use for it.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19011">Marc Harper</a>.</p>
<p>John, I wrote some code to systematically find many examples, and it finds many. Let me know if you have any use for it.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19073</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 30 Aug 2012 11:12:50 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19073</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19053&quot;&gt;Tom Leinster&lt;/a&gt;.

Thanks, Tom!  That &#039;intrinsically more diverse&#039; idea indeed seems closely connected to majorization.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19053">Tom Leinster</a>.</p>
<p>Thanks, Tom!  That &#8216;intrinsically more diverse&#8217; idea indeed seems closely connected to majorization.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19072</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 30 Aug 2012 11:07:56 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19072</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19049&quot;&gt;Arrow&lt;/a&gt;.

$latex \beta &#060; 0$ would make the entropy undefined for probability distributions that vanish at some points, and it would probably have many other evil features as well.  For example, low-probability events would contribute enormously to the entropy.  

In a certain analogy to thermodynamics, which I used to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/&quot; rel=&quot;nofollow&quot;&gt;relate R&#233;nyi entropy to free energy&lt;/a&gt;, $latex \beta &#060; 0$ would correspond to temperatures below absolute zero.  At negative temperatures, a system is more likely to be found in states of high energy than states of low energy.  &lt;a href=&quot;http://en.wikipedia.org/wiki/Negative_temperature&quot; rel=&quot;nofollow&quot;&gt;Negative temperatures are not completely absurd&lt;/a&gt;, but they only make sense for systems with an upper bound on their energy---just as positive temperatures only make sense for systems with a lower bound on their energy.  ]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19049">Arrow</a>.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3C+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &lt; 0" class="latex" /> would make the entropy undefined for probability distributions that vanish at some points, and it would probably have many other evil features as well.  For example, low-probability events would contribute enormously to the entropy.  </p>
<p>In a certain analogy to thermodynamics, which I used to <a href="https://johncarlosbaez.wordpress.com/2011/02/10/rnyi-entropy-and-free-energy/" rel="nofollow">relate R&eacute;nyi entropy to free energy</a>, <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3C+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &lt; 0" class="latex" /> would correspond to temperatures below absolute zero.  At negative temperatures, a system is more likely to be found in states of high energy than states of low energy.  <a href="http://en.wikipedia.org/wiki/Negative_temperature" rel="nofollow">Negative temperatures are not completely absurd</a>, but they only make sense for systems with an upper bound on their energy&#8212;just as positive temperatures only make sense for systems with a lower bound on their energy.  </p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19071</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 30 Aug 2012 10:58:07 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19071</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19048&quot;&gt;Arrow&lt;/a&gt;.

Nice examples, Arrow!  I like your last example the best.

For including code you surround the code by something like this:

[ sourcecode language=&quot;python&quot;]

[ /sourcecode]

except without the space after the [.    I don&#039;t know what range of languages it takes.  It seems to do something halfway reasonable if you leave the language choice out.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19048">Arrow</a>.</p>
<p>Nice examples, Arrow!  I like your last example the best.</p>
<p>For including code you surround the code by something like this:</p>
<p>[ sourcecode language=&#8221;python&#8221;]</p>
<p>[ /sourcecode]</p>
<p>except without the space after the [.    I don&#8217;t know what range of languages it takes.  It seems to do something halfway reasonable if you leave the language choice out.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19057</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 30 Aug 2012 01:28:40 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19057</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19020&quot;&gt;Marc Harper&lt;/a&gt;.

Just for the sake of people who are too lazy to read all the comments, I&#039;ll note that this remark of Marc&#039;s was based on a misunderstanding that was cleared up &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19029&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19020">Marc Harper</a>.</p>
<p>Just for the sake of people who are too lazy to read all the comments, I&#8217;ll note that this remark of Marc&#8217;s was based on a misunderstanding that was cleared up <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19029" rel="nofollow">here</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19056</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 30 Aug 2012 01:26:53 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19056</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19015&quot;&gt;David Speyer&lt;/a&gt;.

I&#039;ll delete &#039;em if you want.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19015">David Speyer</a>.</p>
<p>I&#8217;ll delete &#8217;em if you want.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19055</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Thu, 30 Aug 2012 01:25:06 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11843#comment-19055</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19010&quot;&gt;David Speyer&lt;/a&gt;.

That looks convincing---thanks a lot, David!  For those who haven&#039;t thought about this at at all: we want the function graphed here to be negative for $latex 0 &#060; \beta &#060; 1$ and positive for $latex \beta &#062; 1.$

By the way, there are simple formulas for the R&#233;nyi entropy at $latex \beta = 0$ and $latex \beta = \infty,$ which allow us to understand exactly what&#039;s going on at these points.  Oscar used them to construct his example.  At $latex \beta = 0$ we get the &lt;a href=&quot;http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#Min-entropy&quot; rel=&quot;nofollow&quot;&gt;&lt;b&gt;min-entropy&lt;/b&gt;&lt;/a&gt; and at $latex \beta = \infty$ we get the &lt;b&gt;max-entropy&lt;/b&gt;, which is the logarithm of the number of $latex i$ such that $latex p_i &#062; 0$.  

Since both of Oscar&#039;s probability distributions have the same max-entropy, the curve you graphed approaches zero as $latex \beta \to \infty,$ just as it looks.  

It approaches a certain negative value as $latex \beta \to 0,$ though the eye could be fooled into thinking it approaches zero.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/08/29/an-entropy-challenge/#comment-19010">David Speyer</a>.</p>
<p>That looks convincing&#8212;thanks a lot, David!  For those who haven&#8217;t thought about this at at all: we want the function graphed here to be negative for <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Cbeta+%3C+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="0 &lt; &#92;beta &lt; 1" class="latex" /> and positive for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3E+1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &gt; 1." class="latex" /></p>
<p>By the way, there are simple formulas for the R&eacute;nyi entropy at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cinfty%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = &#92;infty," class="latex" /> which allow us to understand exactly what&#8217;s going on at these points.  Oscar used them to construct his example.  At <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = 0" class="latex" /> we get the <a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#Min-entropy" rel="nofollow"><b>min-entropy</b></a> and at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta = &#92;infty" class="latex" /> we get the <b>max-entropy</b>, which is the logarithm of the number of <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=p_i+%3E+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i &gt; 0" class="latex" />.  </p>
<p>Since both of Oscar&#8217;s probability distributions have the same max-entropy, the curve you graphed approaches zero as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to &#92;infty," class="latex" /> just as it looks.  </p>
<p>It approaches a certain negative value as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+0%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta &#92;to 0," class="latex" /> though the eye could be fooled into thinking it approaches zero.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
