<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Information Geometry (Part 9)	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/</link>
	<description></description>
	<lastBuildDate>Tue, 31 Jan 2017 23:10:31 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Biology as Information Dynamics &#124; Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-87204</link>

		<dc:creator><![CDATA[Biology as Information Dynamics &#124; Azimuth]]></dc:creator>
		<pubDate>Tue, 31 Jan 2017 05:17:18 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-87204</guid>

					<description><![CDATA[This is my talk for the workshop &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2017/01/23/quantifying-biological-complexity/&quot;&gt;Biological Complexity: Can It Be Quantified?&lt;/a&gt;

&#8226; &lt;a href=&quot;http://math.ucr.edu/home/baez/bio_asu/bio_asu_web.pdf&quot; rel=&quot;nofollow&quot;&gt;Biology as information dynamics&lt;/a&gt;.

&lt;blockquote&gt;
&lt;b&gt;Abstract.&lt;/b&gt; If biology is the study of self-replicating entities, and we want to understand the role of information, it makes sense to see how information theory is connected to the &#039;replicator equation&#039;&#8212;a simple model of population dynamics for self-replicating entities. The relevant concept of information turns out to be the information of one probability distribution relative to another, also known as the Kullback&#8211;Liebler divergence. Using this we can see evolution as a learning process, and give a clean general formulation of Fisher&#039;s fundamental theorem of natural selection. 
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>This is my talk for the workshop <a href="https://johncarlosbaez.wordpress.com/2017/01/23/quantifying-biological-complexity/">Biological Complexity: Can It Be Quantified?</a></p>
<p>&bull; <a href="http://math.ucr.edu/home/baez/bio_asu/bio_asu_web.pdf" rel="nofollow">Biology as information dynamics</a>.</p>
<blockquote><p>
<b>Abstract.</b> If biology is the study of self-replicating entities, and we want to understand the role of information, it makes sense to see how information theory is connected to the &#8216;replicator equation&#8217;&mdash;a simple model of population dynamics for self-replicating entities. The relevant concept of information turns out to be the information of one probability distribution relative to another, also known as the Kullback&ndash;Liebler divergence. Using this we can see evolution as a learning process, and give a clean general formulation of Fisher&#8217;s fundamental theorem of natural selection.
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: akirabergman		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-16413</link>

		<dc:creator><![CDATA[akirabergman]]></dc:creator>
		<pubDate>Fri, 29 Jun 2012 03:17:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-16413</guid>

					<description><![CDATA[I have been intrigued by the similarity of the definition of entropy to the formula for the n&#039;th prime number; 

p(n) = n*ln(n) + ...

Is this similarity only accidental or does it have a meaning?]]></description>
			<content:encoded><![CDATA[<p>I have been intrigued by the similarity of the definition of entropy to the formula for the n&#8217;th prime number; </p>
<p>p(n) = n*ln(n) + &#8230;</p>
<p>Is this similarity only accidental or does it have a meaning?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: The Mathematics of Biodiversity (Part 3) « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-16334</link>

		<dc:creator><![CDATA[The Mathematics of Biodiversity (Part 3) « Azimuth]]></dc:creator>
		<pubDate>Wed, 27 Jun 2012 15:50:58 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-16334</guid>

					<description><![CDATA[This would be a version of the replicator equation, which I explained recently in Information Geometry (Part 9). [...]]]></description>
			<content:encoded><![CDATA[<p>This would be a version of the replicator equation, which I explained recently in Information Geometry (Part 9). [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 13) « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-16274</link>

		<dc:creator><![CDATA[Information Geometry (Part 13) « Azimuth]]></dc:creator>
		<pubDate>Tue, 26 Jun 2012 16:19:00 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-16274</guid>

					<description><![CDATA[In Part 9, I told you about the &#8216;replicator equation&#8217;, which says how these fractions change with time [...]]]></description>
			<content:encoded><![CDATA[<p>In Part 9, I told you about the &#8216;replicator equation&#8217;, which says how these fractions change with time [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15785</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 08 Jun 2012 03:11:30 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-15785</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15677&quot;&gt;davidtweed&lt;/a&gt;.

David wrote:

&lt;blockquote&gt;
I think the use of phrases like “absorb information from the environment” are a little too passive. I’d say a more informative analogy is that processes “acquire information from the environment”, 
&lt;/blockquote&gt;

Evolutionary biologists are trained to avoid &lt;a href=&quot;http://en.wikipedia.org/wiki/Teleology#Biology&quot; rel=&quot;nofollow&quot;&gt;&#039;teleological&#039;&lt;/a&gt; or &#039;purposive&#039; accounts:

&lt;blockquote&gt;
Statements which imply that nature has goals, for example where a species is said to do something &quot;in order to&quot; achieve survival, appear teleological, and therefore invalid. Usually, it is possible to rewrite such sentences to avoid the apparent teleology. Some biology courses have incorporated exercises requiring students to rephrase such sentences so that they do not read teleologically. 
&lt;/blockquote&gt;

This is important for avoiding some mistakes.  But I think it&#039;ll be very interesting for evolutionary biology to collide with machine learning, where people with goals are designing systems &lt;i&gt;in order to&lt;/i&gt; achieve those goals.  Deen Abiola&#039;s comments &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15783&quot; rel=&quot;nofollow&quot;&gt;below&lt;/a&gt; show what I mean.

&lt;blockquote&gt;
(E.g., if you really want to maximise your probability of getting some right answer, you’re best served by testing “answers” at widely spaced parts of the configuration space, but biological evolution seems to churn out individuals who are only slightly different from their parents. Why is that? One quick possibility is that “evolutionary inference” thinks the parents are already quite close to the answer, so small modifications are called for. Or maybe it’s one or multiple other reasons…)
&lt;/blockquote&gt;


Here you&#039;re talking about what evolutionary inference &quot;thinks&quot;, which would give you a rap on the knuckles in those biology courses.  &lt;img src=&quot;http://math.ucr.edu/home/baez/emoticons/tongue2.gif&quot; /&gt;

More seriously, but relatedly, biologists are really interested in to what extent we can think of evolution itself as having been optimized for something... instead of just being a fixed method whereby other things get optimized.   The buzzword for this puzzle is &lt;a href=&quot;http://en.wikipedia.org/wiki/Evolvability#Evolution_of_evolvability&quot; rel=&quot;nofollow&quot;&gt;the evolution of evolvability&lt;/a&gt;:

&lt;blockquote&gt;
While variation yielding high evolvability could be useful in the long term, in the short term most of that variation is likely to be a disadvantage. For example, naively it would seem that increasing the mutation rate via a &lt;a href=&quot;http://www2.mnhn.fr/oseb/spip/IMG/pdf/Taddei_et_al_Nature_1997.pdf&quot; rel=&quot;nofollow&quot;&gt;mutator allele&lt;/a&gt; would increase evolvability. But as an extreme example, if the mutation rate is too high then all individuals will be dead or at least carry a heavy mutation load. Short-term selection for low variation most of the time is usually thought likely to be more powerful than long-term selection for evolvability, making it difficult for natural selection to cause the evolution of evolvability.

When recombination is low, mutator alleles may still sometimes hitchhike on the success of adaptive mutations that they cause. In this case, selection can take place at the level of the lineage. This may explain why mutators are often seen during experimental evolution of microbes. Mutator alleles can also evolve more easily when they only increase mutation rates in nearby DNA sequences, not across the whole genome: this is known as a contingency locus.

The evolution of evolvability is less controversial if it occurs via the evolution of sexual reproduction, or via the tendency of variation-generating mechanisms to become more active when an organism is stressed. The yeast prion may also be an example of the evolution of evolvability through &lt;a href=&quot;http://en.wikipedia.org/wiki/Evolutionary_capacitance&quot; rel=&quot;nofollow&quot;&gt;evolutionary capacitance&lt;/a&gt;. An evolutionary capacitor is a switch that turns genetic variation on and off. This is very much like bet-hedging the risk that a future environment will be similar or different. Theoretical models also predict the evolution of evolvability via modularity. When the costs of evolvability are sufficiently short-lived, more evolvable lineages may be the most successful in the long-term.
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15677">davidtweed</a>.</p>
<p>David wrote:</p>
<blockquote><p>
I think the use of phrases like “absorb information from the environment” are a little too passive. I’d say a more informative analogy is that processes “acquire information from the environment”,
</p></blockquote>
<p>Evolutionary biologists are trained to avoid <a href="http://en.wikipedia.org/wiki/Teleology#Biology" rel="nofollow">&#8216;teleological&#8217;</a> or &#8216;purposive&#8217; accounts:</p>
<blockquote><p>
Statements which imply that nature has goals, for example where a species is said to do something &#8220;in order to&#8221; achieve survival, appear teleological, and therefore invalid. Usually, it is possible to rewrite such sentences to avoid the apparent teleology. Some biology courses have incorporated exercises requiring students to rephrase such sentences so that they do not read teleologically.
</p></blockquote>
<p>This is important for avoiding some mistakes.  But I think it&#8217;ll be very interesting for evolutionary biology to collide with machine learning, where people with goals are designing systems <i>in order to</i> achieve those goals.  Deen Abiola&#8217;s comments <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15783" rel="nofollow">below</a> show what I mean.</p>
<blockquote><p>
(E.g., if you really want to maximise your probability of getting some right answer, you’re best served by testing “answers” at widely spaced parts of the configuration space, but biological evolution seems to churn out individuals who are only slightly different from their parents. Why is that? One quick possibility is that “evolutionary inference” thinks the parents are already quite close to the answer, so small modifications are called for. Or maybe it’s one or multiple other reasons…)
</p></blockquote>
<p>Here you&#8217;re talking about what evolutionary inference &#8220;thinks&#8221;, which would give you a rap on the knuckles in those biology courses.  <img src="https://i1.wp.com/math.ucr.edu/home/baez/emoticons/tongue2.gif" /></p>
<p>More seriously, but relatedly, biologists are really interested in to what extent we can think of evolution itself as having been optimized for something&#8230; instead of just being a fixed method whereby other things get optimized.   The buzzword for this puzzle is <a href="http://en.wikipedia.org/wiki/Evolvability#Evolution_of_evolvability" rel="nofollow">the evolution of evolvability</a>:</p>
<blockquote><p>
While variation yielding high evolvability could be useful in the long term, in the short term most of that variation is likely to be a disadvantage. For example, naively it would seem that increasing the mutation rate via a <a href="http://www2.mnhn.fr/oseb/spip/IMG/pdf/Taddei_et_al_Nature_1997.pdf" rel="nofollow">mutator allele</a> would increase evolvability. But as an extreme example, if the mutation rate is too high then all individuals will be dead or at least carry a heavy mutation load. Short-term selection for low variation most of the time is usually thought likely to be more powerful than long-term selection for evolvability, making it difficult for natural selection to cause the evolution of evolvability.</p>
<p>When recombination is low, mutator alleles may still sometimes hitchhike on the success of adaptive mutations that they cause. In this case, selection can take place at the level of the lineage. This may explain why mutators are often seen during experimental evolution of microbes. Mutator alleles can also evolve more easily when they only increase mutation rates in nearby DNA sequences, not across the whole genome: this is known as a contingency locus.</p>
<p>The evolution of evolvability is less controversial if it occurs via the evolution of sexual reproduction, or via the tendency of variation-generating mechanisms to become more active when an organism is stressed. The yeast prion may also be an example of the evolution of evolvability through <a href="http://en.wikipedia.org/wiki/Evolutionary_capacitance" rel="nofollow">evolutionary capacitance</a>. An evolutionary capacitor is a switch that turns genetic variation on and off. This is very much like bet-hedging the risk that a future environment will be similar or different. Theoretical models also predict the evolution of evolvability via modularity. When the costs of evolvability are sufficiently short-lived, more evolvable lineages may be the most successful in the long-term.
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15783</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Fri, 08 Jun 2012 02:09:22 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-15783</guid>

					<description><![CDATA[Over on G+, Deen Abiola wrote:

&lt;blockquote&gt;
Are you aware of the work by machine learning theorist in the area? In particular, the body of work pioneered by Leslie Valiant? The learning power of evolution is characterized in terms of machine learning and its shown to be weaker than many current algorithms (weaker than Probably Approximately Correct, equivalent to correlational statistical queries). Search Valiant Evolution algorithms for interesting reading.

http://people.seas.harvard.edu/~varunk/docs/recombination-K11.pdf

http://www.almaden.ibm.com/cs/people/vitaly/papers/FV_Evolvability_COLT_OP_08.pdf

http://www.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/p619-feldman.pdf﻿
&lt;/blockquote&gt;

John Baez wrote:

&lt;blockquote&gt;
Deen Abiola - Thanks for the references!  I wasn&#039;t aware of these.  I get the feeling that machine learning, evolutionary game theory and the study of biodiversity live in parallel universes with limited communication between them, but I&#039;m just starting to learn all three, so I expect there&#039;s lots of interesting interplay that I haven&#039;t bumped into yet.﻿
&lt;/blockquote&gt;

Deen Abiola wrote:

&lt;blockquote&gt;
Recently, Game Theory has been getting a lot of attention in machine learning - stuff like regret minimization, on line learning and reinforcement learning. Parts of evolutionary game theory see use in coevolutionary optimization - an area I am really interested in.

I think it&#039;s mainly the biodiversity and real biology people that are most segregated. Which is a shame because it is amazing to see these same things keep popping up everywhere. Our knowledge base has too little entropy heh. Maybe category theory can help as a bridge there...

Is information fundamental or does the digital age color our view the way clockwork mechanics did 200 years ago? Maybe I am biased but I vote for information as fundamental.﻿
&lt;/blockquote&gt;

Deen Abiola wrote:

&lt;blockquote&gt;
One of the papers I link to shows how Valiant&#039;s model of evolution&#039;s learning can be vastly sped up by introducing a recombination opertator. It makes a passing reference to evolutionary game theory work when discussing what is required in the reality of natural selection and genetic recombination for the model to hold:

http://people.seas.harvard.edu/~varunk/docs/recombination-K11.pdf 

The strength of this new model is not given. But it is not hard to imagine that machine learning algos can be &quot;smarter&quot;.

Evolution&#039;s learning ability is clearly surpassed in speed by humans - indeed this fact is what many proponents of a super human AI argue will allow us to produce something smarter than us. it&#039;s happened before. In many areas (where the assumptions of an exponential distribution or convexity or smooth differentiable objective function hold) machine learning algos already surpass everything on the planet. 

What humans and to some extent evolution do best and what even the best ML algos struggle with is in managing complexity by a kind of layering of past experience to quickly solve more complex problems (what Summers was hinting at). That is, most search algorithms are not very good at reducing search time by improving the search space as they learn. Instead they blindly grow the search space exponentially with problem size. New methods including deep learning and transfer learning and search space improving heuristics seek to improve ML beyond the idiot savants that only excel in well behaved problems (the kind that is not readily found in reality).﻿
&lt;/blockquote&gt;

Marc Harper wrote:

&lt;blockquote&gt;
Since the mechanisms by which humans learn, in particular how the brain functions fundamentally, is not known, it may be a stretch to say that &quot;Evolution&#039;s learning ability is clearly surpassed in speed by humans&quot;. It could be evolutionary in some way (which has been proposed by many many people). Some of these models are intellectually pleasing though lacking in evidence (memetics), others have produced remarkably complex behaviors in simulations (Neural Darwinism).

Will humans create an AI that surpasses our own abilities without resorting to simply evolving it? I leave you with the following quotations for meditation:

&quot;The theory of evolution by cumulative natural selection is the only theory we know of that is in principle capable of explaining the existence of organized complexity.&quot; – Richard Dawkins, The Blind Watchmaker (1987)

&quot;A blind-variation-and-selective-retention process is fundamental to all inductive achievements, to all genuine increases in knowledge, to all increases in the fit of system to environment.&quot; – Donald T. Campbell (1974)

&quot;Inductive inference is the only process known to us by which essentially new knowledge comes into the world.&quot; – R. A. Fisher, The Design of Experiments (1935)﻿
&lt;/blockquote&gt;

Deen Abiola wrote:

&lt;blockquote&gt;
 I am not looking inside the function. Only the results are needed to make this conclusion. You can capture a very good model of how evolution learns and then ask, what kind of functions can it learn? You can then compare it to humans and computational learning algos. That is what the papers I mentioned above do, one of the authors who has won the computing equivalent of a Nobel prize, for whatever that is worth.

I actually specialize in something called Genetic Programming so I have a lot of respect for what we can learn from nature. However, like its natural motivation, genetic programming tends to be very slow because its manner of following a gradient is implicit rather than explicit. I am seeking a way to be able to more explicitly guide this search.

Evolution learns at a scale measured in hundreds to tens of thousands of years. It learns by generating an incredibly large amount of hypotheses and unless the queries (organisms) are simple it waits many thousands of years to stabilize at a sufficiently optimal distribution on the best set of phenotypes. Human&#039;s learn on a scale of decades to centuries. With new improvements in biotech we have gotten to a point where we can manipulate our own genetics and protein processing without the messy process and glacial pace of evolution. Even before biotech we were able to direct the development of specific phenotypes more quickly than the blind patience of evolution with this thing called breeding.﻿
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>Over on G+, Deen Abiola wrote:</p>
<blockquote><p>
Are you aware of the work by machine learning theorist in the area? In particular, the body of work pioneered by Leslie Valiant? The learning power of evolution is characterized in terms of machine learning and its shown to be weaker than many current algorithms (weaker than Probably Approximately Correct, equivalent to correlational statistical queries). Search Valiant Evolution algorithms for interesting reading.</p>
<p><object data="http://people.seas.harvard.edu/~varunk/docs/recombination-K11.pdf" type="application/pdf" width="100%" height="800" style="height: 800px;"><p><a href="http://people.seas.harvard.edu/~varunk/docs/recombination-K11.pdf">Click to access recombination-K11.pdf</a></p></object></p>
<p><object data="http://www.almaden.ibm.com/cs/people/vitaly/papers/FV_Evolvability_COLT_OP_08.pdf" type="application/pdf" width="100%" height="800" style="height: 800px;"><p><a href="http://www.almaden.ibm.com/cs/people/vitaly/papers/FV_Evolvability_COLT_OP_08.pdf">Click to access FV_Evolvability_COLT_OP_08.pdf</a></p></object></p>
<p><a href="http://www.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/p619-feldman.pdf﻿" rel="nofollow ugc">http://www.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/p619-feldman.pdf﻿</a>
</p></blockquote>
<p>John Baez wrote:</p>
<blockquote><p>
Deen Abiola &#8211; Thanks for the references!  I wasn&#8217;t aware of these.  I get the feeling that machine learning, evolutionary game theory and the study of biodiversity live in parallel universes with limited communication between them, but I&#8217;m just starting to learn all three, so I expect there&#8217;s lots of interesting interplay that I haven&#8217;t bumped into yet.﻿
</p></blockquote>
<p>Deen Abiola wrote:</p>
<blockquote><p>
Recently, Game Theory has been getting a lot of attention in machine learning &#8211; stuff like regret minimization, on line learning and reinforcement learning. Parts of evolutionary game theory see use in coevolutionary optimization &#8211; an area I am really interested in.</p>
<p>I think it&#8217;s mainly the biodiversity and real biology people that are most segregated. Which is a shame because it is amazing to see these same things keep popping up everywhere. Our knowledge base has too little entropy heh. Maybe category theory can help as a bridge there&#8230;</p>
<p>Is information fundamental or does the digital age color our view the way clockwork mechanics did 200 years ago? Maybe I am biased but I vote for information as fundamental.﻿
</p></blockquote>
<p>Deen Abiola wrote:</p>
<blockquote><p>
One of the papers I link to shows how Valiant&#8217;s model of evolution&#8217;s learning can be vastly sped up by introducing a recombination opertator. It makes a passing reference to evolutionary game theory work when discussing what is required in the reality of natural selection and genetic recombination for the model to hold:</p>
<p><object data="http://people.seas.harvard.edu/~varunk/docs/recombination-K11.pdf" type="application/pdf" width="100%" height="800" style="height: 800px;"><p><a href="http://people.seas.harvard.edu/~varunk/docs/recombination-K11.pdf">Click to access recombination-K11.pdf</a></p></object> </p>
<p>The strength of this new model is not given. But it is not hard to imagine that machine learning algos can be &#8220;smarter&#8221;.</p>
<p>Evolution&#8217;s learning ability is clearly surpassed in speed by humans &#8211; indeed this fact is what many proponents of a super human AI argue will allow us to produce something smarter than us. it&#8217;s happened before. In many areas (where the assumptions of an exponential distribution or convexity or smooth differentiable objective function hold) machine learning algos already surpass everything on the planet. </p>
<p>What humans and to some extent evolution do best and what even the best ML algos struggle with is in managing complexity by a kind of layering of past experience to quickly solve more complex problems (what Summers was hinting at). That is, most search algorithms are not very good at reducing search time by improving the search space as they learn. Instead they blindly grow the search space exponentially with problem size. New methods including deep learning and transfer learning and search space improving heuristics seek to improve ML beyond the idiot savants that only excel in well behaved problems (the kind that is not readily found in reality).﻿
</p></blockquote>
<p>Marc Harper wrote:</p>
<blockquote><p>
Since the mechanisms by which humans learn, in particular how the brain functions fundamentally, is not known, it may be a stretch to say that &#8220;Evolution&#8217;s learning ability is clearly surpassed in speed by humans&#8221;. It could be evolutionary in some way (which has been proposed by many many people). Some of these models are intellectually pleasing though lacking in evidence (memetics), others have produced remarkably complex behaviors in simulations (Neural Darwinism).</p>
<p>Will humans create an AI that surpasses our own abilities without resorting to simply evolving it? I leave you with the following quotations for meditation:</p>
<p>&#8220;The theory of evolution by cumulative natural selection is the only theory we know of that is in principle capable of explaining the existence of organized complexity.&#8221; – Richard Dawkins, The Blind Watchmaker (1987)</p>
<p>&#8220;A blind-variation-and-selective-retention process is fundamental to all inductive achievements, to all genuine increases in knowledge, to all increases in the fit of system to environment.&#8221; – Donald T. Campbell (1974)</p>
<p>&#8220;Inductive inference is the only process known to us by which essentially new knowledge comes into the world.&#8221; – R. A. Fisher, The Design of Experiments (1935)﻿
</p></blockquote>
<p>Deen Abiola wrote:</p>
<blockquote><p>
 I am not looking inside the function. Only the results are needed to make this conclusion. You can capture a very good model of how evolution learns and then ask, what kind of functions can it learn? You can then compare it to humans and computational learning algos. That is what the papers I mentioned above do, one of the authors who has won the computing equivalent of a Nobel prize, for whatever that is worth.</p>
<p>I actually specialize in something called Genetic Programming so I have a lot of respect for what we can learn from nature. However, like its natural motivation, genetic programming tends to be very slow because its manner of following a gradient is implicit rather than explicit. I am seeking a way to be able to more explicitly guide this search.</p>
<p>Evolution learns at a scale measured in hundreds to tens of thousands of years. It learns by generating an incredibly large amount of hypotheses and unless the queries (organisms) are simple it waits many thousands of years to stabilize at a sufficiently optimal distribution on the best set of phenotypes. Human&#8217;s learn on a scale of decades to centuries. With new improvements in biotech we have gotten to a point where we can manipulate our own genetics and protein processing without the messy process and glacial pace of evolution. Even before biotech we were able to direct the development of specific phenotypes more quickly than the blind patience of evolution with this thing called breeding.﻿
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Marc Harper		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15724</link>

		<dc:creator><![CDATA[Marc Harper]]></dc:creator>
		<pubDate>Tue, 05 Jun 2012 00:58:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-15724</guid>

					<description><![CDATA[Hi John!

I&#039;m happy to see that you are still interested in the topics. I want to recognize some other researchers that have had similar ideas (and that I based some of my work on). In particular, Cosma Shalizi independently discovered the analogy between the discrete replicator dynamic and Bayesian inference; I.M. Bomze was the first (as far as I can tell) to use relative entropy / cross entropy to analyze the replicator dynamic and proved many important results; and Shun-ichi Amari and his many collaborators wrote briefly about the connection between information geometry and the replicator equation circa 1995 and in subsequent works.

Unfortunately I found out about much of these researchers&#039; work after I had finished my graduate thesis -- some of it would have been much easier to figure out! In any case, there&#039;s credit where credit is due!]]></description>
			<content:encoded><![CDATA[<p>Hi John!</p>
<p>I&#8217;m happy to see that you are still interested in the topics. I want to recognize some other researchers that have had similar ideas (and that I based some of my work on). In particular, Cosma Shalizi independently discovered the analogy between the discrete replicator dynamic and Bayesian inference; I.M. Bomze was the first (as far as I can tell) to use relative entropy / cross entropy to analyze the replicator dynamic and proved many important results; and Shun-ichi Amari and his many collaborators wrote briefly about the connection between information geometry and the replicator equation circa 1995 and in subsequent works.</p>
<p>Unfortunately I found out about much of these researchers&#8217; work after I had finished my graduate thesis &#8212; some of it would have been much easier to figure out! In any case, there&#8217;s credit where credit is due!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Information Geometry (Part 10) « Azimuth		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15699</link>

		<dc:creator><![CDATA[Information Geometry (Part 10) « Azimuth]]></dc:creator>
		<pubDate>Mon, 04 Jun 2012 02:14:13 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-15699</guid>

					<description><![CDATA[ Last time I began explaining the tight relation between three concepts: entropy, information and biodiversity [...]]]></description>
			<content:encoded><![CDATA[<p> Last time I began explaining the tight relation between three concepts: entropy, information and biodiversity [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15687</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sun, 03 Jun 2012 02:51:37 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-15687</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15684&quot;&gt;Frederik De Roo&lt;/a&gt;.

Frederik wrote:

&lt;blockquote&gt;
...doesn’t this depend on what you assume as possible for $latex f(P)$?  
&lt;/blockquote&gt;

Yes!  My remarks in the section &quot;Exponential growth&quot; only apply to the pathetically simple special case where $latex f(P)$ is actually independent of $latex P$, so the population of each species grows or declines exponentially.  This is a very boring case, and it would be almost silly to even mention it except that &lt;i&gt;for short times&lt;/i&gt;, we can approximate the solution of any equation

$latex \displaystyle{ \frac{d}{d t} P_i(t) = f_i(P(t)) P_i(t) } $

by a solution of the linear equation

$latex \displaystyle{ \frac{d}{d t} P_i(t) = f_i(P(0)) P_i(t) } $

This is my ultimate reason for introducing the pathetically simple special case.  However, you&#039;ll note that in this section, I only analyze the behavior of this case &lt;i&gt;as $latex t \to +\infty$.&lt;/i&gt;  So, the lessons here won&#039;t apply to more general cases---at least, not without tons of qualifications.  Next time I&#039;ll look at the short-time behavior of the same pathetically simple special case, and get some more lessons.

I should have made this more clear.  I&#039;m not explaining things very well since I&#039;m learning it and/or making it up as I go.

Your example is a nice one, since it illustrates the importance of inter-species interactions, which are completely absent in the pathetically simple special case I discussed.  I&#039;ll talk more about those later!  

There&#039;s a lot of interesting stuff to say about how information flows in situations where species are competing or cooperating with each other, but it&#039;s more complicated than &quot;so, the entropy drops&quot;. Very often it rises.  

For now, try this:

&#8226; Marc Harper, &lt;a href=&quot;http://arxiv.org/abs/0911.1763&quot; rel=&quot;nofollow&quot;&gt;The replicator equation as an inference dynamic&lt;/a&gt;.


]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15684">Frederik De Roo</a>.</p>
<p>Frederik wrote:</p>
<blockquote><p>
&#8230;doesn’t this depend on what you assume as possible for <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" />?
</p></blockquote>
<p>Yes!  My remarks in the section &#8220;Exponential growth&#8221; only apply to the pathetically simple special case where <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" /> is actually independent of <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P" class="latex" />, so the population of each species grows or declines exponentially.  This is a very boring case, and it would be almost silly to even mention it except that <i>for short times</i>, we can approximate the solution of any equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+P_i%28t%29+%3D+f_i%28P%28t%29%29+P_i%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} P_i(t) = f_i(P(t)) P_i(t) } " class="latex" /></p>
<p>by a solution of the linear equation</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7Bd%7D%7Bd+t%7D+P_i%28t%29+%3D+f_i%28P%280%29%29+P_i%28t%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{d}{d t} P_i(t) = f_i(P(0)) P_i(t) } " class="latex" /></p>
<p>This is my ultimate reason for introducing the pathetically simple special case.  However, you&#8217;ll note that in this section, I only analyze the behavior of this case <i>as <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%2B%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t &#92;to +&#92;infty" class="latex" />.</i>  So, the lessons here won&#8217;t apply to more general cases&#8212;at least, not without tons of qualifications.  Next time I&#8217;ll look at the short-time behavior of the same pathetically simple special case, and get some more lessons.</p>
<p>I should have made this more clear.  I&#8217;m not explaining things very well since I&#8217;m learning it and/or making it up as I go.</p>
<p>Your example is a nice one, since it illustrates the importance of inter-species interactions, which are completely absent in the pathetically simple special case I discussed.  I&#8217;ll talk more about those later!  </p>
<p>There&#8217;s a lot of interesting stuff to say about how information flows in situations where species are competing or cooperating with each other, but it&#8217;s more complicated than &#8220;so, the entropy drops&#8221;. Very often it rises.  </p>
<p>For now, try this:</p>
<p>&bull; Marc Harper, <a href="http://arxiv.org/abs/0911.1763" rel="nofollow">The replicator equation as an inference dynamic</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Frederik De Roo		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/06/01/information-geometry-part-9/#comment-15684</link>

		<dc:creator><![CDATA[Frederik De Roo]]></dc:creator>
		<pubDate>Sat, 02 Jun 2012 19:59:09 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=9904#comment-15684</guid>

					<description><![CDATA[&lt;blockquote&gt; As some replicators die off while others prosper, they gather information their environment, and this hypothesis gets refined. So, the entropy S(p) drops. We can make this even more precise.
&lt;/blockquote&gt;

If you allow me to display my ignorance here: doesn&#039;t this depend on what you assume as possible for $latex f(P)$? (for example, the constant $latex f(P)$)

Suppose we take two species $latex P_1$ and $latex P_2$ who have some symbiotic relationship, but have to share a finite area. E.g. suppose that ($latex \beta$ is a positive coefficient):

$latex \frac{dP_1}{dt}=\beta \frac{P_2-P_1}{P_1+P_2}$

and similar for $latex P_2$, but with $latex P_1$ and $latex P_2$ exchanged.

Then for large $latex t,$ $latex p_1$ and $latex p_2$ will both go to 1/2. Also in this case I would say that the replicators gathered information about their environment (even though this example may not be very realistic) however it appears to me that the entropy becomes maximal. Am I doing something wrong?]]></description>
			<content:encoded><![CDATA[<blockquote><p> As some replicators die off while others prosper, they gather information their environment, and this hypothesis gets refined. So, the entropy S(p) drops. We can make this even more precise.
</p></blockquote>
<p>If you allow me to display my ignorance here: doesn&#8217;t this depend on what you assume as possible for <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" />? (for example, the constant <img src="https://s0.wp.com/latex.php?latex=f%28P%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(P)" class="latex" />)</p>
<p>Suppose we take two species <img src="https://s0.wp.com/latex.php?latex=P_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=P_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_2" class="latex" /> who have some symbiotic relationship, but have to share a finite area. E.g. suppose that (<img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> is a positive coefficient):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdP_1%7D%7Bdt%7D%3D%5Cbeta+%5Cfrac%7BP_2-P_1%7D%7BP_1%2BP_2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;frac{dP_1}{dt}=&#92;beta &#92;frac{P_2-P_1}{P_1+P_2}" class="latex" /></p>
<p>and similar for <img src="https://s0.wp.com/latex.php?latex=P_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_2" class="latex" />, but with <img src="https://s0.wp.com/latex.php?latex=P_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=P_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="P_2" class="latex" /> exchanged.</p>
<p>Then for large <img src="https://s0.wp.com/latex.php?latex=t%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="t," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=p_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_2" class="latex" /> will both go to 1/2. Also in this case I would say that the replicators gathered information about their environment (even though this example may not be very realistic) however it appears to me that the entropy becomes maximal. Am I doing something wrong?</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
