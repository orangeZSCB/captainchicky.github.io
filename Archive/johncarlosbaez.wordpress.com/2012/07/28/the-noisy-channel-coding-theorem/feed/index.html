<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: The Noisy Channel Coding Theorem	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/</link>
	<description></description>
	<lastBuildDate>Sat, 04 Mar 2017 19:00:04 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: L		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-88396</link>

		<dc:creator><![CDATA[L]]></dc:creator>
		<pubDate>Sat, 04 Mar 2017 19:00:04 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-88396</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-88349&quot;&gt;John Baez&lt;/a&gt;.

Thanks for responding!! For other readers, the quantitative statement is that the number of of $latex \beta$ typical words (those words $latex x$ with $&#124; latex &#124;-1/n * log (p(x)) - H(X) &#124; &#060; \beta$ ) can be bounded by $latex 2^{N(H(X) + \beta)}$ (this follows by direct computation from $latex 1 \geq \Sigma_{x Typical} P(x)$). When $H(X) &#060; log &#124;X&#124;$ (which is exactly the non-uniform distribution case) and $\latex \beta$ sufficiently small, $H(X) + \beta - log &#124;X&#124; &#060; 0$, so the ratio of typical words to all words goes to zero as $N$ grows.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-88349">John Baez</a>.</p>
<p>Thanks for responding!! For other readers, the quantitative statement is that the number of of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta" class="latex" /> typical words (those words <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x" class="latex" /> with $| latex |-1/n * log (p(x)) &#8211; H(X) | &lt; \beta$ ) can be bounded by <img src="https://s0.wp.com/latex.php?latex=2%5E%7BN%28H%28X%29+%2B+%5Cbeta%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="2^{N(H(X) + &#92;beta)}" class="latex" /> (this follows by direct computation from <img src="https://s0.wp.com/latex.php?latex=1+%5Cgeq+%5CSigma_%7Bx+Typical%7D+P%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 &#92;geq &#92;Sigma_{x Typical} P(x)" class="latex" />). When $H(X) &lt; log |X|$ (which is exactly the non-uniform distribution case) and $\latex \beta$ sufficiently small, $H(X) + \beta &#8211; log |X| &lt; 0$, so the ratio of typical words to all words goes to zero as $N$ grows.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-88349</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Sat, 04 Mar 2017 01:03:01 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-88349</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-88333&quot;&gt;L&lt;/a&gt;.

I haven&#039;t thought about this for about 5 years, but: when the probability distribution on $latex A$ is uniform, I believe you are right that the &#039;typical set&#039; is all of $latex A^n$.  In this case no compression is possible.  For every other probability distribution on $latex A$, the typical set will become small compared to $latex A^n$ when $latex n \to \infty$.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-88333">L</a>.</p>
<p>I haven&#8217;t thought about this for about 5 years, but: when the probability distribution on <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> is uniform, I believe you are right that the &#8216;typical set&#8217; is all of <img src="https://s0.wp.com/latex.php?latex=A%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^n" class="latex" />.  In this case no compression is possible.  For every other probability distribution on <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" />, the typical set will become small compared to <img src="https://s0.wp.com/latex.php?latex=A%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^n" class="latex" /> when <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="n &#92;to &#92;infty" class="latex" />.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: L		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-88333</link>

		<dc:creator><![CDATA[L]]></dc:creator>
		<pubDate>Fri, 03 Mar 2017 18:29:52 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-88333</guid>

					<description><![CDATA[Do you think you can expand on this : &quot;The ‘typical set’ has few elements compared to the whole set of strings with n letters&quot;...? If we start with the uniform distribution of a finite set A, then every string is in the typical set of A^n - they are equiprobable - so I guess the statement needs some qualification, or perhaps I am confused. Does this have something to do with the uniform distribution having the largest entropy?]]></description>
			<content:encoded><![CDATA[<p>Do you think you can expand on this : &#8220;The ‘typical set’ has few elements compared to the whole set of strings with n letters&#8221;&#8230;? If we start with the uniform distribution of a finite set A, then every string is in the typical set of A^n &#8211; they are equiprobable &#8211; so I guess the statement needs some qualification, or perhaps I am confused. Does this have something to do with the uniform distribution having the largest entropy?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: arch1		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17397</link>

		<dc:creator><![CDATA[arch1]]></dc:creator>
		<pubDate>Tue, 31 Jul 2012 14:29:36 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-17397</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17370&quot;&gt;John Baez&lt;/a&gt;.

Thanks for the extra context John.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17370">John Baez</a>.</p>
<p>Thanks for the extra context John.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Florifulgurator		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17378</link>

		<dc:creator><![CDATA[Florifulgurator]]></dc:creator>
		<pubDate>Tue, 31 Jul 2012 08:55:22 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-17378</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17370&quot;&gt;John Baez&lt;/a&gt;.

Sounds like yummy breakfast. But no Cramer large deviation stuff? Last century I ran a seminar on Cramer&#039;s theorem, iterated logarithm, arcsin law etc. Now suddenly it seems Shannon is yummy, too.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17370">John Baez</a>.</p>
<p>Sounds like yummy breakfast. But no Cramer large deviation stuff? Last century I ran a seminar on Cramer&#8217;s theorem, iterated logarithm, arcsin law etc. Now suddenly it seems Shannon is yummy, too.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17372</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 31 Jul 2012 04:45:37 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-17372</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17353&quot;&gt;romain&lt;/a&gt;.

That&#039;s nice!  Thanks!  

I hope I have enough energy to say more about the asymptotic equipartition property.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17353">romain</a>.</p>
<p>That&#8217;s nice!  Thanks!  </p>
<p>I hope I have enough energy to say more about the asymptotic equipartition property.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17371</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 31 Jul 2012 04:31:11 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-17371</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17351&quot;&gt;Jamie Vicary&lt;/a&gt;.

Let&#039;s keep talking about all this stuff.

By the way, Jamie, speaking of error correction...

Everyone please remember: on all Wordpress blogs, LaTeX is done like this:

&#036;latex  E = mc^2$

with the word &#039;latex&#039; directly following the first dollar sign, no space.  Double dollar signs don&#039;t work here.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17351">Jamie Vicary</a>.</p>
<p>Let&#8217;s keep talking about all this stuff.</p>
<p>By the way, Jamie, speaking of error correction&#8230;</p>
<p>Everyone please remember: on all WordPress blogs, LaTeX is done like this:</p>
<p>&#036;latex  E = mc^2$</p>
<p>with the word &#8216;latex&#8217; directly following the first dollar sign, no space.  Double dollar signs don&#8217;t work here.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17370</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 31 Jul 2012 03:36:39 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-17370</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17366&quot;&gt;arch1&lt;/a&gt;.

I didn&#039;t mean to say the  asymptotic equipartition property is extremely hard.  However, the rest of the proof looks easy in comparison, so one is inclined to look at this part and say &quot;yuck, that&#039;s some technical fact I&#039;d rather take on faith&quot;.   It seems like the pit in the peach.   But I was trying to convince everyone that unlike the pit in the peach, it&#039;s highly nutritious, and tasty in its own way.

I stated a watered-down version of the asymptotic equipartition theorem: just for purposes of exposition, I assumed that each letter in the string was drawn independently from the same probability distribution on letters.  In other words, I was assuming that they&#039;re &#039;independent identically distributed&#039; random variables.  This is clearly too restrictive---it sure ain&#039;t true for English text!

The statement and proof gets a bit harder when we do the full-fledged thing: you can see a proof &lt;a href=&quot;http://en.wikipedia.org/wiki/Asymptotic_equipartition_property#AEP_for_discrete-time_i.i.d._sources&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for the i.i.d. case and &lt;a href=&quot;http://en.wikipedia.org/wiki/Asymptotic_equipartition_property#AEP_for_discrete-time_finite-valued_stationary_ergodic_sources&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for the more general case.  The more general case uses Lévy&#039;s martingale convergence theorem,  Markov&#039;s inequality and Borel-Cantelli lemma.  For some people that would be very scary, while others eat such stuff for breakfast.

But anyway, regardless of whether the proof is hard, the result seems both interesting and highly believable: not shocking.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17366">arch1</a>.</p>
<p>I didn&#8217;t mean to say the  asymptotic equipartition property is extremely hard.  However, the rest of the proof looks easy in comparison, so one is inclined to look at this part and say &#8220;yuck, that&#8217;s some technical fact I&#8217;d rather take on faith&#8221;.   It seems like the pit in the peach.   But I was trying to convince everyone that unlike the pit in the peach, it&#8217;s highly nutritious, and tasty in its own way.</p>
<p>I stated a watered-down version of the asymptotic equipartition theorem: just for purposes of exposition, I assumed that each letter in the string was drawn independently from the same probability distribution on letters.  In other words, I was assuming that they&#8217;re &#8216;independent identically distributed&#8217; random variables.  This is clearly too restrictive&#8212;it sure ain&#8217;t true for English text!</p>
<p>The statement and proof gets a bit harder when we do the full-fledged thing: you can see a proof <a href="http://en.wikipedia.org/wiki/Asymptotic_equipartition_property#AEP_for_discrete-time_i.i.d._sources" rel="nofollow">here</a> for the i.i.d. case and <a href="http://en.wikipedia.org/wiki/Asymptotic_equipartition_property#AEP_for_discrete-time_finite-valued_stationary_ergodic_sources" rel="nofollow">here</a> for the more general case.  The more general case uses Lévy&#8217;s martingale convergence theorem,  Markov&#8217;s inequality and Borel-Cantelli lemma.  For some people that would be very scary, while others eat such stuff for breakfast.</p>
<p>But anyway, regardless of whether the proof is hard, the result seems both interesting and highly believable: not shocking.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: blake561able		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17369</link>

		<dc:creator><![CDATA[blake561able]]></dc:creator>
		<pubDate>Tue, 31 Jul 2012 01:00:53 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-17369</guid>

					<description><![CDATA[It is interesting that Weyl was Shannon&#039;s adviser at IAS. The communication between those two must have come close to exceeding these limits on information!]]></description>
			<content:encoded><![CDATA[<p>It is interesting that Weyl was Shannon&#8217;s adviser at IAS. The communication between those two must have come close to exceeding these limits on information!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: arch1		</title>
		<link>https://johncarlosbaez.wordpress.com/2012/07/28/the-noisy-channel-coding-theorem/#comment-17366</link>

		<dc:creator><![CDATA[arch1]]></dc:creator>
		<pubDate>Mon, 30 Jul 2012 22:10:15 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=11087#comment-17366</guid>

					<description><![CDATA[John,

My quick skim of your summary of the hard lemma left me wondering why it is a hard lemma*.  Isn&#039;t it just a baby step away from the fact that as n-&#062; infinity, samples of size n look more and more like the underlying distribution from which they are drawn?

*That in itself is progress, since my quick skim of Wikipedia&#039;s summary of the hard lemma left me almost clueless as to the content of the hard lemma (it struck me as very vague:-)]]></description>
			<content:encoded><![CDATA[<p>John,</p>
<p>My quick skim of your summary of the hard lemma left me wondering why it is a hard lemma*.  Isn&#8217;t it just a baby step away from the fact that as n-&gt; infinity, samples of size n look more and more like the underlying distribution from which they are drawn?</p>
<p>*That in itself is progress, since my quick skim of Wikipedia&#8217;s summary of the hard lemma left me almost clueless as to the content of the hard lemma (it struck me as very vague:-)</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
