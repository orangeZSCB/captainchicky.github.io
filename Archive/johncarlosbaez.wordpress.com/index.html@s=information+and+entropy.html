<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title> information and entropy | Search Results  | Azimuth</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://johncarlosbaez.wordpress.com/xmlrpc.php" />
<meta name='robots' content='noindex, follow, max-image-preview:large' />
<link rel='dns-prefetch' href='//s2.wp.com' />
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//s1.wp.com' />
<link rel='dns-prefetch' href='//s.wordpress.com' />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Feed" href="https://johncarlosbaez.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Comments Feed" href="https://johncarlosbaez.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Azimuth &raquo; Search Results for &#8220;information and entropy&#8221; Feed" href="https://johncarlosbaez.wordpress.com/search/information+and+entropy/feed/rss2/" />
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s1.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='all-css-0-1' href='https://s2.wp.com/_static/??-eJyNkttSAyEMhl9IyFKnOl44PguHiKnAMhBaeXvZajvrqfWGIQnfn58AHLKwc2JMDLGJHJqnVIHSMyXiDvyCESvkZuB4LGaWttYbWHEnyLcRGix+VArCXim5lQpMo+DAhNm+ikCm6NKhcg94FqJkQ3Ojza5CREcaw+i6OFoFOeiORQT02nYZKV3HR20df4H+Nn90OsSQs14s6z43Fr6Q+2b73xJFMyVfr+B2/sQ2Uk1jbo4qn5Pid3b1ZMvMRz5m/fPiF7ADOY888HraC8a3y0gebYQxuWCtYqyRWhQfP2XhnuKjurtV9w/badrs3gHJreS0?cssminify=yes' type='text/css' media='all' />
<style id='wp-block-library-inline-css'>
.has-text-align-justify {
	text-align:justify;
}
</style>
<style id='global-styles-inline-css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--normal: 16px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--huge: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-normal-font-size{font-size: var(--wp--preset--font-size--normal) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-huge-font-size{font-size: var(--wp--preset--font-size--huge) !important;}
</style>
<link rel='stylesheet' id='all-css-2-1' href='https://s0.wp.com/_static/??-eJx9i0EKgCAQAD+ULYJRHaK3qJhY666o0fezW126zcAMGNR09LaUDq4kLFN1VCGeIuHpAxXwjgWy1TUwfURsqEP+W7MzyL6hh1a99JnWuEg1zEpNo1T7DZLtMYQ=?cssminify=yes' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3-1' href='https://s2.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h&cssminify=yes' type='text/css' media='print' />
<style id='jetpack-global-styles-frontend-style-inline-css'>
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel='stylesheet' id='all-css-6-1' href='https://s0.wp.com/wp-content/themes/h4/global.css?m=1420737423h&cssminify=yes' type='text/css' media='all' />
<script id='wpcom-actionbar-placeholder-js-extra'>
var actionbardata = {"siteID":"12777403","siteName":"Azimuth","siteURL":"http:\/\/johncarlosbaez.wordpress.com","siteHost":"johncarlosbaez.wordpress.com","icon":"<img alt='' src='https:\/\/s2.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/contempt","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2015%2F06%2F06%2Finformation-and-entropy-in-biological-systems-part-7%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"c031e776ae","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"8f1337fe87\" \/>","referer":"https:\/\/johncarlosbaez.wordpress.com\/?s=information+and+entropy","canFollow":"1","feedID":"62242","statusMessage":"","subsEmailDefault":"instantly","customizeLink":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F%3Fs%3Dinformation%2Band%2Bentropy","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Contempt","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 5,227 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2015%2F06%2F06%2Finformation-and-entropy-in-biological-systems-part-7%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats","notifyNewPosts":"Notify me of new posts","notifyNewPostsDetails":"Receive web and mobile notifications for new posts from this site.","emailNewPosts":"Email me new posts","emailNewPostsDetails":"You can customize your notification settings further <a href=\"https:\/\/wordpress.com\/following\/manage?s=johncarlosbaez.wordpress.com\">here<\/a>.","emailNewComments":"Email me new comments","instantly":"Instantly","daily":"Daily","weekly":"Weekly"}};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJyFykEKwkAMQNELmQ4qtboQz1LbWDJMknGSQXt7K9SFILj6i//CI8Og4igeooWs5oxm/YRNtE34vqxXSgjVsCxAHEhu+sOV5JCLPufPIxlSHdHeM94rlnlNwyR/ETBNpXdc8YXP28OuOx3brt3HF3swRvU='></script>
<script type='text/javascript'>
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://johncarlosbaez.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />

<!-- Jetpack Open Graph Tags -->
<meta property="og:title" content="Search Results for &#8220;information and entropy&#8221; &#8211; Azimuth" />
<meta property="og:site_name" content="Azimuth" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s1.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="https://s2.wp.com/i/webclip.png" />
<link rel='openid.server' href='https://johncarlosbaez.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='https://johncarlosbaez.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="https://johncarlosbaez.wordpress.com/osd.xml" title="Azimuth" />
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com" />
<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Azimuth" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://johncarlosbaez.wordpress.com/feed/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s1.wp.com/i/favicon.ico" /><style type="text/css">
#headerimg{
	background: url(https://johncarlosbaez.files.wordpress.com/2010/08/azimuth_header.jpg) no-repeat;
}
#header h1 a, .description {
	color:#E5F2E9;
}
</style>
<!-- There is no amphtml version available for this URL. -->		<link rel="stylesheet" id="custom-css-css" type="text/css" href="https://s0.wp.com/?custom-css=1&#038;csblog=RBZ9&#038;cscache=6&#038;csrev=7" />
		</head>
<body class="search search-results customizer-styles-applied highlander-enabled highlander-light">

<div id="page">

<div id="header">
	<div id="headerimg" onclick="location.href='https://johncarlosbaez.wordpress.com';" style="cursor: pointer;">
		<h1><a href="https://johncarlosbaez.wordpress.com/">Azimuth</a></h1>
		<div class="description"></div>
	</div>
</div>

<ul id="pagebar" class="menu pagebar"><li ><a href="https://johncarlosbaez.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://johncarlosbaez.wordpress.com/about/">About</a></li>
</ul>

<div id="grad" style="height: 65px; width: 100%; background: url(https://s2.wp.com/wp-content/themes/pub/contempt/images/blue_flower/topgrad.jpg);">&nbsp;</div>

	<div id="content">

	
		
			<div class="post-19837 post type-post status-publish format-standard hentry category-biology category-information-and-entropy" id="post-19837">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/" rel="bookmark">Information and Entropy in Biological Systems (Part&nbsp;7)</a></h2>
				<small>6 June, 2015</small><br />


				<div class="entry">
					<p>In 1961, Rolf Landauer argued that that the least possible amount of energy required to erase one bit of information stored in memory at temperature <img src="https://s0.wp.com/latex.php?latex=T&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T" class="latex" /> is  <img src="https://s0.wp.com/latex.php?latex=kT+%5Cln+2%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="kT &#92;ln 2," class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k" class="latex" /> is Boltzmann&#8217;s constant.</p>
<p>This is called the <a href="https://en.wikipedia.org/wiki/Landauer%27s_principle">Landauer limit</a>, and it came after many decades of arguments concerning Maxwell&#8217;s demon and the relation between information and entropy.</p>
<p>In fact, these arguments are still not finished.  For example, here&#8217;s an argument that the Landauer limit is not as solid as widely believed:</p>
<p>&bull; John D. Norton, <a href="http://www.pitt.edu/~jdnorton/papers/Waiting_SHPMP.pdf">Waiting for Landauer</a>, <i>Studies in History and Philosophy of Modern Physics</i> <b>42</b> (2011), 184&ndash;198.</p>
<p>But something like the Landauer limit almost surely holds under some conditions!  And if it holds, it puts some limits on what organisms can do.  That&#8217;s what David Wolpert spoke about at our workshop! You can see his slides here:</p>
<p>&bull; <a href="http://www.santafe.edu/about/people/profile/David%20Wolpert">David Wolpert</a> &mdash; <a href="http://math.ucr.edu/home/baez/nimbios/nimbios_wolpert.pdf">The Landauer limit and thermodynamics of biological organisms</a>.</p>
<p>You can also watch a video:</p>
<div align="center">
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="560" height="315" src="https://www.youtube.com/embed/NbzucR2k3pU?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/#comments">46 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/06/06/information-and-entropy-in-biological-systems-part-7/" rel="bookmark" title="Permanent Link to Information and Entropy in Biological Systems (Part&nbsp;7)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-19821 post type-post status-publish format-standard hentry category-biology category-information-and-entropy" id="post-19821">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/06/01/information-and-entropy-in-biological-systems-part-6/" rel="bookmark">Information and Entropy in Biological Systems (Part&nbsp;6)</a></h2>
				<small>1 June, 2015</small><br />


				<div class="entry">
					<p>The resounding lack of comment to this series of posts confirms my theory that a blog post that says &#8220;go somewhere else and read something&#8221; will never be popular.  Even if it&#8217;s &#8220;go somewhere else and watch a video&#8221;, this is too much like saying</p>
<blockquote><p>
  Hi!  Want to talk?  Okay, go into that other room and watch TV, then come back when you&#8217;re done and we&#8217;ll talk about it.
</p></blockquote>
<p>But no matter: our workshop on <a href="http://www.nimbios.org/workshops/WS_entropy">Information and Entropy in Biological Systems</a> was really exciting!  I want to make it available to the world as much as possible.  I&#8217;m running around too much to create lovingly hand-crafted summaries of each talk&#8212;and I know you&#8217;re punishing me for that, with your silence.  But I&#8217;ll keep on going, just to get the material out there.</p>
<p><a href="http://www.marcharper.net/">Marc Harper</a> spoke about information in evolutionary game theory, and we have a nice video of that.  I&#8217;ve been excited about his work for quite a while, because it shows that the analogy between &#8216;evolution&#8217; and &#8216;learning&#8217; can be made mathematically precise.  I summarized some of his ideas in my <a href="http://math.ucr.edu/home/baez/information/">information geometry series</a>, and I&#8217;ve also gotten him to write two articles for this blog:</p>
<p>&bull; Marc Harper, <a href="https://johncarlosbaez.wordpress.com/2014/01/22/relative-entropy-in-evolutionary-dynamics/">Relative entropy in evolutionary dynamics</a>, <i>Azimuth</i>, 22 January 2014.</p>
<p>&bull; Marc Harper, <a href="https://johncarlosbaez.wordpress.com/2015/03/24/stationary-stability-in-finite-populations/">Stationary stability in finite populations</a>, <i>Azimuth</i>, 24 March 2015.</p>
<p>Here are the slides and video of his talk:</p>
<p>&bull; Marc Harper, <a href="http://math.ucr.edu/home/baez/nimbios/nimbios_harper.pdf">Information transport and evolutionary dynamics</a>.</p>
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="560" height="315" src="https://www.youtube.com/embed/hevnySd5LRs?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/06/01/information-and-entropy-in-biological-systems-part-6/#comments">11 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/06/01/information-and-entropy-in-biological-systems-part-6/" rel="bookmark" title="Permanent Link to Information and Entropy in Biological Systems (Part&nbsp;6)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-19807 post type-post status-publish format-standard hentry category-biodiversity category-biology category-information-and-entropy" id="post-19807">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/05/30/information-and-entropy-in-biological-systems-part-5/" rel="bookmark">Information and Entropy in Biological Systems (Part&nbsp;5)</a></h2>
				<small>30 May, 2015</small><br />


				<div class="entry">
					<p><a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a> of U. C. Berkeley spoke about the maximum entropy method as a method of predicting patterns in ecology.   <a href="http://www.lsa.umich.edu/eeb/people/ci.ostlingannette_ci.detail">Annette Ostling</a> of the University of Michigan spoke about some competing theories, such as the &#8216;neutral model&#8217; of biodiversity&#8212;a theory that sounds much too simple to be right, yet fits the data surprisingly well!</p>
<p>We managed to get a video of Ostling&#8217;s talk, but not Harte&#8217;s.  Luckily, you can see the slides of both.  You can also see a summary of Harte&#8217;s book <i>Maximum Entropy and Ecology</i>:</p>
<p>&bull; John Baez, <a href="https://johncarlosbaez.wordpress.com/2013/02/21/maximum-entropy-and-ecology/">Maximum entropy and ecology</a>, <i>Azimuth</i>, 21 February 2013.</p>
<p>Here are his talk slides and abstract:</p>
<p>• John Harte, <a href="http://math.ucr.edu/home/baez/nimbios/nimbios_harte.pdf">Maximum entropy as a foundation for theory building in ecology</a>.</p>
<blockquote><p>
<strong>Abstract.</strong> Constrained maximization of information entropy (MaxEnt) yields least-biased probability distributions. In statistical physics, this powerful inference method yields classical statistical mechanics/thermodynamics under the constraints imposed by conservation laws.  I apply MaxEnt to macroecology, the study of the distribution, abundance, and energetics of species in ecosystems.  With constraints derived from ratios of ecological state variables, I show that MaxEnt yields realistic abundance distributions, species-area relationships, spatial aggregation patterns, and body-size distributions over a wide range of taxonomic groups, habitats and spatial scales.  I conclude with a brief summary of some of the major opportunities at the frontier of MaxEnt-based macroecological theory.  </p></blockquote>
<p>Here is a video of Ostling&#8217;s talk, as well as her slides and some papers she recommended:</p>
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="450" height="254" src="https://www.youtube.com/embed/-VMvHdjA2UM?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
<p>&bull; Annette Ostling, <a href="http://www.nimbios.org/wordpress-training/entropy/wp-content/uploads/sites/24/2015/04/aostling_NIMBioS_talk.pdf">The neutral theory of biodiversity and other competitors to maximum entropy</a>.</p>
<blockquote><p>
<strong>Abstract:</strong> I am a bit of the odd man out in that I will not talk that much about information and entropy, but instead about neutral theory and niche theory in ecology. My interest in coming to this workshop is in part out of an interest in what greater insights we can get into neutral models and stochastic population dynamics in general using entropy and information theory.</p>
<p>I will present the niche and neutral theories of the maintenance of diversity of competing species in ecology, and explain the dynamics included in neutral models in ecology. I will also briefly explain how one can derive a species abundance distribution from neutral models. I will present the view that neutral models have the potential to serve as more process-based null models than previously used in ecology for detecting the signature of niches and habitat filtering. However, tests of neutral theory in ecology have not as of yet been as useful as tests of neutral theory in evolutionary biology, because they leave open the possibility that pattern is influenced by “demographic complexity” rather than niches. I will mention briefly some of the work I’ve been doing to try to construct better tests of neutral theory.</p>
<p>Finally I’ll mention some connections that have been made so far between predictions of entropy theory and predictions of neutral theory in ecology and evolution.</p></blockquote>
<p>
These papers present interesting relations between ecology and statistical mechanics.  Check out the nice &#8216;analogy chart&#8217; in the second one!</p>
<p>&bull; M. G. Bowler, <a href="http://www.nimbios.org/wordpress-training/entropy/wp-content/uploads/sites/24/2015/04/bowler.pdf">Species abundance distributions, statistical mechanics and the priors of MaxEnt</a>, <i>Theoretical Population Biology</i> <b>92</b> (2014), 69&ndash;77.</p>
<blockquote><p> <b>Abstract.</b> The methods of Maximum Entropy have been deployed for some years to address the problem of species abundance distributions. In this approach, it is important to identify the correct weighting factors, or priors, to be applied before maximising the entropy function subject to constraints. The forms of such priors depend not only on the exact problem but can also depend on the way it is set up; priors are determined by the underlying dynamics of the complex system under consideration. The problem is one of statistical mechanics and it is the properties of the system that yield the correct MaxEnt priors, appropriate to the way the problem is framed. Here I calculate, in several different ways, the species abundance distribution resulting when individuals in a community are born and die independently. In<br />
the usual formulation the prior distribution for the number of species over the number of individuals is 1/n; the problem can be reformulated in terms of the distribution of individuals over species classes, with a uniform prior. Results are obtained using master equations for the dynamics and separately through the combinatoric methods of elementary statistical mechanics; the MaxEnt priors then emerge a posteriori. The first object is to establish the log series species abundance distribution as the outcome of per capita guild dynamics. The second is to clarify the true nature and origin of priors in the language of MaxEnt. Finally, I consider how it may come about that the distribution is similar to log series in the event that filled niches dominate species abundance. For the general ecologist, there are two messages. First, that species abundance distributions are determined largely by population sorting through fractional processes (resulting in the 1/n factor) and secondly that useful information is likely to be found only in departures from the log series. For the MaxEnt practitioner, the message is that the prior with respect to which the entropy is to be maximised is determined by the nature of the problem and the way in which it is formulated.
</p></blockquote>
<p>&bull; Guy Sella and Aaron E. Hirsh, <a href="http://www.nimbios.org/wordpress-training/entropy/wp-content/uploads/sites/24/2015/04/Sella_Hirsch.pdf">The application of statistical physics to evolutionary biology</a>, <i>Proc. Nat. Acad. Sci.</i> <b>102</b> (2005), 9541&ndash;9546.</p>
<blockquote><p>
A number of fundamental mathematical models of the evolutionary process exhibit dynamics that can be difficult to understand analytically. Here we show that a precise mathematical analogy can be drawn between certain evolutionary and thermodynamic systems, allowing application of the powerful machinery of statistical physics to analysis of a family of evolutionary models. Analytical results that follow directly from this approach include the steady-state distribution of fixed genotypes and the load in finite populations. The analogy with statistical physics also reveals that, contrary to a basic tenet of the nearly neutral theory of molecular evolution, the frequencies of adaptive and deleterious substitutions at steady state are equal. Finally, just as the free energy function quantitatively characterizes the balance between energy and entropy, a free fitness function provides an analytical expression for the balance between natural selection and stochastic drift.
</p></blockquote>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/05/30/information-and-entropy-in-biological-systems-part-5/#comments">1 Comment</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biodiversity/" rel="category tag">biodiversity</a>, <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/05/30/information-and-entropy-in-biological-systems-part-5/" rel="bookmark" title="Permanent Link to Information and Entropy in Biological Systems (Part&nbsp;5)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-19793 post type-post status-publish format-standard hentry category-biology category-information-and-entropy" id="post-19793">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/05/21/information-and-entropy-in-biological-systems-part-4/" rel="bookmark">Information and Entropy in Biological Systems (Part&nbsp;4)</a></h2>
				<small>21 May, 2015</small><br />


				<div class="entry">
					<p>I kicked off the workshop on Information and Entropy in Biological Systems with a broad overview of the many ways information theory and entropy get used in biology:</p>
<p>• John Baez, <a href="http://math.ucr.edu/home/baez/nimbios/nimbios_baez.pdf">Information and entropy in biological systems</a>.</p>
<blockquote><p>
<strong>Abstract.</strong> Information and entropy are being used in biology in many different ways: for example, to study biological communication systems, the &#8216;action-perception loop&#8217;, the thermodynamic foundations of biology, the structure of ecosystems, measures of biodiversity, and evolution. Can we unify these? To do this, we must learn to talk to each other. This will be easier if we share some basic concepts which I&#8217;ll sketch here.
</p></blockquote>
<p>The talk is full of links, <font color="blue">in blue</font>. If you click on these you can get more details.  You can also watch a video of my talk:</p>
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="560" height="315" src="https://www.youtube.com/embed/GEv2u9zSCdQ?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/05/21/information-and-entropy-in-biological-systems-part-4/#respond">Leave a Comment &#187;</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/05/21/information-and-entropy-in-biological-systems-part-4/" rel="bookmark" title="Permanent Link to Information and Entropy in Biological Systems (Part&nbsp;4)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-19781 post type-post status-publish format-standard hentry category-biology category-information-and-entropy" id="post-19781">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/05/20/information-and-entropy-in-biological-systems-part-3-2/" rel="bookmark">Information and Entropy in Biological Systems (Part&nbsp;3)</a></h2>
				<small>20 May, 2015</small><br />


				<div class="entry">
					<p>We had a great <a href="https://johncarlosbaez.wordpress.com/2014/07/04/entropy-and-information-in-biological-systems-part-2/">workshop on information and entropy in biological systems</a>, and now you can <a href="https://www.youtube.com/playlist?list=PLRyq_4VPZ9g-3869ozbY_eEp6jZhWL0UE">see what it was like</a>.   I think I&#8217;ll post these talks one a time, or maybe a few at a time, because they&#8217;d be overwhelming taken all at once.</p>
<p>So, let&#8217;s dive into <a href="http://people.healthsciences.ucla.edu/research/institution/personnel?personnel_id=45756">Chris Lee&#8217;s</a> exciting ideas about organisms as &#8216;information evolving machines&#8217; that may provide &#8216;disinformation&#8217; to their competitors.  Near the end of his talk, he discusses some new results on an ever-popular topic: the Prisoner&#8217;s Dilemma.  You may know about this classic book:</p>
<p>&bull; Robert Axelrod, <i>The Evolution of Cooperation</i>, Basic Books, New York, 1984.  Some passages available <a href="http://www-personal.umich.edu/~axe/Axelrod_Evol_of_Coop_excerpts.pdf">free online</a>.</p>
<p>If you don&#8217;t, read it now!  He showed that the simple &#8216;tit for tat&#8217; strategy did very well in some experiments where the game was played repeatedly and strategies who did well got to &#8216;reproduce&#8217; themselves.  This result was very exciting, so a lot of people have done research on it.  More recently  a paper on this subject by William Press and Freeman Dyson received a lot of hype.  I think this is a good place to learn about that:</p>
<p>&bull; Mike Shulman, <a href="https://golem.ph.utexas.edu/category/2012/07/zerodeterminant_strategies_in.html">Zero determinant strategies in the iterated Prisoner&#8217;s Dilemma</a>, <i>The n-Category Caf&eacute;</i>, 19 July 2012.</p>
<p>Chris Lee&#8217;s new work on the Prisoner&#8217;s Dilemma is here, cowritten with two other people who attended the workshop:</p>
<p>&bull; <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0120625">The art of war: beyond memory-one strategies in population games</a>, <i>PLOS One</i>, 24 March 2015.</p>
<blockquote><p>
<strong>Abstract.</strong> We show that the history of play in a population game contains exploitable information that can be successfully used by sophisticated strategies to defeat memory-one opponents, including zero determinant strategies. The history allows a player to label opponents by their strategies, enabling a player to determine the population distribution and to act differentially based on the opponent’s strategy in each pairwise interaction. For the Prisoner’s Dilemma, these advantages lead to the natural formation of cooperative coalitions among similarly behaving players and eventually to unilateral defection against opposing player types. We show analytically and empirically that optimal play in population games depends strongly on the population distribution. For example, the optimal strategy for a minority player type against a resident tit-for-tat (TFT) population is &#8216;always cooperate&#8217; (ALLC), while for a majority player type the optimal strategy versus TFT players is &#8216;always defect&#8217; (ALLD). Such behaviors are not accessible to memory-one strategies. Drawing inspiration from Sun Tzu’s the Art of War, we implemented a non-memory-one strategy for population games based on techniques from machine learning and statistical inference that can exploit the history of play in this manner. Via simulation we find that this strategy is essentially uninvadable and can successfully invade (significantly more likely than a neutral mutant) essentially all known memory-one strategies for the Prisoner’s Dilemma, including ALLC (always cooperate), ALLD (always defect), tit-for-tat (TFT), win-stay-lose-shift (WSLS), and zero determinant (ZD) strategies, including extortionate and generous strategies.</p></blockquote>
<p>And now for the talk!  Click on the talk title here for Chris Lee&#8217;s slides, or go down and watch the video:</p>
<p>&bull; Chris Lee, <a href="http://math.ucr.edu/home/baez/nimbios/nimbios_lee.pdf">Empirical information, potential information and disinformation as signatures of distinct classes of information evolving machines</a>.</p>
<blockquote><p>
<strong>Abstract.</strong> Information theory is an intuitively attractive way of thinking about biological evolution, because it seems to capture a core aspect of biology&mdash;life as a solution to &#8220;information problems&#8221;&mdash;in a fundamental way. However, there are non-trivial questions about how to apply that idea, and whether it has actual predictive value. For example, should we think of biological systems as being actually driven by an information metric? One idea that can draw useful links between information theory, evolution and statistical inference is the definition of an information evolving machine (IEM) as a system whose elements represent distinct predictions, and whose weights represent an information (prediction power) metric, typically as a function of sampling some iterative observation process. I first show how this idea provides useful results for describing a statistical inference process, including its maximum entropy bound for optimal inference, and how its sampling-based metrics (&#8220;empirical information&#8221;, Ie, for prediction power; and &#8220;potential information&#8221;, I<sub>p</sub>, for latent prediction power) relate to classical definitions such as mutual information and relative entropy. These results suggest classification of IEMs into several distinct types: </p>
<p>1. I<sub>e</sub> machine: e.g. a population of competing genotypes evolving under selection and mutation is an IEM that computes an Ie equivalent to fitness, and whose gradient (I<sub>p</sub>) acts strictly locally, on mutations that it actually samples. Its transition rates between steady states will decrease exponentially as a function of evolutionary distance. </p>
<p>2. &#8220;I<sub>p</sub> tunneling&#8221; machine: a statistical inference process summing over a population of models to compute both I<sub>e</sub>, I<sub>p</sub> can directly detect &#8220;latent&#8221; information in the observations (not captured by its model), which it can follow to &#8220;tunnel&#8221; rapidly to a new steady state. </p>
<p>3. disinformation machine (multiscale IEM): an ecosystem of species is an IEM whose elements (species) are themselves IEMs that can interact. When an attacker IEM can reduce a target IEM&#8217;s prediction power (I<sub>e</sub>) by sending it a misleading signal, this &#8220;disinformation dynamic&#8221; can alter the evolutionary landscape in interesting ways, by opening up paths for rapid co-evolution to distant steady-states. This is especially true when the disinformation attack targets a feature of high fitness value, yielding a combination of strong negative selection for retention of the target feature, plus strong positive selection for escaping the disinformation attack. I will illustrate with examples from statistical inference and evolutionary game theory. These concepts, though basic, may provide useful connections between diverse themes in the workshop.
</p></blockquote>
<span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" width="560" height="315" src="https://www.youtube.com/embed/RFGu6QqFJDY?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/05/20/information-and-entropy-in-biological-systems-part-3-2/#comments">1 Comment</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/05/20/information-and-entropy-in-biological-systems-part-3-2/" rel="bookmark" title="Permanent Link to Information and Entropy in Biological Systems (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-19514 post type-post status-publish format-standard hentry category-conferences category-information-and-entropy" id="post-19514">
				<h2><a href="https://johncarlosbaez.wordpress.com/2015/04/06/information-and-entropy-in-biological-systems-part-3/" rel="bookmark">Information and Entropy in Biological Systems (Part&nbsp;3)</a></h2>
				<small>6 April, 2015</small><br />


				<div class="entry">
					<p>I think you can watch live streaming video of our workshop on <a href="http://www.nimbios.org/workshops/WS_entropy">Information and Entropy in Biological Systems</a>, which runs Wednesday April 8th to Friday April 10th.  Later, videos will be made available in a permanent location.</p>
<p>To watch the workshop live, go <b><a href="http://www.nimbios.org/videos/livestream">here</a></b>. Go down to where it says</p>
<blockquote><p>
Investigative Workshop: Information and Entropy in Biological Systems
</p></blockquote>
<p>Then click where it says <b>live link</b>.  There&#8217;s nothing there <em>now</em>, but I&#8217;m hoping there will be when the show starts!</p>
<p>Below you can see the schedule of talks and a list of participants.  The hours are in Eastern Daylight Time: add 4 hours to get Greenwich Mean Time.  The talks start at 10 am EDT, which is 2 pm GMT.</p>
<h3>Schedule</h3>
<p>There will be 1&frac12; hours of talks in the morning and 1&frac12; hours in the afternoon for each of the 3 days, Wednesday April 8th to Friday April 10th.  The rest of the time will be for discussions on different topics.  We&#8217;ll break up into groups, based on what people want to discuss.</p>
<p>Each invited speaker will give a 30-minute talk summarizing the key ideas in some area, <i>not</i> their latest research so much as what everyone should know to start interesting conversations.  After that, 15 minutes for questions and/or coffee.</p>
<p>Here&#8217;s the schedule.  You can already see slides or other material for the talks with links!</p>
<h4> Wednesday April 8 </h4>
<p>&bull; 9:45-10:00 &#8212; the usual introductory fussing around.<br />
&bull; 10:00-10:30 &#8212; John Baez, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/03/25/introductory-talk/">Information and entropy in biological systems</a>.<br />
&bull; 10:30-11:00 &#8212; questions, coffee.<br />
&bull; 11:00-11:30 &#8212; Chris Lee, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/03/27/empirical-information-potential-information-and-disinformation/">Empirical information, potential information and disinformation</a>.<br />
&bull; 11:30-11:45 &#8212; questions.</p>
<p>&bull; 11:45-1:30 &#8212; lunch, conversations.</p>
<p>&bull; 1:30-2:00 &#8212; John Harte, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/03/25/maximum-entropy-as-a-foundation-for-theory-building-in-ecology/">Maximum entropy as a foundation for theory building in ecology</a>.<br />
&bull; 2:00-2:15 &#8212; questions, coffee.<br />
&bull; 2:15-2:45 &#8212; Annette Ostling, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/04/08/the-neutral-theory-of-biodiversity-and-other-competitors-to-maximum-entropy/">The neutral theory of biodiversity and other competitors to the principle of maximum entropy</a>.<br />
&bull; 2:45-3:00 &#8212; questions, coffee.<br />
&bull; 3:00-5:30 &#8212; break up into groups for discussions.</p>
<p>&bull; 5:30 &#8212; reception.</p>
<h4> Thursday April 9 </h4>
<p>&bull; 10:00-10:30 &#8212; David Wolpert, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/04/09/the-landauer-limit-and-thermodynamics-of-biological-organisms/">The Landauer limit and thermodynamics of biological organisms</a>.<br />
&bull; 10:30-11:00 &#8212; questions, coffee.<br />
&bull; 11:00-11:30 &#8212; Susanne Still, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/04/08/efficient-computation-and-data-representation/">Efficient computation and data modeling</a>.<br />
&bull; 11:30-11:45 &#8212; questions.</p>
<p>&bull; 11:45-1:30 &#8212; <strong>group photo</strong>, lunch, conversations.</p>
<p>&bull; 1:30-2:00 &#8212; Matina Donaldson-Matasci, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/04/02/the-fitness-value-of-information-in-an-uncertain-environment/">The fitness value of information in an uncertain environment</a>.<br />
&bull; 2:00-2:15 &#8212; questions, coffee.<br />
&bull; 2:15-2:45 &#8212; Roderick Dewar, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/04/10/maximum-entropy-and-maximum-entropy-production-in-biological-systems/">Maximum entropy and maximum entropy production in biological systems: survival of the likeliest?</a><br />
&bull; 2:45-3:00 &#8212; questions, coffee.<br />
&bull; 3:00-6:00 &#8212; break up into groups for discussions.</p>
<h4> Friday April 10 </h4>
<p>&bull; 10:00-10:30 &#8212; Marc Harper, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/04/02/information-transport-and-evolutionary-dynamics/">Information transport and evolutionary dynamics</a>.<br />
&bull; 10:30-11:00 &#8212; questions, coffee.<br />
&bull; 11:00-11:30 &#8212; Tobias Fritz, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/03/27/characterizations-of-shannon-and-renyi-entropy/">Characterizations of Shannon and R&eacute;nyi entropy</a>.<br />
&bull; 11:30-11:45 &#8212; questions.</p>
<p>&bull; 11:45-1:30 &#8212; lunch, conversations.</p>
<p>&bull; 1:30-2:00 &#8212; Christina Cobbold, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/03/30/biodiversity-measures-and-the-role-of-species-similarity/">Biodiversity measures and the role of species similarity</a>.<br />
&bull; 2:00-2:15 &#8212; questions, coffee.<br />
&bull; 2:15-2:45 &#8212;  Tom Leinster, <a href="http://www.nimbios.org/wordpress-training/entropy/2015/03/29/maximizing-biological-diversity/">Maximizing biological diversity</a>.<br />
&bull; 2:45-3:00 &#8212; questions, coffee.<br />
&bull; 3:00-6:00 &#8212; break up into groups for discussions.</p>
<h3> Participants </h3>
<p>Here are the confirmed participants.   This list may change a little bit:</p>
<p>&bull; John Baez &#8211; mathematical physicist.</p>
<p>&bull; Romain Brasselet &#8211; postdoc in cognitive neuroscience knowledgeable about information-theoretic methods and methods of estimating entropy from samples of probability distributions.</p>
<p>&bull;  Katharina Brinck &#8211; grad student at Centre for Complexity Science at Imperial College; did masters at John Harte’s lab, where she extended his Maximum Entropy Theory of Ecology (METE) to trophic food webs, to study how  entropy maximization on the macro scale together with MEP on the scale of individuals drive the structural development of model ecosystems.</p>
<p>&bull; Christina Cobbold &#8211; mathematical biologist, has studied the role of species similarity in measuring biodiversity.</p>
<p>&bull;  Troy Day &#8211; mathematical biologist, works with population dynamics, host-parasite dynamics, etc.; influential and could help move population dynamics to a more information-theoretic foundation.</p>
<p>&bull; Roderick Dewar &#8211; physicist who studies the principle of maximal entropy production.</p>
<p>&bull; Barrett Deris &#8211; MIT postdoc studying the studying the factors that influence evolvability of drug resistance in bacteria.</p>
<p>&bull; Charlotte de Vries &#8211; a biology master&#8217;s student who studied particle physics to the master&#8217;s level at Oxford and the Perimeter Institute.   Interested in information theory.</p>
<p>&bull; Matina Donaldson-Matasci &#8211; a biologist who studies information, uncertainty and collective behavior.</p>
<p>&bull; Chris Ellison &#8211; a postdoc who worked with James Crutchfield on &#8220;information-theoretic measures of structure and memory in stationary, stochastic systems &#8211; primarily, finite state hidden Markov models&#8221;.  He coauthored &#8220;Intersection Information based on Common Randomness&#8221;, <a href="http://arxiv.org/abs/1310.1538" rel="nofollow">http://arxiv.org/abs/1310.1538</a>.  The idea: &#8220;The introduction of the partial information decomposition generated a flurry of proposals for defining an intersection information that quantifies how much of &#8220;the same information&#8221; two or more random variables specify about a target random variable. As of yet, none is wholly satisfactory.&#8221;  Works on mutual information between organisms and environment (along with David Krakauer and Jessica Flack), and also entropy rates.</p>
<p>&bull;  Cameron Freer &#8211;  MIT postdoc in Brain and Cognitive Sciences working on maximum entropy production principles, algorithmic entropy etc.</p>
<p>&bull; Tobias Fritz &#8211; a physicist who has worked on &#8220;resource theories&#8221; and haracterizations of Shannon and R&eacute;nyi entropy and on resource theories.</p>
<p>&bull; Dashiell Fryer &#8211; works with Marc Harper on information geometry and evolutionary game theory.</p>
<p>&bull; Michael Gilchrist &#8211; an evolutionary biologist studying how errors and costs of protein translation affect the codon usage observed within a genome.  Works at NIMBioS.</p>
<p>&bull; Manoj Gopalkrishnan &#8211; an expert on chemical reaction networks who understands entropy-like Lyapunov functions for these systems.</p>
<p>&bull; Marc Harper &#8211; works on evolutionary game theory using ideas from information theory, information geometry, etc.</p>
<p>&bull; John Harte &#8211; an ecologist who uses the maximum entropy method to predict the structure of ecosystems.</p>
<p>&bull; Ellen Hines &#8211; studies habitat modeling and mapping for marine endangered species and ecosystems, sea level change scenarios, documenting of human use and values.  Her lab has used MaxEnt methods.</p>
<p>&bull; Elizabeth Hobson &#8211; behavior ecology postdoc developing methods to quantify social complexity in animals.  Works at NIMBioS.</p>
<p>&bull; John Jungk &#8211; works on graph theory and biology.</p>
<p>&bull; Chris Lee &#8211; in bioinformatics and genomics; applies information theory to experiment design and evolutionary biology.</p>
<p>&bull;  Maria Leites &#8211; works on dynamics, bifurcations and applications of coupled systems of non-linear ordinary differential equations with applications to ecology, epidemiology, and transcriptional regulatory networks.   Interested in information theory.</p>
<p>&bull; Tom Leinster &#8211; a mathematician who applies category theory to study various concepts of &#8216;magnitude&#8217;, including biodiversity and entropy.</p>
<p>&bull;  Timothy Lezon &#8211; a systems biologist in the Drug Discovery Institute at Pitt, who has used entropy to characterize phenotypic heterogeneity in populations of cultured cells.</p>
<p>&bull;  Maria Ortiz Mancera &#8211; statistician working at CONABIO,  the National Commission for Knowledge and Use of Biodiversity, in Mexico.</p>
<p>&bull;   Yajun Mei &#8211; statistician who uses Kullback-Leibler divergence and how to efficiently compute entropy for the two-state hidden Markov models.</p>
<p>&bull; Robert Molzon &#8211; mathematical economist who has studied deterministic approximation of stochastic evolutionary dynamics.</p>
<p>&bull;   David Murrugarra &#8211; works on discrete models in mathematical biology; interested in learning about information theory.</p>
<p>&bull; Annette Ostling &#8211; studies community ecology, focusing on the influence of interspecific competition on community structure, and what insights patterns of community structure might provide about the mechanisms by which competing species coexist.</p>
<p>&bull; Connie Phong &#8211; grad student at Chicago&#8217;s Institute of Genomics and System biology, working on how &#8220;certain biochemical network motifs are more attuned than others at maintaining strong input to output relationships under fluctuating conditions.&#8221;</p>
<p>&bull; Petr Plechak &#8211; works on information-theoretic tools for estimating and minimizing errors in coarse-graining stochastic systems.  Wrote <a href="http://arxiv.org/abs/1304.7700.">&#8220;Information-theoretic tools for parametrized coarse-graining of non-equilibrium extended systems&#8221;</a>.</p>
<p>&bull; Blake Polllard &#8211; physics grad student working with John Baez on various generalizations of Shannon and Renyi entropy, and how these entropies change with time in Markov processes and open Markov processes.</p>
<p>&bull; Timothee Poisot &#8211; works on species interaction networks; developed a &#8220;new suite of tools for probabilistic interaction networks&#8221;.</p>
<p>&bull;  Richard Reeve &#8211; works on biodiversity studies and the spread of antibiotic resistance.  Ran a program on entropy-based biodiversity measures at a mathematics institute in Barcelona.</p>
<p>&bull; Rob Shaw &#8211; works on entropy and information in biotic and pre-biotic systems.</p>
<p>&bull; Matteo Smerlak &#8211; postdoc working on nonequilibrium thermodynamics and its applications to biology, especially population biology and cell replication.</p>
<p>&bull; Susanne Still &#8211; a computer scientist who studies the role of thermodynamics and information theory in prediction.</p>
<p>&bull; Alexander Wissner-Gross &#8211; Institute Fellow at the Harvard University Institute for Applied Computational Science and Research Affiliate at the MIT Media Laboratory, interested in lots of things.</p>
<p>&bull; David Wolpert &#8211; works at the Santa Fe Institute on i) information theory and game theory, ii) the second law of thermodynamics and dynamics of complexity, iii) multi-information source optimization, iv) the mathematical underpinnings of reality, v) evolution of organizations.</p>
<p>&bull; Matthew Zefferman &#8211; works on evolutionary game theory, institutional economics and models of gene-culture co-evolution.   No work on information, but a postdoc at NIMBioS.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/04/06/information-and-entropy-in-biological-systems-part-3/#comments">3 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2015/04/06/information-and-entropy-in-biological-systems-part-3/" rel="bookmark" title="Permanent Link to Information and Entropy in Biological Systems (Part&nbsp;3)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-18298 post type-post status-publish format-standard hentry category-biology category-conferences category-information-and-entropy" id="post-18298">
				<h2><a href="https://johncarlosbaez.wordpress.com/2014/07/04/entropy-and-information-in-biological-systems-part-2/" rel="bookmark">Entropy and Information in Biological Systems (Part&nbsp;2)</a></h2>
				<small>4 July, 2014</small><br />


				<div class="entry">
					<p><a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a>, <a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a> and I are running a workshop!  Now you can apply here to attend:</p>
<p>&bull; <a href="http://www.nimbios.org/workshops/WS_entropy">Information and entropy in biological systems</a>, National Institute for Mathematical and Biological Synthesis, Knoxville Tennesee, Wednesday-Friday, 8-10 April 2015.  </p>
<p>Click the link, read the stuff and scroll down to &#8220;CLICK HERE&#8221; to apply.  The deadline is 12 November 2014.</p>
<p>Financial support for travel, meals, and lodging is available for workshop attendees who need it.  We will choose among the applicants and invite 10-15 of them.  </p>
<div align="center"><a href="http://www.nimbios.org/"><img width="450" src="https://i0.wp.com/www.utk.edu/tntoday/images/nimbios_logo_lg.jpg" /></a></div>
<h3> The idea </h3>
<p>Information theory and entropy methods are becoming powerful tools in biology, from the level of individual cells, to whole ecosystems, to experimental design, model-building, and the measurement of biodiversity. The aim of this investigative workshop is to synthesize different ways of applying these concepts to help systematize and unify work in biological systems. Early attempts at &#8220;grand syntheses&#8221; often misfired, but applications of information theory and entropy to specific highly focused topics in biology have been increasingly successful. In ecology, entropy maximization methods have proven successful in predicting the distribution and abundance of species. Entropy is also widely used as a measure of biodiversity. Work on the role of information in game theory has shed new light on evolution. As a population evolves, it can be seen as gaining information about its environment. The principle of maximum entropy production has emerged as a fascinating yet controversial approach to predicting the behavior of biological systems, from individual organisms to whole ecosystems. This investigative workshop will bring together top researchers from these diverse fields to share insights and methods and address some long-standing conceptual problems.</p>
<p>So, here are the goals of our workshop:</p>
<p>&bull; To study the validity of the principle of Maximum Entropy Production (MEP), which states that biological systems &#8211; and indeed all open, non-equilibrium systems &#8211; act to produce entropy at the maximum rate.</p>
<p>&bull; To familiarize all the participants with applications to ecology of the MaxEnt method: choosing the probabilistic hypothesis with the highest entropy subject to the constraints of our data. We will compare MaxEnt with competing approaches and examine whether MaxEnt provides a sufficient justification for the principle of MEP.</p>
<p>&bull; To clarify relations between known characterizations of entropy, the use of entropy as a measure of biodiversity, and the use of MaxEnt methods in ecology.</p>
<p>&bull; To develop the concept of evolutionary games as &#8220;learning&#8221; processes in which information is gained over time.</p>
<p>&bull; To study the interplay between information theory and the thermodynamics of individual cells and organelles.</p>
<p>For more details, go <a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/">here</a>.  </p>
<p>If you&#8217;ve got colleagues who might be interested in this, please let them know.  You can download a PDF suitable for printing and putting on a bulletin board by clicking on this:</p>
<div align="center">
<a href="http://math.ucr.edu/home/baez/nimbios/WS_entropy_flier.pdf"><br />
<img src="https://i0.wp.com/math.ucr.edu/home/baez/nimbios/WS_entropy_flier.jpg" /></a>
</div>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/07/04/entropy-and-information-in-biological-systems-part-2/#comments">6 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2014/07/04/entropy-and-information-in-biological-systems-part-2/" rel="bookmark" title="Permanent Link to Entropy and Information in Biological Systems (Part&nbsp;2)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-16829 post type-post status-publish format-standard hentry category-biology category-conferences category-information-and-entropy category-mathematics" id="post-16829">
				<h2><a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/" rel="bookmark">Entropy and Information in Biological Systems (Part&nbsp;1)</a></h2>
				<small>2 November, 2013</small><br />


				<div class="entry">
					<p><a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a> is an ecologist who uses maximum entropy methods to predict the distribution, abundance and energy usage of species.  <a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a> uses information theory in bioinformatics and evolutionary game theory.   Harper, Harte and I are organizing a workshop on entropy and information in biological systems, and I&#8217;m really excited about it!</p>
<p>It&#8217;ll take place at the <a href="http://www.nimbios.org/">National Institute for Mathematical and Biological Synthesis</a> in Knoxville Tennesee.    We are scheduling it for Wednesday-Friday, April 8-10, 2015.  When the date gets confirmed, I&#8217;ll post an advertisement so you can apply to attend.</p>
<p>Writing the proposal was fun, because we got to pull together lots of interesting people who are applying information theory and entropy to biology in quite different ways.   So, here it is!</p>
<div align="center"><a href="http://www.nimbios.org/"><img width="450" src="https://i0.wp.com/www.utk.edu/tntoday/images/nimbios_logo_lg.jpg" /></a></div>
<h3> Proposal </h3>
<p>Ever since Shannon initiated research on information theory in 1948, there have been hopes that the concept of information could serve as a tool to help systematize and unify work in biology.  The link between information and <i>entropy</i> was noted very early on, and it suggested that a full thermodynamic understanding of biology would naturally involve the information processing and storage that are characteristic of living organisms.  However, the subject is full of conceptual pitfalls for the unwary, and progress has been slower than initially expected.  Premature attempts at &#8216;grand syntheses&#8217; have often misfired.  But applications of information theory and entropy to specific highly focused topics in biology have been increasingly successful, such as:</p>
<p>&bull;  the maximum entropy principle in ecology,<br />
&bull;   Shannon and R&eacute;nyi entropies as measures of biodiversity,<br />
&bull;  information theory in evolutionary game theory,<br />
&bull;  information and the thermodynamics of individual cells.</p>
<p>Because they work in diverse fields, researchers in these specific topics have had little opportunity to trade insights and take stock of the progress so far.  The aim of the workshop is to do just this.  </p>
<p>In what follows, participants&#8217; names are in boldface, while the main goals of the workshop are in italics.</p>
<p><b><a href="http://biology.anu.edu.au/roderick_dewar/">Roderick Dewar</a></b> is a key advocate of the principle of Maximum Entropy Production, which says that biological systems&#8212;and indeed all open, non-equilibrium systems&#8212;act to produce entropy at the maximum rate.  Along with others, he has applied this principle to make testable predictions in a wide range of biological systems, from ATP synthesis [DJZ2006] to respiration and photosynthesis of individual plants [D2010] and plant communities.  He has also sought to derive this principle from ideas in statistical mechanics [D2004, D2009], but it remains controversial.  </p>
<p><i>The first goal of this workshop is to study the validity of this principle</i>.</p>
<p>While they may be related, the principle of Maximum Entropy Production should not be confused with the MaxEnt inference procedure, which says that we should choose the probabilistic hypothesis with the highest entropy subject to the constraints provided by our data.  MaxEnt was first explicitly advocated by Jaynes.  He noted that it is already implicit in the procedures of statistical mechanics, but convincingly argued that it can also be applied to situations where entropy is more &#8216;informational&#8217; than &#8216;thermodynamic&#8217; in character.  </p>
<p>Recently <b><a href="http://socrates.berkeley.edu/~hartelab/">John Harte</a></b> has applied MaxEnt in this way to ecology, using it to make specific testable predictions for the distribution, abundance and energy usage of species across spatial scales and across habitats and taxonomic groups [Harte2008, Harte2009, Harte2011].  <b><a href="http://webapps.lsa.umich.edu/eeb/directory/faculty/aostling/">Annette Ostling</a></b> is an expert on other theories that attempt to explain the same data, such as the &#8216;neutral model&#8217; [AOE2008, ODLSG2009, O2005, O2012]. <b><a href="http://biology.anu.edu.au/roderick_dewar/">Dewar</a></b> has also used MaxEnt in ecology [D2008], and he has argued that it underlies the principle of Maximum Entropy Production.    </p>
<p><i>Thus, a second goal of this workshop is to familiarize all the participants with applications of the MaxEnt method to ecology, compare it with competing approaches, and study whether MaxEnt provides a sufficient justification for the principle of Maximum Entropy Production.</i></p>
<p>Entropy is not merely a predictive tool in ecology: it is also widely used as a measure of biodiversity.  Here Shannon&#8217;s original concept of entropy naturally generalizes to &#8216;R&eacute;nyi entropy&#8217;, which depends on a parameter <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;ge 0" class="latex" />.  This equals</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+H_%5Calpha%28p%29+%3D+%5Cfrac%7B1%7D%7B1-%5Calpha%7D+%5Clog+%5Csum_i+p_i%5E%5Calpha++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ H_&#92;alpha(p) = &#92;frac{1}{1-&#92;alpha} &#92;log &#92;sum_i p_i^&#92;alpha  } " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th type (which could mean species, some other taxon, etc.).    In the limit <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha &#92;to 1" class="latex" /> this reduces to the Shannon entropy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++H%28p%29+%3D+-+%5Csum_i+p_i+%5Clog+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  H(p) = - &#92;sum_i p_i &#92;log p_i } " class="latex" /></p>
<p>As <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha" class="latex" /> increases, we give less weight to rare types of organisms.  <b><a href="http://www.maths.gla.ac.uk/~cc/">Christina Cobbold</a></b> and <b><a href="http://www.maths.ed.ac.uk/~tl/">Tom Leinster</a></b> have described a systematic and highly flexible system of biodiversity measurement, with R&eacute;nyi entropy at its heart [CL2012].    They consider both the case where all we have are the numbers <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" />, and the more subtle case where we take the distance between different types of organisms into account.  </p>
<p><b><a href="http://math.ucr.edu/home/baez/">John Baez</a></b> has explained the role of R&eacute;nyi entropy in thermodynamics [B2011], and together with <b><a href="http://www.maths.ed.ac.uk/~tl/"></a><a>Tom Leinster</a></b> and <b><a href="http://users.icfo.es/Tobias.Fritz/">Tobias Fritz</a></b> he has proved other theorems characterizing entropy which explain its importance for information processing [BFL2011].  However, these ideas have not yet been connected to the widespread use of entropy in biodiversity studies.  More importantly, the use of entropy as a measure of biodiversity has not been clearly connected to MaxEnt methods in ecology.  Does the success of MaxEnt methods imply a tendency for ecosystems to maximize biodiversity subject to the constraints of resource availability?  This seems surprising, but a more nuanced statement along these general lines might be correct.    </p>
<p><i>So, a third goal of this workshop is to clarify relations between known characterizations of entropy, the use of entropy as a measure of biodiversity, and the use of MaxEnt methods in ecology.</i></p>
<p>As the amount of data to analyze in genomics continues to surpass the ability of humans to analyze it, we can expect automated experiment design to become ever more important.   In <b><a href="http://thinking.bioinformatics.ucla.edu/">Chris Lee</a></b> and <b><a href="http://people.mbi.ucla.edu/marcharper/">Marc Harper</a></b>’s RoboMendel program [LH2013], a mathematically precise concept of &#8216;potential information&#8217;&#8212;how much information is left to learn&#8212;plays a crucial role in deciding what experiment to do next, given the data obtained so far.  It will be useful for them to interact with <b><a href="http://www.princeton.edu/~wbialek/wbialek.html">William Bialek</a></b>, who has expertise in estimating entropy from empirical data and using it to constrain properties of models [BBS, BNS2001, BNS2002], and <b><a href="http://www2.hawaii.edu/~sstill/">Susanne Still</a></b>, who applies information theory to automated theory building and biology [CES2010, PS2012].</p>
<p>However, there is another link between biology and potential information.  <b><a href="http://people.mbi.ucla.edu/marcharper/">Harper</a></b> has noted that in an ecosystem where the population of each type of organism grows at a rate proportional to its fitness (which may depend on the fraction of organisms of each type), the quantity </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+I%28q%7C%7Cp%29+%3D+%5Csum_i+q_i+%5Cln%28q_i%2Fp_i%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ I(q||p) = &#92;sum_i q_i &#92;ln(q_i/p_i) } " class="latex" /></p>
<p>always decreases if there is an evolutionarily stable state [Harper2009].  Here <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the fraction of organisms of the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="i" class="latex" />th genotype at a given time, while <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> is this fraction in the evolutionarily stable state.  This quantity is often called the Shannon information of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> &#8216;relative to&#8217; <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" />.  But in fact, it is precisely the same as <b><a href="http://thinking.bioinformatics.ucla.edu/">Lee</a></b> and <b><a href="http://people.mbi.ucla.edu/marcharper/">Harper</a></b>’s potential information!  Indeed, there is a precise mathematical analogy between evolutionary games and processes where a probabilistic hypothesis is refined by repeated experiments.  </p>
<p><i>Thus, a fourth goal of this workshop is to develop the concept of evolutionary games as &#8216;learning&#8217; processes in which information is gained over time.</i>  </p>
<p>We shall try to synthesize this with <b><a href="http://octavia.zoology.washington.edu/">Carl Bergstrom</a></b> and <b><a href="http://www.matina.org/">Matina Donaldson-Matasci</a></b>’s work on the &#8216;fitness value of information&#8217;: a measure of how much increase in fitness a population can obtain per bit of extra information [BL2004, DBL2010, DM2013].   Following <b><a href="http://people.mbi.ucla.edu/marcharper/">Harper</a></b>, we shall consider not only relative Shannon entropy, but also relative R&eacute;nyi entropy, as a measure of information gain [Harper2011].</p>
<p><i>A fifth and final goal of this workshop is to study the interplay between information theory and the thermodynamics of individual cells and organelles.</i></p>
<p><b><a href="http://www2.hawaii.edu/~sstill/">Susanne Still</a></b> has studied the thermodynamics of prediction in biological systems [BCSS2012].   And in a celebrated related piece of work, <b><a href="http://web.mit.edu/physics/people/faculty/england_jeremy.html">Jeremy England</a></b> used thermodynamic arguments to a derive a lower bound for the amount of entropy generated during a process of self-replication of a bacterial cell [England2013].  Interestingly, he showed that <i>E. coli</i> comes within a factor of 3 of this lower bound.   </p>
<p>In short, information theory and entropy methods are becoming powerful tools in biology, from the level of individual cells, to whole ecosystems, to experimental design, model-building, and the measurement of biodiversity. The time is ripe for an investigative workshop that brings together experts from different fields and lets them share insights and methods and begin to tackle some of the big remaining questions.</p>
<h3> Bibliography </h3>
<p>[AOE2008] D. Alonso, A. Ostling and R. Etienne, <a href="http://www-personal.umich.edu/~aostling/papers/alonso2008.pdf">The assumption of symmetry and species abundance distributions</a>, <i>Ecology Letters</i> <b>11</b> (2008), 93&#8211;105.</p>
<p>[TMMABB2012} D. Amodei, W. Bialek, M. J. Berry II, O. Marre, T. Mora, and G. Tkacik, <a href="http://arxiv.org/abs/1207.6319">The simplest maximum entropy model for collective behavior in a neural network</a>, arXiv:1207.6319 (2012).</p>
<p>[B2011] J. Baez, <a href="http://arxiv.org/abs/1102.2098">R&eacute;nyi entropy and free energy</a>, arXiv:1102.2098 (2011).</p>
<p>[BFL2011] J. Baez, T. Fritz and T. Leinster, <a href="http://arxiv.org/abs/1106.1791">A characterization of entropy in terms of information loss</a>, <i>Entropy</i> <b>13</b> (2011), 1945&#8211;1957.</p>
<p>[B2011] J. Baez and M. Stay, <a href="http://arxiv.org/abs/1010.2067">Algorithmic thermodynamics</a>, <i>Math. Struct. Comp. Sci.</i> <b>22</b> (2012), 771&#8211;787.</p>
<p>[BCSS2012] A. J. Bell, G. E. Crooks, S. Still and D. A Sivak, <a href="http://arxiv.org/abs/1203.3271">The thermodynamics of prediction</a>, <i>Phys. Rev. Lett.</i> <b>109</b> (2012), 120604.</p>
<p>[BL2004] C. T. Bergstrom and M. Lachmann, <a href="http://octavia.zoology.washington.edu/publications/BergstromAndLachmann04.pdf">Shannon information and biological fitness</a>, in <i>IEEE Information Theory Workshop 2004</i>, IEEE, 2004, pp. 50-54.</p>
<p>[BBS] M. J. Berry II, W. Bialek and E. Schneidman, <a href="http://arxiv.org/abs/physics/0212114">An information theoretic approach to the functional classification of neurons</a>, in <i>Advances in Neural Information Processing Systems 15</i>, MIT Press, 2005.</p>
<p>[BNS2001] W. Bialek, I. Nemenman and N. Tishby, <a href="http://www.princeton.edu/~wbialek/our_papers/bnt_01a.pdf">Predictability, complexity and learning</a>, <i>Neural Computation</i> <b>13</b> (2001), 2409&#8211;2463.</p>
<p>[BNS2002] W. Bialek, I. Nemenman and F. Shafee, <a href="http://books.nips.cc/papers/files/nips14/LT22.pdf">Entropy and inference, revisited</a>, in <i>Advances in Neural Information Processing Systems 14</i>, MIT Press, 2002.</p>
<p>[CL2012] C. Cobbold and T. Leinster, <a href="http://www.maths.ed.ac.uk/~tl/mdiss.pdf">Measuring diversity: the importance of species similarity</a>, <i>Ecology</i> <b>93</b> (2012), 477&#8211;489.</p>
<p>[CES2010] J. P. Crutchfield, S. Still and C. Ellison, <a href="http://arxiv.org/abs/0708.1580">Optimal causal inference: estimating stored information and approximating causal architecture</a>, <i>Chaos</i> <b>20</b> (2010), 037111.</p>
<p>[D2004] R. C. Dewar, Maximum entropy production and non-equilibrium statistical mechanics, in <i>Non-Equilibrium Thermodynamics and Entropy Production: Life, Earth and Beyond</i>, eds. A. Kleidon and R. Lorenz, Springer, New York, 2004, 41&#8211;55.</p>
<p>[DJZ2006] R. C. Dewar, D. Juret&iacute;c,  P. Zupanov&iacute;c, <a href="http://www.pmfst.hr/~juretic/CPLETT23896.pdf">The functional design of the rotary enzyme ATP synthase is consistent with maximum entropy production</a>, <i>Chem. Phys. Lett.</i> <b>430</b> (2006), 177&#8211;182. </p>
<p>[D2008] R. C. Dewar, A. Port&eacute;, <a href="http://arxiv.org/abs/q-bio/0703061">Statistical mechanics unifies different ecological patterns</a>, <i>J. Theor. Bio.</i> <b>251</b> (2008), 389&#8211;403. </p>
<p>[D2009] R. C. Dewar, <a href="http://www.mdpi.com/1099-4300/11/4/931/pdf">Maximum entropy production as an inference algorithm that translates physical assumptions into macroscopic predictions: don&#8217;t shoot the messenger</a>, <i>Entropy</i> <b>11</b> (2009), 931&#8211;944. </p>
<p>[D2010] R. C. Dewar, <a href="http://rstb.royalsocietypublishing.org/content/365/1545/1429.full">Maximum entropy production and plant optimization theories</a>, <i>Phil. Trans. Roy. Soc. B</i> <b>365</b> (2010) 1429&#8211;1435.</p>
<p>[DBL2010} M. C. Donaldson-Matasci, C. T. Bergstrom, and<br />
M. Lachmann, <a href="http://arxiv.org/abs/q-bio/0510007">The fitness value of information</a>, <i>Oikos</i> <b>119</b> (2010), 219-230.</p>
<p>[DM2013] M. C. Donaldson-Matasci, G. DeGrandi-Hoffman, and A. Dornhaus, Bigger is better: honey bee colonies as distributed information-gathering systems, <i>Animal Behaviour</i> <b>85</b> (2013), 585&#8211;592.</p>
<p>[England2013] J. L. England, <a href="http://arxiv.org/abs/1209.1179">Statistical physics of self-replication</a>, <i>J. Chem. Phys.</i> <b>139</b> (2013), 121923.</p>
<p>[ODLSG2009} J. L. Green,  J. K. Lake, J. P. O’Dwyer, A. Ostling and V. M. Savage, <a href="http://www-personal.umich.edu/~aostling/papers/ODwyer2009.pdf">An integrative framework for stochastic, size-structured community assembly</a>, <i>PNAS</i> <b>106</b> (2009), 6170&#8211;6175.</p>
<p>[Harper2009] M. Harper, <a href="http://arxiv.org/abs/0911.1383">Information geometry and evolutionary game theory</a>, arXiv:0911.1383 (2009).</p>
<p>[Harper2011] M. Harper, <a href="http://arxiv.org/abs/0911.1764">Escort evolutionary game theory</a>, <i>Physica D</i> <b>240</b> (2011), 1411&#8211;1415.</p>
<p>[Harte2008] J. Harte, T. Zillio, E. Conlisk and A. Smith, Maximum entropy and the state-variable approach to macroecology, <i>Ecology</i> <b>89</b> (2008), 2700&#8211;2711.</p>
<p>[Harte2009] J. Harte, A. Smith and D. Storch, Biodiversity scales from plots to biomes with a universal species-area curve, <i>Ecology Letters</i> <b>12</b> (2009), 789–797.</p>
<p>[Harte2011] J. Harte, <i>Maximum Entropy and Ecology: A Theory of Abundance, Distribution, and Energetics</i>, Oxford U. Press, Oxford, 2011.</p>
<p>[LH2013] M. Harper and C. Lee, <a href="http://arxiv.org/abs/1210.4808">Basic experiment planning via information metrics: the RoboMendel problem</a>, arXiv:1210.4808 (2012).</p>
<p>[O2005] A. Ostling, <a href="http://www-personal.umich.edu/~aostling/papers/O2005.pdf">Neutral theory tested by birds</a>, <i>Nature</i> <b>436</b> (2005), 635.</p>
<p>[O2012] A. Ostling, <a href="http://www-personal.umich.edu/~aostling/papers/O2012fit.pdf">Do fitness-equalizing tradeoffs lead to neutral communities?</a>, <i>Theoretical Ecology</i> <b>5</b> (2012), 181&#8211;194. </p>
<p>[PS2012] D. Precup and S. Still, <a href="http://www2.hawaii.edu/~sstill/StillPrecup2011.pdf">An information-theoretic approach to curiosity-driven reinforcement learning</a>, <i>Theory in Biosciences</i> <b>131</b> (2012), 139&#8211;148.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/#comments">35 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/biology/" rel="category tag">biology</a>, <a href="https://johncarlosbaez.wordpress.com/category/conferences/" rel="category tag">conferences</a>, <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2013/11/02/entropy-and-information-in-biological-systems/" rel="bookmark" title="Permanent Link to Entropy and Information in Biological Systems (Part&nbsp;1)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31635 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31635">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/" rel="bookmark">Information Geometry (Part&nbsp;21)</a></h2>
				<small>17 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Last time</a> I ended with a formula for the &#8216;Gibbs distribution&#8217;: the probability distribution that maximizes entropy subject to constraints on the expected values of some observables.</p>
<p>This formula is well-known, but I&#8217;d like to derive it here.   My argument won&#8217;t be up to the highest standards of rigor: I&#8217;ll do a bunch of computations, and it would take more work to state conditions under which these computations are justified.   But even a nonrigorous approach is worthwhile, since the computations will give us more than the mere formula for the Gibbs distribution.</p>
<p>I&#8217;ll start by reminding you of what I claimed last time.   I&#8217;ll state it in a way that removes all unnecessary distractions, so go back to <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Part 20</a> if you want more explanation.</p>
<h3> The Gibbs distribution</h3>
<p>Take a measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu." class="latex" />    Suppose there is a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> that maximizes the <b>entropy</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-%5Cint_%5COmega+%5Cpi%28x%29+%5Cln+%5Cpi%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -&#92;int_&#92;Omega &#92;pi(x) &#92;ln &#92;pi(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>subject to the requirement that some integrable functions <img src="https://s0.wp.com/latex.php?latex=A%5E1%2C+%5Cdots%2C+A%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^1, &#92;dots, A^n" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> have expected values equal to some chosen list of numbers <img src="https://s0.wp.com/latex.php?latex=q%5E1%2C+%5Cdots%2C+q%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^1, &#92;dots, q^n." class="latex" /></p>
<p>(Unlike last time, now I&#8217;m writing <img src="https://s0.wp.com/latex.php?latex=A%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i" class="latex" /> with superscripts rather than subscripts, because I&#8217;ll be using the Einstein summation convention: I&#8217;ll sum over any repeated index that appears once as a a superscript and once as a subscript.)</p>
<p>Furthermore, suppose <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> depends smoothly on <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathbb%7BR%7D%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in &#92;mathbb{R}^n." class="latex" />    I&#8217;ll call it <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> to indicate its dependence on <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   Then, I claim <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the so-called <b>Gibbs distribution</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cpi_q%28x%29+%3D%5Cfrac%7Be%5E%7B-p_i+A%5Ei%28x%29%7D%7D%7B%5Cint_%5COmega+e%5E%7B-p_i+A%5Ei%28x%29%7D+%5C%2C+d%5Cmu%28x%29%7D+++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;pi_q(x) =&#92;frac{e^{-p_i A^i(x)}}{&#92;int_&#92;Omega e^{-p_i A^i(x)} &#92;, d&#92;mu(x)}   }" class="latex" /></p>
<p>where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%28q%29%7D%7B%5Cpartial+q%5Ei%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f(q)}{&#92;partial q^i} }" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Cint_%5COmega+%5Cpi_q%28x%29+%5Cln+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;int_&#92;Omega &#92;pi_q(x) &#92;ln &#92;pi_q(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>is the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>Let&#8217;s show this is true!</p>
<h3> Finding the Gibbs distribution</h3>
<p>So, we are trying to find a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> that maximizes entropy subject to these constraints:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cpi%28x%29+A%5Ei%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q%5Ei+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;pi(x) A^i(x) &#92;, d&#92;mu(x) = q^i } " class="latex" /></p>
<p>We can solve this problem using <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>.  We need one Lagrange multiplier, say <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i," class="latex" /> for each of the above constraints.  But it&#8217;s easiest if we start by letting <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> range over all of <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega)," class="latex" /> that is, the space of all integrable functions on <img src="https://s0.wp.com/latex.php?latex=%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega." class="latex" />  Then, because we want <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> to be a probability distribution, we need to impose one extra constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cpi%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;pi(x) &#92;, d&#92;mu(x) = 1 } " class="latex" /></p>
<p>To do this we need an extra Lagrange multiplier, say <img src="https://s0.wp.com/latex.php?latex=%5Cgamma.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma." class="latex" /></p>
<p>So, that&#8217;s what we&#8217;ll do!  We&#8217;ll look for critical points of this function on <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%29%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega):" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+-+%5Cbeta_i++%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+-+%5Cgamma+%5Cint+%5Cpi%5C%2C++d%5Cmu++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu - &#92;beta_i  &#92;int &#92;pi A^i &#92;, d&#92;mu - &#92;gamma &#92;int &#92;pi&#92;,  d&#92;mu  }" class="latex" /></p>
<p>Here I&#8217;m using some tricks to keep things short.  First, I&#8217;m dropping the dummy variable <i>x</i> which appeared in all of the integrals we had: I&#8217;m leaving it implicit.  Second, all my integrals are over <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> so I won&#8217;t say that.  And third, I&#8217;m using the Einstein summation convention, so there&#8217;s a sum over <i>i</i> implicit here.</p>
<p>Okay, now let&#8217;s do the <a href="https://en.wikipedia.org/wiki/Functional_derivative">variational derivative</a> required to find a critical point of this function.   When I was a math major taking physics classes, the way physicists did variational derivatives seemed like black magic to me.  Then I spent months reading how mathematicians rigorously justified these techniques.  I don&#8217;t feel like a massive digression into this right now, so I&#8217;ll just do the calculations&#8212;and if they seem like black magic, I&#8217;m sorry!</p>
<p>We need to find <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> obeying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cleft%28-+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+-+%5Cbeta_i++%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+-+%5Cgamma+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%5Cright%29+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;left(- &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu - &#92;beta_i  &#92;int &#92;pi A^i &#92;, d&#92;mu - &#92;gamma &#92;int &#92;pi &#92;, d&#92;mu &#92;right) = 0 } " class="latex" /></p>
<p>or in other words</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cleft%28%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+%2B+%5Cbeta_i++%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+%2B+%5Cgamma+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%5Cright%29+%3D+0+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;left(&#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu + &#92;beta_i  &#92;int &#92;pi A^i &#92;, d&#92;mu + &#92;gamma &#92;int &#92;pi &#92;, d&#92;mu &#92;right) = 0 } " class="latex" /></p>
<p>First we need to simplify this expression.   The only part that takes any work, if you know how to do variational derivatives, is the first term.  Since the derivative of <img src="https://s0.wp.com/latex.php?latex=z+%5Cln+z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="z &#92;ln z" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=1+%2B+%5Cln+z%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 + &#92;ln z," class="latex" /> we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu++%3D+1+%2B+%5Cln+%5Cpi%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu  = 1 + &#92;ln &#92;pi(x) } " class="latex" /></p>
<p>The second and third terms are easy, so we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cleft%28+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu+%2B+%5Cbeta_i+%5Cint+%5Cpi+A%5Ei+%5C%2C+d%5Cmu+%2B+%5Cgamma+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%5Cright%29+%3D%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;left( &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu + &#92;beta_i &#92;int &#92;pi A^i &#92;, d&#92;mu + &#92;gamma &#92;int &#92;pi &#92;, d&#92;mu &#92;right) =} " class="latex" /></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp;  <img src="https://s0.wp.com/latex.php?latex=1+%2B+%5Cln+%5Cpi%28x%29+%2B+%5Cbeta_i+A%5Ei%28x%29+%2B+%5Cgamma++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="1 + &#92;ln &#92;pi(x) + &#92;beta_i A^i(x) + &#92;gamma  " class="latex" /></p>
<p>Thus, we need to solve this equation:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+1+%2B+%5Cln+%5Cpi%28x%29+%2B+%5Cbeta_i+A%5Ei%28x%29+%2B+%5Cgamma++%3D+0%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ 1 + &#92;ln &#92;pi(x) + &#92;beta_i A^i(x) + &#92;gamma  = 0} " class="latex" /></p>
<p>That&#8217;s easy to do:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi%28x%29+%3D+e%5E%7B-1+-+%5Cgamma+-+%5Cbeta_i+A%5Ei%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi(x) = e^{-1 - &#92;gamma - &#92;beta_i A^i(x)} " class="latex" /></p>
<p>Good!   It&#8217;s starting to look like the Gibbs distribution!</p>
<p>We now need to choose the Lagrange multipliers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> to make the constraints hold.  To satisfy this constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint+%5Cpi+%5C%2C+d%5Cmu+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int &#92;pi &#92;, d&#92;mu = 1 } " class="latex" /></p>
<p>we must choose <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;gamma" class="latex" /> so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint++e%5E%7B-1+-+%5Cgamma+-+%5Cbeta_i+A%5Ei+%7D+%5C%2C+d%5Cmu+%3D+1+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int  e^{-1 - &#92;gamma - &#92;beta_i A^i } &#92;, d&#92;mu = 1 } " class="latex" /></p>
<p>or in other words</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+e%5E%7B1+%2B+%5Cgamma%7D+%3D+%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ e^{1 + &#92;gamma} = &#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu } " class="latex" /></p>
<p>Plugging this into our earlier formula</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi%28x%29+%3D+e%5E%7B-1+-+%5Cgamma+-+%5Cbeta_i+A%5Ei%28x%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi(x) = e^{-1 - &#92;gamma - &#92;beta_i A^i(x)} " class="latex" /></p>
<p>we get this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi_q%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi_q(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{&#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu} } " class="latex" /></p>
<p>Great!  Even more like the Gibbs distribution!</p>
<p>By the way, you must have noticed the &#8220;1&#8221; that showed up here:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta+%5Cpi%28x%29%7D+%5Cint+%5Cpi+%5Cln+%5Cpi+%5C%2C+d%5Cmu++%3D+1+%2B+%5Cln+%5Cpi%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;frac{&#92;delta}{&#92;delta &#92;pi(x)} &#92;int &#92;pi &#92;ln &#92;pi &#92;, d&#92;mu  = 1 + &#92;ln &#92;pi(x) } " class="latex" /></p>
<p>It buzzed around like an annoying fly in the otherwise beautiful calculation, but eventually went away.  This is the same irksome &#8220;1&#8221; that showed up in <a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Part 19</a>.   Someday I&#8217;d like to say a bit more about it.</p>
<p>Now, where were we?  We were trying to show that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cpi_q%28x%29+%3D%5Cfrac%7Be%5E%7B-p_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-p_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+++%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;pi_q(x) =&#92;frac{e^{-p_i A^i(x)}}{&#92;int e^{-p_i A^i} &#92;, d&#92;mu}   }" class="latex" /></p>
<p>minimizes entropy subject to our constraints.  So far we&#8217;ve shown</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{&#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu} } " class="latex" /></p>
<p>is a critical point.   It&#8217;s clear that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi%28x%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi(x) &#92;ge 0" class="latex" /></p>
<p>so <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> really is a probability distribution.  We should show it actually <i>maximizes</i> entropy subject to our constraints, but I will skip that.   Given that, <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> will be our claimed Gibbs distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> if we can show</p>
<p><img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i = &#92;beta_i" class="latex" /></p>
<p>This is interesting!   It&#8217;s saying our Lagrange multipliers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> actually equal the so-called <b>conjugate variables</b> <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q^i} }" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=f%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q)" class="latex" /> is the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q:" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+f%28q%29+%3D+-+%5Cint_%5COmega+%5Cpi_q%28x%29+%5Cln+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ f(q) = - &#92;int_&#92;Omega &#92;pi_q(x) &#92;ln &#92;pi_q(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>There are two ways to show this: the easy way and the hard way.  The easy way is to reflect on the meaning of Lagrange multipliers, and I&#8217;ll sketch that way first.  The hard way is to use brute force: just compute <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> and show it equals <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i." class="latex" />   This is a good test of our computational muscle&#8212;but more importantly, it will help us discover some interesting facts about the Gibbs distribution.</p>
<h3> The easy way</h3>
<p>Consider a simple Lagrange multiplier problem where you&#8217;re trying to find a critical point of a smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+%5Cmathbb%7BR%7D%5E2+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon &#92;mathbb{R}^2 &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>subject to the constraint</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%3D+c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g = c" class="latex" /></p>
<p>for some smooth function</p>
<p><img src="https://s0.wp.com/latex.php?latex=g+%5Ccolon+%5Cmathbb%7BR%7D%5E2+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g &#92;colon &#92;mathbb{R}^2 &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>and constant <i>c</i>.   (The function <i>f</i> here has nothing to do with the <i>f</i> in the previous sections.)   To answer this we introduce a Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> and seek points where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla+%28+f+-+%5Clambda+g%29+%3D+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla ( f - &#92;lambda g) = 0" class="latex" /></p>
<p>This works because the above equation says</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f+%3D+%5Clambda+%5Cnabla+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla f = &#92;lambda &#92;nabla g" class="latex" /></p>
<p>Geometrically this means we&#8217;re at a point where the gradient of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> points at right angles to the level surface of <img src="https://s0.wp.com/latex.php?latex=g%3A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g:" class="latex" /></p>
<p><a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"><img loading="lazy" data-attachment-id="31685" data-permalink="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/lagrange_multipliers/" data-orig-file="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png" data-orig-size="2560,1843" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lagrange_multipliers" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=300" data-large-file="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=450" src="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=450&#038;h=324" alt="" class="aligncenter size-full wp-image-31685" width="450" height="324" srcset="https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=450&amp;h=324 450w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=900&amp;h=648 900w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=150&amp;h=108 150w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=300&amp;h=216 300w, https://johncarlosbaez.files.wordpress.com/2021/08/lagrange_multipliers.png?w=768&amp;h=553 768w" sizes="(max-width: 450px) 100vw, 450px" /></a></p>
<p>Thus, to first order we can&#8217;t change <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> by moving along the level surface of <img src="https://s0.wp.com/latex.php?latex=g.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g." class="latex" /></p>
<p>But also, if we start at a point where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f+%3D+%5Clambda+%5Cnabla+g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;nabla f = &#92;lambda &#92;nabla g" class="latex" /></p>
<p>and we begin moving in any direction, the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> will change at a rate equal to <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda" class="latex" /> times the rate of change of <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="g" class="latex" />.  That&#8217;s just what the equation says!   And this fact gives a conceptual meaning to the Lagrange multiplier <img src="https://s0.wp.com/latex.php?latex=%5Clambda.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;lambda." class="latex" /></p>
<p>Our situation is more complicated, since our functions are defined on the infinite-dimensional space <img src="https://s0.wp.com/latex.php?latex=L%5E1%28%5COmega%29%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="L^1(&#92;Omega)," class="latex" /> and we have an <i>n</i>-tuple of constraints  with an <i>n</i>-tuple of Lagrange multipliers.   But the same principle holds.</p>
<p>So, when we are at a solution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> of our constrained entropy-maximization problem, and we start moving the point <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> by changing the value of the <i>i</i>th constraint, namely <img src="https://s0.wp.com/latex.php?latex=q%5Ei%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i," class="latex" /> the rate at which the entropy changes will be <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> times the rate of change of <img src="https://s0.wp.com/latex.php?latex=q%5Ei.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^i." class="latex" />    So, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%3D+%5Cbeta_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i} = &#92;beta_i }" class="latex" /></p>
<p>But this is just what we needed to show!</p>
<h3> The hard way</h3>
<p>Here&#8217;s another way to show</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%3D+%5Cbeta_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i} = &#92;beta_i }" class="latex" /></p>
<p>We start by solving our constrained entropy-maximization problem using Lagrange multipliers.  As already shown, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi_q%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7B%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi_q(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{&#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu} } " class="latex" /></p>
<p>Then we&#8217;ll compute the entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28q%29+%3D+-+%5Cint+%5Cpi_q+%5Cln+%5Cpi_q+%5C%2C+d%5Cmu+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q) = - &#92;int &#92;pi_q &#92;ln &#92;pi_q &#92;, d&#92;mu " class="latex" /></p>
<p>Then we&#8217;ll differentiate this with respect to <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> and show we get <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i." class="latex" /></p>
<p>Let&#8217;s try it!   The calculation is a bit heavy, so let&#8217;s write <img src="https://s0.wp.com/latex.php?latex=Z%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(q)" class="latex" /> for the so-called <b>partition function</b></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Z%28q%29+%3D+%5Cint+e%5E%7B-+%5Cbeta_i+A%5Ei%7D+%5C%2C+d%5Cmu+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Z(q) = &#92;int e^{- &#92;beta_i A^i} &#92;, d&#92;mu } " class="latex" /></p>
<p>so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cpi_q%28x%29+%3D+%5Cfrac%7Be%5E%7B-+%5Cbeta_i+A%5Ei%28x%29%7D%7D%7BZ%28q%29%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;pi_q(x) = &#92;frac{e^{- &#92;beta_i A^i(x)}}{Z(q)} } " class="latex" /></p>
<p>and the entropy is</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++f%28q%29+%26%3D%26+-+%5Cdisplaystyle%7B+%5Cint++%5Cpi_q++%5Cln+%5Cleft%28+%5Cfrac%7Be%5E%7B-+%5Cbeta_k+A%5Ek%7D%7D%7BZ%28q%29%7D+%5Cright%29++%5C%2C+d%5Cmu+%7D++%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cint+%5Cpi_q+%5Cleft%28%5Cbeta_k+A%5Ek+%2B+%5Cln+Z%28q%29+%5Cright%29++%5C%2C+d%5Cmu+%7D+%5C%5C+%5C%5C++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  f(q) &amp;=&amp; - &#92;displaystyle{ &#92;int  &#92;pi_q  &#92;ln &#92;left( &#92;frac{e^{- &#92;beta_k A^k}}{Z(q)} &#92;right)  &#92;, d&#92;mu }  &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;int &#92;pi_q &#92;left(&#92;beta_k A^k + &#92;ln Z(q) &#92;right)  &#92;, d&#92;mu } &#92;&#92; &#92;&#92;  &#92;end{array}  " class="latex" /></p>
<p>This is the sum of two terms.  The first term</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint+%5Cpi_q+%5Cbeta_k+A%5Ek+++%5C%2C+d%5Cmu+%3D++%5Cbeta_k+%5Cint+%5Cpi_q+A%5Ek+++%5C%2C+d%5Cmu%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int &#92;pi_q &#92;beta_k A^k   &#92;, d&#92;mu =  &#92;beta_k &#92;int &#92;pi_q A^k   &#92;, d&#92;mu} " class="latex" /></p>
<p>is <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_k" class="latex" /> times the expected value of <img src="https://s0.wp.com/latex.php?latex=A%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^k" class="latex" /> with respect to the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> all summed over <img src="https://s0.wp.com/latex.php?latex=k.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="k." class="latex" />    But the expected value of <img src="https://s0.wp.com/latex.php?latex=A%5Ek&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A^k" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=q%5Ek%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q^k," class="latex" /> so we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint++%5Cpi_q+%5Cbeta_k+A%5Ek+%5C%2C+d%5Cmu+%7D+%3D++%5Cbeta_k+q%5Ek+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int  &#92;pi_q &#92;beta_k A^k &#92;, d&#92;mu } =  &#92;beta_k q^k " class="latex" /></p>
<p>The second term is easier:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega++%5Cpi_q+%5Cln+Z%28q%29+%5C%2C+d%5Cmu+%3D+%5Cln+Z%28q%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega  &#92;pi_q &#92;ln Z(q) &#92;, d&#92;mu = &#92;ln Z(q) }" class="latex" /></p>
<p>since <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%28x%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q(x)" class="latex" /> integrates to 1 and the partition function <img src="https://s0.wp.com/latex.php?latex=Z%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(q)" class="latex" /> doesn&#8217;t depend on <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>Putting together these two terms we get an interesting formula for the entropy:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28q%29+%3D+%5Cbeta_k+q%5Ek+%2B+%5Cln+Z%28q%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q) = &#92;beta_k q^k + &#92;ln Z(q) " class="latex" /></p>
<p>This formula is one reason this brute-force approach is actually worthwhile!  I&#8217;ll say more about it later.</p>
<p>But for now, let&#8217;s use this formula to show what we&#8217;re trying to show, namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%3D+%5Cbeta_i+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i} = &#92;beta_i }" class="latex" /></p>
<p>For starters,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D%7D+%26%3D%26+%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cleft%28%5Cbeta_k+q%5Ek+%2B+%5Cln+Z%28q%29+%5Cright%29+%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_k+++%5Cfrac%7B%5Cpartial+q%5Ek%7D%7B%5Cpartial+q%5Ei%7D+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29+++%7D++%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_k++%5Cdelta%5Ek_i+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29+++%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_i+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29++%7D++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i}} &amp;=&amp; &#92;displaystyle{&#92;frac{&#92;partial}{&#92;partial q^i} &#92;left(&#92;beta_k q^k + &#92;ln Z(q) &#92;right) } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_k   &#92;frac{&#92;partial q^k}{&#92;partial q^i} + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)   }  &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_k  &#92;delta^k_i + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)   } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_i + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)  }  &#92;end{array}  " class="latex" /></p>
<p>where we played a little Kronecker delta game with the second term.</p>
<p>Now we just need to compute the third term:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29+%7D+%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+Z%28q%29+%7D+%5C%5C++%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D++%5Cint+e%5E%7B-+%5Cbeta_j+A%5Ej%7D++%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cleft%28e%5E%7B-+%5Cbeta_j+A%5Ej%7D%5Cright%29+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D%5Cleft%28+-+%5Cbeta_k+A%5Ek+%5Cright%29++e%5E%7B-+%5Cbeta_j+A%5Ej%7D+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+-%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D++A%5Ek+++e%5E%7B-+%5Cbeta_j+A%5Ej%7D+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+-+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cint++A%5Ek+++e%5E%7B-+%5Cbeta_j+A%5Ej%7D+%5C%2C+d%5Cmu++%7D++%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+-+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%7D++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{ &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q) } &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;frac{&#92;partial}{&#92;partial q^i} Z(q) } &#92;&#92;  &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;frac{&#92;partial}{&#92;partial q^i}  &#92;int e^{- &#92;beta_j A^j}  &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;int &#92;frac{&#92;partial}{&#92;partial q^i} &#92;left(e^{- &#92;beta_j A^j}&#92;right) &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{1}{Z(q)} &#92;int &#92;frac{&#92;partial}{&#92;partial q^i}&#92;left( - &#92;beta_k A^k &#92;right)  e^{- &#92;beta_j A^j} &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ -&#92;frac{1}{Z(q)} &#92;int &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i}  A^k   e^{- &#92;beta_j A^j} &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ - &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} &#92;frac{1}{Z(q)} &#92;int  A^k   e^{- &#92;beta_j A^j} &#92;, d&#92;mu  }  &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ - &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k }  &#92;end{array}  " class="latex" /></p>
<p>Ah, you don&#8217;t know how good it feels, after years of category theory, to be doing calculations like this again!</p>
<p>Now we can finish the job we started:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Bccl%7D++%5Cdisplaystyle%7B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D%7D+%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_i+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q%5Ei%7D+%5Cln+Z%28q%29++%7D+%5C%5C+%5C%5C++%26%3D%26++%5Cdisplaystyle%7B+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek+%2B+%5Cbeta_i+-+%5Cfrac%7B%5Cpartial+%5Cbeta_k%7D%7B%5Cpartial+q%5Ei%7D+q%5Ek++%7D+%5C%5C+%5C%5C++%26%3D%26+%5Cbeta_i++%5Cend%7Barray%7D++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;begin{array}{ccl}  &#92;displaystyle{&#92;frac{&#92;partial f}{&#92;partial q^i}} &amp;=&amp;  &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_i + &#92;frac{&#92;partial}{&#92;partial q^i} &#92;ln Z(q)  } &#92;&#92; &#92;&#92;  &amp;=&amp;  &#92;displaystyle{ &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k + &#92;beta_i - &#92;frac{&#92;partial &#92;beta_k}{&#92;partial q^i} q^k  } &#92;&#92; &#92;&#92;  &amp;=&amp; &#92;beta_i  &#92;end{array}  " class="latex" /></p>
<p>Voilà!</p>
<h3> Conclusions</h3>
<p>We&#8217;ve learned the formula for the probability distribution that maximizes entropy subject to some constraints on the expected values of observables.  But more importantly, we&#8217;ve seen that the anonymous Lagrange multipliers <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> that show up in this problem are actually the partial derivatives of entropy!  They equal</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%5Ei%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q^i} } " class="latex" /></p>
<p>Thus, they are rich in meaning.   From what we&#8217;ve seen earlier, they are &#8216;surprisals&#8217;.   They are <i>analogous to momentum</i> in classical mechanics and have the meaning of <i>intensive variables</i> in thermodynamics:</p>
<div align="center">
<table border="1">
<tbody>
<tr>
<td></td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;<i>q</i> &nbsp;</td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">probabilities</td>
</tr>
<tr>
<td>&nbsp;<i>p</i>&nbsp;</td>
<td style="text-align:center;"> momentum</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">surprisals</td>
</tr>
<tr>
<td>&nbsp;<i>S</i> &nbsp;</td>
<td style="text-align:center;"> action</td>
<td style="text-align:center;">entropy</td>
<td style="text-align:center;"> Shannon entropy</td>
</tr>
</tbody>
</table>
</div>
<p>Furthermore, by showing <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i+%3D+p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i = p_i" class="latex" /> the hard way we discovered an interesting fact.  There&#8217;s a relation between the entropy and the logarithm of the partition function:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28q%29+%3D+p_i+q%5Ei+%2B+%5Cln+Z%28q%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f(q) = p_i q^i + &#92;ln Z(q) " class="latex" /></p>
<p>(We proved this formula with <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;beta_i" class="latex" /> replacing <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> but now we know those are equal.)</p>
<p>This formula suggests that the logarithm of the partition function is important&#8212;and it is!  It&#8217;s closely related to the concept of <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)#Relation_to_thermodynamic_variables">free energy</a>&#8212;even though &#8216;energy&#8217;, free or otherwise, doesn&#8217;t show up at the level of generality we&#8217;re working at now.</p>
<p>This formula should also remind you of the tautological 1-form on the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q," class="latex" /> namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%3D+p_i+dq%5Ei&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;theta = p_i dq^i" class="latex" /></p>
<p>It should remind you even more of the contact 1-form on the contact manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}," class="latex" /> namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+-dS+%2B+p_i+dq%5Ei+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;alpha = -dS + p_i dq^i " class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S" class="latex" /> is a coordinate on the contact manifold that&#8217;s a kind of abstract stand-in for our entropy function <img src="https://s0.wp.com/latex.php?latex=f.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f." class="latex" /></p>
<p>So, it&#8217;s clear there&#8217;s a lot more to say: we&#8217;re seeing hints of things here and there, but not yet the full picture.</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p>• <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/#comments">11 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;21)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
			<div class="post-31552 post type-post status-publish format-standard hentry category-information-and-entropy category-mathematics category-physics category-probability" id="post-31552">
				<h2><a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/" rel="bookmark">Information Geometry (Part&nbsp;20)</a></h2>
				<small>14 August, 2021</small><br />


				<div class="entry">
					<p><a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Last time</a> we worked out an analogy between classical mechanics, thermodynamics and probability theory.  The latter two look suspiciously similar:</p>
<div align="center">
<table border="1">
<tbody>
<tr>
<td></td>
<td style="text-align:center;">&nbsp;<b>Classical Mechanics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Thermodynamics</b>&nbsp;</td>
<td style="text-align:center;">&nbsp;<b>Probability Theory</b>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;<i>q</i> &nbsp;</td>
<td style="text-align:center;">position</td>
<td style="text-align:center;">extensive variables</td>
<td style="text-align:center;">probabilities</td>
</tr>
<tr>
<td>&nbsp;<i>p</i>&nbsp;</td>
<td style="text-align:center;"> momentum</td>
<td style="text-align:center;">intensive variables</td>
<td style="text-align:center;">surprisals</td>
</tr>
<tr>
<td>&nbsp;<i>S</i> &nbsp;</td>
<td style="text-align:center;"> action</td>
<td style="text-align:center;">entropy</td>
<td style="text-align:center;"> Shannon entropy</td>
</tr>
</tbody>
</table>
</div>
<p>This is no coincidence.  After all, in the subject of statistical mechanics we <i>explain</i> classical thermodynamics using probability theory&#8212;and entropy is revealed to be Shannon entropy (or its quantum analogue).</p>
<p>Now I want to make this precise.</p>
<p>To connect classical thermodynamics to probability theory, I&#8217;ll start by discussing &#8216;statistical manifolds&#8217;.  I introduced the idea of a statistical manifold in <a href="https://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>: it&#8217;s a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> equipped with a map sending each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> to a probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> on some measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega." class="latex" />   Now I&#8217;ll say how these fit into the second column of the above chart.</p>
<p>Then I&#8217;ll talk about statistical manifolds of a special sort used in thermodynamics, which I&#8217;ll call &#8216;Gibbsian&#8217;, since they really go back to Josiah Willard Gibbs.</p>
<div align="center"><a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs"><img src="https://johncarlosbaez.files.wordpress.com/2021/08/josiah_willard_gibbs_-from_mms-.jpg?w=200" alt="" width="200" /></a></div>
<p>In a Gibbsian statistical manifold, for each <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a &#8216;Gibbs distribution&#8217;.   Physically, these Gibbs distributions describe <a href="https://en.wikipedia.org/wiki/Thermodynamic_equilibrium">thermodynamic equilibria</a>.  For example, if you specify the volume, energy and number of particles in a box of gas, there will be a Gibbs distribution describing what the particles do in thermodynamic equilibrium under these conditions.   Mathematically, Gibbs distributions <i>maximize entropy</i> subject to some constraints specified by the point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q." class="latex" /></p>
<p>More precisely: in a Gibbsian statistical manifold we have a list of observables <img src="https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots+%2C+A_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_1, &#92;dots , A_n" class="latex" /> whose expected values serve as coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n" class="latex" /> for points <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the probability distribution that maximizes entropy subject to the constraint that the expected value of <img src="https://s0.wp.com/latex.php?latex=A_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_i" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   We can derive most of the interesting formulas of thermodynamics starting from this!</p>
<h3> Statistical manifolds</h3>
<p>Let&#8217;s fix a measure space <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;mu." class="latex" />  A <b>statistical manifold</b> is then a manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> equipped with a smooth map <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> assigning to each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> a probability distribution on <img src="https://s0.wp.com/latex.php?latex=%5COmega%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega," class="latex" /> which I&#8217;ll call <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" />   So, <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a function on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cpi_q+%5C%2C+d%5Cmu+%3D+1+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;pi_q &#92;, d&#92;mu = 1 }" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%28x%29+%5Cge+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q(x) &#92;ge 0" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>The idea here is that the space of <i>all</i> probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Omega" class="latex" /> may be too huge to understand in as much detail as we&#8217;d like, so instead we describe <i>some</i> of these probability distributions&#8212;a family parametrized by points of some manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" />&#8212;using the map <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" />   This is the basic idea behind <a href="https://en.wikipedia.org/wiki/Parametric_statistics">parametric statistics</a>.</p>
<p>Information geometry is the geometry of statistical manifolds.  Any statistical manifold comes with a bunch of interesting geometrical structures.   One is the &#8216;Fisher information metric&#8217;, a Riemannian metric I explained in <a href="https://math.ucr.edu/home/baez/information/information_geometry_7.html">Part 7</a>.  Another is a 1-parameter family of connections on the tangent bundle <img src="https://s0.wp.com/latex.php?latex=T+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T Q," class="latex" /> which is important in Amari&#8217;s approach to information geometry.   You can read about this here:</p>
<p>• Hiroshi Matsuzoe, <a href="https://projecteuclid.org/download/pdf_1/euclid.aspm/1543086326">Statistical manifolds and affine differential geometry</a>, in <i>Advanced Studies in Pure Mathematics 57</i>, pp. 303–321.</p>
<p>I don&#8217;t want to talk about it now&#8212;I just wanted to reassure you that I&#8217;m not completely ignorant of it!</p>
<p>I want to focus on the story I&#8217;ve been telling, which is about <i>entropy</i>.   Our statistical manifold <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> comes with a smooth <b>entropy</b> function</p>
<p><img src="https://s0.wp.com/latex.php?latex=f+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f &#92;colon Q &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>namely</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++f%28q%29+%3D+-%5Cint_%5COmega+%5Cpi_q%28x%29+%5C%2C+%5Cln+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29++++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  f(q) = -&#92;int_&#92;Omega &#92;pi_q(x) &#92;, &#92;ln &#92;pi_q(x) &#92;, d&#92;mu(x)    } " class="latex" /></p>
<p>We can use this entropy function to do many of the things we usually do in thermodynamics!   For example, at any point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> where this function is differentiable, its differential gives a cotangent vector</p>
<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%28df%29_q+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p = (df)_q " class="latex" /></p>
<p>which has an important physical meaning.   In coordinates we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>and we call <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> the <b>intensive variable conjugate to</b> <img src="https://s0.wp.com/latex.php?latex=q_i.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i." class="latex" />   For example if <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> is energy, <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> will be &#8216;coolness&#8217;: the reciprocal of temperature.</p>
<p>Defining <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> this way gives a Lagrangian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5C%7B+%28q%2Cp%29+%5Cin+T%5E%5Cast+Q+%3A+%5C%3B+x+%5Cin+M%2C+%5C%3B+p+%3D++%28df%29_x+%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;{ (q,p) &#92;in T^&#92;ast Q : &#92;; x &#92;in M, &#92;; p =  (df)_x &#92;} " class="latex" /></p>
<p>of the cotangent bundle <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q." class="latex" />   We can also get contact geometry into the game by defining a contact manifold <img src="https://s0.wp.com/latex.php?latex=T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="T^&#92;ast Q &#92;times &#92;mathbb{R}" class="latex" /> and a Legendrian submanifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CSigma+%3D+%5C%7B+%28q%2Cp%2CS%29+%5Cin+T%5E%5Cast+Q+%5Ctimes+%5Cmathbb%7BR%7D+%3A+%5C%3B+x+%5Cin+M%2C+%5C%3B+p+%3D++%28df%29_q+%2C+S+%3D+f%28q%29+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;Sigma = &#92;{ (q,p,S) &#92;in T^&#92;ast Q &#92;times &#92;mathbb{R} : &#92;; x &#92;in M, &#92;; p =  (df)_q , S = f(q) &#92;}" class="latex" /></p>
<p>But I&#8217;ve been talking about these ideas for the last three episodes, so I won&#8217;t say more just now!   Instead, I want to throw a new idea into the pot.</p>
<h3> Gibbsian statistical manifolds</h3>
<p>Thermodynamics, and statistical mechanics, spend a lot of time dealing with statistical manifold of a special sort I&#8217;ll call &#8216;Gibbsian&#8217;.  In these, each probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a &#8216;Gibbs distribution&#8217;, meaning that it maximizes entropy subject to certain constraints specified by the point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q." class="latex" /></p>
<p>How does this work?  For starters, an integrable function</p>
<p><img src="https://s0.wp.com/latex.php?latex=A+%5Ccolon+%5COmega+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A &#92;colon &#92;Omega &#92;to &#92;mathbb{R}" class="latex" /></p>
<p>is called a <b>random variable</b>, or in physics an <b>observable</b>.  The <b>expected value</b> of an observable is a smooth real-valued function on our statistical manifold</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle+%5Ccolon+Q+%5Cto+%5Cmathbb%7BR%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle &#92;colon Q &#92;to &#92;mathbb{R} " class="latex" /></p>
<p>given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Clangle+A+%5Crangle%28q%29+%3D+%5Cint_%5COmega+A%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;langle A &#92;rangle(q) = &#92;int_&#92;Omega A(x) &#92;pi_q(x) &#92;, d&#92;mu(x) } " class="latex" /></p>
<p>In other words, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A &#92;rangle" class="latex" /> is a function whose value at at any point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q" class="latex" /> is the expected value of <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A" class="latex" /> with respect to the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>Now, suppose our statistical manifold is <i>n</i>-dimensional and we have <i>n</i> observables <img src="https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots%2C+A_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="A_1, &#92;dots, A_n." class="latex" />   Their expected values will be smooth functions on our manifold&#8212;and sometimes these functions will be a coordinate system!</p>
<p>This may sound rather unlikely, but it&#8217;s really not so outlandish.  Indeed, if there&#8217;s a point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> such that the differentials of the functions <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle" class="latex" /> are linearly independent at this point, these functions will be a coordinate system in some neighborhood of this point, by the <a href="https://en.wikipedia.org/wiki/Inverse_function_theorem">inverse function theorem</a>.  So, we can take this neighborhood, use <i>it</i> as our statistical manifold, and the functions <img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle" class="latex" /> will be coordinates.</p>
<p>So, let&#8217;s assume the expected values of our observables give a coordinate system on <img src="https://s0.wp.com/latex.php?latex=Q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q." class="latex" />   Let&#8217;s call these coordinates <img src="https://s0.wp.com/latex.php?latex=q_1%2C+%5Cdots%2C+q_n%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1, &#92;dots, q_n," class="latex" /> so that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+A_i+%5Crangle%28q%29+%3D+q_i++&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;langle A_i &#92;rangle(q) = q_i  " class="latex" /></p>
<p>Now for the kicker: we say our statistical manifold is <b>Gibbsian</b> if for each point <img src="https://s0.wp.com/latex.php?latex=q+%5Cin+Q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q &#92;in Q," class="latex" /> <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is the probability distribution that <i>maximizes entropy subject to the above condition!</i></p>
<p>Which condition?   The condition saying that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+A_i%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega A_i(x) &#92;pi_q(x) &#92;, d&#92;mu(x) = q_i } " class="latex" /></p>
<p>for all <i>i</i>.   This is just the previous equation spelled out so that you can see it&#8217;s a condition on <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>This assumption of the entropy-maximizing nature of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is a very powerful, because it implies a useful and nontrivial formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" />   It&#8217;s called the <b><a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs distribution</a></b>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B++%5Cpi_q%28x%29+%3D+%5Cfrac%7B1%7D%7BZ%28q%29%7D+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29%5Cright%29+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{  &#92;pi_q(x) = &#92;frac{1}{Z(q)} &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x)&#92;right) }" class="latex" /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5COmega.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;in &#92;Omega." class="latex" /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> is the intensive variable conjugate to <img src="https://s0.wp.com/latex.php?latex=q_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i," class="latex" /> while <img src="https://s0.wp.com/latex.php?latex=Z%28q%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Z(q)" class="latex" /> is the <b>partition function</b>: the thing we must divide by to make sure <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> integrates to 1.  In other words:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+Z%28q%29+%3D+%5Cint_%5COmega+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29+%5Cright%29+%5C%2C+d%5Cmu%28x%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ Z(q) = &#92;int_&#92;Omega &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x) &#92;right) &#92;, d&#92;mu(x)   } " class="latex" /></p>
<p>By the way, this formula may look confusing at first, since the left side depends on the point <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> in our statistical manifold, while there&#8217;s no <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q" class="latex" /> visible in the right side!  Do you see what&#8217;s going on?</p>
<p>I&#8217;ll tell you: the conjugate variable <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> sitting on the right side of the above formula, depends on <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" />   Remember, we got it by taking the partial derivative of the entropy in the <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" /> direction</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+p_i+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_i%7D+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ p_i = &#92;frac{&#92;partial f}{&#92;partial q_i} } " class="latex" /></p>
<p>and the evaluating this derivative at the point <img src="https://s0.wp.com/latex.php?latex=q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q." class="latex" /></p>
<p>But wait a minute!  <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> here is the entropy&#8212;but the entropy of <i>what?</i></p>
<p>The entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> of course!</p>
<p>So there&#8217;s something circular about our formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /> To know <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q," class="latex" /> you need to know the conjugate variables <img src="https://s0.wp.com/latex.php?latex=p_i%2C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i," class="latex" /> but to compute these you need to know the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q." class="latex" /></p>
<p>This is actually okay.  While circular, the formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is still <i>true</i>.  It&#8217;s harder to work with than you might hope.  But it&#8217;s still extremely useful.</p>
<p>Next time I&#8217;ll prove that this formula for <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> is true, and do a few things with it.   All this material was discovered by Gibbs in the late 1800&#8217;s, and it&#8217;s lurking any good book on statistical mechanics&#8212;but not phrased in the language of statistical manifolds.  The physics textbooks usually consider special cases, like a box of gas where:</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_1" class="latex" /> is energy, <img src="https://s0.wp.com/latex.php?latex=p_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1" class="latex" /> is 1/temperature.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_2" class="latex" /> is volume, <img src="https://s0.wp.com/latex.php?latex=p_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_2" class="latex" /> is &ndash;pressure/temperature.</p>
<p>&bull; <img src="https://s0.wp.com/latex.php?latex=q_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_3" class="latex" /> is the number of particles, <img src="https://s0.wp.com/latex.php?latex=p_3&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_3" class="latex" /> is chemical potential / pressure.</p>
<p>While these special cases are important and interesting, I&#8217;d rather be general!</p>
<h3> Technical comments</h3>
<p>I said &#8220;Any statistical manifold comes with a bunch of interesting geometrical structures&#8221;, but in fact some conditions are required.  For example, the Fisher information metric is only well-defined and nondegenerate under some conditions on the map <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" />   For example, if <img src="https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi" class="latex" /> maps every point of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="Q" class="latex" /> to the same probability distribution, the Fisher information metric will <i>vanish</i>.</p>
<p>Similarly, the entropy function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="f" class="latex" /> is only smooth under some conditions on <img src="https://s0.wp.com/latex.php?latex=%5Cpi.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi." class="latex" /></p>
<p>Furthermore, the integral</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+%5Cexp%5Cleft%28-%5Csum_%7Bi+%3D+1%7D%5En+p_i+A_i%28x%29+%5Cright%29+%5C%2C+d%5Cmu%28x%29+++%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega &#92;exp&#92;left(-&#92;sum_{i = 1}^n p_i A_i(x) &#92;right) &#92;, d&#92;mu(x)   } " class="latex" /></p>
<p>may not converge for all values of the numbers <img src="https://s0.wp.com/latex.php?latex=p_1%2C+%5Cdots%2C+p_n.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, &#92;dots, p_n." class="latex" />   But in my discussion of Gibbsian statistical manifolds, I was assuming that an entropy-maximizing probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;pi_q" class="latex" /> with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+%5Cint_%5COmega+A_i%28x%29+%5Cpi_q%28x%29+%5C%2C+d%5Cmu%28x%29+%3D+q_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ &#92;int_&#92;Omega A_i(x) &#92;pi_q(x) &#92;, d&#92;mu(x) = q_i } " class="latex" /></p>
<p>actually <i>exists</i>.   In this case the probability distribution is also unique (almost everywhere).</p>
<hr />
<p>For all my old posts on information geometry, go here:</p>
<p>• <a href="https://math.ucr.edu/home/baez/information/">Information geometry</a>.</p>
									</div>

				<p class="postmetadata">
				<img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/speech_bubble.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/#comments">16 Comments</a>				| <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/documents.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/" rel="category tag">information and entropy</a>, <a href="https://johncarlosbaez.wordpress.com/category/mathematics/" rel="category tag">mathematics</a>, <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>, <a href="https://johncarlosbaez.wordpress.com/category/probability/" rel="category tag">probability</a>								 | <img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/permalink.gif" alt="" /> <a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/" rel="bookmark" title="Permanent Link to Information Geometry (Part&nbsp;20)">Permalink</a>
<br /><img src="https://s2.wp.com/wp-content/themes/pub/contempt/images/blog/figure_ver1.gif" alt="" /> Posted by John Baez				</p>
			</div>
			<hr />
		
		<div class="navigation">
			<div class="alignleft"><a href="https://johncarlosbaez.wordpress.com/page/2/?s=information+and+entropy" >&laquo; Previous Entries</a></div>
			<div class="alignright"></div>
		</div>

	
	</div>

	<div id="sidebar">
				<ul>

		 <li>

				<p>You have searched the <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> blog archives for <strong>&#8216;information and entropy&#8217;</strong>. If you are unable to find anything in these search results, you can try one of these links.</p>

					</li> 
		<li id="recent-posts-3" class="widget widget_recent_entries">
		<h2 class="widgettitle">latest posts:</h2>

		<ul>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/">Classical Mechanics versus Thermodynamics (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/">Maxwell&#8217;s Relations (Part&nbsp;3)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/">Maxwell&#8217;s Relations (Part&nbsp;2)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/">Maxwell&#8217;s Relations (Part&nbsp;1)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/09/13/the-cyclic-identity-for-partial-derivatives/">The Cyclic Identity for Partial&nbsp;Derivatives</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/17/information-geometry-part-21/">Information Geometry (Part&nbsp;21)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/14/information-geometry-part-20/">Information Geometry (Part&nbsp;20)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/08/information-geometry-part-19/">Information Geometry (Part&nbsp;19)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/08/05/information-geometry-part-18/">Information Geometry (Part&nbsp;18)</a>
									</li>
											<li>
					<a href="https://johncarlosbaez.wordpress.com/2021/07/29/structured-vs-decorated-cospans-part-2/">Structured vs Decorated Cospans (Part&nbsp;2)</a>
									</li>
					</ul>

		</li>
<li id="recent-comments-2" class="widget widget_recent_comments"><h2 class="widgettitle">latest comments:</h2>
				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="amarashiki" class="recentcommentsavatartop" style="height:32px; width:32px;"><a href="http://gravatar.com/amarashiki" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/62b2df0762257e75433ad6f161488c3a?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstexttop" style=""><a href="http://gravatar.com/amarashiki" rel="nofollow">amarashiki</a> on <a href="https://johncarlosbaez.wordpress.com/2012/01/23/classical-mechanics-versus-thermodynamics-part-2/#comment-172566">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172560">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172559">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="domenico" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://2.gravatar.com/avatar/ba06491deb8346d20356ac2ae05893ee?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">domenico on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172558">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/22/maxwells-relations-part-3/#comment-172557">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/18/maxwells-relations-part-two/#comment-172556">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Maxwell Relations | Equivalent eXchange" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://equivalentexchange.blog/2021/09/25/maxwell-relations/" rel="nofollow">Maxwell Relations |&hellip;</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/17/maxwells-relations-part-1/#comment-172555">Maxwell&#8217;s Relations (Par&hellip;</a></td></tr><tr><td title="Robert A. Wilson" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="https://robwilson1.wordpress.com" rel="nofollow"><img alt='' src='https://2.gravatar.com/avatar/24dc7e2371491f36fd4538b3474920b5?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="https://robwilson1.wordpress.com" rel="nofollow">Robert A. Wilson</a> on <a href="https://johncarlosbaez.wordpress.com/2021/04/04/the-koide-formula/#comment-172553">The Koide Formula</a></td></tr><tr><td title="John Baez" class="recentcommentsavatarend" style="height:32px; width:32px;"><a href="http://math.ucr.edu/home/baez/" rel="nofollow"><img alt='' src='https://0.gravatar.com/avatar/34784534843022b3541c8ddd693718cb?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></a></td><td class="recentcommentstextend" style=""><a href="http://math.ucr.edu/home/baez/" rel="nofollow">John Baez</a> on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172545">Classical Mechanics versus The&hellip;</a></td></tr><tr><td title="Wolfgang" class="recentcommentsavatarend" style="height:32px; width:32px;"><img alt='' src='https://1.gravatar.com/avatar/d3c6d7ec8069e25c08a0a11263581925?s=32&#038;d=identicon&#038;r=G' class='avatar avatar-32' height='32' width='32' /></td><td class="recentcommentstextend" style="">Wolfgang on <a href="https://johncarlosbaez.wordpress.com/2021/09/23/classical-mechanics-versus-thermodynamics-part-3/#comment-172543">Classical Mechanics versus The&hellip;</a></td></tr>				</table>
				</li>
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">How To Write Math Here:</h2>
			<div class="textwidget"><p>You can <a href="http://en.wikibooks.org/wiki/LaTeX/Mathematics">include math in your comments using LaTeX</a>,  but you need to do it this way:</p>
<p>&#036;latex  E = mc^2&#036;</p>
<p>You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some described <a href="http://en.support.wordpress.com/latex/">here</a>.  You can't preview comments here, but I'm happy to fix errors.</p>
</div>
		</li>
<li id="categories-2" class="widget widget_categories"><h2 class="widgettitle">Read Posts On:</h2>

			<ul>
					<li class="cat-item cat-item-177"><a href="https://johncarlosbaez.wordpress.com/category/art/">art</a> (3)
</li>
	<li class="cat-item cat-item-4003"><a href="https://johncarlosbaez.wordpress.com/category/astronomy/">astronomy</a> (29)
</li>
	<li class="cat-item cat-item-8262191"><a href="https://johncarlosbaez.wordpress.com/category/azimuth/">azimuth</a> (60)
</li>
	<li class="cat-item cat-item-86856"><a href="https://johncarlosbaez.wordpress.com/category/biodiversity/">biodiversity</a> (38)
</li>
	<li class="cat-item cat-item-4936"><a href="https://johncarlosbaez.wordpress.com/category/biology/">biology</a> (107)
</li>
	<li class="cat-item cat-item-355244"><a href="https://johncarlosbaez.wordpress.com/category/carbon-emissions/">carbon emissions</a> (78)
</li>
	<li class="cat-item cat-item-5936"><a href="https://johncarlosbaez.wordpress.com/category/chemistry/">chemistry</a> (74)
</li>
	<li class="cat-item cat-item-6108"><a href="https://johncarlosbaez.wordpress.com/category/climate/">climate</a> (155)
</li>
	<li class="cat-item cat-item-5043"><a href="https://johncarlosbaez.wordpress.com/category/computer-science/">computer science</a> (57)
</li>
	<li class="cat-item cat-item-9204"><a href="https://johncarlosbaez.wordpress.com/category/conferences/">conferences</a> (81)
</li>
	<li class="cat-item cat-item-1098"><a href="https://johncarlosbaez.wordpress.com/category/culture/">culture</a> (4)
</li>
	<li class="cat-item cat-item-657"><a href="https://johncarlosbaez.wordpress.com/category/economics/">economics</a> (32)
</li>
	<li class="cat-item cat-item-1212"><a href="https://johncarlosbaez.wordpress.com/category/energy/">energy</a> (50)
</li>
	<li class="cat-item cat-item-25393"><a href="https://johncarlosbaez.wordpress.com/category/engineering/">engineering</a> (11)
</li>
	<li class="cat-item cat-item-14852"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/game-theory/">game theory</a> (29)
</li>
	<li class="cat-item cat-item-1215"><a href="https://johncarlosbaez.wordpress.com/category/geography/">geography</a> (4)
</li>
	<li class="cat-item cat-item-337"><a href="https://johncarlosbaez.wordpress.com/category/health/">health</a> (4)
</li>
	<li class="cat-item cat-item-678"><a href="https://johncarlosbaez.wordpress.com/category/history/">history</a> (2)
</li>
	<li class="cat-item cat-item-23375499"><a href="https://johncarlosbaez.wordpress.com/category/information-and-entropy/">information and entropy</a> (92)
</li>
	<li class="cat-item cat-item-3558"><a href="https://johncarlosbaez.wordpress.com/category/jobs/">jobs</a> (11)
</li>
	<li class="cat-item cat-item-5465"><a href="https://johncarlosbaez.wordpress.com/category/journals/">journals</a> (5)
</li>
	<li class="cat-item cat-item-3582"><a href="https://johncarlosbaez.wordpress.com/category/mathematics/">mathematics</a> (478)
</li>
	<li class="cat-item cat-item-18"><a href="https://johncarlosbaez.wordpress.com/category/music/">music</a> (3)
</li>
	<li class="cat-item cat-item-3968"><a href="https://johncarlosbaez.wordpress.com/category/networks/">networks</a> (185)
</li>
	<li class="cat-item cat-item-154934"><a href="https://johncarlosbaez.wordpress.com/category/oceans/">oceans</a> (13)
</li>
	<li class="cat-item cat-item-1211"><a href="https://johncarlosbaez.wordpress.com/category/physics/">physics</a> (204)
</li>
	<li class="cat-item cat-item-10451"><a href="https://johncarlosbaez.wordpress.com/category/probability/">probability</a> (92)
</li>
	<li class="cat-item cat-item-4909"><a href="https://johncarlosbaez.wordpress.com/category/psychology/">psychology</a> (6)
</li>
	<li class="cat-item cat-item-3330"><a href="https://johncarlosbaez.wordpress.com/category/publishing/">publishing</a> (19)
</li>
	<li class="cat-item cat-item-46615"><a href="https://johncarlosbaez.wordpress.com/category/puzzles/">puzzles</a> (14)
</li>
	<li class="cat-item cat-item-4140243"><a href="https://johncarlosbaez.wordpress.com/category/quantum-technologies/">quantum technologies</a> (28)
</li>
	<li class="cat-item cat-item-562"><a href="https://johncarlosbaez.wordpress.com/category/questions/">questions</a> (3)
</li>
	<li class="cat-item cat-item-93974"><a href="https://johncarlosbaez.wordpress.com/category/risks/">risks</a> (48)
</li>
	<li class="cat-item cat-item-37893"><a href="https://johncarlosbaez.wordpress.com/category/seminars/">seminars</a> (21)
</li>
	<li class="cat-item cat-item-581"><a href="https://johncarlosbaez.wordpress.com/category/software/">software</a> (19)
</li>
	<li class="cat-item cat-item-39438"><a href="https://johncarlosbaez.wordpress.com/category/strategies/">strategies</a> (36)
</li>
	<li class="cat-item cat-item-6877"><a href="https://johncarlosbaez.wordpress.com/category/sustainability/">sustainability</a> (71)
</li>
	<li class="cat-item cat-item-66608272"><a href="https://johncarlosbaez.wordpress.com/category/the-practice-of-science/">the practice of science</a> (27)
</li>
	<li class="cat-item cat-item-61590"><a href="https://johncarlosbaez.wordpress.com/category/this-weeks-finds/">this week&#039;s finds</a> (18)
</li>
			</ul>

			</li>
<li id="linkcat-20924250" class="widget widget_links"><h2 class="widgettitle">also visit these:</h2>

	<ul class='xoxo blogroll'>
<li><a href="http://www.azimuthproject.org/azimuth/show/Azimuth+Blog" title="Go here to see what’s on this blog, organized by topic or author!">Azimuth Blog Overview</a></li>
<li><a href="http://www.azimuthproject.org/azimuth/show/HomePage" title="for scientists and engineers who want to save the planet">Azimuth Project</a></li>
<li><a href="http://bittooth.blogspot.com/" title="David Summers on energy: oil, gas and more">Bit Tooth Energy</a></li>
<li><a href="http://bravenewclimate.com/" title="Barry Brooks on climate and energy policy">Brave New Climate</a></li>
<li><a href="http://physics.ucsd.edu/do-the-math/" title="UC San Diego prof uses physics and estimation to assess energy, growth, options">Do the Math</a></li>
<li><a href="http://dotearth.blogs.nytimes.com/" title="Andrew Revkin’s environmental blog on the New York Times">Dot Earth</a></li>
<li><a href="http://e360.yale.edu/" title="News from the Yale School of Forestry &amp; Environmental Studies">Environment 360</a></li>
<li><a href="http://planet3.org/" title="A metablog on sustainability">Planet3.0</a></li>
<li><a href="http://www.realclimate.org/" title="climate science from climate scientists">RealClimate</a></li>
<li><a href="http://www.easterbrook.ca/steve/" title="Steve Easterbrook&#8217;s blog on software engineering and climate modeling">Serendipity</a></li>
<li><a href="http://scienceofdoom.com/" title="climate science in perspective">The Science of Doom</a></li>
<li><a href="http://e360.yale.edu/" title="opinion, analysis, reporting and debate on environmental issues">Yale Environment 360</a></li>

	</ul>
</li>

<li id="rss_links-3" class="widget widget_rss_links"><h2 class="widgettitle">RSS feeds:</h2>
<p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p class="size-small"><a class="feed-image-link" href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments"><img src="https://johncarlosbaez.wordpress.com/i/rss/red-small.png?m=1391188133h" alt="RSS Feed" /></a>&nbsp;<a href="https://johncarlosbaez.wordpress.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p>
</li>
<li id="blog_subscription-3" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widgettitle"><label for="subscribe-field">Email Subscription:</label></h2>

			<form
				action="https://subscribe.wordpress.com"
				method="post"
				accept-charset="utf-8"
				id=""
			>
				<p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
					<div class="jetpack-subscribe-count">
						<p>
						Join 5,227 other followers						</p>
					</div>
									<p id="subscribe-email">
					<label
						id="subscribe-field-label"
						for="subscribe-field"
						class="screen-reader-text"
					>
						Email Address:					</label>

					<input
							type="email"
							name="email"
							
							style="width: 95%; padding: 1px 10px"
							placeholder="Enter your email address"
							value=""
							id="subscribe-field"
						/>				</p>

				<p id="subscribe-submit"
									>
                    <input type="hidden" name="action" value="subscribe"/>
                    <input type="hidden" name="blog_id" value="12777403"/>
                    <input type="hidden" name="source" value="https://johncarlosbaez.wordpress.com/?s=information+and+entropy"/>
                    <input type="hidden" name="sub-type" value="widget"/>
                    <input type="hidden" name="redirect_fragment" value="blog_subscription-3"/>
					<input type="hidden" id="_wpnonce" name="_wpnonce" value="8f1337fe87" />                    <button type="submit"
	                    	                        class="wp-block-button__link"
	                    		                	                >
	                    Sign me up!                    </button>
                </p>
            </form>
			
</li>
<li id="search-3" class="widget widget_search"><h2 class="widgettitle">SEARCH:</h2>
<form method="get" id="searchform" action="https://johncarlosbaez.wordpress.com/">
<div><label for="s" class="search-label">Search</label><input type="text" value="information and entropy" name="s" id="s" />
<input type="submit" id="searchsubmit" value="Search" />
</div>
</form></li>
<li id="blog-stats-2" class="widget widget_blog-stats"><h2 class="widgettitle">Blog Stats:</h2>
		<ul>
			<li>4,177,056 hits</li>
		</ul>
		</li>
		</ul>
	</div>



<div id="footer">
	<p>
	<br />
	<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a>
	</p>
</div>

</div>

		<!--  -->
<script src='//0.gravatar.com/js/gprofiles.js?ver=202138y' id='grofiles-cards-js'></script>
<script id='wpgroho-js-extra'>
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1610363240h'></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-62b2df0762257e75433ad6f161488c3a">
	</div>
	<div class="grofile-hash-map-34784534843022b3541c8ddd693718cb">
	</div>
	<div class="grofile-hash-map-ba06491deb8346d20356ac2ae05893ee">
	</div>
	<div class="grofile-hash-map-24dc7e2371491f36fd4538b3474920b5">
	</div>
	<div class="grofile-hash-map-d3c6d7ec8069e25c08a0a11263581925">
	</div>
	</div>

<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20210915";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210920";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div
				class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions"
				itemscope
				itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"/>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"/>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"/>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"/>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"/>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
							</span>
						</a>
												<a href="#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"/>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea
													name="comment"
													class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea"
													id="jp-carousel-comment-form-comment-field"
													placeholder="Write a Comment..."
												></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field" />
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field" />
															</fieldset>
																											</div>
													<input
														type="submit"
														name="submit"
														class="jp-carousel-comment-form-button"
														id="jp-carousel-comment-form-button-submit"
														value="Post Comment" />
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"/>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"/>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		<link rel='stylesheet' id='all-css-0-2' href='https://s0.wp.com/_static/??/wp-content/mu-plugins/carousel/swiper-bundle.css,/wp-content/mu-plugins/carousel/jetpack-carousel.css?m=1630955947j&cssminify=yes' type='text/css' media='all' />
<script id='jetpack-carousel-js-extra'>
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/johncarlosbaez.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"31af8ab3d4","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/johncarlosbaez.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fjohncarlosbaez.wordpress.com%2F2021%2F08%2F17%2Finformation-geometry-part-21%2F","blog_id":"12777403","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=12777403&v=wpcom&tz=0&user_id=0&subd=johncarlosbaez","is_public":"1"};
</script>
<script crossorigin='anonymous' type='text/javascript' src='https://s2.wp.com/_static/??-eJx9jt0OgjAMhV/I0sCFCRfGZxlbQzbZj2sn8PZOI4lBw1V7TvP1HJwT6BiEgqBjNPSwmtLSOD7h18kXSFMZbWBUxtsAg8roFQvluoFkpW+8h+o/dy+U189o5qSjh5TjskKm6rFsjA16Kob4BVVJfiDT1KCDIlrlWJgmdCSp5sNmHDCzNSMJI5eBdbZJbAw/vf9kwLv6Tlbu6i/tuev7tuvOrXsCIKR8xw=='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script src="//stats.wp.com/w.js?62" defer></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'12777403','blog_tz':'0','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'12777403','v':'wpcom','tz':'0','user_id':'0','subd':'johncarlosbaez'}]);
_stq.push(['extra', {'crypt':'UE40eW5QN0p8M2Y/RE1TaVhzUzFMbjdWNHpwZGhTayxPSUFCMGNrd29+Smw0TDhnZmRTK0hlRi9QSGh6bi9GXVhBJWIlZlR5U1JMLU8/MkNtblkvY1d6eVImTEouL2UleTBRZmlNdkp0aWJaNFJhPS0uTVpHa1hIc25EbVJyQmw/SkZ+Q3lLcjU/NDBvQTZVNThpLXB0U3J1elBvZCZiTkEzXXlsLi1PU25PbltLcyZONi1Lay1ia3RtU0ZEcmxjdDdab2FqP3lnNTdjREcvfi93LG1GJS5WSk1jVm9BP0F6XWJteXEsSVAzXy9zQXcwbXVuW2t2QnU3QmFxTzRBVHl8UkZhb1VJWHpPbFlLRC5BbVdIeTFvWUZXfjVJRnNwSy1rN0dJRGxNUHcuQU40NH5salFmcy1xbnlfcnFBblh6JU9UbiY5bVJlc2wvcXZpSklDeA=='}]);
_stq.push([ 'clickTrackerInit', '12777403', '0' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>
</body>
</html>