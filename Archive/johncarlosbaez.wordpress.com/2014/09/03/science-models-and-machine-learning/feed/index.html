<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Science, Models and Machine Learning	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/</link>
	<description></description>
	<lastBuildDate>Mon, 04 Apr 2016 00:01:43 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Holger K. von Jouanne-Diedrich		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-76771</link>

		<dc:creator><![CDATA[Holger K. von Jouanne-Diedrich]]></dc:creator>
		<pubDate>Wed, 27 Jan 2016 16:52:26 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-76771</guid>

					<description><![CDATA[The link to https://www.ceremade.dauphine.fr/~peyre/numerical-tour/tours/optim_8_homotopy/index_04.png is broken]]></description>
			<content:encoded><![CDATA[<p>The link to <a href="https://www.ceremade.dauphine.fr/~peyre/numerical-tour/tours/optim_8_homotopy/index_04.png" rel="nofollow ugc">https://www.ceremade.dauphine.fr/~peyre/numerical-tour/tours/optim_8_homotopy/index_04.png</a> is broken</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davetweed		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56690</link>

		<dc:creator><![CDATA[davetweed]]></dc:creator>
		<pubDate>Tue, 16 Sep 2014 18:52:55 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56690</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288&quot;&gt;davetweed&lt;/a&gt;.

I&#039;ve been a bit snowed under so haven&#039;t got time to write a longer reply, but you might find &lt;a href=&quot;http://sebastianraschka.com/Articles/2014_kernel_pca.html&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt; shows some of the practical issues in using a kernel method (in this case Kernel PCA).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288">davetweed</a>.</p>
<p>I&#8217;ve been a bit snowed under so haven&#8217;t got time to write a longer reply, but you might find <a href="http://sebastianraschka.com/Articles/2014_kernel_pca.html" rel="nofollow">this link</a> shows some of the practical issues in using a kernel method (in this case Kernel PCA).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56464</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Thu, 11 Sep 2014 06:10:58 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56464</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288&quot;&gt;davetweed&lt;/a&gt;.

thanks for the reply.
Somehow I feel still somewhat uneasy about that dualization methods and that trial and error methods. How is the trial and error method computationally implemented? Is that fast enough?

Wikipedia writes on the page of &lt;a href=&quot;http://en.wikipedia.org/wiki/Artificial_neural_network&quot; rel=&quot;nofollow&quot;&gt;neural nets&lt;/a&gt; that:
&lt;blockquote&gt;In the 1990s, neural networks were overtaken in popularity in machine learning by support vector machines and other, much simpler methods such as linear classifiers. Renewed interest in neural nets was sparked in the 2000s by the advent of deep learning.&lt;/blockquote&gt; and in fact there are even challenges on that topic:
&lt;a href=&quot;http://googleresearch.blogspot.de/2014/09/building-deeper-understanding-of-images.html&quot; rel=&quot;nofollow&quot;&gt;

Since as I understood the whole inner city of London is scanned with CCTV cameras and individuals are traced by computers for behaving against the norm - like rakishly put: deviate from their way between two subway stations for e.g. peeing against a lantern - the trial and error algorithms must be quite fast, at least in computer vision.&lt;/a&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288">davetweed</a>.</p>
<p>thanks for the reply.<br />
Somehow I feel still somewhat uneasy about that dualization methods and that trial and error methods. How is the trial and error method computationally implemented? Is that fast enough?</p>
<p>Wikipedia writes on the page of <a href="http://en.wikipedia.org/wiki/Artificial_neural_network" rel="nofollow">neural nets</a> that:</p>
<blockquote><p>In the 1990s, neural networks were overtaken in popularity in machine learning by support vector machines and other, much simpler methods such as linear classifiers. Renewed interest in neural nets was sparked in the 2000s by the advent of deep learning.</p></blockquote>
<p> and in fact there are even challenges on that topic:<br />
<a href="http://googleresearch.blogspot.de/2014/09/building-deeper-understanding-of-images.html" rel="nofollow"></p>
<p>Since as I understood the whole inner city of London is scanned with CCTV cameras and individuals are traced by computers for behaving against the norm &#8211; like rakishly put: deviate from their way between two subway stations for e.g. peeing against a lantern &#8211; the trial and error algorithms must be quite fast, at least in computer vision.</a></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56408</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Wed, 10 Sep 2014 01:45:44 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56408</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56402&quot;&gt;John Baez&lt;/a&gt;.

&lt;a href=&quot;&quot; rel=&quot;nofollow&quot;&gt;Deen Abiola&lt;/a&gt; wrote:

&lt;blockquote&gt;
This is a mischaracterization: 

&lt;blockquote&gt;
If I start using differential equations (Navier Stokes) apparently I&#039;m doing &#039;proper modelling&#039;, whereas if I apply non-parametric regression I&#039;m doing something &#039;magic&#039;.
&lt;/blockquote&gt;  

No, the difference is non-parametric regression is blind. With Navier Stokes, in part by deductive reasoning, we know the compression is very good, we can throw it at all sorts of unexpected situations and get good predictions. We know the boundaries of the system. In the case of non-parametric regression, there is always a possibility that some unknown situation will break it. And the question of how representative your data is.

Consider say protein folding, earthquakes or weather prediction. You can throw lots of machine learning at them but in all cases, models driven by theory are best. With protein folding there are lots of systems which give decent results but break for unexpected situations. A theory of how biological systems fold these in a timely manner - the constraints which allow this possibility - would be of far greater utility in solving this problem than Density Estimation. For earthquakes without some idea of all the variables involved just throwing more data, if it&#039;s the wrong data won&#039;t reduce your error.

Finally, for weather prediction, machine learning or statistics works up to a point but the best predictors are physically based models because in a sense they&#039;re generative. The set of functions they represent are more than continuous functions. Unlike function learning, theories can be at least as powerful as Turing machines. ML there is a tool like any other, the best results come from a mix of human judgement, physics based models and statistics.﻿
&lt;/blockquote&gt;

&lt;a href=&quot;https://plus.google.com/u/0/113162081395652203523&quot; rel=&quot;nofollow&quot;&gt;Thomas Dietterich&lt;/a&gt; wrote:

&lt;blockquote&gt;
I agree with Neil Lawrence . I&#039;ve recently been working with the methods of Functional Data Analysis (http://en.wikipedia.org/wiki/Functional_data_analysis). These are non-parametric basis function models that share deep connections to kernel methods. But an interesting aspect of them is that you can build lots of prior knowledge (e.g., about the &quot;physics&quot;) into the regularization term rather than into the model term. So for example, you can use a regularizer such as a linear differential operator that penalizes fits that are far from matching the behavior of linear differential equations. In other words, you can express a prior that the evolution of a variable should be &quot;close to&quot; a differential equation model without knowing anything about the structure of the model. 

Another area where physics meets Bayes is data assimilation into numerical weather prediction models. The initial conditions are unknown (and the models are known to be wrong, particularly in modeling sub-pixel and boundary-layer phenomena). So they use an ensemble Kalman filter to update the state estimates every 6 hours from (noisy) observations of the atmosphere. We know that the gaussian assumptions of the Kalman filter are wrong and the dynamics in the atmospheric model are wrong, but together, they give very nice results.

In summary, I think we have barely begun to explore the ways in which flexible non-parametric statistical/ML models can be combined with mechanistic models﻿.
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56402">John Baez</a>.</p>
<p><a href="" rel="nofollow">Deen Abiola</a> wrote:</p>
<blockquote><p>
This is a mischaracterization: </p>
<blockquote><p>
If I start using differential equations (Navier Stokes) apparently I&#8217;m doing &#8216;proper modelling&#8217;, whereas if I apply non-parametric regression I&#8217;m doing something &#8216;magic&#8217;.
</p></blockquote>
<p>No, the difference is non-parametric regression is blind. With Navier Stokes, in part by deductive reasoning, we know the compression is very good, we can throw it at all sorts of unexpected situations and get good predictions. We know the boundaries of the system. In the case of non-parametric regression, there is always a possibility that some unknown situation will break it. And the question of how representative your data is.</p>
<p>Consider say protein folding, earthquakes or weather prediction. You can throw lots of machine learning at them but in all cases, models driven by theory are best. With protein folding there are lots of systems which give decent results but break for unexpected situations. A theory of how biological systems fold these in a timely manner &#8211; the constraints which allow this possibility &#8211; would be of far greater utility in solving this problem than Density Estimation. For earthquakes without some idea of all the variables involved just throwing more data, if it&#8217;s the wrong data won&#8217;t reduce your error.</p>
<p>Finally, for weather prediction, machine learning or statistics works up to a point but the best predictors are physically based models because in a sense they&#8217;re generative. The set of functions they represent are more than continuous functions. Unlike function learning, theories can be at least as powerful as Turing machines. ML there is a tool like any other, the best results come from a mix of human judgement, physics based models and statistics.﻿
</p></blockquote>
<p><a href="https://plus.google.com/u/0/113162081395652203523" rel="nofollow">Thomas Dietterich</a> wrote:</p>
<blockquote><p>
I agree with Neil Lawrence . I&#8217;ve recently been working with the methods of Functional Data Analysis (<a href="http://en.wikipedia.org/wiki/Functional_data_analysis" rel="nofollow ugc">http://en.wikipedia.org/wiki/Functional_data_analysis</a>). These are non-parametric basis function models that share deep connections to kernel methods. But an interesting aspect of them is that you can build lots of prior knowledge (e.g., about the &#8220;physics&#8221;) into the regularization term rather than into the model term. So for example, you can use a regularizer such as a linear differential operator that penalizes fits that are far from matching the behavior of linear differential equations. In other words, you can express a prior that the evolution of a variable should be &#8220;close to&#8221; a differential equation model without knowing anything about the structure of the model. </p>
<p>Another area where physics meets Bayes is data assimilation into numerical weather prediction models. The initial conditions are unknown (and the models are known to be wrong, particularly in modeling sub-pixel and boundary-layer phenomena). So they use an ensemble Kalman filter to update the state estimates every 6 hours from (noisy) observations of the atmosphere. We know that the gaussian assumptions of the Kalman filter are wrong and the dynamics in the atmospheric model are wrong, but together, they give very nice results.</p>
<p>In summary, I think we have barely begun to explore the ways in which flexible non-parametric statistical/ML models can be combined with mechanistic models﻿.
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56402</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Tue, 09 Sep 2014 23:31:11 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56402</guid>

					<description><![CDATA[There&#039;s a parallel discussion going on about this post &lt;a href=&quot;&quot; rel=&quot;nofollow&quot;&gt;over on G+&lt;/a&gt;, and &lt;a href=&quot;https://plus.google.com/u/0/116220678599902155344&quot; rel=&quot;nofollow&quot;&gt;Neil Lawrence&lt;/a&gt; wrote:

&lt;blockquote&gt; 
This is a really interesting thread. I should be following up much more on some of the links coming out of it, so apologies for the brain dump that follows (it&#039;s late, I just released 1600 paper decisions and I&#039;m still dealing with the repercussions, as well as the back log of work that&#039;s been building). However, in the end, I couldn&#039;t resist, so here it comes. I&#039;ve followed John&#039;s posts for quite a long time ... and this is the first time I&#039;ve indulged! Although maybe this should have been a separate post or a blog post.

There is a smooth continuum between data driven models and mechanistic models. To such an extent that I don&#039;t think Laplace and Gauss would have noticed the difference. The mainstay technique of my branch of ML, so called &#039;Bayesian inference&#039; was actually mainly developed by Laplace and later invoked by Gauss with the goal of fitting mechanistic models to data. Gauss&#039;s claim on least squares emerges from his attempt to predict Ceres&#039;s orbit with a &#039;Gaussian&#039; distribution for the noise. To these mathematicians/scientists the modelling was one thing, it included stochastic components (the noise) and deterministic components derived from Kepler&#039;s laws or Newtonian mechanics.

Since then, things seem to have separated into two camps. If I start using differential equations (Navier Stokes) apparently I&#039;m doing &#039;proper modelling&#039;, whereas if I apply non-parametric regression I&#039;m doing something &#039;magic&#039;. The Navier Stokes equations are an abstraction of what really goes on in fluid mechanics (which is a bunch of molecules bumping in to each other). It includes what I refer to as &lt;b&gt;strong&lt;/b&gt; mechanistic assumptions about how those interactions should appear at the macroscopic scale. However, if the data departs in any way from the underlying equation, the model is unable to respond, other than by changing a few parameters. Conversely, a data-driven nonlinear regression includes only very &lt;b&gt;weak&lt;/b&gt; mechanistic assumptions. In particular, they tend to make some assumption about smoothness.

Now, to me, that is simply two ends of the same coin. Smoothness assumptions tend to be (generally) more applicable than Navier Stokes because they derive from much vaguer beliefs about how the world operates, for example they very often seem to emerge from the law of large numbers. However, we can also be more specific about particular physical systems where they emerge. Some variants of diffusion (e.g. the heat equation) can lead to a Gaussian form for the Green&#039;s function. For these systems, if we consider the initial conditions to be unknown, and choose to place a probabilistic process prior over them, then after a given time we expect temperature to be distributed according to a Gaussian process model. The covariance function of the process (or kernel as it&#039;s known in the world of support vector machines) is the widely used &#039;Gaussian&#039; kernel (which I prefer to call the &#039;exponentiated quadratic&#039;). It is this &#039;kernel&#039; that provides the feature map \Phi referred to in the original post above. However, if you pick at it, it has an underlying (&lt;b&gt;weak&lt;/b&gt;) mechanistic interpretation.

In most &lt;b&gt;strongly&lt;/b&gt; mechanistic modelling initial conditions are given deterministic values, and so the probabilistic nature (and associated flexibility) of the model remains inapparent. Trying to explore the gap between these two approaches is the main aim of my group&#039;s work (and that of collaborators and colleagues) in &quot;Latent Force Models&quot;, here we blur the line between the two domains. In the ideal world we&#039;d like to place probabilistic priors over the initial conditions for even Navier Stokes equations. In the meantime we have managed to do it for some simple spatio-temporal systems (http://arxiv.org/pdf/1107.2699v1.pdf Section 5.1). Our inspiration is often Systems Biology, but it could well be climate modelling, I just think that&#039;s a much harder field to get into (in terms of time one would need to invest).

I should also mention other interesting directions: many researchers are using &#039;emulators&#039; which are statistical models (weakly mechanistic!) designed to approximate the output of a complex simulators that I would term &lt;b&gt;strongly&lt;/b&gt; mechanistic (e.g. the MUCM project http://www.mucm.ac.uk/). Another really interesting direction is Approximate Bayesian Computation. Where the aim is to bring the probabilistic techniques of &#039;Bayesian inference&#039; (sorry for quotes, but I dislike the term Bayesian, see a previous G+ post of mine) to large &#039;strongly mechanistic&#039; models. I helped instigate a workshop on ABC at this year&#039;s NIPS. I&#039;ve not made any contributions in this area, I just think it&#039;s interesting and ML people should know more about it ... that&#039;s why I used the term &#039;instigate&#039; rather than organise: http://xianblog.wordpress.com/2014/09/09/abcnips-call-for-papers/﻿
&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>There&#8217;s a parallel discussion going on about this post <a href="" rel="nofollow">over on G+</a>, and <a href="https://plus.google.com/u/0/116220678599902155344" rel="nofollow">Neil Lawrence</a> wrote:</p>
<blockquote><p>
This is a really interesting thread. I should be following up much more on some of the links coming out of it, so apologies for the brain dump that follows (it&#8217;s late, I just released 1600 paper decisions and I&#8217;m still dealing with the repercussions, as well as the back log of work that&#8217;s been building). However, in the end, I couldn&#8217;t resist, so here it comes. I&#8217;ve followed John&#8217;s posts for quite a long time &#8230; and this is the first time I&#8217;ve indulged! Although maybe this should have been a separate post or a blog post.</p>
<p>There is a smooth continuum between data driven models and mechanistic models. To such an extent that I don&#8217;t think Laplace and Gauss would have noticed the difference. The mainstay technique of my branch of ML, so called &#8216;Bayesian inference&#8217; was actually mainly developed by Laplace and later invoked by Gauss with the goal of fitting mechanistic models to data. Gauss&#8217;s claim on least squares emerges from his attempt to predict Ceres&#8217;s orbit with a &#8216;Gaussian&#8217; distribution for the noise. To these mathematicians/scientists the modelling was one thing, it included stochastic components (the noise) and deterministic components derived from Kepler&#8217;s laws or Newtonian mechanics.</p>
<p>Since then, things seem to have separated into two camps. If I start using differential equations (Navier Stokes) apparently I&#8217;m doing &#8216;proper modelling&#8217;, whereas if I apply non-parametric regression I&#8217;m doing something &#8216;magic&#8217;. The Navier Stokes equations are an abstraction of what really goes on in fluid mechanics (which is a bunch of molecules bumping in to each other). It includes what I refer to as <b>strong</b> mechanistic assumptions about how those interactions should appear at the macroscopic scale. However, if the data departs in any way from the underlying equation, the model is unable to respond, other than by changing a few parameters. Conversely, a data-driven nonlinear regression includes only very <b>weak</b> mechanistic assumptions. In particular, they tend to make some assumption about smoothness.</p>
<p>Now, to me, that is simply two ends of the same coin. Smoothness assumptions tend to be (generally) more applicable than Navier Stokes because they derive from much vaguer beliefs about how the world operates, for example they very often seem to emerge from the law of large numbers. However, we can also be more specific about particular physical systems where they emerge. Some variants of diffusion (e.g. the heat equation) can lead to a Gaussian form for the Green&#8217;s function. For these systems, if we consider the initial conditions to be unknown, and choose to place a probabilistic process prior over them, then after a given time we expect temperature to be distributed according to a Gaussian process model. The covariance function of the process (or kernel as it&#8217;s known in the world of support vector machines) is the widely used &#8216;Gaussian&#8217; kernel (which I prefer to call the &#8216;exponentiated quadratic&#8217;). It is this &#8216;kernel&#8217; that provides the feature map \Phi referred to in the original post above. However, if you pick at it, it has an underlying (<b>weak</b>) mechanistic interpretation.</p>
<p>In most <b>strongly</b> mechanistic modelling initial conditions are given deterministic values, and so the probabilistic nature (and associated flexibility) of the model remains inapparent. Trying to explore the gap between these two approaches is the main aim of my group&#8217;s work (and that of collaborators and colleagues) in &#8220;Latent Force Models&#8221;, here we blur the line between the two domains. In the ideal world we&#8217;d like to place probabilistic priors over the initial conditions for even Navier Stokes equations. In the meantime we have managed to do it for some simple spatio-temporal systems (<a href="http://arxiv.org/pdf/1107.2699v1.pdf" rel="nofollow ugc">http://arxiv.org/pdf/1107.2699v1.pdf</a> Section 5.1). Our inspiration is often Systems Biology, but it could well be climate modelling, I just think that&#8217;s a much harder field to get into (in terms of time one would need to invest).</p>
<p>I should also mention other interesting directions: many researchers are using &#8217;emulators&#8217; which are statistical models (weakly mechanistic!) designed to approximate the output of a complex simulators that I would term <b>strongly</b> mechanistic (e.g. the MUCM project <a href="http://www.mucm.ac.uk/" rel="nofollow ugc">http://www.mucm.ac.uk/</a>). Another really interesting direction is Approximate Bayesian Computation. Where the aim is to bring the probabilistic techniques of &#8216;Bayesian inference&#8217; (sorry for quotes, but I dislike the term Bayesian, see a previous G+ post of mine) to large &#8216;strongly mechanistic&#8217; models. I helped instigate a workshop on ABC at this year&#8217;s NIPS. I&#8217;ve not made any contributions in this area, I just think it&#8217;s interesting and ML people should know more about it &#8230; that&#8217;s why I used the term &#8216;instigate&#8217; rather than organise: <a href="http://xianblog.wordpress.com/2014/09/09/abcnips-call-for-papers/﻿" rel="nofollow ugc">http://xianblog.wordpress.com/2014/09/09/abcnips-call-for-papers/﻿</a>
</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davetweed		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56327</link>

		<dc:creator><![CDATA[davetweed]]></dc:creator>
		<pubDate>Mon, 08 Sep 2014 17:33:11 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56327</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288&quot;&gt;davetweed&lt;/a&gt;.

You&#039;ve pretty much got it. The duality that wikipedia mentions is &quot;just&quot; &lt;a href=&quot;http://en.wikipedia.org/wiki/Duality_%28optimization%29&quot; rel=&quot;nofollow&quot;&gt;one of the constrained optimization dualities&lt;/a&gt;, so while it&#039;s still a strange fact it&#039;s not something unique to the SVM.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288">davetweed</a>.</p>
<p>You&#8217;ve pretty much got it. The duality that wikipedia mentions is &#8220;just&#8221; <a href="http://en.wikipedia.org/wiki/Duality_%28optimization%29" rel="nofollow">one of the constrained optimization dualities</a>, so while it&#8217;s still a strange fact it&#8217;s not something unique to the SVM.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56306</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Mon, 08 Sep 2014 05:26:44 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56306</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288&quot;&gt;davetweed&lt;/a&gt;.

thanks for the reply.

&lt;blockquote&gt;For the example in my comment $latex \phi((x,y))=(x^2,y^2)$&lt;/blockquote&gt;

If I understand correctly &lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; writes that in case that the data is linearly separable the kernel would be (after that ominous &quot;dualization&quot;):  $latex x_i \cdot x_j$. So in the example that would be $latex x \cdot y$, if the set of points would be linearly separable, which it isn&#039;t, that&#039;s why you choose $latex \phi$. Is that right? However if I am able to write a kernel as a dot-product $latex \phi(x_i),\phi(x_j)$ that doesn&#039;t seem to guarantee that the corresponding set is automatically linearly separable. You wrote in that direction

&lt;blockquote&gt;Different mappings will do better or worse in making the data separable using a hyperplane.&lt;/blockquote&gt;

And so in the moment I say Yes. But....like if I choose $latex phi$ to be the identity I will have the same problems as before. That is Mercer&#039;s theorem doesn&#039;t seem to be enough. There must be something, which tells you when that points get linearly separable (for the corresponding $latex \phi$). What you seem to propose is that this is done by trial and error. And this &quot;trial and error&quot; prodedure is what you call the choice in preprocessing? Did I understand you right?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288">davetweed</a>.</p>
<p>thanks for the reply.</p>
<blockquote><p>For the example in my comment <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28%28x%2Cy%29%29%3D%28x%5E2%2Cy%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi((x,y))=(x^2,y^2)" class="latex" /></p></blockquote>
<p>If I understand correctly <a href="http://en.wikipedia.org/wiki/Support_vector_machine" rel="nofollow">Wikipedia</a> writes that in case that the data is linearly separable the kernel would be (after that ominous &#8220;dualization&#8221;):  <img src="https://s0.wp.com/latex.php?latex=x_i+%5Ccdot+x_j&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x_i &#92;cdot x_j" class="latex" />. So in the example that would be <img src="https://s0.wp.com/latex.php?latex=x+%5Ccdot+y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="x &#92;cdot y" class="latex" />, if the set of points would be linearly separable, which it isn&#8217;t, that&#8217;s why you choose <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" />. Is that right? However if I am able to write a kernel as a dot-product <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28x_i%29%2C%5Cphi%28x_j%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi(x_i),&#92;phi(x_j)" class="latex" /> that doesn&#8217;t seem to guarantee that the corresponding set is automatically linearly separable. You wrote in that direction</p>
<blockquote><p>Different mappings will do better or worse in making the data separable using a hyperplane.</p></blockquote>
<p>And so in the moment I say Yes. But&#8230;.like if I choose <img src="https://s0.wp.com/latex.php?latex=phi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="phi" class="latex" /> to be the identity I will have the same problems as before. That is Mercer&#8217;s theorem doesn&#8217;t seem to be enough. There must be something, which tells you when that points get linearly separable (for the corresponding <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" />). What you seem to propose is that this is done by trial and error. And this &#8220;trial and error&#8221; prodedure is what you call the choice in preprocessing? Did I understand you right?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davetweed		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56288</link>

		<dc:creator><![CDATA[davetweed]]></dc:creator>
		<pubDate>Sun, 07 Sep 2014 19:34:48 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56288</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56047&quot;&gt;davetweed&lt;/a&gt;.

Firstly, it&#039;s perhaps relevant to clarify that &lt;em&gt;kernel functions&lt;/em&gt; are an &quot;optimization&quot; of the computation process and that the fundamental picture can be seen just in terms of the &lt;em&gt;nonlinear map&lt;/em&gt; $latex \phi$. The kernel and nonlinear map are always related by 

$latex K(a,b)=\phi(a)\cdot\phi(b)$ 

Knowing $latex \phi$ you can find $latex K$ directly, while &lt;a href=&quot;http://en.wikipedia.org/wiki/Mercer%27s_theorem&quot; rel=&quot;nofollow&quot;&gt;tells you how to go from a suitable $latex K$ to $latex \phi$&lt;/a&gt;. For the example in my comment 

$latex \phi((x,y))=(x^2,y^2)$ 

with corresponding kernel 

$latex K((x,y),(u,v))=x^2u^2+y^2v^2$ 

But there&#039;s nothing special about that $latex \phi$: it could be 

$latex \phi((x,y))=(x+y,x^3,x \exp(y))$ 

or any other non-linear map. Different mappings will do better or worse in making the data separable using a hyperplane.

To see about the issue of choice, let&#039;s look at the series of steps (in theory):

1. You give me a data set and ask for an SVM model.

2. I decide on a mapping $latex \phi$ (maybe after looking at the data).

3. I can stick the mapping and the data set into a standard SVM software package. In theory it applies $latex \phi$ to &quot;preprocess&quot; the dataset and then fits a maximum margin hyperplane to that transformed dataset.

4. I look at the performance of the classifier. If it&#039;s not very good I might return to step 2 and try a different mapping.

5. I give you back the parameters for this SVM classifier.

The confusing bit comes because in 3 the package doesn&#039;t literally apply $latex \phi$ to the dataset, instead it uses the kernel function to essentially do the $latex \phi$ mapping and the dot product using the one function $latex K$. It&#039;s also the case that frequently you don&#039;t choose $latex \phi$ but go straight to choosing $latex K$.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56047">davetweed</a>.</p>
<p>Firstly, it&#8217;s perhaps relevant to clarify that <em>kernel functions</em> are an &#8220;optimization&#8221; of the computation process and that the fundamental picture can be seen just in terms of the <em>nonlinear map</em> <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" />. The kernel and nonlinear map are always related by </p>
<p><img src="https://s0.wp.com/latex.php?latex=K%28a%2Cb%29%3D%5Cphi%28a%29%5Ccdot%5Cphi%28b%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K(a,b)=&#92;phi(a)&#92;cdot&#92;phi(b)" class="latex" /> </p>
<p>Knowing <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> you can find <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> directly, while <a href="http://en.wikipedia.org/wiki/Mercer%27s_theorem" rel="nofollow">tells you how to go from a suitable <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /></a>. For the example in my comment </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28%28x%2Cy%29%29%3D%28x%5E2%2Cy%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi((x,y))=(x^2,y^2)" class="latex" /> </p>
<p>with corresponding kernel </p>
<p><img src="https://s0.wp.com/latex.php?latex=K%28%28x%2Cy%29%2C%28u%2Cv%29%29%3Dx%5E2u%5E2%2By%5E2v%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K((x,y),(u,v))=x^2u^2+y^2v^2" class="latex" /> </p>
<p>But there&#8217;s nothing special about that <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" />: it could be </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28%28x%2Cy%29%29%3D%28x%2By%2Cx%5E3%2Cx+%5Cexp%28y%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi((x,y))=(x+y,x^3,x &#92;exp(y))" class="latex" /> </p>
<p>or any other non-linear map. Different mappings will do better or worse in making the data separable using a hyperplane.</p>
<p>To see about the issue of choice, let&#8217;s look at the series of steps (in theory):</p>
<p>1. You give me a data set and ask for an SVM model.</p>
<p>2. I decide on a mapping <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> (maybe after looking at the data).</p>
<p>3. I can stick the mapping and the data set into a standard SVM software package. In theory it applies <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> to &#8220;preprocess&#8221; the dataset and then fits a maximum margin hyperplane to that transformed dataset.</p>
<p>4. I look at the performance of the classifier. If it&#8217;s not very good I might return to step 2 and try a different mapping.</p>
<p>5. I give you back the parameters for this SVM classifier.</p>
<p>The confusing bit comes because in 3 the package doesn&#8217;t literally apply <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> to the dataset, instead it uses the kernel function to essentially do the <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> mapping and the dot product using the one function <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" />. It&#8217;s also the case that frequently you don&#8217;t choose <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;phi" class="latex" /> but go straight to choosing <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="K" class="latex" />.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davetweed		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56287</link>

		<dc:creator><![CDATA[davetweed]]></dc:creator>
		<pubDate>Sun, 07 Sep 2014 19:07:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56287</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56154&quot;&gt;davetweed&lt;/a&gt;.

When I say &quot;may not buy you anything&quot;, all I mean is that cross validation isn&#039;t doing very much if your data set is so small (relative to the complexity of the problem) that you&#039;d be doing it with tiny training and validation sets, or as you mention when you have no idea which variables ought to considered outputs of the others (or even make sense. In the quote I&#039;m just weakly arguing against the idea that there&#039;s some &quot;mechanical checklist&quot; that you should follow when applying machine learning; people should be considering what they need to do to in their particular situation with their data.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56154">davetweed</a>.</p>
<p>When I say &#8220;may not buy you anything&#8221;, all I mean is that cross validation isn&#8217;t doing very much if your data set is so small (relative to the complexity of the problem) that you&#8217;d be doing it with tiny training and validation sets, or as you mention when you have no idea which variables ought to considered outputs of the others (or even make sense. In the quote I&#8217;m just weakly arguing against the idea that there&#8217;s some &#8220;mechanical checklist&#8221; that you should follow when applying machine learning; people should be considering what they need to do to in their particular situation with their data.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56265</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Sun, 07 Sep 2014 07:14:08 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=18532#comment-56265</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56047&quot;&gt;davetweed&lt;/a&gt;.

thanks for the explanation.
&lt;blockquote&gt;choosing (X,Y)=(x^2,y^2) happened outside the algorithm&lt;/blockquote&gt;
Unfortunately I am not sure if I have fully understood what you mean with preprocessing. Do you mean that the algorithm goes through various cases as seems to be indicated by &lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_classification&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;, but then where is the choice ? or is this preprocessing supposed to be done by a human?

By that whats written in Wikipedia I can&#039;t really understand how exactly that &quot;kernel trick&quot; works and Wikipedia wrote:
&lt;blockquote&gt;The transformation may be nonlinear and the transformed space high dimensional;&lt;/blockquote&gt;
Does the &quot;may be&quot; extend to the &quot;transformed space high dimensional&quot; and thus in particular could also include lower dimensional? What would be the kernel in your case? (wikipedia writes only cryptically that
&lt;blockquote&gt;one can show that the dual of the SVM reduces to the following optimization problem:&lt;/blockquote&gt; where it is not even clear here to me what they mean with &quot;dual&quot; in this case.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/09/03/science-models-and-machine-learning/#comment-56047">davetweed</a>.</p>
<p>thanks for the explanation.</p>
<blockquote><p>choosing (X,Y)=(x^2,y^2) happened outside the algorithm</p></blockquote>
<p>Unfortunately I am not sure if I have fully understood what you mean with preprocessing. Do you mean that the algorithm goes through various cases as seems to be indicated by <a href="http://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_classification" rel="nofollow">Wikipedia</a>, but then where is the choice ? or is this preprocessing supposed to be done by a human?</p>
<p>By that whats written in Wikipedia I can&#8217;t really understand how exactly that &#8220;kernel trick&#8221; works and Wikipedia wrote:</p>
<blockquote><p>The transformation may be nonlinear and the transformed space high dimensional;</p></blockquote>
<p>Does the &#8220;may be&#8221; extend to the &#8220;transformed space high dimensional&#8221; and thus in particular could also include lower dimensional? What would be the kernel in your case? (wikipedia writes only cryptically that</p>
<blockquote><p>one can show that the dual of the SVM reduces to the following optimization problem:</p></blockquote>
<p> where it is not even clear here to me what they mean with &#8220;dual&#8221; in this case.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
