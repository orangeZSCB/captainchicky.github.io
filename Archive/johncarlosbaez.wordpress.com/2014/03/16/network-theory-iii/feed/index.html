<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Network Theory III	</title>
	<atom:link href="https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/feed/" rel="self" type="application/rss+xml" />
	<link>https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/</link>
	<description></description>
	<lastBuildDate>Mon, 14 Apr 2014 16:51:34 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Gleaning&#8230; &#171; recurring dreams		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/#comment-40331</link>

		<dc:creator><![CDATA[Gleaning&#8230; &#171; recurring dreams]]></dc:creator>
		<pubDate>Mon, 14 Apr 2014 16:51:34 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17693#comment-40331</guid>

					<description><![CDATA[[&#8230;] John Baez&#8217;s This Week&#8217;s Finds and Azimuth blog also make math sexy. But there&#8217;s too much stuff to fit into this tiny list. I&#8217;ll just highlight his Network Theory series. Some of the topics it touches on include information theory and Bayesian networks. [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] John Baez&#8217;s This Week&#8217;s Finds and Azimuth blog also make math sexy. But there&#8217;s too much stuff to fit into this tiny list. I&#8217;ll just highlight his Network Theory series. Some of the topics it touches on include information theory and Bayesian networks. [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Baez		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/#comment-38733</link>

		<dc:creator><![CDATA[John Baez]]></dc:creator>
		<pubDate>Mon, 17 Mar 2014 15:45:02 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17693#comment-38733</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/#comment-38712&quot;&gt;nad&lt;/a&gt;.

Nadja wrote:

&lt;blockquote&gt;
It seems there are typos on page 10

$latex S(X,p)-S(X,r) \rightarrow S(X,p)-S(Z,r)$

and

$latex S(X,p)-S(X,q)+S(X,q)-S(X,r) \to$
$latex S(X,p)-S(Y,q)+S(Y,q)-S(Z,r) $

&lt;/blockquote&gt;

Wow, you&#039;re right!  I&#039;m not surprised I made this typo; I&#039;m surprised that nobody attending my talk noticed it.  This proves something I should have remembered: at a talk, almost nobody carefully reads equations on the slides, because they are too busy listening to the speaker.  So, it&#039;s good to avoid showing them equations.   (Equations are a bit easier to absorb when sitting at your computer at home.)

&lt;blockquote&gt;
Where does the data processing inequality come from?
&lt;/blockquote&gt;

In my talk I mentioned the data processing inequality because it&#039;s one way to prove the fact I needed: &lt;i&gt;when you apply a function to a random variable, its entropy goes down&lt;/i&gt;.  I figured that since I was talking to an audience of computer scientists working on quantum information theory, a lot of them would know and love the data processing inequality.  But it&#039;s probably easier to show the inequality I needed &lt;i&gt;directly&lt;/i&gt;.

All I needed was this.  Suppose you have a list of probabilities $latex p_i$ that sum to 1.  Suppose you partition them into bunches, add up the numbers in each bunch and get numbers $latex q_i$: a shorter list of probabilities that sum to 1.  Then

$latex S(q) \le S(p) $ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; (<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2665.png" alt="♥" class="wp-smiley" style="height: 1em; max-height: 1em;" />)

or in other words

$latex \displaystyle{ - \sum_i q_i \ln q_i \le - \sum_i p_i \ln p_i } $

I imagine the easiest way to show this is to show it inductively, starting from this: if $latex p_1, p_2$ are two probabilities that sum to &#8804; 1, and 

$latex q = p_1 + p_2$

then

$latex -q \ln q \le p_1 \ln p_1 \; + \; p_2 \ln p_2 $

I forget how to show this, but it should be easy, so I&#039;ll leave it as an exercise for the reader!

The Scholarpedia page on &lt;a href=&quot;http://www.scholarpedia.org/article/Mutual_information&quot; rel=&quot;nofollow&quot;&gt;mutual information&lt;/a&gt; gives a statement and easy proof of the data processing inequality---search it for the phrase &#039;data processing&#039;.

This inequality says that if we have two random variables, X and Y, whose mutual information is I(X;Y), and a third random variable Z that is a function of Y, we have I(X;Z) &#8804; I(X;Y).  In other words, &lt;i&gt;there&#039;s no way that Z can know more about X than Y did, because Z is obtained from Y&lt;/i&gt;.  

If we apply this in the case where X = Y, we get I(X;Z) &#8804; I(X;X).  Using the definition of mutual information we can simplify both sides and get S(Z) &#8804; S(X).  

So, I&#039;m saying that if Z is a function of X then 

S(Z) &#8804; S(X)

and this is another way of stating inequality (<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2665.png" alt="♥" class="wp-smiley" style="height: 1em; max-height: 1em;" />) above.  However, you need to know about random variables and mutual information to understand this---so if you don&#039;t, please ignore what I just said!  It&#039;s easier to just prove (<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2665.png" alt="♥" class="wp-smiley" style="height: 1em; max-height: 1em;" />) directly.

&lt;blockquote&gt;
It seems you get the information loss between $latex p$ and $latex \tilde{q}$ from the relative Shannon entropy

$latex \displaystyle{ -\sum_i p_i \ln{\frac{p_i}{q_i}} }$

via setting

$latex \displaystyle{ q_i = \tilde{q}_i^{\frac{\tilde{q}_i}{p_i}} }$

&lt;/blockquote&gt;

No, we don&#039;t do anything like this---I&#039;d never even thought of this trick.  Raising a probability to a power like that looks very scary!  I&#039;ve done similar things when introducing &#039;temperature&#039; into probability theory, but then you need to divide by a fudge factor (the partition function) to make sure the probabilities still sum to one.  ]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/#comment-38712">nad</a>.</p>
<p>Nadja wrote:</p>
<blockquote><p>
It seems there are typos on page 10</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28X%2Cp%29-S%28X%2Cr%29+%5Crightarrow+S%28X%2Cp%29-S%28Z%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X,p)-S(X,r) &#92;rightarrow S(X,p)-S(Z,r)" class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28X%2Cp%29-S%28X%2Cq%29%2BS%28X%2Cq%29-S%28X%2Cr%29+%5Cto&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X,p)-S(X,q)+S(X,q)-S(X,r) &#92;to" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=S%28X%2Cp%29-S%28Y%2Cq%29%2BS%28Y%2Cq%29-S%28Z%2Cr%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X,p)-S(Y,q)+S(Y,q)-S(Z,r) " class="latex" /></p>
</blockquote>
<p>Wow, you&#8217;re right!  I&#8217;m not surprised I made this typo; I&#8217;m surprised that nobody attending my talk noticed it.  This proves something I should have remembered: at a talk, almost nobody carefully reads equations on the slides, because they are too busy listening to the speaker.  So, it&#8217;s good to avoid showing them equations.   (Equations are a bit easier to absorb when sitting at your computer at home.)</p>
<blockquote><p>
Where does the data processing inequality come from?
</p></blockquote>
<p>In my talk I mentioned the data processing inequality because it&#8217;s one way to prove the fact I needed: <i>when you apply a function to a random variable, its entropy goes down</i>.  I figured that since I was talking to an audience of computer scientists working on quantum information theory, a lot of them would know and love the data processing inequality.  But it&#8217;s probably easier to show the inequality I needed <i>directly</i>.</p>
<p>All I needed was this.  Suppose you have a list of probabilities <img src="https://s0.wp.com/latex.php?latex=p_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_i" class="latex" /> that sum to 1.  Suppose you partition them into bunches, add up the numbers in each bunch and get numbers <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i" class="latex" />: a shorter list of probabilities that sum to 1.  Then</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28q%29+%5Cle+S%28p%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(q) &#92;le S(p) " class="latex" /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (♥)</p>
<p>or in other words</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-+%5Csum_i+q_i+%5Cln+q_i+%5Cle+-+%5Csum_i+p_i+%5Cln+p_i+%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ - &#92;sum_i q_i &#92;ln q_i &#92;le - &#92;sum_i p_i &#92;ln p_i } " class="latex" /></p>
<p>I imagine the easiest way to show this is to show it inductively, starting from this: if <img src="https://s0.wp.com/latex.php?latex=p_1%2C+p_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p_1, p_2" class="latex" /> are two probabilities that sum to &le; 1, and </p>
<p><img src="https://s0.wp.com/latex.php?latex=q+%3D+p_1+%2B+p_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q = p_1 + p_2" class="latex" /></p>
<p>then</p>
<p><img src="https://s0.wp.com/latex.php?latex=-q+%5Cln+q+%5Cle+p_1+%5Cln+p_1+%5C%3B+%2B+%5C%3B+p_2+%5Cln+p_2+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-q &#92;ln q &#92;le p_1 &#92;ln p_1 &#92;; + &#92;; p_2 &#92;ln p_2 " class="latex" /></p>
<p>I forget how to show this, but it should be easy, so I&#8217;ll leave it as an exercise for the reader!</p>
<p>The Scholarpedia page on <a href="http://www.scholarpedia.org/article/Mutual_information" rel="nofollow">mutual information</a> gives a statement and easy proof of the data processing inequality&#8212;search it for the phrase &#8216;data processing&#8217;.</p>
<p>This inequality says that if we have two random variables, X and Y, whose mutual information is I(X;Y), and a third random variable Z that is a function of Y, we have I(X;Z) &le; I(X;Y).  In other words, <i>there&#8217;s no way that Z can know more about X than Y did, because Z is obtained from Y</i>.  </p>
<p>If we apply this in the case where X = Y, we get I(X;Z) &le; I(X;X).  Using the definition of mutual information we can simplify both sides and get S(Z) &le; S(X).  </p>
<p>So, I&#8217;m saying that if Z is a function of X then </p>
<p>S(Z) &le; S(X)</p>
<p>and this is another way of stating inequality (♥) above.  However, you need to know about random variables and mutual information to understand this&#8212;so if you don&#8217;t, please ignore what I just said!  It&#8217;s easier to just prove (♥) directly.</p>
<blockquote><p>
It seems you get the information loss between <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7Bq%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tilde{q}" class="latex" /> from the relative Shannon entropy</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+-%5Csum_i+p_i+%5Cln%7B%5Cfrac%7Bp_i%7D%7Bq_i%7D%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ -&#92;sum_i p_i &#92;ln{&#92;frac{p_i}{q_i}} }" class="latex" /></p>
<p>via setting</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%7B+q_i+%3D+%5Ctilde%7Bq%7D_i%5E%7B%5Cfrac%7B%5Ctilde%7Bq%7D_i%7D%7Bp_i%7D%7D+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;displaystyle{ q_i = &#92;tilde{q}_i^{&#92;frac{&#92;tilde{q}_i}{p_i}} }" class="latex" /></p>
</blockquote>
<p>No, we don&#8217;t do anything like this&#8212;I&#8217;d never even thought of this trick.  Raising a probability to a power like that looks very scary!  I&#8217;ve done similar things when introducing &#8216;temperature&#8217; into probability theory, but then you need to divide by a fudge factor (the partition function) to make sure the probabilities still sum to one.  </p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: nad		</title>
		<link>https://johncarlosbaez.wordpress.com/2014/03/16/network-theory-iii/#comment-38712</link>

		<dc:creator><![CDATA[nad]]></dc:creator>
		<pubDate>Mon, 17 Mar 2014 10:22:22 +0000</pubDate>
		<guid isPermaLink="false">http://johncarlosbaez.wordpress.com/?p=17693#comment-38712</guid>

					<description><![CDATA[Where does the data processing inequality come from?

It seems there are typos on page 10

$latex S(X,p)-S(X,r) \rightarrow S(X,p)-S(Z,r) $

and

$latex S(X,p)-S(X,q)+S(X,q)-S(X,r) \to $ $latex S(X,p)-S(Y,q)+S(Y,q)-S(Z,r)$ 

It seems you get the information loss between $latex p$ and $latex \tilde{q}$ from the relative Shannon entropy 

$latex -\sum_i p_i \ln{\frac{p_i}{q_i}}$ 

via setting 

$latex q_i = \tilde{q}_i^{\frac{\tilde{q}_i}{p_i}}$ 

(eventually apart from a minus sign), so this condition that the functor should vanish on morphisms in FP seems somehow connected with a kind of  convex structure on FinStat. Do you use this in the proof? (Sorry, no time to look at the proof).]]></description>
			<content:encoded><![CDATA[<p>Where does the data processing inequality come from?</p>
<p>It seems there are typos on page 10</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28X%2Cp%29-S%28X%2Cr%29+%5Crightarrow+S%28X%2Cp%29-S%28Z%2Cr%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X,p)-S(X,r) &#92;rightarrow S(X,p)-S(Z,r) " class="latex" /></p>
<p>and</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28X%2Cp%29-S%28X%2Cq%29%2BS%28X%2Cq%29-S%28X%2Cr%29+%5Cto+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X,p)-S(X,q)+S(X,q)-S(X,r) &#92;to " class="latex" /> <img src="https://s0.wp.com/latex.php?latex=S%28X%2Cp%29-S%28Y%2Cq%29%2BS%28Y%2Cq%29-S%28Z%2Cr%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="S(X,p)-S(Y,q)+S(Y,q)-S(Z,r)" class="latex" /> </p>
<p>It seems you get the information loss between <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7Bq%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="&#92;tilde{q}" class="latex" /> from the relative Shannon entropy </p>
<p><img src="https://s0.wp.com/latex.php?latex=-%5Csum_i+p_i+%5Cln%7B%5Cfrac%7Bp_i%7D%7Bq_i%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="-&#92;sum_i p_i &#92;ln{&#92;frac{p_i}{q_i}}" class="latex" /> </p>
<p>via setting </p>
<p><img src="https://s0.wp.com/latex.php?latex=q_i+%3D+%5Ctilde%7Bq%7D_i%5E%7B%5Cfrac%7B%5Ctilde%7Bq%7D_i%7D%7Bp_i%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" alt="q_i = &#92;tilde{q}_i^{&#92;frac{&#92;tilde{q}_i}{p_i}}" class="latex" /> </p>
<p>(eventually apart from a minus sign), so this condition that the functor should vanish on morphisms in FP seems somehow connected with a kind of  convex structure on FinStat. Do you use this in the proof? (Sorry, no time to look at the proof).</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
